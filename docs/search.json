[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Sciencing Data",
    "section": "",
    "text": "Sandwiches: robust covariance error estimation\n\n\n\n\n\n\nOLS\n\n\nconfidence intervals\n\n\nclustered errors\n\n\nheteroskedastic errors\n\n\n\n\n\n\n\n\n\nFeb 22, 2024\n\n\nChris Kelly\n\n\n\n\n\n\n\n\n\n\n\n\nBLUE coefficients: bias and efficiency\n\n\n\n\n\n\nOLS\n\n\nleast-squares\n\n\nGauss-markov\n\n\ncoefficients\n\n\nBLUE\n\n\nbias\n\n\nefficiency\n\n\n\n\n\n\n\n\n\nFeb 21, 2024\n\n\nChris Kelly\n\n\n\n\n\n\n\n\n\n\n\n\nDeriving OLS coefficients (matrix algebra)\n\n\n\n\n\n\nOLS\n\n\nleast-squares\n\n\ncoefficients\n\n\npartial differentiation\n\n\nhessian matrix\n\n\n\n\n\n\n\n\n\nFeb 20, 2024\n\n\nChris Kelly\n\n\n\n\n\n\n\n\n\n\n\n\nMinimizing mse tends to mean, MAE tends to median\n\n\n\n\n\n\ndifferentation\n\n\ncost functions\n\n\nMSE\n\n\nMAE\n\n\n\n\n\n\n\n\n\nFeb 8, 2024\n\n\nChris Kelly\n\n\n\n\n\n\n\n\n\n\n\n\nDifferentiation formulae\n\n\n\n\n\n\ndifferentation\n\n\npython\n\n\n\n\n\n\n\n\n\nFeb 8, 2024\n\n\nChris Kelly\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/blue_ols.html",
    "href": "posts/blue_ols.html",
    "title": "BLUE coefficients: bias and efficiency",
    "section": "",
    "text": "What are we exploring?\n\n\n\nProving that the OLS coefficient is the best linear unbiased estimator."
  },
  {
    "objectID": "posts/blue_ols.html#are-the-ols-coefficients-blue",
    "href": "posts/blue_ols.html#are-the-ols-coefficients-blue",
    "title": "BLUE coefficients: bias and efficiency",
    "section": "Are the OLS coefficients “BLUE”?",
    "text": "Are the OLS coefficients “BLUE”?\nWe find a unique solution to the set of coefficients that minimize the sum of squared residuals analytically (see its derivation here):\n\\[\n\\hat{\\beta}^{OLS}=(X^{\\intercal}X)^{-1}X^{\\intercal}y\n\\]\nHowever, how do we know if these coefficients are the best ones we can estimate?\nFor the estimated coefficients to be the Best Linear Unbiased Estimator (i.e. “BLUE”):\n\nThe best estimator has to be unbiased: \\(E[\\hat{\\beta}^*] = \\beta\\)\nAnd among all possible linear, unbiased estimators, it must have the smallest variance: \\(V[\\hat{\\beta}^{*}] &lt; V[\\hat{\\beta}^{Z}]\\)\n\nWe want to ensure our OLS estimate is the best, i.e. that \\(\\hat{\\beta}^{OLS} = \\hat{\\beta}^{*}\\). To achieve this, first we need to confirm it is unbiased. Then given this is true, we can check that the coefficient is most efficient vs all other unbiased estimators.\n\n\n\n\n\n\nGauss Markov Assumptions\n\n\n\nAlong the way, we will outline the Gauss-Markov assumptions utilised that ensure the OLS coefficient is BLUE."
  },
  {
    "objectID": "posts/blue_ols.html#setting-the-scene",
    "href": "posts/blue_ols.html#setting-the-scene",
    "title": "BLUE coefficients: bias and efficiency",
    "section": "Setting the scene",
    "text": "Setting the scene\n\nThe true coefficient and model\n\\(\\beta\\) is the true (unobserved) relationship between all the relevant explanatory features, \\(X\\), and their associated observed outcomes, \\(y\\). In other words, if we observed the entire population of data, it is the relationship we would find.\nConcretely, we assume the outcome is a linear function of all its relevant features. This “true model” perfectly predicts the outcome, except for random noise \\(\\epsilon\\) that influences the observed outcome: \\(y = X\\beta + \\epsilon\\)\n\n\nThe estimated coefficient\n\\(\\hat{\\beta}\\) is our estimated coefficient for the true relationship \\(\\beta\\). In reality, we estimate \\(\\hat{\\beta}\\) from the small, finite sample of size \\(n\\) that is collected, not the whole population. Given any random sample could be collected, we can term the coefficient resulting from the optimum estimation proceedure to be \\(\\hat{\\beta}^*\\). We want to understand if \\(\\hat{\\beta}^{OLS} = \\hat{\\beta}^*\\).\n\n\nThe expected estimated coefficient\n\\(E[\\hat{\\beta}]\\) is the “expected” estimated coefficient. Imagine we repeat the action of estimating the coefficient \\(\\hat{\\beta}\\) many times, each time collecting a new sample (where each observation is sampled i.i.d), and recording the value for the estimated coefficient. \\(E[\\hat{\\beta}]\\) would then be the average of all of those estimated coefficients. If the OLS coefficient is unbiased, then the expected coefficient estimate should be equal to the true one, \\(E[\\hat{\\beta}^{OLS}]=\\beta\\).\n\n\nThe variance of the estimated coefficient\n\\(V[\\hat{\\beta}]\\) is the variance of the estimated coefficient. It determines how much we might expect our estimate \\(\\hat{\\beta}\\) to differ from the true \\(\\beta\\) for any sample drawn. Given the OLS coefficient has been shown to be unbiased, if it is BLUE we expect its variance to be lower than another other unbiased choice \\(\\hat{\\beta}^{Z}\\). Concretely, we want to find \\(V[\\hat{\\beta}^{OLS}] &lt; V[\\hat{\\beta}^{Z}]\\)."
  },
  {
    "objectID": "posts/blue_ols.html#bias",
    "href": "posts/blue_ols.html#bias",
    "title": "BLUE coefficients: bias and efficiency",
    "section": "Bias",
    "text": "Bias\nOften our small finite samples of size \\(n\\) are not a perfect reflection of the population they are drawn from. This “sampling error” means we might estimate a different relationship between \\(X\\) and \\(y\\) than the true relationship of the population, i.e. \\(\\hat{\\beta} \\neq \\beta\\).\nHowever, we should expect our estimated coefficient to be equal to the true value on average. This means we do not want to have a bias towards the estimate being systematically too small or too large, for example. In other words, if we repeated the whole proceedure thousands of times (each time taking new samples, and estimating a coefficient from the new sample) then the average of all the estimated coefficients values should be equal to the true value, i.e. \\(E[\\hat{\\beta}] = \\beta\\).\nRecall that we believe there is a true model that follows the form:\n\\[\ny = X\\beta + \\epsilon\n\\]\n\n\n\n\n\n\nGM1: Linearity \n\n\n\nThe formula above relies on the first Gauss-Markov assumption - that the dependent variable \\(y\\) is assumed to be a linear function of the variables \\(X\\). Note that implies that the proper functional form has been selected (i.e. the relationship is linear) and there are no omitted variables - a huge assumption!\n\n\nIf we substitute this into our estimated coefficient:\n\\[\n\\displaylines{\n\\begin{align}\n\\hat{\\beta}^{OLS} & = (X^{\\intercal}X)^{-1}X^{\\intercal}y\n\\\\ & = (X^{\\intercal}X)^{-1}X^{\\intercal}(X\\beta+\\epsilon)\n\\\\ & = (X^{\\intercal}X)^{-1}X^{\\intercal}X\\beta+(X^{\\intercal}X)^{-1}X^{\\intercal}\\epsilon\n\\\\ & = \\beta+(X^{\\intercal}X)^{-1}X^{\\intercal}\\epsilon\n\\end{align}\n}\n\\]\nwe show that the estimated coefficient \\(\\hat{\\beta}^{OLS}\\) will differ from the true value depending on the random error \\(\\epsilon\\) associated with the particular finite sample collected.\nNow let’s now take the expectation, to determine when the coefficient is unbiased. In other words, what is the “average” coefficient if we took the sample many times: \\[\n\\displaylines{\n\\begin{align}\nE[\\hat{\\beta}^{OLS}]\n& = \\beta +(X^{\\intercal}X)^{-1}X^{\\intercal}E[\\epsilon]\n\\\\ & = \\beta & \\iff E[\\epsilon] & = 0\n\\end{align}\n\\\\\n}\n\\]\nWe find that the coefficient is unbiased as long as the expected error is also zero.\n\n\n\n\n\n\nGM2: Strict Exogeneity \n\n\n\nThe second Gauss-Markov assumption is strict exogeneity, where the expected error is zero for all feature values: \\(E[\\epsilon|X] = 0\\). By definition, the weaker exogeneity statement of \\(E[\\epsilon] = 0\\) is implied by having the expected error conditional being equal to zero."
  },
  {
    "objectID": "posts/blue_ols.html#efficiency",
    "href": "posts/blue_ols.html#efficiency",
    "title": "BLUE coefficients: bias and efficiency",
    "section": "Efficiency",
    "text": "Efficiency\nTo be the “best”, the OLS estimator also needs to be efficient. This means that it has the lowest variance of all unbiased estimators. This section looks to prove this.\n\nCoefficient variance for OLS\nFirst, let’s derive the variance from the coefficients estimated using OLS, termed \\(V[\\hat{\\beta}^{OLS}]\\). As before, we substitute the true model \\(y = X\\beta + \\epsilon\\) into the coefficient estimated through OLS:\n\\[\n\\displaylines{\n\\begin{align}\n\\hat{\\beta}^{OLS}-\\beta\n& = \\beta +  ((X^{\\intercal}X)^{-1}X^{\\intercal}\\epsilon) - \\beta \\\\\n& = (X^{\\intercal}X)^{-1}X^{\\intercal}\\epsilon\n\\\\ \\\\\n\\therefore\n(\\hat{\\beta}^{OLS}-\\beta)(\\hat{\\beta}^{OLS}-\\beta)^{\\intercal}\n& = ((X^{\\intercal}X)^{-1}X^{\\intercal}\\epsilon)((X^{\\intercal}X)^{-1}X^{\\intercal}\\epsilon)^{\\intercal}\n\\\\ & =\n(X^{\\intercal}X)^{-1}X^{\\intercal}\\epsilon\\epsilon^{\\intercal}X(X^{\\intercal}X)^{-1} \\\\ \\\\\n\\therefore\nV(\\hat{\\beta}^{OLS}) & = E[(\\hat{\\beta}^{OLS}-\\beta)(\\hat{\\beta}^{OLS}-\\beta)^{\\intercal}]\n\\\\ & =\n(X^{\\intercal}X)^{-1}X^{\\intercal}E[\\epsilon\\epsilon^{\\intercal}]X(X^{\\intercal}X)^{-1}\n\\end{align}\n}\n\\]\nThis is sometimes called the sandwich estimator - post soon to follow on this!\n\n\nCoefficient variance assuming “spherical errors”\nWe can simplify this further by appling some assumptions to the estimated error variance \\(E[\\epsilon\\epsilon^{\\intercal}]\\):\n\\[\n\\displaylines{\n\\begin{align}\nE[\\epsilon \\epsilon^{\\intercal}] & =  \n\\begin{bmatrix}\nE[\\epsilon_1^2] & \\cdots & E[\\epsilon_1\\epsilon_n] \\\\\n\\vdots & \\ddots & \\vdots \\\\\nE[\\epsilon_n\\epsilon_1] & \\cdots & E[\\epsilon_n^2]\n\\end{bmatrix} \\\\ \\\\\n& = \\begin{bmatrix}\n\\frac{1}{n}\\sum_{i=1}^{n}{\\epsilon_i^2} & \\cdots & 0 \\\\\n\\vdots & \\ddots & \\vdots \\\\\n0 & \\cdots & \\frac{1}{n}\\sum_{i=1}^{n}{\\epsilon_i^2}\n\\end{bmatrix} \\\\ \\\\\n& = \\begin{bmatrix}\n\\hat{\\sigma}^2 & \\cdots & 0 \\\\\n\\vdots & \\ddots & \\vdots \\\\\n0 & \\cdots & \\hat{\\sigma}^2\n\\end{bmatrix} \\\\ \\\\\n& = \\hat{\\sigma}^2I\n\\end{align}\n}\n\\]\nHow can we jump to this result? Well we are making two assumptions:\n\nNo serial correlation: \\(\\rho_{\\epsilon_{i},\\epsilon_{i \\neq j}} = 0\\). No correlation between sample errors means that \\(E[\\epsilon_i \\epsilon_{j \\neq i}] = 0\\), and hence the off-diagonals of the error covariance matrix are zero.\nHomoskedasticity: the assumption of uniform error variance for all samples means that \\(V[\\epsilon_i^2] = V[\\epsilon_{j \\neq i}^2] = \\hat{\\sigma}^2\\). And our best approximation for \\(\\hat{\\sigma}^2\\) is simply taking the average squared error: \\(\\hat{\\sigma}^2 = \\frac{1}{n}\\sum_{i=1}^{n}{\\epsilon_i^2}\\)\n\n\n\n\n\n\n\nGM3: Spherical errors \n\n\n\nThe third Gauss-Markov assumption is spherical errors, \\(E[\\epsilon\\epsilon^{\\intercal}|X] = 0\\). This means that the outer product of the expected errors is a scalar matrix, which implies no serial correlation and homoskedasticity.\n\n\n\n\nIt is especially important to make the right assumptions about \\(E[\\epsilon\\epsilon^{\\intercal}|X]\\) as it impacts where our estimate of the standard errors is correct! We will dive into what happens to SE if we violate these assumptions in another post.\nSince we now see that \\(\\hat{\\sigma}^2\\) is a scalar matrix, we can thus simplfy the variance formula further:\n\\[\n\\displaylines{\n\\begin{align}\nV(\\hat{\\beta}^{OLS}) & =\n(X^{\\intercal}X)^{-1}X^{\\intercal}E[\\epsilon\\epsilon^{\\intercal}]X(X^{\\intercal}X)^{-1}\n\\\\ & =\n(X^{\\intercal}X)^{-1}X^{\\intercal} \\hat{\\sigma}^2I X(X^{\\intercal}X)^{-1}\n\\\\ & =\n\\hat{\\sigma}^2\n\\cancel{(X^{\\intercal}X)^{-1}}\n\\cancel{X^{\\intercal} X}\n(X^{\\intercal}X)^{-1}\n\\\\ & = \\hat{\\sigma}^2(X^{\\intercal}X)^{-1}\n\\end{align}\n}\n\\]\n\n\nFormulating an alternative unbiased coefficient\nNext step - lets formulate another estimator, \\(\\hat{\\beta}^{z}\\), which differs from \\(\\hat{\\beta}^{OLS}\\) by a non-zero matrix \\(A\\). See how they both differ below:\n\\[\n\\displaylines{\n\\begin{align}\n\\hat{\\beta}^{OLS} & =(X^{\\intercal}X)^{-1}X^{\\intercal}y \\\\\n\\hat{\\beta}^{Z} & =\\left((X^{\\intercal}X)^{-1}X^{\\intercal}+A\\right)y\n\\end{align}\n}\n\\]\nNow we need to ensure this new estimator is not biased. So by taking the expectation in the same was as for OLS…\n\\[\n\\displaylines{\n\\begin{align}\nE[\\hat{\\beta}^{Z}] & = E\\left[ \\left((X^{\\intercal}X)^{-1}X^{\\intercal}+A\\right)y \\right]\n\\\\ & = \\left((X^{\\intercal}X)^{-1}X^{\\intercal}+A\\right)(X\\beta+ \\cancel{E\\left[\\epsilon \\right]}) & \\because E[\\epsilon] = 0\n\\\\ & = (X^{\\intercal}X)^{-1}X^{\\intercal}X\\beta+AX\\beta\n\\\\ & = \\beta+AX\\beta\n\\end{align}\n}\n\\]\nSo the estimator is only unbiased iff \\(AX=0\\). This is important to note when comparing the variance between unbiased coefficients - see below!\n\n\nVariance of the alternative unbiased coefficient\nJust like before, we calculate the variance:\n\\[\n\\displaylines{\n\\begin{align}\nV[\\hat{\\beta}^{Z}]\n& = V\\left[ \\left((X^{\\intercal}X)^{-1}X^{\\intercal}+A\\right)y \\right]\n\\\\ & = \\left((X^{\\intercal}X)^{-1}X^{\\intercal}+A\\right) V[y] \\left((X^{\\intercal}X)^{-1}X^{\\intercal}+A\\right)^{\\intercal}\n\\\\ & = \\hat{\\sigma}^2 \\left((X^{\\intercal}X)^{-1}X^{\\intercal}+A\\right) \\left((X^{\\intercal}X)^{-1}X^{\\intercal}+A\\right)^{\\intercal}\n& \\because E[\\epsilon \\epsilon^{\\intercal}|X] = 0\n\\\\ & = \\hat{\\sigma}^2\\left((X^{\\intercal}X)^{-1}X^{\\intercal}+A\\right) \\left(X(X^{\\intercal}X)^{-1}+A^{\\intercal}\\right)\n\\\\ & = \\hat{\\sigma}^2 \\left(\n(X^{\\intercal}X)^{-1}X^{\\intercal} X(X^{\\intercal}X)^{-1} + AX(X^{\\intercal}X)^{-1} + (X^{\\intercal}X)^{-1}X^{\\intercal}A^{\\intercal} + AA^{\\intercal}\n\\right)\n\\\\ & = \\hat{\\sigma}^2\n(X^{\\intercal}X)^{-1} + \\hat{\\sigma}^2AA^{\\intercal}\n& \\because AX = 0\n\\\\ & = V[\\beta^{OLS}] + \\hat{\\sigma}^2AA^{\\intercal}\n\\end{align}\n}\n\\]\nNow since AA is surely a positive semi-definite matrix, then we know that \\(V[\\hat{\\beta}^{Z}] &gt; V[\\hat{\\beta}^{OLS}]\\).\nWe have shown that \\(\\hat{\\beta}^{OLS}\\) has the smallest variance among all unbiased estimators!"
  },
  {
    "objectID": "posts/blue_ols.html#summarising-the-gauss-markov-assumptions",
    "href": "posts/blue_ols.html#summarising-the-gauss-markov-assumptions",
    "title": "BLUE coefficients: bias and efficiency",
    "section": "Summarising the Gauss-markov assumptions",
    "text": "Summarising the Gauss-markov assumptions\nAlong the way, we showed where assumptions were needed to ensure the OLS coefficient estimation is BLUE.\nWe actually missed one out, but it is actually quite trivial to see from the OLS coefficient formula:\n\\[\n\\hat{\\beta}^{OLS}=(X^{\\intercal}X)^{-1}X^{\\intercal}y\n\\]\n\n\n\n\n\n\nGM4: Full rank \n\n\n\nThe matrix \\(X\\) must be of full rank \\(k\\), so that it is possible to invert the matrix \\(X^{\\intercal}X\\). This is equivalent to having no perfect multi-collinearity.\n\n\nWe have now collected our full set of Gauss-Markov assumptions required for the OLS coefficient to be BLUE:\n\nLinearity\nStrict Exogeneity\nSpherical Errors\nFull rank"
  },
  {
    "objectID": "posts/mse_mean_mae_median.html",
    "href": "posts/mse_mean_mae_median.html",
    "title": "Minimizing mse tends to mean, MAE tends to median",
    "section": "",
    "text": "What we are solving\n\n\n\nWhy minimizing the mean absolute error tends towards the median of the sample.\n\n\nIn the absence of informative features, an ML algorithm minimizing the sum of squared errors will tend towards predicting the mean of the sample. However, minimizing the sum of absolute errors will tend towards predicting the median of the sample.\nThis post dives into why this is the case.\n\nMinimizing residual sum-of-squares\nLet’s define the residual for sample \\(i\\) as \\(\\epsilon_i\\). We now want to find the prediction \\(\\hat{y}\\) that minimizes the sum of all squared residuals (i.e. where the gradient is zero):\n\\[\n\\displaylines{\n\\begin{align}\n\\min_\\hat{y}{\\left[\\sum_{i=1}^N{\\epsilon_i^2}\\right]}\n\\Rightarrow &\n\\frac{\\partial}{\\partial \\hat{y}} \\sum_{i=1}^N{\\epsilon_i^2} \\\\ = &\n\\frac{\\partial \\left( \\sum_{i=1}^N{\\epsilon_i^2} \\right) }{\\partial\\epsilon}\n\\left( \\frac{\\partial\\epsilon}{\\partial \\hat{y} } \\right) \\\\ = &\n\\sum_{i=1}^N 2\\epsilon_i \\left( \\frac{\\partial\\epsilon_i}{\\partial \\hat{y} }\\right) = 0\n\\end{align}\n}\n\\]\nWe can now substitue in \\(\\epsilon_i = y - \\hat{y}\\):\n\\[\n\\displaylines{\n\\begin{align}\n\\sum_{i=1}^N 2\\epsilon_i \\left( \\frac{\\partial\\epsilon_i}{\\partial \\hat{y} }\\right) &\n= \\sum_{i=1}^N2( y_i- \\hat{y})\\left(\\frac{\\partial( y_i-\\hat{y})}{\\partial \\hat{y}}\\right) \\\\  &\n= \\sum_{i=1}^N2( y_i- \\hat{y} )(-1) \\\\ &\n= \\sum_{i=1}^N2( y_i) - 2ny = 0 \\\\ &\n\\therefore n \\hat{y} = \\sum_{i=1}^N( y_i) \\\\ &\n\\therefore \\hat{y} = \\frac{\\sum_{i=1}^N{y_i}}{n} = \\bar{y}\n\\end{align}\n}\n\\]\nThus we can see that the prediction that minimizes the sum of squared residuals, is simply the mean.\n\n\nMinimize sum of absolute residuals\nWe now do the same think again, but this time look to minimize the sum of all absolute residuals instead.\n\\[\n\\displaylines{\n\\begin{align}\n\\min_\\hat{y}{\\left[\\sum_{i=1}^N{\\mid \\epsilon_i \\mid}\\right]}\n\\Rightarrow &\n\\frac{\\partial}{\\partial \\hat{y}} \\sum_{i=1}^N{\\left(\\epsilon_i^2\\right)^{1/2}} \\\\ = &\n\\frac{\\partial \\sum_{i=1}^N{ \\left(\\epsilon_i^2\\right)^{1/2} } }{\\partial\\epsilon_i^2}\n\\times \\frac{\\partial\\epsilon_i^2}{\\partial \\epsilon_i }\n\\times \\frac{\\partial\\epsilon_i}{\\partial \\hat{y} }  \\\\ = &\n\\frac{1}{2} \\sum_{i=1}^N{ \\left(\\epsilon_i^2\\right)^{-1/2} }\n\\times 2 \\epsilon_i\n\\times \\frac{\\partial\\epsilon_i}{\\partial \\hat{y} }  \\\\ = &\n\\sum_{i=1}^N{ \\left(\\epsilon_i^2\\right)^{-1/2} } \\times \\epsilon_i\n\\times \\frac{\\partial\\epsilon_i}{\\partial \\hat{y} }  \\\\ = &\n\\sum_{i=1}^N \\frac{\\epsilon_i}{\\mid \\epsilon_i \\mid} \\left( \\frac{\\partial\\epsilon_i}{\\partial \\hat{y} }\\right) = 0\n\\end{align}\n}\n\\]\nAnd similarly to before, we can now substitute in \\(\\epsilon_i = y - \\hat{y}\\):\n\\[\n\\displaylines{\n\\begin{align}\n\\sum_{i=1}^N \\frac{\\epsilon_i}{\\mid \\epsilon_i \\mid} \\left( \\frac{\\partial\\epsilon_i}{\\partial \\hat{y} }\\right) &\n= \\sum_{i=1}^N \\frac{ y_i- \\hat{y} }{\\mid y_i- \\hat{y} \\mid}\\left(\\frac{\\partial( y_i-\\hat{y})}{\\partial \\hat{y}}\\right) \\\\  &\n= \\sum_{i=1}^N \\frac{ y_i- \\hat{y} }{\\mid y_i- \\hat{y} \\mid}(-1) = 0\n\\end{align}\n}\n\\]\nNow \\(f(x) = \\frac{ x }{\\mid x \\mid}\\) is an cool transformation, keeping its sign but getting rid of the magnitude of the size of \\(x\\), i.e.:\n\n\\(f(x &lt; 0) = -1\\)\n\\(f(x &gt; 0) = 1\\)\n\nSo to ensure that \\(\\sum f(\\epsilon_i)=0\\), we need to pick a value for \\(\\hat{y}\\) that means half of the errors are \\(&lt;0\\) and half of the errors are \\(&gt;0\\).\nSo that means half the errors must be negative, and half are positive. So \\(\\hat{y}\\) has to be the median value!\nFin."
  },
  {
    "objectID": "posts/ols_coef_derivation.html",
    "href": "posts/ols_coef_derivation.html",
    "title": "Deriving OLS coefficients (matrix algebra)",
    "section": "",
    "text": "What we are exploring\n\n\n\nDeriving a unique, analytical solution to the set of coefficients that minimize the sum of squared residuals."
  },
  {
    "objectID": "posts/ols_coef_derivation.html#summary",
    "href": "posts/ols_coef_derivation.html#summary",
    "title": "Deriving OLS coefficients (matrix algebra)",
    "section": "Summary",
    "text": "Summary\nThe cost function for OLS is the sum of squared residuals, \\(\\hat{\\epsilon}^{\\intercal}\\hat{\\epsilon}\\). In order to fit a good linear model, we want to find optimum values for the estimated vector of coefficients, \\(\\hat{\\beta}^*\\), that minimizes this cost function.\nFirst we do partial differentiation of the cost function with respect to the coefficients. Finding the coefficient values where the partial differential is equal to zero reveals the stationary points of the cost function. For OLS in particular, we can find a unique solution for the choice of coefficients that can be found analytically. The hessian matrix then further proves that this is a global minima."
  },
  {
    "objectID": "posts/ols_coef_derivation.html#deriving-the-optimum-coefficients",
    "href": "posts/ols_coef_derivation.html#deriving-the-optimum-coefficients",
    "title": "Deriving OLS coefficients (matrix algebra)",
    "section": "Deriving the optimum coefficients",
    "text": "Deriving the optimum coefficients\n\n0. Defining the notation\nFor a sample \\(i\\), we observe an outcome \\(y_i\\). \\(y\\) is a vector of all \\(n\\) observed outcomes.\n\\[\n\\underset{n \\times 1} {y} =\n\\begin{bmatrix}\n    y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_{n-1} \\\\ y_n\n\\end{bmatrix}\n\\]\nWe also observe \\(k\\) features for every sample \\(i\\). \\(X\\) is a matrix of these observed features. Note the first column is usually all ones, to include an intercept to optimize (or “bias” term).\n\\[\n\\underset{n \\times k} {X} =\n\\begin{bmatrix}\n  1 & x_{11} & \\cdots & x_{1,k-1} & x_{1,k} \\\\\n  1 & x_{21} & \\cdots & x_{2,k-1} & x_{2,k} \\\\\n  1 & x_{31} & \\cdots & x_{3,k-1} & x_{3,k} \\\\\n  \\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\\n  1 & x_{n-2,1} & \\cdots & x_{n-2,k-1} & x_{n-2,k} \\\\\n  1 & x_{n-1,1} & \\cdots & x_{n-1,k-1} & x_{n-1,k} \\\\\n  1 & x_{n,1} & \\cdots & x_{n,k-1} & x_{n,k}\n\\end{bmatrix}\n\\]\nThe contribution of each feature to the prediction is estimated by the coefficients \\(\\hat{\\beta}\\).\n\\[\n\\underset{k \\times 1} {\\hat{\\beta}} =\n\\begin{bmatrix}\n  \\beta_0 \\\\\n  \\beta_1 \\\\\n  \\vdots \\\\\n  \\beta_{k-1} \\\\\n  \\beta_{k}\n\\end{bmatrix}\n\\]\nWe make predictions, \\(\\hat{y}\\), by calculating the dot product of the features \\(X\\) and the coefficients \\(\\hat{\\beta}\\).\n\\[\n\\hat{y} = X \\hat{\\beta}\n\\]\nwhich is shorthand for this:\n\\[\n\\displaylines{\n\\begin{align}\n\\begin{bmatrix}\n  \\hat{y}_1 \\\\ \\hat{y}_2 \\\\ \\hat{y}_3 \\\\ \\vdots \\\\ \\hat{y}_{n-2} \\\\ \\hat{y}_{n-1} \\\\ \\hat{y}_n\n\\end{bmatrix} & =\n\\begin{bmatrix}\n  1 & x_{11} & \\cdots & x_{1,k-1} & x_{1,k} \\\\\n  1 & x_{21} & \\cdots & x_{2,k-1} & x_{2,k} \\\\\n  1 & x_{31} & \\cdots & x_{3,k-1} & x_{3,k} \\\\\n  \\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\\n  1 & x_{n-2,1} & \\cdots & x_{n-2,k-1} & x_{n-2,k} \\\\\n  1 & x_{n-1,1} & \\cdots & x_{n-1,k-1} & x_{n-1,k} \\\\\n  1 & x_{n,1} & \\cdots & x_{n,k-1} & x_{n,k}\n\\end{bmatrix}\n\\begin{bmatrix}\n  \\hat{\\beta}_0 \\\\\n  \\hat{\\beta}_1 \\\\\n  \\vdots \\\\\n  \\hat{\\beta}_{k-1} \\\\\n  \\hat{\\beta}_{k}\n\\end{bmatrix}\n\\\\ \\\\ & =\n\\begin{bmatrix}\n  \\hat{\\beta}_0 + \\hat{\\beta}_{1}x_{1,1} + \\cdots + \\hat{\\beta}_{k-1}x_{1,k-1} + \\hat{\\beta}_{k}x_{1,k} \\\\\n  \\hat{\\beta}_0 + \\hat{\\beta}_{1}x_{2,1} + \\cdots + \\hat{\\beta}_{k-1}x_{2,k-1} + \\hat{\\beta}_{k}x_{2,k} \\\\\n  \\hat{\\beta}_0 + \\hat{\\beta}_{1}x_{3,1} + \\cdots + \\hat{\\beta}_{k-1}x_{3,k-1} + \\hat{\\beta}_{k}x_{3,k} \\\\\n  \\vdots \\\\\n  \\hat{\\beta}_0 + \\hat{\\beta}_{1}x_{n-2,1} + \\cdots + \\hat{\\beta}_{k-1}x_{2,k-1} + \\hat{\\beta}_{k}x_{n-2,k} \\\\\n  \\hat{\\beta}_0 + \\hat{\\beta}_{1}x_{n-1,1} + \\cdots + \\hat{\\beta}_{k-1}x_{2,k-1} + \\hat{\\beta}_{k}x_{n-1,k} \\\\\n  \\hat{\\beta}_0 + \\hat{\\beta}_{1}x_{n,1} + \\cdots + \\hat{\\beta}_{k-1}x_{2,k-1} + \\hat{\\beta}_{k}x_{n,k} \\\\\n\\end{bmatrix}\n\\end{align}\n}\n\\]\nThe residual is the difference between the true outcome and the model prediction.\n\\[\n\\hat{\\epsilon} = y_i -\\hat{y}_i\n\\]\nwhich is shorthand for this:\n\\[\n\\begin{bmatrix}\n  \\hat{\\epsilon_1} \\\\\n  \\hat{\\epsilon_2} \\\\\n  \\hat{\\epsilon_3} \\\\\n  \\vdots \\\\\n  \\hat{\\epsilon_{n-2}} \\\\\n  \\hat{\\epsilon_{n-1}} \\\\\n  \\hat{\\epsilon_{n}}\n\\end{bmatrix} =\n\\begin{bmatrix}\n    y_1 \\\\ y_2 \\\\ y_3 \\\\ \\vdots \\\\ y_{n-2} \\\\ y_{n-1} \\\\ y_n\n\\end{bmatrix} -\n\\begin{bmatrix}\n  \\hat{y}_1 \\\\ \\hat{y}_2 \\\\ \\hat{y}_3 \\\\ \\vdots \\\\ \\hat{y}_{n-2} \\\\ \\hat{y}_{n-1} \\\\ \\hat{y}_n\n\\end{bmatrix}\n\\]\nOur aim is to find the optimum vector of coefficients, \\(\\hat{\\beta}^*\\), that minimizes the sum of squared residuals:\n\\[\n\\min_{\\beta} \\left( \\epsilon^{\\intercal}\\epsilon \\right)\n\\]\n\n\n1. Expand the sum of squared residuals\nThe first step involves expanding the sum of squared residuals, and substituting in \\(X \\hat{\\beta}\\) for \\(\\hat{y}\\). \\[\n\\displaylines{\n\\begin{align}\n\\sum_{i=1}^n{\\hat{\\epsilon}_i^2} & = \\hat{\\epsilon}^{\\intercal}\\hat{\\epsilon}\n\\\\& =(y-X\\hat{\\beta})^{\\intercal}(y-X\\hat{\\beta})\n\\\\& = y^{\\intercal}y - y^{\\intercal}X\\hat{\\beta}-\n\\hat{\\beta}^{\\intercal} X^{\\intercal}y+\n\\hat{\\beta}^{\\intercal}X^{\\intercal}X\\hat{\\beta}\n\\\\& = y^{\\intercal}y - 2y^{\\intercal}X\\hat{\\beta}\n+\\hat{\\beta}^{\\intercal}X^{\\intercal}X\\hat{\\beta}\n\\end{align}\n}\n\\]\n\n\nNote we can simply add the two middle terms, since are both scalars:\n\\[\n\\displaylines{\ny^{\\intercal}X\\hat{\\beta} =\n\\hat{\\beta}^{\\intercal} X^{\\intercal}y \\\\\n\\because \\underset{1 \\times n}{y^{\\intercal}} \\times\n\\underset{n \\times k}{X} \\times\n\\hat{\\underset{k \\times 1}{\\beta}}\n=\n\\hat{\\underset{1 \\times k}{\\beta}^{\\intercal}} \\times\n\\underset{k \\times n}{X^{\\intercal}} \\times\n\\underset{n \\times 1}{y}\n}\n\\]\n\n\n2. Partially differentiate RSS with respect to beta\nThe second step involves partially differentiating the cost function with respect to its parameters, to understand how it changes as the coefficients vary.\n\\[\n\\displaylines{\n\\begin{align}\n\\frac{\\partial}{\\partial\\hat{\\beta}}\\hat{\\epsilon}^{\\intercal}\\hat{\\epsilon} & \\equiv\n\\begin{bmatrix}\n    \\frac{\\partial}{\\partial\\hat{\\beta}_1}\\hat{\\epsilon}^{\\intercal}\\hat{\\epsilon} \\\\\n    \\frac{\\partial}{\\partial\\hat{\\beta}_2}\\hat{\\epsilon}^{\\intercal}\\hat{\\epsilon} \\\\\n    \\vdots \\\\\n    \\frac{\\partial}{\\partial\\hat{\\beta}_k}\\hat{\\epsilon}^{\\intercal}\\hat{\\epsilon}\n\\end{bmatrix}\n\\\\ & = \\frac{d}{d\\hat{\\beta}}(\ny^{\\intercal}y - 2y^{\\intercal}X\\hat{\\beta}\n+\\hat{\\beta}^{\\intercal}X^{\\intercal}X\\hat{\\beta}) \\\\ & = 0 - 2X^{\\intercal}y +((X^{\\intercal}X)\\hat{\\beta} + (X^{\\intercal}X)^{\\intercal}\\hat{\\beta})\n\\\\ & = -2X^{\\intercal}y +  2((X^{\\intercal}X)\\hat{\\beta})\n\\end{align}\n}\n\\]\n\n\nTwo matrix differentiation rules used here for reference:\n\\[\n\\displaylines{\n\\frac{\\partial}{dx}(Ax) = A^{\\intercal}x \\\\\n\\frac{\\partial}{dx}(xAx) = Ax + A^{\\intercal}x\n}\n\\]\nAnd note \\(X^{\\intercal}X = (X^{\\intercal}X)^{\\intercal}\\) by definition, so we can add the two last terms.\n\n\n3. Find the coefficient values at the stationary point\nNow we find the choices of \\(\\beta\\) where the partial differential is equal to zero. These stationary points for the cost function are either at its maximum or minimum.\nFor OLS - we actually only find one unique solution!\n\\[\n\\displaylines{\n\\begin{align}\n\\cancel{2}X^{\\intercal}y +  \\cancel{2}((X^{\\intercal}X)\\hat{\\beta}) & = 0 \\\\\n\\therefore (X^{\\intercal}X)\\hat{\\beta} & = X^{\\intercal}y \\\\\n\\therefore \\hat{\\beta} & = (X^{\\intercal}X)^{-1}X^{\\intercal}y\n\\end{align}\n}\n\\]\n\n\nNote the need to invert \\(X^{\\intercal}X\\). This is only possible for a full rank matrix.\nThe first term is the (inverse) variance matrix of \\(X\\). This term normalizes the coefficient with respect to the magnitude of \\(X\\).\nThe second term is the covariance matrix between \\(X\\) and \\(y\\). This incorporates the linear relationship between the two in the coefficient.\nHence, the coefficient can be interpreted as the estimated change in \\(y\\) given a one unit change in \\(X\\).\n\n\n4. Check the stationary point is a global minimum (hessian matrix)\nFinally, we derive the hessian matrix, by double-differentiating the cost function with respect to the coefficients:\n\\[\n\\displaylines{\n\\frac{\\partial^2}{\\partial\\hat{\\beta}\\partial\\hat{\\beta}^{\\intercal}}\n\\left(\n  \\hat{\\epsilon}^{\\intercal}\\hat{\\epsilon}\n\\right)\n\\\\ = \\frac{\\partial}{\\partial \\hat{\\beta}^{\\intercal}} \\left( -2X^{\\intercal}y +  2((X^{\\intercal}X)\\hat{\\beta}) \\right)\n\\\\ = 2(X^{\\intercal}X)^{\\intercal}\n}\n\\]\nSince \\(X^{\\intercal}X\\) is clearly positive definite, the cost function is convex. Thus, we know our unique solution for \\(\\beta\\) where the partial differential is at zero is indeed a global minimum for the cost function."
  },
  {
    "objectID": "posts/ols_coef_derivation.html#final-reflections",
    "href": "posts/ols_coef_derivation.html#final-reflections",
    "title": "Deriving OLS coefficients (matrix algebra)",
    "section": "Final reflections",
    "text": "Final reflections\nUnlike logistic regression, or the multiple hidden-layer structure of neural networks, we can “jump” straight to the optimum coefficients for OLS. Why can we do this? Well chiefly its because OLS is a bit of a special case:\n\nThe minima is a global minima: The hessian matrix is positive definite, and hence the cost function is strictly convex. This means we know that when a choice of coefficients is found that ensure the partially differentiated cost function is equal zero, this minima is also a global one, not a local one.\nThere is only one solution for the optimum coefficient: We assume that the matrix is full rank (every feature provides additional predictive power) and that the number of predictors is smaller than the number of obervations. This also means that partially differentiating is okay to do!\nA closed-form solution can be found The predictions are generated from \\(X\\) using a simple, purely algebraic function, i.e. the sum-product of \\(X\\) by \\(\\beta\\). This means we can find an analytical solution to the optimal choice \\(\\beta^*\\). Note this often isn’t possible since non-linear activation functions (i.e. link functions) are often transcendental.\n\nWe will dive into this in another post.\nFin."
  },
  {
    "objectID": "posts/differentiation.html",
    "href": "posts/differentiation.html",
    "title": "Differentiation formulae",
    "section": "",
    "text": "What we are solving\n\n\n\nDeriving why \\(\\frac{d}{dx} \\left(x^{n} \\right) = nx^{n-1}\\)"
  },
  {
    "objectID": "posts/differentiation.html#formulating-an-approach-to-the-problem",
    "href": "posts/differentiation.html#formulating-an-approach-to-the-problem",
    "title": "Differentiation formulae",
    "section": "Formulating an approach to the problem",
    "text": "Formulating an approach to the problem\nThe gradient between two points, \\(x\\) and \\(a\\), is equal to the change in the y axis divided by the change in the x axis.\nFor example, if \\(y=f(x)=x^n\\), then we can approximate the gradient as follows:\n\\[\n\\frac{\\Delta y}{\\Delta x} =\n\\frac{f(x)-f(a)}{x-a} =\n\\frac{x^n-a^n}{x-a}\n\\]\nThis isn’t the same as saying a line which passes through both of these points has the same gradient though. There is a gap between this linear approximation and the exact curve.\nHowever, the smaller the movement across the x-axis, the closer the approximation is to the actual curve. For example, in Figure 1 below, the line drawn between \\(f(1)\\) and \\(f(2)\\) is closer to the gradient at \\(f(1)\\) of the true curve, compared to the line drawn between \\(f(1)\\) and \\(f(3)\\).\n\n\nCode\nimport plotly.graph_objects as go\nimport numpy as np\n\ndef interpolate(n,x1,x2,num=50):\n  x = np.linspace(x1,x2,num)\n  m = (x2**n - x1**n)/(x2 - x1)\n  c = x1**n - m*x1\n  y = m*x+c\n  return({'x':x,'y':y})\n\nn=3\nactual = {'x': np.arange(0,4,0.1)}\nactual['y'] = actual['x']**n\nlb, ub = 1, [3.5,3,2]\nlin0 = interpolate(n,lb,ub[0])\nlin1 = interpolate(n,lb,ub[1])\nlin2 = interpolate(n,lb,ub[2])\n\nfig = go.Figure(data = go.Scatter(mode='lines'))\nfig.add_trace( go.Scatter( x=actual['x'], y=actual['y'], line=dict(dash='solid'), name=f\"f(x)=x^{n}\" ))\nfig.add_trace( go.Scatter( x=lin0['x'], y=lin0['y'], name=f\"f({lb}) -&gt; f({ub[0]})\", line=dict(dash='dot') ))\nfig.add_trace( go.Scatter( x=lin1['x'], y=lin1['y'], name=f\"f({lb}) -&gt; f({ub[1]})\", line=dict(dash='dot') ))\nfig.add_trace( go.Scatter( x=lin2['x'], y=lin2['y'], name=f\"f({lb}) -&gt; f({ub[2]})\", line=dict(dash='dot') ))\nfig.show()\n\n\n\n\n                                                \n\n\nFigure 1: The smaller the change in x, the closer the linear approximation is to the true gradient of f(x)\n\n\n\n\nConsequently, we want to determine the gradient at the limit i.e. where \\(a \\rightarrow x\\), to get the true gradient."
  },
  {
    "objectID": "posts/differentiation.html#factoring-out-x-a",
    "href": "posts/differentiation.html#factoring-out-x-a",
    "title": "Differentiation formulae",
    "section": "Factoring out \\((x-a)\\)",
    "text": "Factoring out \\((x-a)\\)\nFirst though, let’s factor out the \\(x-a\\) term for simplicity. Let’s derive a generic formula for this:\n\nIf \\(n=2\\), then \\(x^2-a^2 = (x-a)(x+a)\\)\nIf \\(n=3\\), then \\(x^3-a^3 = (x-a)(x^2+xa+a^2)\\)\nIf \\(n=4\\), then \\(x^4-a^4 = (x-a)(x^3+x^2a+xa^2+a^3)\\)\nIf \\(n=5\\), then \\(x^5-a^5 = (x-a)(x^4+x^3a+x^2a^2+xa^3+a^4)\\)\nAnd so on. In fact for any \\(n\\), we can derive \\(x^n-a^n = (x-a)\\sum_{i=1}^n \\left( x^{n-i}a^{i-1} \\right)\\)\n\nAnd we can now sub that into our formula, and the \\(x-a\\) cancels out:\n\\[\n\\frac{x^n-a^n}{x-a} =\n\\frac{x-a}{x-a} \\sum_{i=1}^n \\left( x^{n-i}a^{i-1} \\right) =\n\\sum_{i=1}^n \\left( x^{n-i}a^{i-1} \\right)\n\\]"
  },
  {
    "objectID": "posts/differentiation.html#getting-our-result",
    "href": "posts/differentiation.html#getting-our-result",
    "title": "Differentiation formulae",
    "section": "Getting our result",
    "text": "Getting our result\nAnd now let’s calculate the result in the limit, where \\(x\\) approaches \\(a\\):\n\\[\n\\lim_{x \\rightarrow a} \\sum_{i=1}^n \\left( x^{n-i}a^{i-1} \\right) \\sim\n\\sum_{i=1}^n \\left( a^{n-i}a^{i-1} \\right) =\n\\sum_{i=1}^n \\left( a^{n-1} \\right) =\nna^{n-1}\n\\]\nHence \\(\\frac{d}{dx} \\left(x^{n} \\right) = nx^{n-1}\\).\nFin."
  },
  {
    "objectID": "posts/sandwich_estimators.html",
    "href": "posts/sandwich_estimators.html",
    "title": "Sandwiches: robust covariance error estimation",
    "section": "",
    "text": "What are we exploring?\n\n\n\nEstimating the correct coefficient variance under different error assumptions"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About me!\nComing soon :)"
  },
  {
    "objectID": "posts/sandwich_estimators.html#introducing-sandwiches",
    "href": "posts/sandwich_estimators.html#introducing-sandwiches",
    "title": "Sandwiches: robust covariance error estimation",
    "section": "Introducing sandwiches",
    "text": "Introducing sandwiches\nThe variance for the OLS coefficient estimator is equal to the following:\n\\[\n\\displaylines{\nV(\\hat{\\beta}) =\n(X^{\\intercal}X)^{-1}X^{\\intercal}E[\\epsilon\\epsilon^{\\intercal}]X(X^{\\intercal}X)^{-1}\n}\n\\]\nThis can be though to as a sandwich:\n\nThe “bread” either side: \\((X^{\\intercal}X)^{-1}X^{\\intercal}\\) on the left and its transpose \\(X(X^{\\intercal}X)^{-1}\\) on the right\nThe “meat” in the middle: what we assume for \\(E[\\epsilon\\epsilon^{\\intercal}]\\)\n\nNote that this is the same as the error variance, since \\(V[\\epsilon]=E[\\epsilon\\epsilon^{\\intercal}]-E[\\epsilon]E[\\epsilon^{\\intercal}]\\) and \\(E[\\epsilon] = 0\\)\n\n\nOur coefficient will only be efficient if these assumptions about the expected error are correct! We will explore what happens when the errors are assumed to be homoskedastic, heteroskedastic or clustered."
  },
  {
    "objectID": "posts/sandwich_estimators.html#cucumber-filling-constant-error-variance",
    "href": "posts/sandwich_estimators.html#cucumber-filling-constant-error-variance",
    "title": "Sandwiches: robust covariance error estimation",
    "section": "Cucumber filling: Constant Error Variance 🥒",
    "text": "Cucumber filling: Constant Error Variance 🥒\nUsual OLS is efficient if the true model has “spherical errors”. What does this mean in practice?\n\nErrors are homoskedastic: \\(V(\\epsilon_i)=\\sigma^2\\) for all observations\nErrors are serially uncorrelated: \\(cov(\\epsilon_i,\\epsilon_{j\\neq i})=0\\)\n\nWhat does this look like for \\(E[\\epsilon\\epsilon^{\\intercal}]\\)?\n\nThe diagonal of the matrix is a constant value (scalar), \\(\\sigma^2\\)\nThe off-diagonals are all zero\n\n\\[\n\\hat{\\sigma}^2\\underset{n\\times n}{I} =\n\\begin{bmatrix}\n\\hat{\\sigma}^2 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n0 & \\hat{\\sigma}^2 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n0 & 0 & \\hat{\\sigma}^2 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n0 & 0 & 0 & \\hat{\\sigma}^2 & 0 & 0 & 0 & 0 & 0\\\\\n0 & 0 & 0 & 0 & \\hat{\\sigma}^2 & 0 & 0 & 0 & 0\\\\\n0 & 0 & 0 & 0 & 0 & \\hat{\\sigma}^2 & 0 & 0 & 0\\\\\n0 & 0 & 0 & 0 & 0 & 0 & \\hat{\\sigma}^2 & 0 & 0\\\\\n0 & 0 & 0 & 0 & 0 & 0 & 0 & \\hat{\\sigma}^2 & 0\\\\\n0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & \\hat{\\sigma}^2\n\\end{bmatrix}\n\\]\nA good estimation of the constant error variance \\(\\sigma^2\\) is to apply the standard formula to the residuals (i.e. method of moments):\n\\[\n\\hat{\\sigma^2}=\\frac{1}{n-k}\\sum{\\hat{\\epsilon_i}^2} \\equiv \\frac{\\epsilon^{\\intercal}\\epsilon }{n-k}\n\\]\n\n\n\n\n\n\nThus our sandwich is:\n\n\n\n\\[\n\\underset{k \\times k}{(X^{\\intercal}X)}^{-1}\n\\underset{k \\times n}\n{X^{\\intercal}}\n\\left(\\frac{1}{n-k} \\times \\underset{1 \\times 1}{(\\epsilon^{\\intercal}\\epsilon)}\n\\times\\underset{n \\times n}{I} \\right)\n\\underset{n \\times k}{X}\n\\underset{k \\times k}{(X^{\\intercal}X)}^{-1}\n\\]\n\n\nIn this scenario, the only things that impact the standard error of the coefficient \\(\\beta_k\\) is:\n\nThe variance of all the residuals, \\(\\sigma^2\\)\nThe variance of the feature \\(V(X_k)\\)\n\n\n\nNote that under spherical errors, the sandwich can be simplified: \\[\n\\displaylines{\n\\begin{align}\nV[\\hat{\\beta}]\n& = (X^{\\intercal}X)^{-1}X^{\\intercal}E[\\epsilon\\epsilon^{\\intercal}]X(X^{\\intercal}X)^{-1}\n\\\\ & = (X^{\\intercal}X)^{-1}X^{\\intercal}\\sigma^2IX(X^{\\intercal}X)^{-1}\n\\\\ & = \\sigma^2(X^{\\intercal}X)^{-1}\\cancel{X^{\\intercal}X}\\cancel{(X^{\\intercal}X)^{-1}}\n\\\\ & = \\sigma^2(X^{\\intercal}X)^{-1}\n\\end{align}\n}\n\\]\n\\[\n1\n\\]"
  },
  {
    "objectID": "posts/sandwich_estimators.html#cucumber-filling-constant-error-variance-1",
    "href": "posts/sandwich_estimators.html#cucumber-filling-constant-error-variance-1",
    "title": "Sandwiches: robust covariance error estimation",
    "section": "🥒 Cucumber filling: Constant Error Variance",
    "text": "🥒 Cucumber filling: Constant Error Variance\nAs shown before, usual OLS is efficient if the true model has “spherical errors. What does this mean in practice?\n\nErrors are homoskedastic: \\(V(\\epsilon_i)=\\sigma^2\\) for all observations\nErrors are serially uncorrelated: \\(cov(\\epsilon_i,\\epsilon_{j\\neq i})=0\\)\n\nWhat does this look like for \\(E[\\epsilon\\epsilon^{\\intercal}]\\)?\n\nThe diagonal of the matrix is a constant value (scalar), \\(\\sigma^2\\)\nThe off-diagonals are all zero\n\n\\[\n\\sigma^2\\underset{n\\times n}{I} = \\begin{bmatrix}\n\\sigma^2 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n0 & \\sigma^2 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n0 & 0 & \\sigma^2 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n0 & 0 & 0 & \\sigma^2 & 0 & 0 & 0 & 0 & 0\\\\\n0 & 0 & 0 & 0 & \\sigma^2 & 0 & 0 & 0 & 0\\\\\n0 & 0 & 0 & 0 & 0 & \\sigma^2 & 0 & 0 & 0\\\\\n0 & 0 & 0 & 0 & 0 & 0 & \\sigma^2 & 0 & 0\\\\\n0 & 0 & 0 & 0 & 0 & 0 & 0 & \\sigma^2 & 0\\\\\n0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & \\sigma^2\n\\end{bmatrix}\n\\]\n\nA good estimation of the constant error variance \\(\\sigma^2\\) is the standard formula (i.e. method of moments):\n\n\\[\n\\hat{\\sigma^2}=\\frac{1}{n-k}\\sum{\\hat{\\epsilon_i}^2} \\equiv \\frac{\\epsilon^{\\intercal}\\epsilon }{n-k}\n\\]\n\n💡 Thus our sandwich is: \\(\\underset{k \\times k}{(X^{\\intercal}X)}^{-1}\n\\underset{k \\times n}\n{X^{\\intercal}}\n\\left(\\frac{1}{n-k} \\times \\underset{1 \\times 1}{(\\epsilon^{\\intercal}\\epsilon)}\n\\times\\underset{n \\times n}{I} \\right)\n\\underset{n \\times k}{X}\n\\underset{k \\times k}{(X^{\\intercal}X)}^{-1}\\)\n\nIn this scenario, the only things that impact the standard error of coefficient \\(k\\) is:\n\nThe variance of all the errors, \\(\\sigma^2\\)\nThe variance of the feature \\(V(X_k)\\)\nFinally, note that under homoskedasticity, the sandwich can be simplified:\n\\[\n  V[\\hat{\\beta}] = (X^{\\intercal}X)^{-1}X^{\\intercal}E[\\epsilon\\epsilon^{\\intercal}]X(X^{\\intercal}X)^{-1} \\\\\n  = (X^{\\intercal}X)^{-1}X^{\\intercal}\\sigma^2IX(X^{\\intercal}X)^{-1} \\\\\n  = \\sigma^2(X^{\\intercal}X)^{-1}\\cancel{X^{\\intercal}X}\\cancel{(X^{\\intercal}X)^{-1}}\n  \\]"
  },
  {
    "objectID": "posts/sandwich_estimators.html#cucumber-constant-error-variance",
    "href": "posts/sandwich_estimators.html#cucumber-constant-error-variance",
    "title": "Sandwiches: robust covariance error estimation",
    "section": "Cucumber: Constant Error Variance 🥒",
    "text": "Cucumber: Constant Error Variance 🥒\nUsual OLS is efficient if the true model has “spherical errors”. What does this mean in practice?\n\nErrors are homoskedastic: \\(V(\\epsilon_i)=\\sigma^2\\) for all observations\nErrors are serially uncorrelated: \\(cov(\\epsilon_i,\\epsilon_{j\\neq i})=0\\)\n\nWhat does this look like for \\(E[\\epsilon\\epsilon^{\\intercal}]\\)?\n\nThe diagonal of the matrix is a constant value (scalar), \\(\\sigma^2\\)\nThe off-diagonals are all zero\n\n\\[\n\\hat{\\sigma}^2\\underset{n\\times n}{I} =\n\\begin{bmatrix}\n\\hat{\\sigma}^2 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n0 & \\hat{\\sigma}^2 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n0 & 0 & \\hat{\\sigma}^2 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n0 & 0 & 0 & \\hat{\\sigma}^2 & 0 & 0 & 0 & 0 & 0\\\\\n0 & 0 & 0 & 0 & \\hat{\\sigma}^2 & 0 & 0 & 0 & 0\\\\\n0 & 0 & 0 & 0 & 0 & \\hat{\\sigma}^2 & 0 & 0 & 0\\\\\n0 & 0 & 0 & 0 & 0 & 0 & \\hat{\\sigma}^2 & 0 & 0\\\\\n0 & 0 & 0 & 0 & 0 & 0 & 0 & \\hat{\\sigma}^2 & 0\\\\\n0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & \\hat{\\sigma}^2\n\\end{bmatrix}\n\\]\nA good estimation of the constant error variance \\(\\sigma^2\\) is to apply the standard formula to the residuals (i.e. method of moments):\n\\[\n\\hat{\\sigma^2}=\\frac{1}{n-k}\\sum{\\hat{\\epsilon_i}^2} \\equiv \\frac{\\epsilon^{\\intercal}\\epsilon }{n-k}\n\\]\n\n\n\n\n\n\nThus our “cucumber sandwich” is:\n\n\n\n\\[\n\\underset{k \\times k}{(X^{\\intercal}X)}^{-1}\n\\underset{k \\times n}\n{X^{\\intercal}}\n\\left(\\frac{1}{n-k} \\times \\underset{1 \\times 1}{(\\epsilon^{\\intercal}\\epsilon)}\n\\times\\underset{n \\times n}{I} \\right)\n\\underset{n \\times k}{X}\n\\underset{k \\times k}{(X^{\\intercal}X)}^{-1}\n\\]\n\n\nIn this scenario, the only things that impact the standard error of the coefficient \\(\\beta_k\\) is:\n\nThe variance of all the residuals, \\(\\sigma^2\\)\nThe variance of the feature \\(V(X_k)\\)\n\n\n\nNote that under spherical errors, the sandwich can be simplified: \\[\n\\displaylines{\n\\begin{align}\nV[\\hat{\\beta}]\n& = (X^{\\intercal}X)^{-1}X^{\\intercal}E[\\epsilon\\epsilon^{\\intercal}]X(X^{\\intercal}X)^{-1}\n\\\\ & = (X^{\\intercal}X)^{-1}X^{\\intercal}\\sigma^2IX(X^{\\intercal}X)^{-1}\n\\\\ & = \\sigma^2(X^{\\intercal}X)^{-1}\\cancel{X^{\\intercal}X}\\cancel{(X^{\\intercal}X)^{-1}}\n\\\\ & = \\sigma^2(X^{\\intercal}X)^{-1}\n\\end{align}\n}\n\\]"
  },
  {
    "objectID": "posts/sandwich_estimators.html#ham-heteroskedastic-errors",
    "href": "posts/sandwich_estimators.html#ham-heteroskedastic-errors",
    "title": "Sandwiches: robust covariance error estimation",
    "section": "Ham: Heteroskedastic errors 🍖",
    "text": "Ham: Heteroskedastic errors 🍖\nHeteroskedastic correction is needed if:\n\nErrors vary for every individual: \\(V(\\epsilon_i)=\\sigma_i^2\\) for all observations\nBut they are still independent aka serially uncorrelated: \\(cov(\\epsilon_i,\\epsilon_{j\\neq i})=0\\)\n\nWhat does this look like for \\(E[\\epsilon\\epsilon^{\\intercal}]\\)?\n\nThe diagonal of the matrix is the estimate of variance which is unique for each observation, \\(\\sigma_i^2\\)\nThe off-diagonals are all zero\n\\[\n  \\underset{n \\times n}{\\sigma^2} = \\begin{bmatrix}\n  \\sigma_1^2 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n  0 & \\sigma_2^2 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n  0 & 0 & \\sigma_3^2 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n  0 & 0 & 0 & \\sigma_4^2 & 0 & 0 & 0 & 0 & 0\\\\\n  0 & 0 & 0 & 0 & \\sigma_5^2 & 0 & 0 & 0 & 0\\\\\n  0 & 0 & 0 & 0 & 0 & \\sigma_6^2 & 0 & 0 & 0\\\\\n  0 & 0 & 0 & 0 & 0 & 0 & \\sigma_7^2 & 0 & 0\\\\\n  0 & 0 & 0 & 0 & 0 & 0 & 0 & \\sigma_8^2 & 0\\\\\n  0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & \\sigma_{..}^2\\\\\n  \\end{bmatrix}\n  \\]\nA good estimation of the vector of heteroskedastic error variances \\(\\sigma^2\\) is again to apply the standard formula to the residuals for each individual:\n\n\\[\n\\hat{\\sigma_i^2}=\\frac{1}{n-k}\\sum{\\hat{\\epsilon_i}^2}\n\\]\n\n\n\n\n\n\nThus our “ham sandwich” is:\n\n\n\n\\[\n\\underset{k \\times k}{(X^{\\intercal}X)}^{-1}\n\\underset{k \\times n}{X^{\\intercal}}\n\\left(\\frac{1}{n-k} \\times \\underset{1 \\times n}{(\\epsilon\\odot \\epsilon)} ^{\\intercal}\n\\times\\underset{n \\times n}{I} \\right)\n\\underset{n \\times k}{X}\n\\underset{k \\times k}{(X^{\\intercal}X)}^{-1}\n\\]\n\n\nSimilar to homoskedastic errors, the things that impacts the standard error of coefficient \\(k\\) is:\n\nThe variance of all the individual errors, \\(\\sigma_i^2\\)\nThe variance of the feature \\(V(X_k)\\)"
  },
  {
    "objectID": "posts/sandwich_estimators.html#salmon-spherical-errors",
    "href": "posts/sandwich_estimators.html#salmon-spherical-errors",
    "title": "Sandwiches: robust covariance error estimation",
    "section": "Salmon: Spherical Errors 🐟",
    "text": "Salmon: Spherical Errors 🐟\nUsual OLS is efficient if the true model has “spherical errors”. What does this mean in practice?\n\nErrors are homoskedastic: \\(V(\\epsilon_i)=\\sigma^2\\) for all observations\nErrors are serially uncorrelated: \\(cov(\\epsilon_i,\\epsilon_{j\\neq i})=0\\)\n\nWhat does this look like for \\(E[\\epsilon\\epsilon^{\\intercal}]\\)?\n\nThe diagonal of the matrix is a constant value (scalar), \\(\\sigma^2\\)\nThe off-diagonals are all zero\n\n\\[\n\\hat{\\sigma}^2\\underset{n\\times n}{I} =\n\\begin{bmatrix}\n\\hat{\\sigma}^2 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n0 & \\hat{\\sigma}^2 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n0 & 0 & \\hat{\\sigma}^2 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n0 & 0 & 0 & \\hat{\\sigma}^2 & 0 & 0 & 0 & 0 & 0\\\\\n0 & 0 & 0 & 0 & \\hat{\\sigma}^2 & 0 & 0 & 0 & 0\\\\\n0 & 0 & 0 & 0 & 0 & \\hat{\\sigma}^2 & 0 & 0 & 0\\\\\n0 & 0 & 0 & 0 & 0 & 0 & \\hat{\\sigma}^2 & 0 & 0\\\\\n0 & 0 & 0 & 0 & 0 & 0 & 0 & \\hat{\\sigma}^2 & 0\\\\\n0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & \\hat{\\sigma}^2\n\\end{bmatrix}\n\\]\nA good estimation of the constant error variance \\(\\sigma^2\\) is to apply the standard formula to the residuals (i.e. method of moments):\n\\[\n\\hat{\\sigma^2}=\\frac{1}{n-k}\\sum{\\hat{\\epsilon_i}^2} \\equiv \\frac{\\epsilon^{\\intercal}\\epsilon }{n-k}\n\\]\n\n\n\n\n\n\nThus our “salmon sandwich” is:\n\n\n\n\\[\n\\underset{k \\times k}{(X^{\\intercal}X)}^{-1}\n\\underset{k \\times n}\n{X^{\\intercal}}\n\\left(\\frac{1}{n-k} \\times \\underset{1 \\times 1}{(\\epsilon^{\\intercal}\\epsilon)}\n\\times\\underset{n \\times n}{I} \\right)\n\\underset{n \\times k}{X}\n\\underset{k \\times k}{(X^{\\intercal}X)}^{-1}\n\\]\n\n\nIn this scenario, the only things that impact the standard error of the coefficient \\(\\beta_k\\) is:\n\nThe variance of all the residuals, \\(\\sigma^2\\)\nThe variance of the feature \\(V(X_k)\\)\n\n\n\nNote that under spherical errors, the sandwich can be simplified: \\[\n\\displaylines{\n\\begin{align}\nV[\\hat{\\beta}]\n& = (X^{\\intercal}X)^{-1}X^{\\intercal}E[\\epsilon\\epsilon^{\\intercal}]X(X^{\\intercal}X)^{-1}\n\\\\ & = (X^{\\intercal}X)^{-1}X^{\\intercal}\\sigma^2IX(X^{\\intercal}X)^{-1}\n\\\\ & = \\sigma^2(X^{\\intercal}X)^{-1}\\cancel{X^{\\intercal}X}\\cancel{(X^{\\intercal}X)^{-1}}\n\\\\ & = \\sigma^2(X^{\\intercal}X)^{-1}\n\\end{align}\n}\n\\]"
  },
  {
    "objectID": "posts/sandwich_estimators.html#cheese-clustered-errors",
    "href": "posts/sandwich_estimators.html#cheese-clustered-errors",
    "title": "Sandwiches: robust covariance error estimation",
    "section": "Cheese: Clustered Errors 🧀",
    "text": "Cheese: Clustered Errors 🧀\nCluster-robust errors are correct if:\n\nErrors vary for every individual: \\(V(\\epsilon_i)=\\sigma_i^2\\) for all observations i.e. still heteroskedastic\nAND errors within the same cluster \\(C_l\\) are serially correlated: \\(cov(\\epsilon_i,\\epsilon_{j}) \\neq 0 \\text{ if } \\epsilon_i,\\epsilon_j \\in C_l\\)\n\nNote - errors between clusters are assumed not to be serially correlated though i.e. \\(cov(\\epsilon_i,\\epsilon_{j}) =0 \\text{ if } \\epsilon_i \\in C_l,\\epsilon_j \\in C_{m \\neq l}\\)\n\n\nWhat does this look like for \\(\\epsilon\\epsilon^{\\intercal}\\)?\n\nThe diagonal of the matrix is the estimate of variance which is unique for each observation, \\(\\sigma_i^2\\)\nThe off-diagonals are also populated with the covariance - but only when they are both in the same cluster\n\nTake an example where observations 1, 2 and 3 are in cluster A, observations 4 and 5 are in cluster B, observations 6, 7 and 8 are in cluster C etc.\n\\[\n\\underset{n \\times n}{\\epsilon\\epsilon^{\\intercal}} \\sim \\begin{bmatrix}\n\\epsilon_1^2 & \\epsilon_1\\epsilon_2 & \\epsilon_1\\epsilon_3 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n\\epsilon_2\\epsilon_1 & \\epsilon_2^2 & \\epsilon_2\\epsilon_3 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n\\epsilon_3\\epsilon_1 & \\epsilon_3\\epsilon_2 & \\epsilon_3^2 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n0 & 0 & 0 & \\epsilon_4^2 & \\epsilon_4\\epsilon_5 & 0 & 0 & 0 & 0\\\\\n0 & 0 & 0 & \\epsilon_5\\epsilon_4 & \\epsilon_5^2 & 0 & 0 & 0 & 0\\\\\n0 & 0 & 0 & 0 & 0 & \\epsilon_6^2 & \\epsilon_6\\epsilon_7 & \\epsilon_6\\epsilon_8 & 0\\\\\n0 & 0 & 0 & 0 & 0 & \\epsilon_7\\epsilon_6 & \\epsilon_7^2 & \\epsilon_7\\epsilon_8 & 0\\\\\n0 & 0 & 0 & 0 & 0 & \\epsilon_8\\epsilon_6 & \\epsilon_8\\epsilon_7 & \\epsilon_8^2 & 0\\\\\n0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & \\epsilon_{..}^2\\\\\n\\end{bmatrix}\n\\]\nAdditionally, we have to do a finite-sample correction as well on the degrees of freedom, based on the number of clusters \\(n_c\\)\n\n💡 Thus our sandwich is: \\(\\underset{k \\times k}{(X^{\\intercal}X)}^{-1}\n\\underset{k \\times n}{X^{\\intercal}}\n\\left(\\frac{n-1}{n-k}\\frac{n_c}{n_c-1} \\times\n\\underset{n \\times n}{(\\epsilon \\epsilon^{\\intercal})}\n\\right)\n\\underset{n \\times k}{X}\n\\underset{k \\times k}{(X^{\\intercal}X)}^{-1}\\)\n\n\nIn this scenario, there are a few additional things that impact the standard error of coefficient \\(k\\):\n\nIf errors are correlated within clusters, this will increase the error.\nIf features are correlated within clusters, this will also increase the error (due to the off-diagonals in the error variance matrix)\nAnd if both the errors and feature correlations are the same sign, this will also increase the standard error.\n\nNote the sandwich formula above is equivalent to calculating mini sandwiches per cluster before summing. This is preferable to creating an enormous matrix from the error outer product and zeroing out covariances that aren’t the same cluster\ndef cluster_XeeX(cluster_index):\n    j = X[:,'cluster_id']== cluster_id # j is the indexes of the cluster\n    ec = e[j] # ec is a vector of errors for that cluster\n    sigma_sqc = ec.dot(ec.transpose()) # outerproduct of clustered errors\n    Xc = X[j,:] # Xc is a matrix of features for that cluster\n    XeeXc = Xc.transpose().dot(sigma_sqc).dot(Xc) # X'ee'X for that cluster \n        return(XeeXc) # shape is kxk\n\nXeeXc = np.sum([cluster_XeeX(i) for i in np.unique(cl)], axis = 0)\nn_cl = len(np.unique(cl)) # number of clusters\n\n       # finite-sample correction factor.    # sum XeeX across all clusters\nXeeX = ((n-1) / (n-k)) * (n_cl / (n_cl-1)) * np.sum(XeeXc, axis = 0)\nsandwich = var_X_inv.dot(XeeX).var_X_inv\n\n\n💡 Thus our sandwich can also be written: \\(\\underset{k \\times k}{(X^{\\intercal}X)^{-1}}\n\\left(\n\\frac{n-1}{n-k}\\frac{n_c}{n_c-1} \\times\n\\sum_{c=1}^{n_c}{\\underset{k \\times k}{(X_c^{\\intercal} \\epsilon_c \\epsilon_c^{\\intercal}X_c)}}\n\\right)\n\\underset{k \\times k}\n{(X^{\\intercal}X)^{-1}}\\)\n\nOne word of caution about averaging of clusters (requires a large number of clusters)"
  },
  {
    "objectID": "posts/sandwich_estimators.html#salmon-bagel-spherical-errors",
    "href": "posts/sandwich_estimators.html#salmon-bagel-spherical-errors",
    "title": "Sandwiches: robust covariance error estimation",
    "section": "Salmon bagel: Spherical Errors 🐟",
    "text": "Salmon bagel: Spherical Errors 🐟\nUsual OLS is efficient if the true model has “spherical errors”. What does this mean in practice?\n\nErrors are homoskedastic: \\(V(\\epsilon_i)=\\sigma^2\\) for all observations\nErrors are serially uncorrelated: \\(cov(\\epsilon_i,\\epsilon_{j\\neq i})=0\\)\n\nWhat does this look like for \\(E[\\epsilon\\epsilon^{\\intercal}]\\)?\n\nThe diagonal of the matrix is a constant value (scalar), \\(\\sigma^2\\)\nThe off-diagonals are all zero\n\n\\[\n\\hat{\\sigma}^2\\underset{n\\times n}{I} =\n\\begin{bmatrix}\n\\hat{\\sigma}^2 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n0 & \\hat{\\sigma}^2 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n0 & 0 & \\hat{\\sigma}^2 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n0 & 0 & 0 & \\hat{\\sigma}^2 & 0 & 0 & 0 & 0 & 0\\\\\n0 & 0 & 0 & 0 & \\hat{\\sigma}^2 & 0 & 0 & 0 & 0\\\\\n0 & 0 & 0 & 0 & 0 & \\hat{\\sigma}^2 & 0 & 0 & 0\\\\\n0 & 0 & 0 & 0 & 0 & 0 & \\hat{\\sigma}^2 & 0 & 0\\\\\n0 & 0 & 0 & 0 & 0 & 0 & 0 & \\hat{\\sigma}^2 & 0\\\\\n0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & \\hat{\\sigma}^2\n\\end{bmatrix}\n\\]\nA good estimation of the constant error variance \\(\\sigma^2\\) is to apply the standard formula to the residuals (i.e. method of moments):\n\\[\n\\hat{\\sigma^2}=\\frac{1}{n-k}\\sum{\\hat{\\epsilon_i}^2} \\equiv \\frac{\\epsilon^{\\intercal}\\epsilon }{n-k}\n\\]\n\n\n\n\n\n\nThus our “salmon sandwich” is:\n\n\n\n\\[\n\\underset{k \\times k}{(X^{\\intercal}X)}^{-1}\n\\underset{k \\times n}\n{X^{\\intercal}}\n\\left(\\frac{1}{n-k} \\times \\underset{1 \\times 1}{(\\epsilon^{\\intercal}\\epsilon)}\n\\times\\underset{n \\times n}{I} \\right)\n\\underset{n \\times k}{X}\n\\underset{k \\times k}{(X^{\\intercal}X)}^{-1}\n\\]\n\n\n\n\nNote that under spherical errors, the sandwich can be simplified: \\[\n\\displaylines{\n\\begin{align}\nV[\\hat{\\beta}]\n& = (X^{\\intercal}X)^{-1}X^{\\intercal}E[\\epsilon\\epsilon^{\\intercal}]X(X^{\\intercal}X)^{-1}\n\\\\ & = (X^{\\intercal}X)^{-1}X^{\\intercal}\\sigma^2IX(X^{\\intercal}X)^{-1}\n\\\\ & = \\sigma^2(X^{\\intercal}X)^{-1}\\cancel{X^{\\intercal}X}\\cancel{(X^{\\intercal}X)^{-1}}\n\\\\ & = \\sigma^2(X^{\\intercal}X)^{-1}\n\\end{align}\n}\n\\]\nIn this scenario, the only things that impact the standard error of the coefficient \\(\\beta_k\\) is:\n\nThe variance of all the residuals, \\(\\sigma^2\\)\nThe variance of the feature \\(V(X_k)\\)"
  },
  {
    "objectID": "posts/sandwich_estimators.html#ham-sarnie-heteroskedastic-errors",
    "href": "posts/sandwich_estimators.html#ham-sarnie-heteroskedastic-errors",
    "title": "Sandwiches: robust covariance error estimation",
    "section": "Ham sarnie: Heteroskedastic errors 🍖",
    "text": "Ham sarnie: Heteroskedastic errors 🍖\nHeteroskedastic correction is needed if:\n\nErrors vary for every individual: \\(V(\\epsilon_i)=\\sigma_i^2\\) for all observations\nBut they are still independent aka serially uncorrelated: \\(cov(\\epsilon_i,\\epsilon_{j\\neq i})=0\\)\n\nWhat does this look like for \\(E[\\epsilon\\epsilon^{\\intercal}]\\)?\n\nThe diagonal of the matrix is the estimate of variance which is unique for each observation, \\(\\sigma_i^2\\)\nThe off-diagonals are all zero\n\\[\n  \\underset{n \\times n}{\\sigma^2} = \\begin{bmatrix}\n  \\sigma_1^2 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n  0 & \\sigma_2^2 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n  0 & 0 & \\sigma_3^2 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n  0 & 0 & 0 & \\sigma_4^2 & 0 & 0 & 0 & 0 & 0\\\\\n  0 & 0 & 0 & 0 & \\sigma_5^2 & 0 & 0 & 0 & 0\\\\\n  0 & 0 & 0 & 0 & 0 & \\sigma_6^2 & 0 & 0 & 0\\\\\n  0 & 0 & 0 & 0 & 0 & 0 & \\sigma_7^2 & 0 & 0\\\\\n  0 & 0 & 0 & 0 & 0 & 0 & 0 & \\sigma_8^2 & 0\\\\\n  0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & \\sigma_{..}^2\\\\\n  \\end{bmatrix}\n  \\]\nA good estimation of the vector of heteroskedastic error variances \\(\\sigma^2\\) is again to apply the standard formula to the residuals for each individual:\n\n\\[\n\\hat{\\sigma_i^2}=\\frac{1}{n-k}\\sum{\\hat{\\epsilon_i}^2}\n\\]\n\n\n\n\n\n\nThus our “ham sandwich” is:\n\n\n\n\\[\n\\underset{k \\times k}{(X^{\\intercal}X)}^{-1}\n\\underset{k \\times n}{X^{\\intercal}}\n\\left(\\frac{1}{n-k} \\times \\underset{1 \\times n}{(\\epsilon\\odot \\epsilon)} ^{\\intercal}\n\\times\\underset{n \\times n}{I} \\right)\n\\underset{n \\times k}{X}\n\\underset{k \\times k}{(X^{\\intercal}X)}^{-1}\n\\]\n\n\nSimilar to homoskedastic errors, the things that impacts the standard error of coefficient \\(k\\) is:\n\nThe variance of all the individual errors, \\(\\sigma_i^2\\)\nThe variance of the feature \\(V(X_k)\\)"
  },
  {
    "objectID": "posts/sandwich_estimators.html#cheese-baguette-clustered-errors",
    "href": "posts/sandwich_estimators.html#cheese-baguette-clustered-errors",
    "title": "Sandwiches: robust covariance error estimation",
    "section": "Cheese baguette: Clustered Errors 🧀",
    "text": "Cheese baguette: Clustered Errors 🧀\nCluster-robust errors are correct if:\n\nErrors vary for every individual: \\(V(\\epsilon_i)=\\sigma_i^2\\) for all observations i.e. still heteroskedastic\nAND errors within the same cluster \\(C_l\\) are serially correlated: \\(cov(\\epsilon_i,\\epsilon_{j}) \\neq 0 \\text{ if } \\epsilon_i,\\epsilon_j \\in C_l\\)\n\nNote - errors between clusters are assumed not to be serially correlated though i.e. \\(cov(\\epsilon_i,\\epsilon_{j}) =0 \\text{ if } \\epsilon_i \\in C_l,\\epsilon_j \\in C_{m \\neq l}\\)\n\n\nWhat does this look like for \\(\\epsilon\\epsilon^{\\intercal}\\)?\n\nThe diagonal of the matrix is the estimate of variance which is unique for each observation, \\(\\sigma_i^2\\)\nThe off-diagonals are also populated with the covariance - but only when they are both in the same cluster\n\nTake an example where observations 1, 2 and 3 are in cluster A, observations 4 and 5 are in cluster B, observations 6, 7 and 8 are in cluster C etc.\n\\[\n\\underset{n \\times n}{\\epsilon\\epsilon^{\\intercal}} \\sim \\begin{bmatrix}\n\\epsilon_1^2 & \\epsilon_1\\epsilon_2 & \\epsilon_1\\epsilon_3 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n\\epsilon_2\\epsilon_1 & \\epsilon_2^2 & \\epsilon_2\\epsilon_3 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n\\epsilon_3\\epsilon_1 & \\epsilon_3\\epsilon_2 & \\epsilon_3^2 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n0 & 0 & 0 & \\epsilon_4^2 & \\epsilon_4\\epsilon_5 & 0 & 0 & 0 & 0\\\\\n0 & 0 & 0 & \\epsilon_5\\epsilon_4 & \\epsilon_5^2 & 0 & 0 & 0 & 0\\\\\n0 & 0 & 0 & 0 & 0 & \\epsilon_6^2 & \\epsilon_6\\epsilon_7 & \\epsilon_6\\epsilon_8 & 0\\\\\n0 & 0 & 0 & 0 & 0 & \\epsilon_7\\epsilon_6 & \\epsilon_7^2 & \\epsilon_7\\epsilon_8 & 0\\\\\n0 & 0 & 0 & 0 & 0 & \\epsilon_8\\epsilon_6 & \\epsilon_8\\epsilon_7 & \\epsilon_8^2 & 0\\\\\n0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & \\epsilon_{..}^2\\\\\n\\end{bmatrix}\n\\]\nAdditionally, we have to do a finite-sample correction as well on the degrees of freedom, based on the number of clusters \\(n_c\\)\n\n💡 Thus our sandwich is: \\(\\underset{k \\times k}{(X^{\\intercal}X)}^{-1}\n\\underset{k \\times n}{X^{\\intercal}}\n\\left(\\frac{n-1}{n-k}\\frac{n_c}{n_c-1} \\times\n\\underset{n \\times n}{(\\epsilon \\epsilon^{\\intercal})}\n\\right)\n\\underset{n \\times k}{X}\n\\underset{k \\times k}{(X^{\\intercal}X)}^{-1}\\)\n\n\nIn this scenario, there are a few additional things that impact the standard error of coefficient \\(k\\):\n\nIf errors are correlated within clusters, this will increase the error.\nIf features are correlated within clusters, this will also increase the error (due to the off-diagonals in the error variance matrix)\nAnd if both the errors and feature correlations are the same sign, this will also increase the standard error.\n\nNote the sandwich formula above is equivalent to calculating mini sandwiches per cluster before summing. This is preferable to creating an enormous matrix from the error outer product and zeroing out covariances that aren’t the same cluster\ndef cluster_XeeX(cluster_index):\n    j = X[:,'cluster_id']== cluster_id # j is the indexes of the cluster\n    ec = e[j] # ec is a vector of errors for that cluster\n    sigma_sqc = ec.dot(ec.transpose()) # outerproduct of clustered errors\n    Xc = X[j,:] # Xc is a matrix of features for that cluster\n    XeeXc = Xc.transpose().dot(sigma_sqc).dot(Xc) # X'ee'X for that cluster \n        return(XeeXc) # shape is kxk\n\nXeeXc = np.sum([cluster_XeeX(i) for i in np.unique(cl)], axis = 0)\nn_cl = len(np.unique(cl)) # number of clusters\n\n       # finite-sample correction factor.    # sum XeeX across all clusters\nXeeX = ((n-1) / (n-k)) * (n_cl / (n_cl-1)) * np.sum(XeeXc, axis = 0)\nsandwich = var_X_inv.dot(XeeX).var_X_inv\n\n\n💡 Thus our sandwich can also be written: \\(\\underset{k \\times k}{(X^{\\intercal}X)^{-1}}\n\\left(\n\\frac{n-1}{n-k}\\frac{n_c}{n_c-1} \\times\n\\sum_{c=1}^{n_c}{\\underset{k \\times k}{(X_c^{\\intercal} \\epsilon_c \\epsilon_c^{\\intercal}X_c)}}\n\\right)\n\\underset{k \\times k}\n{(X^{\\intercal}X)^{-1}}\\)\n\nOne word of caution about averaging of clusters (requires a large number of clusters)"
  },
  {
    "objectID": "posts/sandwich_estimators.html#cheese-roll-clustered-errors",
    "href": "posts/sandwich_estimators.html#cheese-roll-clustered-errors",
    "title": "Sandwiches: robust covariance error estimation",
    "section": "Cheese roll: Clustered Errors 🧀",
    "text": "Cheese roll: Clustered Errors 🧀\nCluster-robust errors are needed if:\n\nErrors vary for every individual: \\(V(\\epsilon_i)=\\sigma_i^2\\) for all observations i.e. still heteroskedastic\nAND errors within the same cluster \\(C_l\\) are serially correlated: \\(cov(\\epsilon_i,\\epsilon_{j}) \\neq 0 \\text{ if } \\epsilon_i,\\epsilon_j \\in C_l\\)\n\nNote - errors between clusters are assumed not to be serially correlated though i.e. \\(cov(\\epsilon_i,\\epsilon_{j}) =0 \\text{ if } \\epsilon_i \\in C_l,\\epsilon_j \\in C_{m \\neq l}\\)\n\n\nWhat does this look like for \\(E[\\epsilon\\epsilon^{\\intercal}]\\)?\n\nThe diagonal of the matrix is the estimate of variance which is unique for each observation, \\(\\sigma_i^2\\)\nThe off-diagonals are also populated with the covariance - but only when they are both in the same cluster\n\nHere is an example where observations 1, 2 and 3 are in cluster A, observations 4 and 5 are in cluster B, observations 6, 7 and 8 are in cluster C etc.\n\\[\n\\underset{n \\times n}{\\epsilon\\epsilon^{\\intercal}} \\sim \\begin{bmatrix}\n\\epsilon_1^2 & \\epsilon_1\\epsilon_2 & \\epsilon_1\\epsilon_3 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n\\epsilon_2\\epsilon_1 & \\epsilon_2^2 & \\epsilon_2\\epsilon_3 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n\\epsilon_3\\epsilon_1 & \\epsilon_3\\epsilon_2 & \\epsilon_3^2 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n0 & 0 & 0 & \\epsilon_4^2 & \\epsilon_4\\epsilon_5 & 0 & 0 & 0 & 0\\\\\n0 & 0 & 0 & \\epsilon_5\\epsilon_4 & \\epsilon_5^2 & 0 & 0 & 0 & 0\\\\\n0 & 0 & 0 & 0 & 0 & \\epsilon_6^2 & \\epsilon_6\\epsilon_7 & \\epsilon_6\\epsilon_8 & 0\\\\\n0 & 0 & 0 & 0 & 0 & \\epsilon_7\\epsilon_6 & \\epsilon_7^2 & \\epsilon_7\\epsilon_8 & 0\\\\\n0 & 0 & 0 & 0 & 0 & \\epsilon_8\\epsilon_6 & \\epsilon_8\\epsilon_7 & \\epsilon_8^2 & 0\\\\\n0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & \\epsilon_{..}^2\\\\\n\\end{bmatrix}\n\\]\nAdditionally, we have to do a finite-sample correction as well on the degrees of freedom, based on the number of clusters \\(n_c\\)\n\n\n\n\n\n\nThus our cheese sandwich is:\n\n\n\n\\(\\underset{k \\times k}{(X^{\\intercal}X)}^{-1}\n\\underset{k \\times n}{X^{\\intercal}}\n\\left(\\frac{n-1}{n-k}\\frac{n_c}{n_c-1} \\times\n\\underset{n \\times n}{(\\epsilon \\epsilon^{\\intercal})}\n\\right)\n\\underset{n \\times k}{X}\n\\underset{k \\times k}{(X^{\\intercal}X)}^{-1}\\)\n\n\nIn this scenario, there are a few additional things that impact the standard error of coefficient \\(k\\):\n\nIf errors are correlated within clusters, this will increase the error.\nIf features are correlated within clusters, this will also increase the error (due to the off-diagonals in the error variance matrix)\nAnd if both the errors and feature correlations are the same sign, this will also increase the standard error.\nAs well as the variance of the individual errors, \\(\\sigma_i^2\\), and the variance of each feature \\(V(X_k)\\), as before"
  }
]