[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Sciencing Data",
    "section": "",
    "text": "Deriving OLS coefficients (matrix algebra)\n\n\n\n\n\n\nOLS\n\n\nleast-squares\n\n\ncoefficients\n\n\npartial differentiation\n\n\nhessian matrix\n\n\n\n\n\n\n\n\n\nFeb 20, 2024\n\n\nChris Kelly\n\n\n\n\n\n\n\n\n\n\n\n\nBLUE coefficients: bias and efficiency\n\n\n\n\n\n\nOLS\n\n\nleast-squares\n\n\nGauss-markov\n\n\ncoefficients\n\n\nBLUE\n\n\nbias\n\n\nefficiency\n\n\n\n\n\n\n\n\n\nFeb 20, 2024\n\n\nChris Kelly\n\n\n\n\n\n\n\n\n\n\n\n\nSandwich Estimators\n\n\n\n\n\n\nOLS\n\n\nconfidence intervals\n\n\nclustered errors\n\n\nheteroskedastic errors\n\n\n\n\n\n\n\n\n\nFeb 20, 2024\n\n\nChris Kelly\n\n\n\n\n\n\n\n\n\n\n\n\nMinimizing mse tends to mean, MAE tends to median\n\n\n\n\n\n\ndifferentation\n\n\ncost functions\n\n\nMSE\n\n\nMAE\n\n\n\n\n\n\n\n\n\nFeb 8, 2024\n\n\nChris Kelly\n\n\n\n\n\n\n\n\n\n\n\n\nMinimizing RSS equivalent to MLE for OLS\n\n\n\n\n\n\ndifferentation\n\n\ncost functions\n\n\nMSE\n\n\nMAE\n\n\n\n\n\n\n\n\n\nFeb 8, 2024\n\n\nChris Kelly\n\n\n\n\n\n\n\n\n\n\n\n\nDifferentiation formulae\n\n\n\n\n\n\ndifferentation\n\n\npython\n\n\n\n\n\n\n\n\n\nFeb 8, 2024\n\n\nChris Kelly\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/mle_gaussian.html",
    "href": "posts/mle_gaussian.html",
    "title": "Minimizing RSS equivalent to MLE for OLS",
    "section": "",
    "text": "What we are exploring\n\n\n\nShowing that OLS produces the same coefficients as MLE when gaussian errors are used.\n\n\n\nQuick recap of OLS: deriving optimal coefficients through least squares\nThere are many options to choose for the intercept (alpha) and the slope (beta) to best a line that best fits the data, but we want to find the ones that best fit the data:\n\n\nCode\nrequire(plotly)\nset.seed(1)\nX = 0:5\ny = 5 + 2*X + runif(6,-0.5,1)\nplot_ly(type='scatter',mode='lines',line=list(dash='dot')) %&gt;%\n  add_trace(x=X,y=1+1*X,name='alpha=1,beta=0.5') %&gt;%\n  add_trace(x=X,y=5+1*X,name='alpha=5,beta=0.5') %&gt;%\n  add_trace(x=X,y=9+1*X,name='alpha=9,beta=0.5') %&gt;%\n  add_trace(x=X,y=1+2*X,name='alpha=1,beta=2') %&gt;%\n  add_trace(x=X,y=5+2*X,name='alpha=5,beta=2') %&gt;%\n  add_trace(x=X,y=9+2*X,name='alpha=9,beta=2') %&gt;%\n  add_trace(x=X,y=1+4*X,name='alpha=1,beta=4') %&gt;%\n  add_trace(x=X,y=5+4*X,name='alpha=5,beta=4') %&gt;%\n  add_trace(x=X,y=9+4*X,name='alpha=9,beta=4') %&gt;%\n  add_trace(x=X,y=y,mode='markers',name='Observed data',line=list(color='rgba(0,0,0,0)'),marker=list(size=10)) %&gt;%\n  layout(xaxis=list(title='X'), yaxis=list(title='y'))\n\n\n\n\n\n\nWe can see that the choices of alpha and beta that minimize the sum of squared residuals are 5 and 2 respectively:\n\n\nCode\nalpha_options &lt;- seq(3,7,1) # seq(4,6,1) # \nbeta_options &lt;- 2^seq(0,2,0.5) # 2^seq(0.5,1.5,0.5) # \ngraph_options &lt;- \"plot_ly(type='scatter',mode='lines') %&gt;% add_trace(x=X,y=y,mode='markers',name='Observed data') %&gt;% \"\ncost_matrix &lt;- matrix(nrow=length(alpha_options),ncol=length(beta_options),dimnames = list(alpha_options, beta_options))\nfor(alpha in 1:length(alpha_options)) {\n  for(beta in 1:length(beta_options)) {\n    graph_options &lt;- paste0(graph_options, paste0(\" add_trace(x=X,y=\",alpha_options[alpha],\"+\",beta_options[beta],\"*X\",\",name='alpha=\",alpha_options[alpha],\",beta=\",round(beta_options[beta],1),\"') %&gt;%  \\n\"))\n    cost_matrix[alpha,beta] &lt;- sum((y-alpha_options[alpha]-beta_options[beta]*X)^2)\n  }\n}\ngraph_options &lt;- paste0(graph_options, ' layout()')\n# eval(parse(text=graph_options))\nplot_ly(x=alpha_options,y=beta_options,z =~cost_matrix,showscale=FALSE,reversescale=TRUE) %&gt;% \n  layout(scene=list(xaxis=list(title='beta'),yaxis=list(title='alpha'),zaxis=list(title='RSS'))) %&gt;% \n  add_surface(contours = list(z = list(project=list(z=TRUE)))) %&gt;%\n  layout(scene = list(camera = list(eye = list(x = -1.5,y = 1.5,z = 1.5))))\n\n\n\n\n\n\nHowever, we don’t need to do this through an exhaustive gridsearch through all the parameters to minimize the sum of squared residuals, but can yield this by differentiating our regression function with respect to our parameters to find its minimum point.\nThis is simple to see if our features are orthogonal (i.e. in a univariate regression, the intercept is a one-dimensional vector of 1s, and the X a one-dimensional vector) as we can apply partial differentiation by \\(\\beta_j\\):\n\\[\n\\min_\\beta{\\left[\\sum_{i=1}^N{\\epsilon_i^2}\\right]}\n\\Rightarrow \\frac{\\partial}{\\partial\\beta_j} \\sum_{i=1}^N{\\epsilon_i^2}=\\frac{\\partial}{\\partial\\epsilon} \\sum_{i=1}^N{\\epsilon_i^2} \\frac{\\partial\\epsilon}{\\partial\\beta_j} = \\sum_{i=1}^N 2\\epsilon_i\\left(\\frac{\\partial\\epsilon_i}{\\partial\\beta_j}\\right)=0 \\\\\n\\epsilon_i = y_i-\\beta_0-\\beta_1x_i \\Rightarrow \\\\\n\\sum_{i=1}^N2( y_i-\\beta_0-\\beta_1x_i)\\left(\\frac{\\partial( y_i-\\beta_0-\\beta_1x_i)}{\\partial\\beta_j}\\right)=0 \\\\\n\\text{if j=0:} \\\\\n\\Rightarrow \\sum_{i=1}^N2( y_i-\\beta_0-\\beta_1x_i)(-1)=0 \\\\\n\\Rightarrow n\\beta_0 = \\sum_{i=1}^N( y_i-\\beta_1x_i)=\\sum_{i=1}^N{y_i} - \\beta_1\\sum_{i=1}^N{x_i} \\\\\n\\Rightarrow \\beta_0 = \\frac{\\sum_{i=1}^N{y_i}}{n} - \\beta_1\\frac{\\sum_{i=1}^N{x_i}}{n}=\\bar{y}-\\beta_1\\bar{x} \\\\\n\\text{if j=1:} \\\\\n\\Rightarrow \\sum_{i=1}^N2( y_i-\\beta_0-\\beta_1x_i)(-x_i)=0 \\\\\n\\Rightarrow \\sum_{i=1}^N2( y_i-(\\bar{y}-\\beta_1\\bar{x})-\\beta_1x_i)(-x_i)=0 \\\\\n\\Rightarrow \\sum_{i=1}^N( y_i-\\bar{y}-\\beta_1(x_i-\\bar{x}))(-x_i)\n=\\sum_{i=1}^N( y_ix_i-\\bar{y}x_i-\\beta_1x_i^2+\\beta_1\\bar{x}x_i)\n=\\sum_{i=1}^N{y_ix_i}-\\bar{y}\\sum_{i=1}^N{x_i}-\\beta_1\\sum_{i=1}^N{x_i}^2+\\beta_1\\bar{x}\\sum_{i=1}^N{x_i}) \\\\\n=\\sum_{i=1}^N{y_ix_i}-\\bar{y}(N\\bar{x})-\\beta_1\\sum_{i=1}^N{x_i}^2+\\beta_1\\bar{x}(N\\bar{x})=0 \\\\\n\\Rightarrow \\beta_1 = \\frac{\\sum_{i=1}^N{y_ix_i}-N\\bar{x}\\bar{y}}{\\sum_{i=1}^N{x_i^2}-N\\bar{x}^2}\n= \\frac{\\sum_{i=1}^N{y_ix_i}-N\\bar{x}\\bar{y}+(N\\bar{x}\\bar{y}-N\\bar{x}\\bar{y})}{\\sum_{i=1}^N{x_i^2}-N\\bar{x}^2+(N\\bar{x}^2-N\\bar{x}^2)} \\\\\n= \\frac{\\sum_{i=1}^N{y_ix_i}-\\sum_{i=1}^N\\bar{x}\\bar{y}+(\\bar{x}\\sum_{i=1}^N{y_i}-\\bar{y}\\sum_{i=1}^N{x_i})}{\\sum_{i=1}^N{x_i^2}-N\\bar{x}^2+(\\bar{x}\\sum_{i=1}^N{x_i}-\\bar{x}\\sum_{i=1}^N{x_i})}\n= \\frac{\\sum_{i=1}^N{(y_i-\\bar{y})(x_i-\\bar{x})}}{\\sum_{i=1}^N{(x_i-\\bar{x})^2}} \\\\\n= \\frac{cov(x,y)}{var(x)}\n\\]\n(If the regression is multivariate, and the features are not perfectly orthogonal - i.e. is some multicollinearity - then this doesn’t perfectly hold, and can yield different coefficients)\nWe can see then that the best unbiased linear estimator (BLUE) for the intercept and slope is derived from minimizing the sum of squared residuals. These derivations also derive two interesting properties:\n\nFrom line 5, that the mean error is zero: \\(\\sum_{i=1}^N2( y_i-\\beta_0-\\beta_1x_i)(-1)=0 \\Rightarrow \\frac{1}{n}\\sum_{i=1}^N(\\epsilon_i)=0\\)\nFrom line 9, that X is deterministic, and not correlated with the error term: \\(\\sum_{i=1}^N2( y_i-\\beta_0-\\beta_1x_i)(-x_i)=0 \\Rightarrow \\sum_{i=1}^N(\\epsilon_ix_i)=0\\) (note that this is equal to the \\(cov(x_i,e_i)=\\sum_{i=1}^N(\\epsilon_ix_i)-\\sum_{i=1}^N(\\bar{\\epsilon}\\bar{x})\\) since the mean error \\(\\bar{\\epsilon}\\) is zero)\n\n\n\nMaximising likelihood to solve linear regression:\nFirst let’s solve the original linear regression problem by maximum likelihood.\nRather than simply minimizing the residual sum of squares (as usual with the OLS loss function), we want to find the beta that maximises the likelihood of observing the evidence we have, knowing \\(y \\sim N(X'\\beta,1)\\). In other words, the probability of observing \\(y\\) given our data and estimated model parameters is a function of the normal probability density of our squared residuals:\n\\[\np(y|\\beta,X) = \\prod_{i=1}^{n}{\\frac{1}{\\sqrt{2\\pi}}}e^{-\\frac{1}{2}\\epsilon_i^2}\n\\] We can take the negative log of the likelihood function to make it easier to differentiate:\n\\[\n\\max_\\beta{p(y|\\beta,X)} \\Rightarrow \\min_\\beta\\left[{-\\log{\\left(\\prod_{i=1}^{n}{\\frac{1}{\\sqrt{2\\pi}}}e^{-\\frac{1}{2}\\epsilon_i^2}\\right)}}\\right] = \\\\\n\\min_\\beta{\\left[ -\\sum{\\log{\\left(\\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{1}{2}\\epsilon_i^2}\\right)}} \\right]} \\\\\n= \\min_\\beta{\\left[ -\\sum{\\log{\\left(\\frac{1}{\\sqrt{2\\pi}}\\right)}} + -\\sum{\\log{\\left(e^{-\\frac{1}{2}\\epsilon_i^2}\\right)}} \\right]} \\\\\n= \\min_\\beta{\\left[ -\\sum{\\log{((2\\pi)^{-\\frac{1}{2}})}} + -\\sum{\\left(-\\frac{1}{2}\\epsilon_i^2\\right)} \\right]} \\\\\n= \\min_\\beta{\\left[ \\frac{1}{2}\\sum{\\log{(2\\pi)}} + -\\sum{\\left(-\\frac{1}{2}(y_i-X_i'\\beta)^2\\right)} \\right]} \\\\\n= \\min_\\beta{\\left[ \\frac{1}{2}\\sum{\\log{(2\\pi)}} + \\frac{1}{2}(Y-X\\beta)^T(Y-X\\beta) \\right]} \\\\\n= \\min_\\beta{\\left[ \\frac{1}{2}\\sum{\\log{(2\\pi)}} + \\frac{1}{2}\\epsilon^T\\epsilon \\right]} \\\\\n\\leftrightarrow \\beta^*=\\arg\\min_\\beta{\\left[ \\frac{1}{2}\\epsilon^T\\epsilon \\right]}\n\\] Now when we minimise the log-likelihood cost function by differentiating it with respect to \\(\\beta\\) and setting it to zero in order to derive the optimum coefficient, the constant \\(\\log{(2\\pi)}\\) drops out, and we are left with differentiating \\(\\frac{d}{d\\beta}\\epsilon^T\\epsilon=0\\) - the exact equivalent as with frequentist OLS. We can rewrite this in terms of the bayesian paradigm if we think of \\(\\beta\\) as a random variable (rather than a fixed quantity as per frequenist thinking): \\[\np(\\beta|X,Y)=\\frac{p(Y|\\beta,X)p(\\beta|X)}{p(Y|X)}=\\frac{p(Y|\\beta,X)p(\\beta|X)}{\\int p(Y|X,\\beta)p(\\beta|X)d\\beta}\n\\] Where:\n\n\\(p(Y|\\beta,X)\\) is the likelihood function (where we maximise the log-likelihood as above)\n\\(p(Y|X)\\) is the evidence (the data we feed into the model)\n\\(p(\\beta|X)\\) is the prior\n\nIf we assume \\(\\beta\\) is fixed, then \\(p(\\beta|X)=1\\), and thus we get \\(= \\min_\\beta{\\left[\\epsilon^T\\epsilon \\right]} \\Rightarrow \\beta^*=(X^TX)^{-1}X^TY\\) (as per OLS)."
  },
  {
    "objectID": "posts/blue_ols.html",
    "href": "posts/blue_ols.html",
    "title": "BLUE coefficients: bias and efficiency",
    "section": "",
    "text": "What are we exploring?\n\n\n\nProving that the OLS coefficient is the best linear unbiased estimator."
  },
  {
    "objectID": "posts/blue_ols.html#are-the-ols-coefficients-blue",
    "href": "posts/blue_ols.html#are-the-ols-coefficients-blue",
    "title": "BLUE coefficients: bias and efficiency",
    "section": "Are the OLS coefficients “BLUE”?",
    "text": "Are the OLS coefficients “BLUE”?\nWe find a unique solution to the set of coefficients that minimize the sum of squared residuals analytically (see its derivation here):\n\\[\n\\hat{\\beta}^{OLS}=(X^{\\intercal}X)^{-1}X^{\\intercal}y\n\\]\nHowever, how do we know if these coefficients are the best ones we can estimate?\nFor the estimated coefficients to be the Best Linear Unbiased Estimator (i.e. “BLUE”):\n\nThe best estimator has to be unbiased: \\(E[\\hat{\\beta}^*] = \\beta\\)\nAnd among all possible linear, unbiased estimators, it must have the smallest variance: \\(V[\\hat{\\beta}^{*}] &lt; V[\\hat{\\beta}^{Z}]\\)\n\nWe want to ensure our OLS estimate is the best, i.e. that \\(\\hat{\\beta}^{OLS} = \\hat{\\beta}^{*}\\). To achieve this, first we need to confirm it is unbiased. Then given this is true, we can check that the coefficient is most efficient vs all other unbiased estimators.\n\n\n\n\n\n\nGauss Markov Assumptions\n\n\n\nAlong the way, we will outline the Gauss-Markov assumptions utilised that ensure the OLS coefficient is BLUE."
  },
  {
    "objectID": "posts/blue_ols.html#setting-the-scene",
    "href": "posts/blue_ols.html#setting-the-scene",
    "title": "BLUE coefficients: bias and efficiency",
    "section": "Setting the scene",
    "text": "Setting the scene\n\nThe true coefficient and model\n\\(\\beta\\) is the true (unobserved) relationship between all the relevant explanatory features, \\(X\\), and their associated observed outcomes, \\(y\\). In other words, if we observed the entire population of data, it is the relationship we would find.\nConcretely, we assume the outcome is a linear function of all its relevant features. This “true model” perfectly predicts the outcome, except for random noise \\(\\epsilon\\) that influences the observed outcome: \\(y = X\\beta + \\epsilon\\)\n\n\nThe estimated coefficient\n\\(\\hat{\\beta}\\) is our estimated coefficient for the true relationship \\(\\beta\\). In reality, we estimate \\(\\hat{\\beta}\\) from the small, finite sample of size \\(n\\) that is collected, not the whole population. Given any random sample could be collected, we can term the coefficient resulting from the optimum estimation proceedure to be \\(\\hat{\\beta}^*\\). We want to understand if \\(\\hat{\\beta}^{OLS} = \\hat{\\beta}^*\\).\n\n\nThe expected estimated coefficient\n\\(E[\\hat{\\beta}]\\) is the “expected” estimated coefficient. Imagine we repeat the action of estimating the coefficient \\(\\hat{\\beta}\\) many times, each time collecting a new sample (where each observation is sampled i.i.d), and recording the value for the estimated coefficient. \\(E[\\hat{\\beta}]\\) would then be the average of all of those estimated coefficients. If the OLS coefficient is unbiased, then the expected coefficient estimate should be equal to the true one, \\(E[\\hat{\\beta}^{OLS}]=\\beta\\).\n\n\nThe variance of the estimated coefficient\n\\(V[\\hat{\\beta}]\\) is the variance of the estimated coefficient. It determines how much we might expect our estimate \\(\\hat{\\beta}\\) to differ from the true \\(\\beta\\) for any sample drawn. Given the OLS coefficient has been shown to be unbiased, if it is BLUE we expect its variance to be lower than another other unbiased choice \\(\\hat{\\beta}^{Z}\\). Concretely, we want to find \\(V[\\hat{\\beta}^{OLS}] &lt; V[\\hat{\\beta}^{Z}]\\)."
  },
  {
    "objectID": "posts/blue_ols.html#bias",
    "href": "posts/blue_ols.html#bias",
    "title": "BLUE coefficients: bias and efficiency",
    "section": "Bias",
    "text": "Bias\nOften our small finite samples of size \\(n\\) are not a perfect reflection of the population they are drawn from. This “sampling error” means we might estimate a different relationship between \\(X\\) and \\(y\\) than the true relationship of the population, i.e. \\(\\hat{\\beta} \\neq \\beta\\).\nHowever, we should expect our estimated coefficient to be equal to the true value on average. This means we do not want to have a bias towards the estimate being systematically too small or too large, for example. In other words, if we repeated the whole proceedure thousands of times (each time taking new samples, and estimating a coefficient from the new sample) then the average of all the estimated coefficients values should be equal to the true value, i.e. \\(E[\\hat{\\beta}] = \\beta\\).\nRecall that we believe there is a true model that follows the form:\n\\[\ny = X\\beta + \\epsilon\n\\]\n\n\n\n\n\n\nGM1: Linearity \n\n\n\nThe formula above relies on the first Gauss-Markov assumption - that the dependent variable \\(y\\) is assumed to be a linear function of the variables \\(X\\). Note that implies that the proper functional form has been selected (i.e. the relationship is linear) and there are no omitted variables - a huge assumption!\n\n\nIf we substitute this into our estimated coefficient:\n\\[\n\\displaylines{\n\\begin{align}\n\\hat{\\beta}^{OLS} & = (X^{\\intercal}X)^{-1}X^{\\intercal}y\n\\\\ & = (X^{\\intercal}X)^{-1}X^{\\intercal}(X\\beta+\\epsilon)\n\\\\ & = (X^{\\intercal}X)^{-1}X^{\\intercal}X\\beta+(X^{\\intercal}X)^{-1}X^{\\intercal}\\epsilon\n\\\\ & = \\beta+(X^{\\intercal}X)^{-1}X^{\\intercal}\\epsilon\n\\end{align}\n}\n\\]\nwe show that the estimated coefficient \\(\\hat{\\beta}^{OLS}\\) will differ from the true value depending on the random error \\(\\epsilon\\) associated with the particular finite sample collected.\nNow let’s now take the expectation, to determine when the coefficient is unbiased. In other words, what is the “average” coefficient if we took the sample many times: \\[\n\\displaylines{\n\\begin{align}\nE[\\hat{\\beta}^{OLS}]\n& = \\beta +(X^{\\intercal}X)^{-1}X^{\\intercal}E[\\epsilon]\n\\\\ & = \\beta & \\iff E[\\epsilon] & = 0\n\\end{align}\n\\\\\n}\n\\]\nWe find that the coefficient is unbiased as long as the expected error is also zero.\n\n\n\n\n\n\nGM2: Strict Exogeneity \n\n\n\nThe second Gauss-Markov assumption is strict exogeneity, where the expected error is zero for all feature values: \\(E[\\epsilon|X] = 0\\). By definition, the weaker exogeneity statement of \\(E[\\epsilon] = 0\\) is implied by having the expected error conditional being equal to zero."
  },
  {
    "objectID": "posts/blue_ols.html#efficiency",
    "href": "posts/blue_ols.html#efficiency",
    "title": "BLUE coefficients: bias and efficiency",
    "section": "Efficiency",
    "text": "Efficiency\nTo be the “best”, the OLS estimator also needs to be efficient. This means that it has the lowest variance of all unbiased estimators. This section looks to prove this.\n\nCoefficient variance for OLS\nFirst, let’s derive the variance from the coefficients estimated using OLS, termed \\(V[\\hat{\\beta}^{OLS}]\\). As before, we substitute the true model \\(y = X\\beta + \\epsilon\\) into the coefficient estimated through OLS:\n\\[\n\\displaylines{\n\\begin{align}\n\\hat{\\beta}^{OLS}-\\beta\n& = \\beta +  ((X^{\\intercal}X)^{-1}X^{\\intercal}\\epsilon) - \\beta \\\\\n& = (X^{\\intercal}X)^{-1}X^{\\intercal}\\epsilon\n\\\\ \\\\\n\\therefore\n(\\hat{\\beta}^{OLS}-\\beta)(\\hat{\\beta}^{OLS}-\\beta)^{\\intercal}\n& = ((X^{\\intercal}X)^{-1}X^{\\intercal}\\epsilon)((X^{\\intercal}X)^{-1}X^{\\intercal}\\epsilon)^{\\intercal}\n\\\\ & =\n(X^{\\intercal}X)^{-1}X^{\\intercal}\\epsilon\\epsilon^{\\intercal}X(X^{\\intercal}X)^{-1} \\\\ \\\\\n\\therefore\nV(\\hat{\\beta}^{OLS}) & = E[(\\hat{\\beta}^{OLS}-\\beta)(\\hat{\\beta}^{OLS}-\\beta)^{\\intercal}]\n\\\\ & =\n(X^{\\intercal}X)^{-1}X^{\\intercal}E[\\epsilon\\epsilon^{\\intercal}]X(X^{\\intercal}X)^{-1}\n\\end{align}\n}\n\\]\nWe can simplify this further by appling some assumptions to the estimated error variance \\(E[\\epsilon\\epsilon^{\\intercal}]\\):\n\\[\n\\displaylines{\n\\begin{align}\nE[\\epsilon \\epsilon^{\\intercal}] & =  \n\\begin{bmatrix}\nE[\\epsilon_1^2] & \\cdots & E[\\epsilon_1\\epsilon_n] \\\\\n\\vdots & \\ddots & \\vdots \\\\\nE[\\epsilon_n\\epsilon_1] & \\cdots & E[\\epsilon_n^2]\n\\end{bmatrix} \\\\ \\\\\n& = \\begin{bmatrix}\n\\frac{1}{n}\\sum_{i=1}^{n}{\\epsilon_i^2} & \\cdots & 0 \\\\\n\\vdots & \\ddots & \\vdots \\\\\n0 & \\cdots & \\frac{1}{n}\\sum_{i=1}^{n}{\\epsilon_i^2}\n\\end{bmatrix} \\\\ \\\\\n& = \\begin{bmatrix}\n\\hat{\\sigma}^2 & \\cdots & 0 \\\\\n\\vdots & \\ddots & \\vdots \\\\\n0 & \\cdots & \\hat{\\sigma}^2\n\\end{bmatrix} \\\\ \\\\\n& = \\hat{\\sigma}^2I\n\\end{align}\n}\n\\]\nHow can we jump to this result? Well we are making two assumptions:\n\nNo serial correlation: \\(\\rho_{\\epsilon_{i},\\epsilon_{i \\neq j}} = 0\\). No correlation between sample errors means that \\(E[\\epsilon_i \\epsilon_{j \\neq i}] = 0\\), and hence the off-diagonals of the error covariance matrix are zero.\nHomoskedasticity: the assumption of uniform error variance for all samples means that \\(V[\\epsilon_i^2] = V[\\epsilon_{j \\neq i}^2] = \\hat{\\sigma}^2\\). And our best approximation for \\(\\hat{\\sigma}^2\\) is simply taking the average squared error: \\(\\hat{\\sigma}^2 = \\frac{1}{n}\\sum_{i=1}^{n}{\\epsilon_i^2}\\)\n\n\n\n\n\n\n\nGM3: Spherical errors \n\n\n\nThe third Gauss-Markov assumption is spherical errors, \\(E[\\epsilon\\epsilon^{\\intercal}|X] = 0\\). This means that the outer product of the expected errors is a scalar matrix, which implies no serial correlation and homoskedasticity.\n\n\n\n\nIt is especially important to make the right assumptions about \\(E[\\epsilon\\epsilon^{\\intercal}|X]\\) as it impacts where our estimate of the standard errors is correct! We will dive into what happens to SE if we violate these assumptions in another post.\nSince we now see that \\(\\hat{\\sigma}^2\\) is a scalar matrix, we can thus simplfy the variance formula further:\n\\[\n\\displaylines{\n\\begin{align}\nV(\\hat{\\beta}^{OLS}) & =\n(X^{\\intercal}X)^{-1}X^{\\intercal}E[\\epsilon\\epsilon^{\\intercal}]X(X^{\\intercal}X)^{-1}\n\\\\ & =\n(X^{\\intercal}X)^{-1}X^{\\intercal} \\hat{\\sigma}^2I X(X^{\\intercal}X)^{-1}\n\\\\ & =\n\\hat{\\sigma}^2\n\\cancel{(X^{\\intercal}X)^{-1}}\n\\cancel{X^{\\intercal} X}\n(X^{\\intercal}X)^{-1}\n\\\\ & = \\hat{\\sigma}^2(X^{\\intercal}X)^{-1}\n\\end{align}\n}\n\\]\n\n\nFormulating an alternative unbiased coefficient\nNext step - lets formulate another estimator, \\(\\hat{\\beta}^{z}\\), which differs from \\(\\hat{\\beta}^{OLS}\\) by a non-zero matrix \\(A\\). See how they both differ below:\n\\[\n\\displaylines{\n\\begin{align}\n\\hat{\\beta}^{OLS} & =(X^{\\intercal}X)^{-1}X^{\\intercal}y \\\\\n\\hat{\\beta}^{Z} & =\\left((X^{\\intercal}X)^{-1}X^{\\intercal}+A\\right)y\n\\end{align}\n}\n\\]\nNow we need to ensure this new estimator is not biased. So by taking the expectation in the same was as for OLS…\n\\[\n\\displaylines{\n\\begin{align}\nE[\\hat{\\beta}^{Z}] & = E\\left[ \\left((X^{\\intercal}X)^{-1}X^{\\intercal}+A\\right)y \\right]\n\\\\ & = \\left((X^{\\intercal}X)^{-1}X^{\\intercal}+A\\right)(X\\beta+ \\cancel{E\\left[\\epsilon \\right]}) & \\because E[\\epsilon] = 0\n\\\\ & = (X^{\\intercal}X)^{-1}X^{\\intercal}X\\beta+AX\\beta\n\\\\ & = \\beta+AX\\beta\n\\end{align}\n}\n\\]\nSo the estimator is only unbiased iff \\(AX=0\\). This is important to note when comparing the variance between unbiased coefficients - see below!\n\n\nVariance of the alternative unbiased coefficient\nJust like before, we calculate the variance:\n\\[\n\\displaylines{\n\\begin{align}\nV[\\hat{\\beta}^{Z}]\n& = V\\left[ \\left((X^{\\intercal}X)^{-1}X^{\\intercal}+A\\right)y \\right]\n\\\\ & = \\left((X^{\\intercal}X)^{-1}X^{\\intercal}+A\\right) V[y] \\left((X^{\\intercal}X)^{-1}X^{\\intercal}+A\\right)^{\\intercal}\n\\\\ & = \\hat{\\sigma}^2 \\left((X^{\\intercal}X)^{-1}X^{\\intercal}+A\\right) \\left((X^{\\intercal}X)^{-1}X^{\\intercal}+A\\right)^{\\intercal}\n& \\because E[\\epsilon \\epsilon^{\\intercal}|X] = 0\n\\\\ & = \\hat{\\sigma}^2\\left((X^{\\intercal}X)^{-1}X^{\\intercal}+A\\right) \\left(X(X^{\\intercal}X)^{-1}+A^{\\intercal}\\right)\n\\\\ & = \\hat{\\sigma}^2 \\left(\n(X^{\\intercal}X)^{-1}X^{\\intercal} X(X^{\\intercal}X)^{-1} + AX(X^{\\intercal}X)^{-1} + (X^{\\intercal}X)^{-1}X^{\\intercal}A^{\\intercal} + AA^{\\intercal}\n\\right)\n\\\\ & = \\hat{\\sigma}^2\n(X^{\\intercal}X)^{-1} + \\hat{\\sigma}^2AA^{\\intercal}\n& \\because AX = 0\n\\\\ & = V[\\beta^{OLS}] + \\hat{\\sigma}^2AA^{\\intercal}\n\\end{align}\n}\n\\]\nNow since AA is surely a positive semi-definite matrix, then we know that \\(V[\\hat{\\beta}^{Z}] &gt; V[\\hat{\\beta}^{OLS}]\\).\nWe have shown that \\(\\hat{\\beta}^{OLS}\\) has the smallest variance among all unbiased estimators!"
  },
  {
    "objectID": "posts/blue_ols.html#summarising-the-gauss-markov-assumptions",
    "href": "posts/blue_ols.html#summarising-the-gauss-markov-assumptions",
    "title": "BLUE coefficients: bias and efficiency",
    "section": "Summarising the Gauss-markov assumptions",
    "text": "Summarising the Gauss-markov assumptions\nAlong the way, we showed where assumptions were needed to ensure the OLS coefficient estimation is BLUE.\nWe actually missed one out, but it is actually quite trivial to see from the OLS coefficient formula:\n\\[\n\\hat{\\beta}^{OLS}=(X^{\\intercal}X)^{-1}X^{\\intercal}y\n\\]\n\n\n\n\n\n\nGM4: Full rank \n\n\n\nThe matrix \\(X\\) must be of full rank \\(k\\), so that it is possible to invert the matrix \\(X^{\\intercal}X\\). This is equivalent to having no perfect multi-collinearity.\n\n\nWe have now collected our full set of Gauss-Markov assumptions required for the OLS coefficient to be BLUE:\n\nLinearity\nStrict Exogeneity\nSpherical Errors\nFull rank"
  },
  {
    "objectID": "posts/mse_mean_mae_median.html",
    "href": "posts/mse_mean_mae_median.html",
    "title": "Minimizing mse tends to mean, MAE tends to median",
    "section": "",
    "text": "What we are solving\n\n\n\nWhy minimizing the mean absolute error tends towards the median of the sample.\n\n\nIn the absence of informative features, an ML algorithm minimizing the sum of squared errors will tend towards predicting the mean of the sample. However, minimizing the sum of absolute errors will tend towards predicting the median of the sample.\nThis post dives into why this is the case.\n\nMinimizing residual sum-of-squares\nLet’s define the residual for sample \\(i\\) as \\(\\epsilon_i\\). We now want to find the prediction \\(\\hat{y}\\) that minimizes the sum of all squared residuals (i.e. where the gradient is zero):\n\\[\n\\displaylines{\n\\begin{align}\n\\min_\\hat{y}{\\left[\\sum_{i=1}^N{\\epsilon_i^2}\\right]}\n\\Rightarrow &\n\\frac{\\partial}{\\partial \\hat{y}} \\sum_{i=1}^N{\\epsilon_i^2} \\\\ = &\n\\frac{\\partial \\left( \\sum_{i=1}^N{\\epsilon_i^2} \\right) }{\\partial\\epsilon}\n\\left( \\frac{\\partial\\epsilon}{\\partial \\hat{y} } \\right) \\\\ = &\n\\sum_{i=1}^N 2\\epsilon_i \\left( \\frac{\\partial\\epsilon_i}{\\partial \\hat{y} }\\right) = 0\n\\end{align}\n}\n\\]\nWe can now substitue in \\(\\epsilon_i = y - \\hat{y}\\):\n\\[\n\\displaylines{\n\\begin{align}\n\\sum_{i=1}^N 2\\epsilon_i \\left( \\frac{\\partial\\epsilon_i}{\\partial \\hat{y} }\\right) &\n= \\sum_{i=1}^N2( y_i- \\hat{y})\\left(\\frac{\\partial( y_i-\\hat{y})}{\\partial \\hat{y}}\\right) \\\\  &\n= \\sum_{i=1}^N2( y_i- \\hat{y} )(-1) \\\\ &\n= \\sum_{i=1}^N2( y_i) - 2ny = 0 \\\\ &\n\\therefore n \\hat{y} = \\sum_{i=1}^N( y_i) \\\\ &\n\\therefore \\hat{y} = \\frac{\\sum_{i=1}^N{y_i}}{n} = \\bar{y}\n\\end{align}\n}\n\\]\nThus we can see that the prediction that minimizes the sum of squared residuals, is simply the mean.\n\n\nMinimize sum of absolute residuals\nWe now do the same think again, but this time look to minimize the sum of all absolute residuals instead.\n\\[\n\\displaylines{\n\\begin{align}\n\\min_\\hat{y}{\\left[\\sum_{i=1}^N{\\mid \\epsilon_i \\mid}\\right]}\n\\Rightarrow &\n\\frac{\\partial}{\\partial \\hat{y}} \\sum_{i=1}^N{\\left(\\epsilon_i^2\\right)^{1/2}} \\\\ = &\n\\frac{\\partial \\sum_{i=1}^N{ \\left(\\epsilon_i^2\\right)^{1/2} } }{\\partial\\epsilon_i^2}\n\\times \\frac{\\partial\\epsilon_i^2}{\\partial \\epsilon_i }\n\\times \\frac{\\partial\\epsilon_i}{\\partial \\hat{y} }  \\\\ = &\n\\frac{1}{2} \\sum_{i=1}^N{ \\left(\\epsilon_i^2\\right)^{-1/2} }\n\\times 2 \\epsilon_i\n\\times \\frac{\\partial\\epsilon_i}{\\partial \\hat{y} }  \\\\ = &\n\\sum_{i=1}^N{ \\left(\\epsilon_i^2\\right)^{-1/2} } \\times \\epsilon_i\n\\times \\frac{\\partial\\epsilon_i}{\\partial \\hat{y} }  \\\\ = &\n\\sum_{i=1}^N \\frac{\\epsilon_i}{\\mid \\epsilon_i \\mid} \\left( \\frac{\\partial\\epsilon_i}{\\partial \\hat{y} }\\right) = 0\n\\end{align}\n}\n\\]\nAnd similarly to before, we can now substitute in \\(\\epsilon_i = y - \\hat{y}\\):\n\\[\n\\displaylines{\n\\begin{align}\n\\sum_{i=1}^N \\frac{\\epsilon_i}{\\mid \\epsilon_i \\mid} \\left( \\frac{\\partial\\epsilon_i}{\\partial \\hat{y} }\\right) &\n= \\sum_{i=1}^N \\frac{ y_i- \\hat{y} }{\\mid y_i- \\hat{y} \\mid}\\left(\\frac{\\partial( y_i-\\hat{y})}{\\partial \\hat{y}}\\right) \\\\  &\n= \\sum_{i=1}^N \\frac{ y_i- \\hat{y} }{\\mid y_i- \\hat{y} \\mid}(-1) = 0\n\\end{align}\n}\n\\]\nNow \\(f(x) = \\frac{ x }{\\mid x \\mid}\\) is an cool transformation, keeping its sign but getting rid of the magnitude of the size of \\(x\\), i.e.:\n\n\\(f(x &lt; 0) = -1\\)\n\\(f(x &gt; 0) = 1\\)\n\nSo to ensure that \\(\\sum f(\\epsilon_i)=0\\), we need to pick a value for \\(\\hat{y}\\) that means half of the errors are \\(&lt;0\\) and half of the errors are \\(&gt;0\\).\nSo that means half the errors must be negative, and half are positive. So \\(\\hat{y}\\) has to be the median value!\nFin."
  },
  {
    "objectID": "posts/ols_coef_derivation.html",
    "href": "posts/ols_coef_derivation.html",
    "title": "Deriving OLS coefficients (matrix algebra)",
    "section": "",
    "text": "What we are exploring\n\n\n\nDeriving a unique, analytical solution to the set of coefficients that minimize the sum of squared residuals."
  },
  {
    "objectID": "posts/ols_coef_derivation.html#summary",
    "href": "posts/ols_coef_derivation.html#summary",
    "title": "Deriving OLS coefficients (matrix algebra)",
    "section": "Summary",
    "text": "Summary\nThe cost function for OLS is the sum of squared residuals, \\(\\hat{\\epsilon}^{\\intercal}\\hat{\\epsilon}\\). In order to fit a good linear model, we want to find optimum values for the estimated vector of coefficients, \\(\\hat{\\beta}^*\\), that minimizes this cost function.\nFirst we do partial differentiation of the cost function with respect to the coefficients. Finding the coefficient values where the partial differential is equal to zero reveals the stationary points of the cost function. For OLS in particular, we can find a unique solution for the choice of coefficients that can be found analytically. The hessian matrix then further proves that this is a global minima."
  },
  {
    "objectID": "posts/ols_coef_derivation.html#deriving-the-optimum-coefficients",
    "href": "posts/ols_coef_derivation.html#deriving-the-optimum-coefficients",
    "title": "Deriving OLS coefficients (matrix algebra)",
    "section": "Deriving the optimum coefficients",
    "text": "Deriving the optimum coefficients\n\n0. Defining the notation\nFor a sample \\(i\\), we observe an outcome \\(y_i\\). \\(y\\) is a vector of all \\(n\\) observed outcomes.\n\\[\n\\underset{n \\times 1} {y} =\n\\begin{bmatrix}\n    y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_{n-1} \\\\ y_n\n\\end{bmatrix}\n\\]\nWe also observe \\(k\\) features for every sample \\(i\\). \\(X\\) is a matrix of these observed features. Note the first column is usually all ones, to include an intercept to optimize (or “bias” term).\n\\[\n\\underset{n \\times k} {X} =\n\\begin{bmatrix}\n  1 & x_{11} & \\cdots & x_{1,k-1} & x_{1,k} \\\\\n  1 & x_{21} & \\cdots & x_{2,k-1} & x_{2,k} \\\\\n  1 & x_{31} & \\cdots & x_{3,k-1} & x_{3,k} \\\\\n  \\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\\n  1 & x_{n-2,1} & \\cdots & x_{n-2,k-1} & x_{n-2,k} \\\\\n  1 & x_{n-1,1} & \\cdots & x_{n-1,k-1} & x_{n-1,k} \\\\\n  1 & x_{n,1} & \\cdots & x_{n,k-1} & x_{n,k}\n\\end{bmatrix}\n\\]\nThe contribution of each feature to the prediction is estimated by the coefficients \\(\\hat{\\beta}\\).\n\\[\n\\underset{k \\times 1} {\\hat{\\beta}} =\n\\begin{bmatrix}\n  \\beta_0 \\\\\n  \\beta_1 \\\\\n  \\vdots \\\\\n  \\beta_{k-1} \\\\\n  \\beta_{k}\n\\end{bmatrix}\n\\]\nWe make predictions, \\(\\hat{y}\\), by calculating the dot product of the features \\(X\\) and the coefficients \\(\\hat{\\beta}\\).\n\\[\n\\hat{y} = X \\hat{\\beta}\n\\]\nwhich is shorthand for this:\n\\[\n\\displaylines{\n\\begin{align}\n\\begin{bmatrix}\n  \\hat{y}_1 \\\\ \\hat{y}_2 \\\\ \\hat{y}_3 \\\\ \\vdots \\\\ \\hat{y}_{n-2} \\\\ \\hat{y}_{n-1} \\\\ \\hat{y}_n\n\\end{bmatrix} & =\n\\begin{bmatrix}\n  1 & x_{11} & \\cdots & x_{1,k-1} & x_{1,k} \\\\\n  1 & x_{21} & \\cdots & x_{2,k-1} & x_{2,k} \\\\\n  1 & x_{31} & \\cdots & x_{3,k-1} & x_{3,k} \\\\\n  \\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\\n  1 & x_{n-2,1} & \\cdots & x_{n-2,k-1} & x_{n-2,k} \\\\\n  1 & x_{n-1,1} & \\cdots & x_{n-1,k-1} & x_{n-1,k} \\\\\n  1 & x_{n,1} & \\cdots & x_{n,k-1} & x_{n,k}\n\\end{bmatrix}\n\\begin{bmatrix}\n  \\hat{\\beta}_0 \\\\\n  \\hat{\\beta}_1 \\\\\n  \\vdots \\\\\n  \\hat{\\beta}_{k-1} \\\\\n  \\hat{\\beta}_{k}\n\\end{bmatrix}\n\\\\ \\\\ & =\n\\begin{bmatrix}\n  \\hat{\\beta}_0 + \\hat{\\beta}_{1}x_{1,1} + \\cdots + \\hat{\\beta}_{k-1}x_{1,k-1} + \\hat{\\beta}_{k}x_{1,k} \\\\\n  \\hat{\\beta}_0 + \\hat{\\beta}_{1}x_{2,1} + \\cdots + \\hat{\\beta}_{k-1}x_{2,k-1} + \\hat{\\beta}_{k}x_{2,k} \\\\\n  \\hat{\\beta}_0 + \\hat{\\beta}_{1}x_{3,1} + \\cdots + \\hat{\\beta}_{k-1}x_{3,k-1} + \\hat{\\beta}_{k}x_{3,k} \\\\\n  \\vdots \\\\\n  \\hat{\\beta}_0 + \\hat{\\beta}_{1}x_{n-2,1} + \\cdots + \\hat{\\beta}_{k-1}x_{2,k-1} + \\hat{\\beta}_{k}x_{n-2,k} \\\\\n  \\hat{\\beta}_0 + \\hat{\\beta}_{1}x_{n-1,1} + \\cdots + \\hat{\\beta}_{k-1}x_{2,k-1} + \\hat{\\beta}_{k}x_{n-1,k} \\\\\n  \\hat{\\beta}_0 + \\hat{\\beta}_{1}x_{n,1} + \\cdots + \\hat{\\beta}_{k-1}x_{2,k-1} + \\hat{\\beta}_{k}x_{n,k} \\\\\n\\end{bmatrix}\n\\end{align}\n}\n\\]\nThe residual is the difference between the true outcome and the model prediction.\n\\[\n\\hat{\\epsilon} = y_i -\\hat{y}_i\n\\]\nwhich is shorthand for this:\n\\[\n\\begin{bmatrix}\n  \\hat{\\epsilon_1} \\\\\n  \\hat{\\epsilon_2} \\\\\n  \\hat{\\epsilon_3} \\\\\n  \\vdots \\\\\n  \\hat{\\epsilon_{n-2}} \\\\\n  \\hat{\\epsilon_{n-1}} \\\\\n  \\hat{\\epsilon_{n}}\n\\end{bmatrix} =\n\\begin{bmatrix}\n    y_1 \\\\ y_2 \\\\ y_3 \\\\ \\vdots \\\\ y_{n-2} \\\\ y_{n-1} \\\\ y_n\n\\end{bmatrix} -\n\\begin{bmatrix}\n  \\hat{y}_1 \\\\ \\hat{y}_2 \\\\ \\hat{y}_3 \\\\ \\vdots \\\\ \\hat{y}_{n-2} \\\\ \\hat{y}_{n-1} \\\\ \\hat{y}_n\n\\end{bmatrix}\n\\]\nOur aim is to find the optimum vector of coefficients, \\(\\hat{\\beta}^*\\), that minimizes the sum of squared residuals:\n\\[\n\\min_{\\beta} \\left( \\epsilon^{\\intercal}\\epsilon \\right)\n\\]\n\n\n1. Expand the sum of squared residuals\nThe first step involves expanding the sum of squared residuals, and substituting in \\(X \\hat{\\beta}\\) for \\(\\hat{y}\\). \\[\n\\displaylines{\n\\begin{align}\n\\sum_{i=1}^n{\\hat{\\epsilon}_i^2} & = \\hat{\\epsilon}^{\\intercal}\\hat{\\epsilon}\n\\\\& =(y-X\\hat{\\beta})^{\\intercal}(y-X\\hat{\\beta})\n\\\\& = y^{\\intercal}y - y^{\\intercal}X\\hat{\\beta}-\n\\hat{\\beta}^{\\intercal} X^{\\intercal}y+\n\\hat{\\beta}^{\\intercal}X^{\\intercal}X\\hat{\\beta}\n\\\\& = y^{\\intercal}y - 2y^{\\intercal}X\\hat{\\beta}\n+\\hat{\\beta}^{\\intercal}X^{\\intercal}X\\hat{\\beta}\n\\end{align}\n}\n\\]\n\n\nNote we can simply add the two middle terms, since are both scalars:\n\\[\n\\displaylines{\ny^{\\intercal}X\\hat{\\beta} =\n\\hat{\\beta}^{\\intercal} X^{\\intercal}y \\\\\n\\because \\underset{1 \\times n}{y^{\\intercal}} \\times\n\\underset{n \\times k}{X} \\times\n\\hat{\\underset{k \\times 1}{\\beta}}\n=\n\\hat{\\underset{1 \\times k}{\\beta}^{\\intercal}} \\times\n\\underset{k \\times n}{X^{\\intercal}} \\times\n\\underset{n \\times 1}{y}\n}\n\\]\n\n\n2. Partially differentiate RSS with respect to beta\nThe second step involves partially differentiating the cost function with respect to its parameters, to understand how it changes as the coefficients vary.\n\\[\n\\displaylines{\n\\begin{align}\n\\frac{\\partial}{\\partial\\hat{\\beta}}\\hat{\\epsilon}^{\\intercal}\\hat{\\epsilon} & \\equiv\n\\begin{bmatrix}\n    \\frac{\\partial}{\\partial\\hat{\\beta}_1}\\hat{\\epsilon}^{\\intercal}\\hat{\\epsilon} \\\\\n    \\frac{\\partial}{\\partial\\hat{\\beta}_2}\\hat{\\epsilon}^{\\intercal}\\hat{\\epsilon} \\\\\n    \\vdots \\\\\n    \\frac{\\partial}{\\partial\\hat{\\beta}_k}\\hat{\\epsilon}^{\\intercal}\\hat{\\epsilon}\n\\end{bmatrix}\n\\\\ & = \\frac{d}{d\\hat{\\beta}}(\ny^{\\intercal}y - 2y^{\\intercal}X\\hat{\\beta}\n+\\hat{\\beta}^{\\intercal}X^{\\intercal}X\\hat{\\beta}) \\\\ & = 0 - 2X^{\\intercal}y +((X^{\\intercal}X)\\hat{\\beta} + (X^{\\intercal}X)^{\\intercal}\\hat{\\beta})\n\\\\ & = -2X^{\\intercal}y +  2((X^{\\intercal}X)\\hat{\\beta})\n\\end{align}\n}\n\\]\n\n\nTwo matrix differentiation rules used here for reference:\n\\[\n\\displaylines{\n\\frac{\\partial}{dx}(Ax) = A^{\\intercal}x \\\\\n\\frac{\\partial}{dx}(xAx) = Ax + A^{\\intercal}x\n}\n\\]\nAnd note \\(X^{\\intercal}X = (X^{\\intercal}X)^{\\intercal}\\) by definition, so we can add the two last terms.\n\n\n3. Find the coefficient values at the stationary point\nNow we find the choices of \\(\\beta\\) where the partial differential is equal to zero. These stationary points for the cost function are either at its maximum or minimum.\nFor OLS - we actually only find one unique solution!\n\\[\n\\displaylines{\n\\begin{align}\n\\cancel{2}X^{\\intercal}y +  \\cancel{2}((X^{\\intercal}X)\\hat{\\beta}) & = 0 \\\\\n\\therefore (X^{\\intercal}X)\\hat{\\beta} & = X^{\\intercal}y \\\\\n\\therefore \\hat{\\beta} & = (X^{\\intercal}X)^{-1}X^{\\intercal}y\n\\end{align}\n}\n\\]\n\n\nNote the need to invert \\(X^{\\intercal}X\\). This is only possible for a full rank matrix.\nThe first term is the (inverse) variance matrix of \\(X\\). This term normalizes the coefficient with respect to the magnitude of \\(X\\).\nThe second term is the covariance matrix between \\(X\\) and \\(y\\). This incorporates the linear relationship between the two in the coefficient.\nHence, the coefficient can be interpreted as the estimated change in \\(y\\) given a one unit change in \\(X\\).\n\n\n4. Check the stationary point is a global minimum (hessian matrix)\nFinally, we derive the hessian matrix, by double-differentiating the cost function with respect to the coefficients:\n\\[\n\\displaylines{\n\\frac{\\partial^2}{\\partial\\hat{\\beta}\\partial\\hat{\\beta}^{\\intercal}}\n\\left(\n  \\hat{\\epsilon}^{\\intercal}\\hat{\\epsilon}\n\\right)\n\\\\ = \\frac{\\partial}{\\partial \\hat{\\beta}^{\\intercal}} \\left( -2X^{\\intercal}y +  2((X^{\\intercal}X)\\hat{\\beta}) \\right)\n\\\\ = 2(X^{\\intercal}X)^{\\intercal}\n}\n\\]\nSince \\(X^{\\intercal}X\\) is clearly positive definite, the cost function is convex. Thus, we know our unique solution for \\(\\beta\\) where the partial differential is at zero is indeed a global minimum for the cost function."
  },
  {
    "objectID": "posts/ols_coef_derivation.html#final-reflections",
    "href": "posts/ols_coef_derivation.html#final-reflections",
    "title": "Deriving OLS coefficients (matrix algebra)",
    "section": "Final reflections",
    "text": "Final reflections\nUnlike logistic regression, or the multiple hidden-layer structure of neural networks, we can “jump” straight to the optimum coefficients for OLS. Why can we do this? Well chiefly its because OLS is a bit of a special case:\n\nThe minima is a global minima: The hessian matrix is positive definite, and hence the cost function is strictly convex. This means we know that when a choice of coefficients is found that ensure the partially differentiated cost function is equal zero, this minima is also a global one, not a local one.\nThere is only one solution for the optimum coefficient: We assume that the matrix is full rank (every feature provides additional predictive power) and that the number of predictors is smaller than the number of obervations. This also means that partially differentiating is okay to do!\nA closed-form solution can be found The predictions are generated from \\(X\\) using a simple, purely algebraic function, i.e. the sum-product of \\(X\\) by \\(\\beta\\). This means we can find an analytical solution to the optimal choice \\(\\beta^*\\). Note this often isn’t possible since non-linear activation functions (i.e. link functions) are often transcendental.\n\nWe will dive into this in another post.\nFin."
  },
  {
    "objectID": "posts/differentiation.html",
    "href": "posts/differentiation.html",
    "title": "Differentiation formulae",
    "section": "",
    "text": "What we are solving\n\n\n\nDeriving why \\(\\frac{d}{dx} \\left(x^{n} \\right) = nx^{n-1}\\)"
  },
  {
    "objectID": "posts/differentiation.html#formulating-an-approach-to-the-problem",
    "href": "posts/differentiation.html#formulating-an-approach-to-the-problem",
    "title": "Differentiation formulae",
    "section": "Formulating an approach to the problem",
    "text": "Formulating an approach to the problem\nThe gradient between two points, \\(x\\) and \\(a\\), is equal to the change in the y axis divided by the change in the x axis.\nFor example, if \\(y=f(x)=x^n\\), then we can approximate the gradient as follows:\n\\[\n\\frac{\\Delta y}{\\Delta x} =\n\\frac{f(x)-f(a)}{x-a} =\n\\frac{x^n-a^n}{x-a}\n\\]\nThis isn’t the same as saying a line which passes through both of these points has the same gradient though. There is a gap between this linear approximation and the exact curve.\nHowever, the smaller the movement across the x-axis, the closer the approximation is to the actual curve. For example, in Figure 1 below, the line drawn between \\(f(1)\\) and \\(f(2)\\) is closer to the gradient at \\(f(1)\\) of the true curve, compared to the line drawn between \\(f(1)\\) and \\(f(3)\\).\n\n\nCode\nimport plotly.graph_objects as go\nimport numpy as np\n\ndef interpolate(n,x1,x2,num=50):\n  x = np.linspace(x1,x2,num)\n  m = (x2**n - x1**n)/(x2 - x1)\n  c = x1**n - m*x1\n  y = m*x+c\n  return({'x':x,'y':y})\n\nn=3\nactual = {'x': np.arange(0,4,0.1)}\nactual['y'] = actual['x']**n\nlb, ub = 1, [3.5,3,2]\nlin0 = interpolate(n,lb,ub[0])\nlin1 = interpolate(n,lb,ub[1])\nlin2 = interpolate(n,lb,ub[2])\n\nfig = go.Figure(data = go.Scatter(mode='lines'))\nfig.add_trace( go.Scatter( x=actual['x'], y=actual['y'], line=dict(dash='solid'), name=f\"f(x)=x^{n}\" ))\nfig.add_trace( go.Scatter( x=lin0['x'], y=lin0['y'], name=f\"f({lb}) -&gt; f({ub[0]})\", line=dict(dash='dot') ))\nfig.add_trace( go.Scatter( x=lin1['x'], y=lin1['y'], name=f\"f({lb}) -&gt; f({ub[1]})\", line=dict(dash='dot') ))\nfig.add_trace( go.Scatter( x=lin2['x'], y=lin2['y'], name=f\"f({lb}) -&gt; f({ub[2]})\", line=dict(dash='dot') ))\nfig.show()\n\n\n\n\n                                                \n\n\nFigure 1: The smaller the change in x, the closer the linear approximation is to the true gradient of f(x)\n\n\n\n\nConsequently, we want to determine the gradient at the limit i.e. where \\(a \\rightarrow x\\), to get the true gradient."
  },
  {
    "objectID": "posts/differentiation.html#factoring-out-x-a",
    "href": "posts/differentiation.html#factoring-out-x-a",
    "title": "Differentiation formulae",
    "section": "Factoring out \\((x-a)\\)",
    "text": "Factoring out \\((x-a)\\)\nFirst though, let’s factor out the \\(x-a\\) term for simplicity. Let’s derive a generic formula for this:\n\nIf \\(n=2\\), then \\(x^2-a^2 = (x-a)(x+a)\\)\nIf \\(n=3\\), then \\(x^3-a^3 = (x-a)(x^2+xa+a^2)\\)\nIf \\(n=4\\), then \\(x^4-a^4 = (x-a)(x^3+x^2a+xa^2+a^3)\\)\nIf \\(n=5\\), then \\(x^5-a^5 = (x-a)(x^4+x^3a+x^2a^2+xa^3+a^4)\\)\nAnd so on. In fact for any \\(n\\), we can derive \\(x^n-a^n = (x-a)\\sum_{i=1}^n \\left( x^{n-i}a^{i-1} \\right)\\)\n\nAnd we can now sub that into our formula, and the \\(x-a\\) cancels out:\n\\[\n\\frac{x^n-a^n}{x-a} =\n\\frac{x-a}{x-a} \\sum_{i=1}^n \\left( x^{n-i}a^{i-1} \\right) =\n\\sum_{i=1}^n \\left( x^{n-i}a^{i-1} \\right)\n\\]"
  },
  {
    "objectID": "posts/differentiation.html#getting-our-result",
    "href": "posts/differentiation.html#getting-our-result",
    "title": "Differentiation formulae",
    "section": "Getting our result",
    "text": "Getting our result\nAnd now let’s calculate the result in the limit, where \\(x\\) approaches \\(a\\):\n\\[\n\\lim_{x \\rightarrow a} \\sum_{i=1}^n \\left( x^{n-i}a^{i-1} \\right) \\sim\n\\sum_{i=1}^n \\left( a^{n-i}a^{i-1} \\right) =\n\\sum_{i=1}^n \\left( a^{n-1} \\right) =\nna^{n-1}\n\\]\nHence \\(\\frac{d}{dx} \\left(x^{n} \\right) = nx^{n-1}\\).\nFin."
  },
  {
    "objectID": "posts/sandwich_estimators.html",
    "href": "posts/sandwich_estimators.html",
    "title": "Sandwich Estimators",
    "section": "",
    "text": "We have previously derived the variance of the coefficient"
  }
]