[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Sciencing Data",
    "section": "",
    "text": "Deriving the poisson distribution from binomial\n\n\n\n\n\n\nBinomial\n\n\nPoisson\n\n\n\n\n\n\n\n\n\nFeb 28, 2024\n\n\nChris Kelly\n\n\n\n\n\n\n\n\n\n\n\n\nEstimating population variance from a sample\n\n\n\n\n\n\nFundamentals\n\n\nBias\n\n\n\n\n\n\n\n\n\nFeb 26, 2024\n\n\nChris Kelly\n\n\n\n\n\n\n\n\n\n\n\n\nLogistic Regression\n\n\n\n\n\n\nMaximum Likelihood\n\n\nGeneralized Linear Models\n\n\n\n\n\n\n\n\n\nFeb 25, 2024\n\n\nChris Kelly\n\n\n\n\n\n\n\n\n\n\n\n\nShrinkage priors for Lasso and Ridge\n\n\n\n\n\n\nBayes\n\n\nMaximum Likelihood\n\n\nRegularization\n\n\nLaplace\n\n\n\n\n\n\n\n\n\nFeb 24, 2024\n\n\nChris Kelly\n\n\n\n\n\n\n\n\n\n\n\n\nOLS vs MLE with gaussian noise\n\n\n\n\n\n\nLinear Models\n\n\nMaximum Likelihood\n\n\nGauss-Markov\n\n\n\n\n\n\n\n\n\nFeb 23, 2024\n\n\nChris Kelly\n\n\n\n\n\n\n\n\n\n\n\n\nSandwiches: robust covariance error estimation\n\n\n\n\n\n\nLinear Models\n\n\nGauss-Markov\n\n\nStandard errors\n\n\n\n\n\n\n\n\n\nFeb 22, 2024\n\n\nChris Kelly\n\n\n\n\n\n\n\n\n\n\n\n\nBLUE coefficients: bias and efficiency\n\n\n\n\n\n\nLinear Models\n\n\nOLS\n\n\nGauss-Markov\n\n\n\n\n\n\n\n\n\nFeb 21, 2024\n\n\nChris Kelly\n\n\n\n\n\n\n\n\n\n\n\n\nDeriving OLS coefficients (multivariate)\n\n\n\n\n\n\nLinear models\n\n\nOLS\n\n\n\n\n\n\n\n\n\nFeb 20, 2024\n\n\nChris Kelly\n\n\n\n\n\n\n\n\n\n\n\n\nMean Squared Error vs Mean Absolute Error\n\n\n\n\n\n\nCost functions\n\n\n\n\n\n\n\n\n\nFeb 8, 2024\n\n\nChris Kelly\n\n\n\n\n\n\n\n\n\n\n\n\nDifferentiation formulae\n\n\n\n\n\n\nFundamentals\n\n\n\n\n\n\n\n\n\nFeb 8, 2024\n\n\nChris Kelly\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/blue_ols.html",
    "href": "posts/blue_ols.html",
    "title": "BLUE coefficients: bias and efficiency",
    "section": "",
    "text": "What are we exploring?\n\n\n\nProving that the OLS coefficient is the best linear unbiased estimator."
  },
  {
    "objectID": "posts/blue_ols.html#are-the-ols-coefficients-blue",
    "href": "posts/blue_ols.html#are-the-ols-coefficients-blue",
    "title": "BLUE coefficients: bias and efficiency",
    "section": "Are the OLS coefficients “BLUE”?",
    "text": "Are the OLS coefficients “BLUE”?\nWe find a unique solution to the set of coefficients that minimize the sum of squared residuals analytically (see its derivation here):\n\\[\n\\hat{\\beta}^{OLS}=(X^{\\intercal}X)^{-1}X^{\\intercal}y\n\\]\nHowever, how do we know if these coefficients are the best ones we can estimate?\nFor the estimated coefficients to be the Best Linear Unbiased Estimator (i.e. “BLUE”):\n\nThe best estimator has to be unbiased: \\(E[\\hat{\\beta}^*] = \\beta\\)\nAnd among all possible linear, unbiased estimators, it must have the smallest variance: \\(V[\\hat{\\beta}^{*}] &lt; V[\\hat{\\beta}^{Z}]\\)\n\nWe want to ensure our OLS estimate is the best, i.e. that \\(\\hat{\\beta}^{OLS} = \\hat{\\beta}^{*}\\). To achieve this, first we need to confirm it is unbiased. Then given this is true, we can check that the coefficient is most efficient vs all other unbiased estimators.\n\n\n\n\n\n\nGauss Markov Assumptions\n\n\n\nAlong the way, we will outline the Gauss-Markov assumptions utilised that ensure the OLS coefficient is BLUE."
  },
  {
    "objectID": "posts/blue_ols.html#setting-the-scene",
    "href": "posts/blue_ols.html#setting-the-scene",
    "title": "BLUE coefficients: bias and efficiency",
    "section": "Setting the scene",
    "text": "Setting the scene\n\nThe true coefficient and model\n\\(\\beta\\) is the true (unobserved) relationship between all the relevant explanatory features, \\(X\\), and their associated observed outcomes, \\(y\\). In other words, if we observed the entire population of data, it is the relationship we would find.\nConcretely, we assume the outcome is a linear function of all its relevant features. This “true model” perfectly predicts the outcome, except for random noise \\(\\epsilon\\) that influences the observed outcome: \\(y = X\\beta + \\epsilon\\)\n\n\nThe estimated coefficient\n\\(\\hat{\\beta}\\) is our estimated coefficient for the true relationship \\(\\beta\\). In reality, we estimate \\(\\hat{\\beta}\\) from the small, finite sample of size \\(n\\) that is collected, not the whole population. Given any random sample could be collected, we can term the coefficient resulting from the optimum estimation proceedure to be \\(\\hat{\\beta}^*\\). We want to understand if \\(\\hat{\\beta}^{OLS} = \\hat{\\beta}^*\\).\n\n\nThe expected estimated coefficient\n\\(E[\\hat{\\beta}]\\) is the “expected” estimated coefficient. Imagine we repeat the action of estimating the coefficient \\(\\hat{\\beta}\\) many times, each time collecting a new sample (where each observation is sampled i.i.d), and recording the value for the estimated coefficient. \\(E[\\hat{\\beta}]\\) would then be the average of all of those estimated coefficients. If the OLS coefficient is unbiased, then the expected coefficient estimate should be equal to the true one, \\(E[\\hat{\\beta}^{OLS}]=\\beta\\).\n\n\nThe variance of the estimated coefficient\n\\(V[\\hat{\\beta}]\\) is the variance of the estimated coefficient. It determines how much we might expect our estimate \\(\\hat{\\beta}\\) to differ from the true \\(\\beta\\) for any sample drawn. Given the OLS coefficient has been shown to be unbiased, if it is BLUE we expect its variance to be lower than another other unbiased choice \\(\\hat{\\beta}^{Z}\\). Concretely, we want to find \\(V[\\hat{\\beta}^{OLS}] &lt; V[\\hat{\\beta}^{Z}]\\)."
  },
  {
    "objectID": "posts/blue_ols.html#bias",
    "href": "posts/blue_ols.html#bias",
    "title": "BLUE coefficients: bias and efficiency",
    "section": "Bias",
    "text": "Bias\nOften our small finite samples of size \\(n\\) are not a perfect reflection of the population they are drawn from. This “sampling error” means we might estimate a different relationship between \\(X\\) and \\(y\\) than the true relationship of the population, i.e. \\(\\hat{\\beta} \\neq \\beta\\).\nHowever, we should expect our estimated coefficient to be equal to the true value on average. This means we do not want to have a bias towards the estimate being systematically too small or too large, for example. In other words, if we repeated the whole proceedure thousands of times (each time taking new samples, and estimating a coefficient from the new sample) then the average of all the estimated coefficients values should be equal to the true value, i.e. \\(E[\\hat{\\beta}] = \\beta\\).\nRecall that we believe there is a true model that follows the form:\n\\[\ny = X\\beta + \\epsilon\n\\]\n\n\n\n\n\n\nGM1: Linearity \n\n\n\nThe formula above relies on the first Gauss-Markov assumption - that the dependent variable \\(y\\) is assumed to be a linear function of the variables \\(X\\). Note that implies that the proper functional form has been selected (i.e. the relationship is linear) and there are no omitted variables - a huge assumption!\n\n\nIf we substitute this into our estimated coefficient:\n\\[\n\\displaylines{\n\\begin{align}\n\\hat{\\beta}^{OLS} & = (X^{\\intercal}X)^{-1}X^{\\intercal}y\n\\\\ & = (X^{\\intercal}X)^{-1}X^{\\intercal}(X\\beta+\\epsilon)\n\\\\ & = (X^{\\intercal}X)^{-1}X^{\\intercal}X\\beta+(X^{\\intercal}X)^{-1}X^{\\intercal}\\epsilon\n\\\\ & = \\beta+(X^{\\intercal}X)^{-1}X^{\\intercal}\\epsilon\n\\end{align}\n}\n\\]\nwe show that the estimated coefficient \\(\\hat{\\beta}^{OLS}\\) will differ from the true value depending on the random error \\(\\epsilon\\) associated with the particular finite sample collected.\nNow let’s now take the expectation, to determine when the coefficient is unbiased. In other words, what is the “average” coefficient if we took the sample many times: \\[\n\\displaylines{\n\\begin{align}\nE[\\hat{\\beta}^{OLS}]\n& = \\beta +(X^{\\intercal}X)^{-1}X^{\\intercal}E[\\epsilon]\n\\\\ & = \\beta & \\iff E[\\epsilon] & = 0\n\\end{align}\n\\\\\n}\n\\]\nWe find that the coefficient is unbiased as long as the expected error is also zero.\n\n\n\n\n\n\nGM2: Strict Exogeneity \n\n\n\nThe second Gauss-Markov assumption is strict exogeneity, where the expected error is zero for all feature values: \\(E[\\epsilon|X] = 0\\). By definition, the weaker exogeneity statement of \\(E[\\epsilon] = 0\\) is implied by having the expected error conditional being equal to zero."
  },
  {
    "objectID": "posts/blue_ols.html#efficiency",
    "href": "posts/blue_ols.html#efficiency",
    "title": "BLUE coefficients: bias and efficiency",
    "section": "Efficiency",
    "text": "Efficiency\nTo be the “best”, the OLS estimator also needs to be efficient. This means that it has the lowest variance of all unbiased estimators. This section looks to prove this.\n\nCoefficient variance for OLS\nFirst, let’s derive the variance from the coefficients estimated using OLS, termed \\(V[\\hat{\\beta}^{OLS}]\\). As before, we substitute the true model \\(y = X\\beta + \\epsilon\\) into the coefficient estimated through OLS:\n\\[\n\\displaylines{\n\\begin{align}\n\\hat{\\beta}^{OLS}-\\beta\n& = \\beta +  ((X^{\\intercal}X)^{-1}X^{\\intercal}\\epsilon) - \\beta \\\\\n& = (X^{\\intercal}X)^{-1}X^{\\intercal}\\epsilon\n\\\\ \\\\\n\\therefore\n(\\hat{\\beta}^{OLS}-\\beta)(\\hat{\\beta}^{OLS}-\\beta)^{\\intercal}\n& = ((X^{\\intercal}X)^{-1}X^{\\intercal}\\epsilon)((X^{\\intercal}X)^{-1}X^{\\intercal}\\epsilon)^{\\intercal}\n\\\\ & =\n(X^{\\intercal}X)^{-1}X^{\\intercal}\\epsilon\\epsilon^{\\intercal}X(X^{\\intercal}X)^{-1} \\\\ \\\\\n\\therefore\nV(\\hat{\\beta}^{OLS}) & = E[(\\hat{\\beta}^{OLS}-\\beta)(\\hat{\\beta}^{OLS}-\\beta)^{\\intercal}]\n\\\\ & =\n(X^{\\intercal}X)^{-1}X^{\\intercal}E[\\epsilon\\epsilon^{\\intercal}]X(X^{\\intercal}X)^{-1}\n\\end{align}\n}\n\\]\nThis is sometimes called the sandwich estimator - post soon to follow on this!\n\n\nCoefficient variance assuming “spherical errors”\nWe can simplify this further by appling some assumptions to the estimated error variance \\(E[\\epsilon\\epsilon^{\\intercal}]\\):\n\\[\n\\displaylines{\n\\begin{align}\nE[\\epsilon \\epsilon^{\\intercal}] & =  \n\\begin{bmatrix}\nE[\\epsilon_1^2] & \\cdots & E[\\epsilon_1\\epsilon_n] \\\\\n\\vdots & \\ddots & \\vdots \\\\\nE[\\epsilon_n\\epsilon_1] & \\cdots & E[\\epsilon_n^2]\n\\end{bmatrix} \\\\ \\\\\n& = \\begin{bmatrix}\n\\frac{1}{n}\\sum_{i=1}^{n}{\\epsilon_i^2} & \\cdots & 0 \\\\\n\\vdots & \\ddots & \\vdots \\\\\n0 & \\cdots & \\frac{1}{n}\\sum_{i=1}^{n}{\\epsilon_i^2}\n\\end{bmatrix} \\\\ \\\\\n& = \\begin{bmatrix}\n\\hat{\\sigma}^2 & \\cdots & 0 \\\\\n\\vdots & \\ddots & \\vdots \\\\\n0 & \\cdots & \\hat{\\sigma}^2\n\\end{bmatrix} \\\\ \\\\\n& = \\hat{\\sigma}^2I\n\\end{align}\n}\n\\]\nHow can we jump to this result? Well we are making two assumptions:\n\nNo serial correlation: \\(\\rho_{\\epsilon_{i},\\epsilon_{i \\neq j}} = 0\\). No correlation between sample errors means that \\(E[\\epsilon_i \\epsilon_{j \\neq i}] = 0\\), and hence the off-diagonals of the error covariance matrix are zero.\nHomoskedasticity: the assumption of uniform error variance for all samples means that \\(V[\\epsilon_i^2] = V[\\epsilon_{j \\neq i}^2] = \\hat{\\sigma}^2\\). And our best approximation for \\(\\hat{\\sigma}^2\\) is simply taking the average squared error: \\(\\hat{\\sigma}^2 = \\frac{1}{n}\\sum_{i=1}^{n}{\\epsilon_i^2}\\)\n\n\n\n\n\n\n\nGM3: Spherical errors \n\n\n\nThe third Gauss-Markov assumption is spherical errors, \\(E[\\epsilon\\epsilon^{\\intercal}|X] = 0\\). This means that the outer product of the expected errors is a scalar matrix, which implies no serial correlation and homoskedasticity.\n\n\n\n\nIt is especially important to make the right assumptions about \\(E[\\epsilon\\epsilon^{\\intercal}|X]\\) as it impacts where our estimate of the standard errors is correct! We will dive into what happens to SE if we violate these assumptions in another post.\nSince we now see that \\(\\hat{\\sigma}^2\\) is a scalar matrix, we can thus simplfy the variance formula further:\n\\[\n\\displaylines{\n\\begin{align}\nV(\\hat{\\beta}^{OLS}) & =\n(X^{\\intercal}X)^{-1}X^{\\intercal}E[\\epsilon\\epsilon^{\\intercal}]X(X^{\\intercal}X)^{-1}\n\\\\ & =\n(X^{\\intercal}X)^{-1}X^{\\intercal} \\hat{\\sigma}^2I X(X^{\\intercal}X)^{-1}\n\\\\ & =\n\\hat{\\sigma}^2\n\\cancel{(X^{\\intercal}X)^{-1}}\n\\cancel{X^{\\intercal} X}\n(X^{\\intercal}X)^{-1}\n\\\\ & = \\hat{\\sigma}^2(X^{\\intercal}X)^{-1}\n\\end{align}\n}\n\\]\n\n\nFormulating an alternative unbiased coefficient\nNext step - lets formulate another estimator, \\(\\hat{\\beta}^{z}\\), which differs from \\(\\hat{\\beta}^{OLS}\\) by a non-zero matrix \\(A\\). See how they both differ below:\n\\[\n\\displaylines{\n\\begin{align}\n\\hat{\\beta}^{OLS} & =(X^{\\intercal}X)^{-1}X^{\\intercal}y \\\\\n\\hat{\\beta}^{Z} & =\\left((X^{\\intercal}X)^{-1}X^{\\intercal}+A\\right)y\n\\end{align}\n}\n\\]\nNow we need to ensure this new estimator is not biased. So by taking the expectation in the same was as for OLS…\n\\[\n\\displaylines{\n\\begin{align}\nE[\\hat{\\beta}^{Z}] & = E\\left[ \\left((X^{\\intercal}X)^{-1}X^{\\intercal}+A\\right)y \\right]\n\\\\ & = \\left((X^{\\intercal}X)^{-1}X^{\\intercal}+A\\right)(X\\beta+ \\cancel{E\\left[\\epsilon \\right]}) & \\because E[\\epsilon] = 0\n\\\\ & = (X^{\\intercal}X)^{-1}X^{\\intercal}X\\beta+AX\\beta\n\\\\ & = \\beta+AX\\beta\n\\end{align}\n}\n\\]\nSo the estimator is only unbiased iff \\(AX=0\\). This is important to note when comparing the variance between unbiased coefficients - see below!\n\n\nVariance of the alternative unbiased coefficient\nJust like before, we calculate the variance:\n\\[\n\\displaylines{\n\\begin{align}\nV[\\hat{\\beta}^{Z}]\n& = V\\left[ \\left((X^{\\intercal}X)^{-1}X^{\\intercal}+A\\right)y \\right]\n\\\\ & = \\left((X^{\\intercal}X)^{-1}X^{\\intercal}+A\\right) V[y] \\left((X^{\\intercal}X)^{-1}X^{\\intercal}+A\\right)^{\\intercal}\n\\\\ & = \\hat{\\sigma}^2 \\left((X^{\\intercal}X)^{-1}X^{\\intercal}+A\\right) \\left((X^{\\intercal}X)^{-1}X^{\\intercal}+A\\right)^{\\intercal}\n& \\because E[\\epsilon \\epsilon^{\\intercal}|X] = 0\n\\\\ & = \\hat{\\sigma}^2\\left((X^{\\intercal}X)^{-1}X^{\\intercal}+A\\right) \\left(X(X^{\\intercal}X)^{-1}+A^{\\intercal}\\right)\n\\\\ & = \\hat{\\sigma}^2 \\left(\n(X^{\\intercal}X)^{-1}X^{\\intercal} X(X^{\\intercal}X)^{-1} + AX(X^{\\intercal}X)^{-1} + (X^{\\intercal}X)^{-1}X^{\\intercal}A^{\\intercal} + AA^{\\intercal}\n\\right)\n\\\\ & = \\hat{\\sigma}^2\n(X^{\\intercal}X)^{-1} + \\hat{\\sigma}^2AA^{\\intercal}\n& \\because AX = 0\n\\\\ & = V[\\beta^{OLS}] + \\hat{\\sigma}^2AA^{\\intercal}\n\\end{align}\n}\n\\]\nNow since AA is surely a positive semi-definite matrix, then we know that \\(V[\\hat{\\beta}^{Z}] &gt; V[\\hat{\\beta}^{OLS}]\\).\nWe have shown that \\(\\hat{\\beta}^{OLS}\\) has the smallest variance among all unbiased estimators!"
  },
  {
    "objectID": "posts/blue_ols.html#summarising-the-gauss-markov-assumptions",
    "href": "posts/blue_ols.html#summarising-the-gauss-markov-assumptions",
    "title": "BLUE coefficients: bias and efficiency",
    "section": "Summarising the Gauss-markov assumptions",
    "text": "Summarising the Gauss-markov assumptions\nAlong the way, we showed where assumptions were needed to ensure the OLS coefficient estimation is BLUE.\nWe actually missed one out, but it is actually quite trivial to see from the OLS coefficient formula:\n\\[\n\\hat{\\beta}^{OLS}=(X^{\\intercal}X)^{-1}X^{\\intercal}y\n\\]\n\n\n\n\n\n\nGM4: Full rank \n\n\n\nThe matrix \\(X\\) must be of full rank \\(k\\), so that it is possible to invert the matrix \\(X^{\\intercal}X\\). This is equivalent to having no perfect multi-collinearity.\n\n\nWe have now collected our full set of Gauss-Markov assumptions required for the OLS coefficient to be BLUE:\n\nLinearity\nStrict Exogeneity\nSpherical Errors\nFull rank"
  },
  {
    "objectID": "posts/mse_mean_mae_median.html",
    "href": "posts/mse_mean_mae_median.html",
    "title": "Mean Squared Error vs Mean Absolute Error",
    "section": "",
    "text": "What we are solving\n\n\n\nMinimizing MAE tends predictions towards the sample median, whereas for MSE predictions tend towards the sample mean.\n\n\nIn the absence of informative features, an ML algorithm minimizing the sum of squared errors will tend towards predicting the mean of the sample. However, minimizing the sum of absolute errors will tend towards predicting the median of the sample.\nThis post dives into why this is the case.\n\nMinimizing residual sum-of-squares\nLet’s define the residual for sample \\(i\\) as \\(\\epsilon_i\\). We now want to find the prediction \\(\\hat{y}\\) that minimizes the sum of all squared residuals (i.e. where the gradient is zero):\n\\[\n\\displaylines{\n\\begin{align}\n\\min_\\hat{y}{\\left[\\sum_{i=1}^N{\\epsilon_i^2}\\right]}\n\\Rightarrow &\n\\frac{\\partial}{\\partial \\hat{y}} \\sum_{i=1}^N{\\epsilon_i^2} \\\\ = &\n\\frac{\\partial \\left( \\sum_{i=1}^N{\\epsilon_i^2} \\right) }{\\partial\\epsilon}\n\\left( \\frac{\\partial\\epsilon}{\\partial \\hat{y} } \\right) \\\\ = &\n\\sum_{i=1}^N 2\\epsilon_i \\left( \\frac{\\partial\\epsilon_i}{\\partial \\hat{y} }\\right) = 0\n\\end{align}\n}\n\\]\nWe can now substitue in \\(\\epsilon_i = y - \\hat{y}\\):\n\\[\n\\displaylines{\n\\begin{align}\n\\sum_{i=1}^N 2\\epsilon_i \\left( \\frac{\\partial\\epsilon_i}{\\partial \\hat{y} }\\right) &\n= \\sum_{i=1}^N2( y_i- \\hat{y})\\left(\\frac{\\partial( y_i-\\hat{y})}{\\partial \\hat{y}}\\right) \\\\  &\n= \\sum_{i=1}^N2( y_i- \\hat{y} )(-1) \\\\ &\n= \\sum_{i=1}^N2( y_i) - 2ny = 0 \\\\ &\n\\therefore n \\hat{y} = \\sum_{i=1}^N( y_i) \\\\ &\n\\therefore \\hat{y} = \\frac{\\sum_{i=1}^N{y_i}}{n} = \\bar{y}\n\\end{align}\n}\n\\]\nThus we can see that the prediction that minimizes the sum of squared residuals, is simply the mean.\n\n\nMinimize sum of absolute residuals\nWe now do the same think again, but this time look to minimize the sum of all absolute residuals instead.\n\\[\n\\displaylines{\n\\begin{align}\n\\min_\\hat{y}{\\left[\\sum_{i=1}^N{\\mid \\epsilon_i \\mid}\\right]}\n\\Rightarrow &\n\\frac{\\partial}{\\partial \\hat{y}} \\sum_{i=1}^N{\\left(\\epsilon_i^2\\right)^{1/2}} \\\\ = &\n\\frac{\\partial \\sum_{i=1}^N{ \\left(\\epsilon_i^2\\right)^{1/2} } }{\\partial\\epsilon_i^2}\n\\times \\frac{\\partial\\epsilon_i^2}{\\partial \\epsilon_i }\n\\times \\frac{\\partial\\epsilon_i}{\\partial \\hat{y} }  \\\\ = &\n\\frac{1}{2} \\sum_{i=1}^N{ \\left(\\epsilon_i^2\\right)^{-1/2} }\n\\times 2 \\epsilon_i\n\\times \\frac{\\partial\\epsilon_i}{\\partial \\hat{y} }  \\\\ = &\n\\sum_{i=1}^N{ \\left(\\epsilon_i^2\\right)^{-1/2} } \\times \\epsilon_i\n\\times \\frac{\\partial\\epsilon_i}{\\partial \\hat{y} }  \\\\ = &\n\\sum_{i=1}^N \\frac{\\epsilon_i}{\\mid \\epsilon_i \\mid} \\left( \\frac{\\partial\\epsilon_i}{\\partial \\hat{y} }\\right) = 0\n\\end{align}\n}\n\\]\nAnd similarly to before, we can now substitute in \\(\\epsilon_i = y - \\hat{y}\\):\n\\[\n\\displaylines{\n\\begin{align}\n\\sum_{i=1}^N \\frac{\\epsilon_i}{\\mid \\epsilon_i \\mid} \\left( \\frac{\\partial\\epsilon_i}{\\partial \\hat{y} }\\right) &\n= \\sum_{i=1}^N \\frac{ y_i- \\hat{y} }{\\mid y_i- \\hat{y} \\mid}\\left(\\frac{\\partial( y_i-\\hat{y})}{\\partial \\hat{y}}\\right) \\\\  &\n= \\sum_{i=1}^N \\frac{ y_i- \\hat{y} }{\\mid y_i- \\hat{y} \\mid}(-1) = 0\n\\end{align}\n}\n\\]\nNow \\(f(x) = \\frac{ x }{\\mid x \\mid}\\) is an cool transformation, keeping its sign but getting rid of the magnitude of the size of \\(x\\), i.e.:\n\n\\(f(x &lt; 0) = -1\\)\n\\(f(x &gt; 0) = 1\\)\n\nSo to ensure that \\(\\sum f(\\epsilon_i)=0\\), we need to pick a value for \\(\\hat{y}\\) that means half of the errors are \\(&lt;0\\) and half of the errors are \\(&gt;0\\).\nSo that means half the errors must be negative, and half are positive. So \\(\\hat{y}\\) has to be the median value!\nFin."
  },
  {
    "objectID": "posts/ols_coef_derivation.html",
    "href": "posts/ols_coef_derivation.html",
    "title": "Deriving OLS coefficients (matrix algebra)",
    "section": "",
    "text": "What we are exploring\n\n\n\nDeriving a unique, analytical solution to the set of coefficients that minimize the sum of squared residuals."
  },
  {
    "objectID": "posts/ols_coef_derivation.html#summary",
    "href": "posts/ols_coef_derivation.html#summary",
    "title": "Deriving OLS coefficients (matrix algebra)",
    "section": "Summary",
    "text": "Summary\nThe cost function for OLS is the sum of squared residuals, \\(\\hat{\\epsilon}^{\\intercal}\\hat{\\epsilon}\\). In order to fit a good linear model, we want to find optimum values for the estimated vector of coefficients, \\(\\hat{\\beta}^*\\), that minimizes this cost function.\nFirst we do partial differentiation of the cost function with respect to the coefficients. Finding the coefficient values where the partial differential is equal to zero reveals the stationary points of the cost function. For OLS in particular, we can find a unique solution for the choice of coefficients that can be found analytically. The hessian matrix then further proves that this is a global minima."
  },
  {
    "objectID": "posts/ols_coef_derivation.html#deriving-the-optimum-coefficients",
    "href": "posts/ols_coef_derivation.html#deriving-the-optimum-coefficients",
    "title": "Deriving OLS coefficients (matrix algebra)",
    "section": "Deriving the optimum coefficients",
    "text": "Deriving the optimum coefficients\n\n0. Defining the notation\nFor a sample \\(i\\), we observe an outcome \\(y_i\\). \\(y\\) is a vector of all \\(n\\) observed outcomes.\n\\[\n\\underset{n \\times 1} {y} =\n\\begin{bmatrix}\n    y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_{n-1} \\\\ y_n\n\\end{bmatrix}\n\\]\nWe also observe \\(k\\) features for every sample \\(i\\). \\(X\\) is a matrix of these observed features. Note the first column is usually all ones, to include an intercept to optimize (or “bias” term).\n\\[\n\\underset{n \\times k} {X} =\n\\begin{bmatrix}\n  1 & x_{11} & \\cdots & x_{1,k-1} & x_{1,k} \\\\\n  1 & x_{21} & \\cdots & x_{2,k-1} & x_{2,k} \\\\\n  1 & x_{31} & \\cdots & x_{3,k-1} & x_{3,k} \\\\\n  \\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\\n  1 & x_{n-2,1} & \\cdots & x_{n-2,k-1} & x_{n-2,k} \\\\\n  1 & x_{n-1,1} & \\cdots & x_{n-1,k-1} & x_{n-1,k} \\\\\n  1 & x_{n,1} & \\cdots & x_{n,k-1} & x_{n,k}\n\\end{bmatrix}\n\\]\nThe contribution of each feature to the prediction is estimated by the coefficients \\(\\hat{\\beta}\\).\n\\[\n\\underset{k \\times 1} {\\hat{\\beta}} =\n\\begin{bmatrix}\n  \\beta_0 \\\\\n  \\beta_1 \\\\\n  \\vdots \\\\\n  \\beta_{k-1} \\\\\n  \\beta_{k}\n\\end{bmatrix}\n\\]\nWe make predictions, \\(\\hat{y}\\), by calculating the dot product of the features \\(X\\) and the coefficients \\(\\hat{\\beta}\\).\n\\[\n\\hat{y} = X \\hat{\\beta}\n\\]\nwhich is shorthand for this:\n\\[\n\\displaylines{\n\\begin{align}\n\\begin{bmatrix}\n  \\hat{y}_1 \\\\ \\hat{y}_2 \\\\ \\hat{y}_3 \\\\ \\vdots \\\\ \\hat{y}_{n-2} \\\\ \\hat{y}_{n-1} \\\\ \\hat{y}_n\n\\end{bmatrix} & =\n\\begin{bmatrix}\n  1 & x_{11} & \\cdots & x_{1,k-1} & x_{1,k} \\\\\n  1 & x_{21} & \\cdots & x_{2,k-1} & x_{2,k} \\\\\n  1 & x_{31} & \\cdots & x_{3,k-1} & x_{3,k} \\\\\n  \\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\\n  1 & x_{n-2,1} & \\cdots & x_{n-2,k-1} & x_{n-2,k} \\\\\n  1 & x_{n-1,1} & \\cdots & x_{n-1,k-1} & x_{n-1,k} \\\\\n  1 & x_{n,1} & \\cdots & x_{n,k-1} & x_{n,k}\n\\end{bmatrix}\n\\begin{bmatrix}\n  \\hat{\\beta}_0 \\\\\n  \\hat{\\beta}_1 \\\\\n  \\vdots \\\\\n  \\hat{\\beta}_{k-1} \\\\\n  \\hat{\\beta}_{k}\n\\end{bmatrix}\n\\\\ \\\\ & =\n\\begin{bmatrix}\n  \\hat{\\beta}_0 + \\hat{\\beta}_{1}x_{1,1} + \\cdots + \\hat{\\beta}_{k-1}x_{1,k-1} + \\hat{\\beta}_{k}x_{1,k} \\\\\n  \\hat{\\beta}_0 + \\hat{\\beta}_{1}x_{2,1} + \\cdots + \\hat{\\beta}_{k-1}x_{2,k-1} + \\hat{\\beta}_{k}x_{2,k} \\\\\n  \\hat{\\beta}_0 + \\hat{\\beta}_{1}x_{3,1} + \\cdots + \\hat{\\beta}_{k-1}x_{3,k-1} + \\hat{\\beta}_{k}x_{3,k} \\\\\n  \\vdots \\\\\n  \\hat{\\beta}_0 + \\hat{\\beta}_{1}x_{n-2,1} + \\cdots + \\hat{\\beta}_{k-1}x_{2,k-1} + \\hat{\\beta}_{k}x_{n-2,k} \\\\\n  \\hat{\\beta}_0 + \\hat{\\beta}_{1}x_{n-1,1} + \\cdots + \\hat{\\beta}_{k-1}x_{2,k-1} + \\hat{\\beta}_{k}x_{n-1,k} \\\\\n  \\hat{\\beta}_0 + \\hat{\\beta}_{1}x_{n,1} + \\cdots + \\hat{\\beta}_{k-1}x_{2,k-1} + \\hat{\\beta}_{k}x_{n,k} \\\\\n\\end{bmatrix}\n\\end{align}\n}\n\\]\nThe residual is the difference between the true outcome and the model prediction.\n\\[\n\\hat{\\epsilon} = y_i -\\hat{y}_i\n\\]\nwhich is shorthand for this:\n\\[\n\\begin{bmatrix}\n  \\hat{\\epsilon_1} \\\\\n  \\hat{\\epsilon_2} \\\\\n  \\hat{\\epsilon_3} \\\\\n  \\vdots \\\\\n  \\hat{\\epsilon_{n-2}} \\\\\n  \\hat{\\epsilon_{n-1}} \\\\\n  \\hat{\\epsilon_{n}}\n\\end{bmatrix} =\n\\begin{bmatrix}\n    y_1 \\\\ y_2 \\\\ y_3 \\\\ \\vdots \\\\ y_{n-2} \\\\ y_{n-1} \\\\ y_n\n\\end{bmatrix} -\n\\begin{bmatrix}\n  \\hat{y}_1 \\\\ \\hat{y}_2 \\\\ \\hat{y}_3 \\\\ \\vdots \\\\ \\hat{y}_{n-2} \\\\ \\hat{y}_{n-1} \\\\ \\hat{y}_n\n\\end{bmatrix}\n\\]\nOur aim is to find the optimum vector of coefficients, \\(\\hat{\\beta}^*\\), that minimizes the sum of squared residuals:\n\\[\n\\min_{\\beta} \\left( \\epsilon^{\\intercal}\\epsilon \\right)\n\\]\n\n\n1. Expand the sum of squared residuals\nThe first step involves expanding the sum of squared residuals, and substituting in \\(X \\hat{\\beta}\\) for \\(\\hat{y}\\). \\[\n\\displaylines{\n\\begin{align}\n\\sum_{i=1}^n{\\hat{\\epsilon}_i^2} & = \\hat{\\epsilon}^{\\intercal}\\hat{\\epsilon}\n\\\\& =(y-X\\hat{\\beta})^{\\intercal}(y-X\\hat{\\beta})\n\\\\& = y^{\\intercal}y - y^{\\intercal}X\\hat{\\beta}-\n\\hat{\\beta}^{\\intercal} X^{\\intercal}y+\n\\hat{\\beta}^{\\intercal}X^{\\intercal}X\\hat{\\beta}\n\\\\& = y^{\\intercal}y - 2y^{\\intercal}X\\hat{\\beta}\n+\\hat{\\beta}^{\\intercal}X^{\\intercal}X\\hat{\\beta}\n\\end{align}\n}\n\\]\n\n\nNote we can simply add the two middle terms, since are both scalars:\n\\[\n\\displaylines{\ny^{\\intercal}X\\hat{\\beta} =\n\\hat{\\beta}^{\\intercal} X^{\\intercal}y \\\\\n\\because \\underset{1 \\times n}{y^{\\intercal}} \\times\n\\underset{n \\times k}{X} \\times\n\\hat{\\underset{k \\times 1}{\\beta}}\n=\n\\hat{\\underset{1 \\times k}{\\beta}^{\\intercal}} \\times\n\\underset{k \\times n}{X^{\\intercal}} \\times\n\\underset{n \\times 1}{y}\n}\n\\]\n\n\n2. Partially differentiate RSS with respect to beta\nThe second step involves partially differentiating the cost function with respect to its parameters, to understand how it changes as the coefficients vary.\n\\[\n\\displaylines{\n\\begin{align}\n\\frac{\\partial}{\\partial\\hat{\\beta}}\\hat{\\epsilon}^{\\intercal}\\hat{\\epsilon} & \\equiv\n\\begin{bmatrix}\n    \\frac{\\partial}{\\partial\\hat{\\beta}_1}\\hat{\\epsilon}^{\\intercal}\\hat{\\epsilon} \\\\\n    \\frac{\\partial}{\\partial\\hat{\\beta}_2}\\hat{\\epsilon}^{\\intercal}\\hat{\\epsilon} \\\\\n    \\vdots \\\\\n    \\frac{\\partial}{\\partial\\hat{\\beta}_k}\\hat{\\epsilon}^{\\intercal}\\hat{\\epsilon}\n\\end{bmatrix}\n\\\\ & = \\frac{d}{d\\hat{\\beta}}(\ny^{\\intercal}y - 2y^{\\intercal}X\\hat{\\beta}\n+\\hat{\\beta}^{\\intercal}X^{\\intercal}X\\hat{\\beta}) \\\\ & = 0 - 2X^{\\intercal}y +((X^{\\intercal}X)\\hat{\\beta} + (X^{\\intercal}X)^{\\intercal}\\hat{\\beta})\n\\\\ & = -2X^{\\intercal}y +  2((X^{\\intercal}X)\\hat{\\beta})\n\\end{align}\n}\n\\]\n\n\nTwo matrix differentiation rules used here for reference:\n\\[\n\\displaylines{\n\\frac{\\partial}{dx}(Ax) = A^{\\intercal}x \\\\\n\\frac{\\partial}{dx}(xAx) = Ax + A^{\\intercal}x\n}\n\\]\nAnd note \\(X^{\\intercal}X = (X^{\\intercal}X)^{\\intercal}\\) by definition, so we can add the two last terms.\n\n\n3. Find the coefficient values at the stationary point\nNow we find the choices of \\(\\beta\\) where the partial differential is equal to zero. These stationary points for the cost function are either at its maximum or minimum.\nFor OLS - we actually only find one unique solution!\n\\[\n\\displaylines{\n\\begin{align}\n\\cancel{2}X^{\\intercal}y +  \\cancel{2}((X^{\\intercal}X)\\hat{\\beta}) & = 0 \\\\\n\\therefore (X^{\\intercal}X)\\hat{\\beta} & = X^{\\intercal}y \\\\\n\\therefore \\hat{\\beta} & = (X^{\\intercal}X)^{-1}X^{\\intercal}y\n\\end{align}\n}\n\\]\n\n\nNote the need to invert \\(X^{\\intercal}X\\). This is only possible for a full rank matrix.\nThe first term is the (inverse) variance matrix of \\(X\\). This term normalizes the coefficient with respect to the magnitude of \\(X\\).\nThe second term is the covariance matrix between \\(X\\) and \\(y\\). This incorporates the linear relationship between the two in the coefficient.\nHence, the coefficient can be interpreted as the estimated change in \\(y\\) given a one unit change in \\(X\\).\n\n\n4. Check the stationary point is a global minimum (hessian matrix)\nFinally, we derive the hessian matrix, by double-differentiating the cost function with respect to the coefficients:\n\\[\n\\displaylines{\n\\frac{\\partial^2}{\\partial\\hat{\\beta}\\partial\\hat{\\beta}^{\\intercal}}\n\\left(\n  \\hat{\\epsilon}^{\\intercal}\\hat{\\epsilon}\n\\right)\n\\\\ = \\frac{\\partial}{\\partial \\hat{\\beta}^{\\intercal}} \\left( -2X^{\\intercal}y +  2((X^{\\intercal}X)\\hat{\\beta}) \\right)\n\\\\ = 2(X^{\\intercal}X)^{\\intercal}\n}\n\\]\nSince \\(X^{\\intercal}X\\) is clearly positive definite, the cost function is convex. Thus, we know our unique solution for \\(\\beta\\) where the partial differential is at zero is indeed a global minimum for the cost function."
  },
  {
    "objectID": "posts/ols_coef_derivation.html#final-reflections",
    "href": "posts/ols_coef_derivation.html#final-reflections",
    "title": "Deriving OLS coefficients (matrix algebra)",
    "section": "Final reflections",
    "text": "Final reflections\nUnlike logistic regression, or the multiple hidden-layer structure of neural networks, we can “jump” straight to the optimum coefficients for OLS. Why can we do this? Well chiefly its because OLS is a bit of a special case:\n\nThe minima is a global minima: The hessian matrix is positive definite, and hence the cost function is strictly convex. This means we know that when a choice of coefficients is found that ensure the partially differentiated cost function is equal zero, this minima is also a global one, not a local one.\nThere is only one solution for the optimum coefficient: We assume that the matrix is full rank (every feature provides additional predictive power) and that the number of predictors is smaller than the number of obervations. This also means that partially differentiating is okay to do!\nA closed-form solution can be found The predictions are generated from \\(X\\) using a simple, purely algebraic function, i.e. the sum-product of \\(X\\) by \\(\\beta\\). This means we can find an analytical solution to the optimal choice \\(\\beta^*\\). Note this often isn’t possible since non-linear activation functions (i.e. link functions) are often transcendental.\n\nWe will dive into this in another post.\nFin."
  },
  {
    "objectID": "posts/differentiation.html",
    "href": "posts/differentiation.html",
    "title": "Differentiation formulae",
    "section": "",
    "text": "What we are solving\n\n\n\nDeriving why \\(\\frac{d}{dx} \\left(x^{n} \\right) = nx^{n-1}\\)"
  },
  {
    "objectID": "posts/differentiation.html#formulating-an-approach-to-the-problem",
    "href": "posts/differentiation.html#formulating-an-approach-to-the-problem",
    "title": "Differentiation formulae",
    "section": "Formulating an approach to the problem",
    "text": "Formulating an approach to the problem\nThe gradient between two points, \\(x\\) and \\(a\\), is equal to the change in the y axis divided by the change in the x axis.\nFor example, if \\(y=f(x)=x^n\\), then we can approximate the gradient as follows:\n\\[\n\\frac{\\Delta y}{\\Delta x} =\n\\frac{f(x)-f(a)}{x-a} =\n\\frac{x^n-a^n}{x-a}\n\\]\nThis isn’t the same as saying a line which passes through both of these points has the same gradient though. There is a gap between this linear approximation and the exact curve.\nHowever, the smaller the movement across the x-axis, the closer the approximation is to the actual curve. For example, in Figure 1 below, the line drawn between \\(f(1)\\) and \\(f(2)\\) is closer to the gradient at \\(f(1)\\) of the true curve, compared to the line drawn between \\(f(1)\\) and \\(f(3)\\).\n\n\nCode\nimport plotly.graph_objects as go\nimport numpy as np\n\ndef interpolate(n,x1,x2,num=50):\n  x = np.linspace(x1,x2,num)\n  m = (x2**n - x1**n)/(x2 - x1)\n  c = x1**n - m*x1\n  y = m*x+c\n  return({'x':x,'y':y})\n\nn=3\nactual = {'x': np.arange(0,4,0.1)}\nactual['y'] = actual['x']**n\nlb, ub = 1, [3.5,3,2]\nlin0 = interpolate(n,lb,ub[0])\nlin1 = interpolate(n,lb,ub[1])\nlin2 = interpolate(n,lb,ub[2])\n\nfig = go.Figure(data = go.Scatter(mode='lines'))\nfig.add_trace( go.Scatter( x=actual['x'], y=actual['y'], line=dict(dash='solid'), name=f\"f(x)=x^{n}\" ))\nfig.add_trace( go.Scatter( x=lin0['x'], y=lin0['y'], name=f\"f({lb}) -&gt; f({ub[0]})\", line=dict(dash='dot') ))\nfig.add_trace( go.Scatter( x=lin1['x'], y=lin1['y'], name=f\"f({lb}) -&gt; f({ub[1]})\", line=dict(dash='dot') ))\nfig.add_trace( go.Scatter( x=lin2['x'], y=lin2['y'], name=f\"f({lb}) -&gt; f({ub[2]})\", line=dict(dash='dot') ))\nfig.show()\n\n\n\n\n                                                \n\n\nFigure 1: The smaller the change in x, the closer the linear approximation is to the true gradient of f(x)\n\n\n\n\nConsequently, we want to determine the gradient at the limit i.e. where \\(a \\rightarrow x\\), to get the true gradient."
  },
  {
    "objectID": "posts/differentiation.html#factoring-out-x-a",
    "href": "posts/differentiation.html#factoring-out-x-a",
    "title": "Differentiation formulae",
    "section": "Factoring out \\((x-a)\\)",
    "text": "Factoring out \\((x-a)\\)\nFirst though, let’s factor out the \\(x-a\\) term for simplicity. Let’s derive a generic formula for this:\n\nIf \\(n=2\\), then \\(x^2-a^2 = (x-a)(x+a)\\)\nIf \\(n=3\\), then \\(x^3-a^3 = (x-a)(x^2+xa+a^2)\\)\nIf \\(n=4\\), then \\(x^4-a^4 = (x-a)(x^3+x^2a+xa^2+a^3)\\)\nIf \\(n=5\\), then \\(x^5-a^5 = (x-a)(x^4+x^3a+x^2a^2+xa^3+a^4)\\)\nAnd so on. In fact for any \\(n\\), we can derive \\(x^n-a^n = (x-a)\\sum_{i=1}^n \\left( x^{n-i}a^{i-1} \\right)\\)\n\nAnd we can now sub that into our formula, and the \\(x-a\\) cancels out:\n\\[\n\\frac{x^n-a^n}{x-a} =\n\\frac{x-a}{x-a} \\sum_{i=1}^n \\left( x^{n-i}a^{i-1} \\right) =\n\\sum_{i=1}^n \\left( x^{n-i}a^{i-1} \\right)\n\\]"
  },
  {
    "objectID": "posts/differentiation.html#getting-our-result",
    "href": "posts/differentiation.html#getting-our-result",
    "title": "Differentiation formulae",
    "section": "Getting our result",
    "text": "Getting our result\nAnd now let’s calculate the result in the limit, where \\(x\\) approaches \\(a\\):\n\\[\n\\lim_{x \\rightarrow a} \\sum_{i=1}^n \\left( x^{n-i}a^{i-1} \\right) \\sim\n\\sum_{i=1}^n \\left( a^{n-i}a^{i-1} \\right) =\n\\sum_{i=1}^n \\left( a^{n-1} \\right) =\nna^{n-1}\n\\]\nHence \\(\\frac{d}{dx} \\left(x^{n} \\right) = nx^{n-1}\\).\nFin."
  },
  {
    "objectID": "posts/sandwich_estimators.html",
    "href": "posts/sandwich_estimators.html",
    "title": "Sandwiches: robust covariance error estimation",
    "section": "",
    "text": "What are we exploring?\n\n\n\nEstimating the correct coefficient variance under different error assumptions"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I’ve taught myself Data Science over the last few years, and I’ve taken lots of notes to solidify my understanding along the way. I thought it was worthwhile to share them!\nAll notes and code are my own, or references have been added.\nAll images are generated using Microsoft Copilot in Bing."
  },
  {
    "objectID": "posts/sandwich_estimators.html#introducing-sandwiches",
    "href": "posts/sandwich_estimators.html#introducing-sandwiches",
    "title": "Sandwiches: robust covariance error estimation",
    "section": "Introducing sandwiches",
    "text": "Introducing sandwiches\nThe variance for the OLS coefficient estimator is equal to the following:\n\\[\n\\displaylines{\nV(\\hat{\\beta}) =\n(X^{\\intercal}X)^{-1}X^{\\intercal}E[\\epsilon\\epsilon^{\\intercal}]X(X^{\\intercal}X)^{-1}\n}\n\\]\nThis can be though to as a sandwich:\n\nThe “bread” either side: \\((X^{\\intercal}X)^{-1}X^{\\intercal}\\) on the left and its transpose \\(X(X^{\\intercal}X)^{-1}\\) on the right\nThe “meat” in the middle: what we assume for \\(E[\\epsilon\\epsilon^{\\intercal}]\\)\n\nNote that this is the same as the error variance, since \\(V[\\epsilon]=E[\\epsilon\\epsilon^{\\intercal}]-E[\\epsilon]E[\\epsilon^{\\intercal}]\\) and \\(E[\\epsilon] = 0\\)\n\n\nOur coefficient will only be efficient if these assumptions about the expected error are correct! We will explore what happens when the errors are assumed to be homoskedastic, heteroskedastic or clustered."
  },
  {
    "objectID": "posts/sandwich_estimators.html#cucumber-filling-constant-error-variance",
    "href": "posts/sandwich_estimators.html#cucumber-filling-constant-error-variance",
    "title": "Sandwiches: robust covariance error estimation",
    "section": "Cucumber filling: Constant Error Variance 🥒",
    "text": "Cucumber filling: Constant Error Variance 🥒\nUsual OLS is efficient if the true model has “spherical errors”. What does this mean in practice?\n\nErrors are homoskedastic: \\(V(\\epsilon_i)=\\sigma^2\\) for all observations\nErrors are serially uncorrelated: \\(cov(\\epsilon_i,\\epsilon_{j\\neq i})=0\\)\n\nWhat does this look like for \\(E[\\epsilon\\epsilon^{\\intercal}]\\)?\n\nThe diagonal of the matrix is a constant value (scalar), \\(\\sigma^2\\)\nThe off-diagonals are all zero\n\n\\[\n\\hat{\\sigma}^2\\underset{n\\times n}{I} =\n\\begin{bmatrix}\n\\hat{\\sigma}^2 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n0 & \\hat{\\sigma}^2 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n0 & 0 & \\hat{\\sigma}^2 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n0 & 0 & 0 & \\hat{\\sigma}^2 & 0 & 0 & 0 & 0 & 0\\\\\n0 & 0 & 0 & 0 & \\hat{\\sigma}^2 & 0 & 0 & 0 & 0\\\\\n0 & 0 & 0 & 0 & 0 & \\hat{\\sigma}^2 & 0 & 0 & 0\\\\\n0 & 0 & 0 & 0 & 0 & 0 & \\hat{\\sigma}^2 & 0 & 0\\\\\n0 & 0 & 0 & 0 & 0 & 0 & 0 & \\hat{\\sigma}^2 & 0\\\\\n0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & \\hat{\\sigma}^2\n\\end{bmatrix}\n\\]\nA good estimation of the constant error variance \\(\\sigma^2\\) is to apply the standard formula to the residuals (i.e. method of moments):\n\\[\n\\hat{\\sigma^2}=\\frac{1}{n-k}\\sum{\\hat{\\epsilon_i}^2} \\equiv \\frac{\\epsilon^{\\intercal}\\epsilon }{n-k}\n\\]\n\n\n\n\n\n\nThus our sandwich is:\n\n\n\n\\[\n\\underset{k \\times k}{(X^{\\intercal}X)}^{-1}\n\\underset{k \\times n}\n{X^{\\intercal}}\n\\left(\\frac{1}{n-k} \\times \\underset{1 \\times 1}{(\\epsilon^{\\intercal}\\epsilon)}\n\\times\\underset{n \\times n}{I} \\right)\n\\underset{n \\times k}{X}\n\\underset{k \\times k}{(X^{\\intercal}X)}^{-1}\n\\]\n\n\nIn this scenario, the only things that impact the standard error of the coefficient \\(\\beta_k\\) is:\n\nThe variance of all the residuals, \\(\\sigma^2\\)\nThe variance of the feature \\(V(X_k)\\)\n\n\n\nNote that under spherical errors, the sandwich can be simplified: \\[\n\\displaylines{\n\\begin{align}\nV[\\hat{\\beta}]\n& = (X^{\\intercal}X)^{-1}X^{\\intercal}E[\\epsilon\\epsilon^{\\intercal}]X(X^{\\intercal}X)^{-1}\n\\\\ & = (X^{\\intercal}X)^{-1}X^{\\intercal}\\sigma^2IX(X^{\\intercal}X)^{-1}\n\\\\ & = \\sigma^2(X^{\\intercal}X)^{-1}\\cancel{X^{\\intercal}X}\\cancel{(X^{\\intercal}X)^{-1}}\n\\\\ & = \\sigma^2(X^{\\intercal}X)^{-1}\n\\end{align}\n}\n\\]\n\\[\n1\n\\]"
  },
  {
    "objectID": "posts/sandwich_estimators.html#cucumber-filling-constant-error-variance-1",
    "href": "posts/sandwich_estimators.html#cucumber-filling-constant-error-variance-1",
    "title": "Sandwiches: robust covariance error estimation",
    "section": "🥒 Cucumber filling: Constant Error Variance",
    "text": "🥒 Cucumber filling: Constant Error Variance\nAs shown before, usual OLS is efficient if the true model has “spherical errors. What does this mean in practice?\n\nErrors are homoskedastic: \\(V(\\epsilon_i)=\\sigma^2\\) for all observations\nErrors are serially uncorrelated: \\(cov(\\epsilon_i,\\epsilon_{j\\neq i})=0\\)\n\nWhat does this look like for \\(E[\\epsilon\\epsilon^{\\intercal}]\\)?\n\nThe diagonal of the matrix is a constant value (scalar), \\(\\sigma^2\\)\nThe off-diagonals are all zero\n\n\\[\n\\sigma^2\\underset{n\\times n}{I} = \\begin{bmatrix}\n\\sigma^2 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n0 & \\sigma^2 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n0 & 0 & \\sigma^2 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n0 & 0 & 0 & \\sigma^2 & 0 & 0 & 0 & 0 & 0\\\\\n0 & 0 & 0 & 0 & \\sigma^2 & 0 & 0 & 0 & 0\\\\\n0 & 0 & 0 & 0 & 0 & \\sigma^2 & 0 & 0 & 0\\\\\n0 & 0 & 0 & 0 & 0 & 0 & \\sigma^2 & 0 & 0\\\\\n0 & 0 & 0 & 0 & 0 & 0 & 0 & \\sigma^2 & 0\\\\\n0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & \\sigma^2\n\\end{bmatrix}\n\\]\n\nA good estimation of the constant error variance \\(\\sigma^2\\) is the standard formula (i.e. method of moments):\n\n\\[\n\\hat{\\sigma^2}=\\frac{1}{n-k}\\sum{\\hat{\\epsilon_i}^2} \\equiv \\frac{\\epsilon^{\\intercal}\\epsilon }{n-k}\n\\]\n\n💡 Thus our sandwich is: \\(\\underset{k \\times k}{(X^{\\intercal}X)}^{-1}\n\\underset{k \\times n}\n{X^{\\intercal}}\n\\left(\\frac{1}{n-k} \\times \\underset{1 \\times 1}{(\\epsilon^{\\intercal}\\epsilon)}\n\\times\\underset{n \\times n}{I} \\right)\n\\underset{n \\times k}{X}\n\\underset{k \\times k}{(X^{\\intercal}X)}^{-1}\\)\n\nIn this scenario, the only things that impact the standard error of coefficient \\(k\\) is:\n\nThe variance of all the errors, \\(\\sigma^2\\)\nThe variance of the feature \\(V(X_k)\\)\nFinally, note that under homoskedasticity, the sandwich can be simplified:\n\\[\n  V[\\hat{\\beta}] = (X^{\\intercal}X)^{-1}X^{\\intercal}E[\\epsilon\\epsilon^{\\intercal}]X(X^{\\intercal}X)^{-1} \\\\\n  = (X^{\\intercal}X)^{-1}X^{\\intercal}\\sigma^2IX(X^{\\intercal}X)^{-1} \\\\\n  = \\sigma^2(X^{\\intercal}X)^{-1}\\cancel{X^{\\intercal}X}\\cancel{(X^{\\intercal}X)^{-1}}\n  \\]"
  },
  {
    "objectID": "posts/sandwich_estimators.html#cucumber-constant-error-variance",
    "href": "posts/sandwich_estimators.html#cucumber-constant-error-variance",
    "title": "Sandwiches: robust covariance error estimation",
    "section": "Cucumber: Constant Error Variance 🥒",
    "text": "Cucumber: Constant Error Variance 🥒\nUsual OLS is efficient if the true model has “spherical errors”. What does this mean in practice?\n\nErrors are homoskedastic: \\(V(\\epsilon_i)=\\sigma^2\\) for all observations\nErrors are serially uncorrelated: \\(cov(\\epsilon_i,\\epsilon_{j\\neq i})=0\\)\n\nWhat does this look like for \\(E[\\epsilon\\epsilon^{\\intercal}]\\)?\n\nThe diagonal of the matrix is a constant value (scalar), \\(\\sigma^2\\)\nThe off-diagonals are all zero\n\n\\[\n\\hat{\\sigma}^2\\underset{n\\times n}{I} =\n\\begin{bmatrix}\n\\hat{\\sigma}^2 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n0 & \\hat{\\sigma}^2 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n0 & 0 & \\hat{\\sigma}^2 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n0 & 0 & 0 & \\hat{\\sigma}^2 & 0 & 0 & 0 & 0 & 0\\\\\n0 & 0 & 0 & 0 & \\hat{\\sigma}^2 & 0 & 0 & 0 & 0\\\\\n0 & 0 & 0 & 0 & 0 & \\hat{\\sigma}^2 & 0 & 0 & 0\\\\\n0 & 0 & 0 & 0 & 0 & 0 & \\hat{\\sigma}^2 & 0 & 0\\\\\n0 & 0 & 0 & 0 & 0 & 0 & 0 & \\hat{\\sigma}^2 & 0\\\\\n0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & \\hat{\\sigma}^2\n\\end{bmatrix}\n\\]\nA good estimation of the constant error variance \\(\\sigma^2\\) is to apply the standard formula to the residuals (i.e. method of moments):\n\\[\n\\hat{\\sigma^2}=\\frac{1}{n-k}\\sum{\\hat{\\epsilon_i}^2} \\equiv \\frac{\\epsilon^{\\intercal}\\epsilon }{n-k}\n\\]\n\n\n\n\n\n\nThus our “cucumber sandwich” is:\n\n\n\n\\[\n\\underset{k \\times k}{(X^{\\intercal}X)}^{-1}\n\\underset{k \\times n}\n{X^{\\intercal}}\n\\left(\\frac{1}{n-k} \\times \\underset{1 \\times 1}{(\\epsilon^{\\intercal}\\epsilon)}\n\\times\\underset{n \\times n}{I} \\right)\n\\underset{n \\times k}{X}\n\\underset{k \\times k}{(X^{\\intercal}X)}^{-1}\n\\]\n\n\nIn this scenario, the only things that impact the standard error of the coefficient \\(\\beta_k\\) is:\n\nThe variance of all the residuals, \\(\\sigma^2\\)\nThe variance of the feature \\(V(X_k)\\)\n\n\n\nNote that under spherical errors, the sandwich can be simplified: \\[\n\\displaylines{\n\\begin{align}\nV[\\hat{\\beta}]\n& = (X^{\\intercal}X)^{-1}X^{\\intercal}E[\\epsilon\\epsilon^{\\intercal}]X(X^{\\intercal}X)^{-1}\n\\\\ & = (X^{\\intercal}X)^{-1}X^{\\intercal}\\sigma^2IX(X^{\\intercal}X)^{-1}\n\\\\ & = \\sigma^2(X^{\\intercal}X)^{-1}\\cancel{X^{\\intercal}X}\\cancel{(X^{\\intercal}X)^{-1}}\n\\\\ & = \\sigma^2(X^{\\intercal}X)^{-1}\n\\end{align}\n}\n\\]"
  },
  {
    "objectID": "posts/sandwich_estimators.html#ham-heteroskedastic-errors",
    "href": "posts/sandwich_estimators.html#ham-heteroskedastic-errors",
    "title": "Sandwiches: robust covariance error estimation",
    "section": "Ham: Heteroskedastic errors 🍖",
    "text": "Ham: Heteroskedastic errors 🍖\nHeteroskedastic correction is needed if:\n\nErrors vary for every individual: \\(V(\\epsilon_i)=\\sigma_i^2\\) for all observations\nBut they are still independent aka serially uncorrelated: \\(cov(\\epsilon_i,\\epsilon_{j\\neq i})=0\\)\n\nWhat does this look like for \\(E[\\epsilon\\epsilon^{\\intercal}]\\)?\n\nThe diagonal of the matrix is the estimate of variance which is unique for each observation, \\(\\sigma_i^2\\)\nThe off-diagonals are all zero\n\\[\n  \\underset{n \\times n}{\\sigma^2} = \\begin{bmatrix}\n  \\sigma_1^2 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n  0 & \\sigma_2^2 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n  0 & 0 & \\sigma_3^2 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n  0 & 0 & 0 & \\sigma_4^2 & 0 & 0 & 0 & 0 & 0\\\\\n  0 & 0 & 0 & 0 & \\sigma_5^2 & 0 & 0 & 0 & 0\\\\\n  0 & 0 & 0 & 0 & 0 & \\sigma_6^2 & 0 & 0 & 0\\\\\n  0 & 0 & 0 & 0 & 0 & 0 & \\sigma_7^2 & 0 & 0\\\\\n  0 & 0 & 0 & 0 & 0 & 0 & 0 & \\sigma_8^2 & 0\\\\\n  0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & \\sigma_{..}^2\\\\\n  \\end{bmatrix}\n  \\]\nA good estimation of the vector of heteroskedastic error variances \\(\\sigma^2\\) is again to apply the standard formula to the residuals for each individual:\n\n\\[\n\\hat{\\sigma_i^2}=\\frac{1}{n-k}\\sum{\\hat{\\epsilon_i}^2}\n\\]\n\n\n\n\n\n\nThus our “ham sandwich” is:\n\n\n\n\\[\n\\underset{k \\times k}{(X^{\\intercal}X)}^{-1}\n\\underset{k \\times n}{X^{\\intercal}}\n\\left(\\frac{1}{n-k} \\times \\underset{1 \\times n}{(\\epsilon\\odot \\epsilon)} ^{\\intercal}\n\\times\\underset{n \\times n}{I} \\right)\n\\underset{n \\times k}{X}\n\\underset{k \\times k}{(X^{\\intercal}X)}^{-1}\n\\]\n\n\nSimilar to homoskedastic errors, the things that impacts the standard error of coefficient \\(k\\) is:\n\nThe variance of all the individual errors, \\(\\sigma_i^2\\)\nThe variance of the feature \\(V(X_k)\\)"
  },
  {
    "objectID": "posts/sandwich_estimators.html#salmon-spherical-errors",
    "href": "posts/sandwich_estimators.html#salmon-spherical-errors",
    "title": "Sandwiches: robust covariance error estimation",
    "section": "Salmon: Spherical Errors 🐟",
    "text": "Salmon: Spherical Errors 🐟\nUsual OLS is efficient if the true model has “spherical errors”. What does this mean in practice?\n\nErrors are homoskedastic: \\(V(\\epsilon_i)=\\sigma^2\\) for all observations\nErrors are serially uncorrelated: \\(cov(\\epsilon_i,\\epsilon_{j\\neq i})=0\\)\n\nWhat does this look like for \\(E[\\epsilon\\epsilon^{\\intercal}]\\)?\n\nThe diagonal of the matrix is a constant value (scalar), \\(\\sigma^2\\)\nThe off-diagonals are all zero\n\n\\[\n\\hat{\\sigma}^2\\underset{n\\times n}{I} =\n\\begin{bmatrix}\n\\hat{\\sigma}^2 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n0 & \\hat{\\sigma}^2 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n0 & 0 & \\hat{\\sigma}^2 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n0 & 0 & 0 & \\hat{\\sigma}^2 & 0 & 0 & 0 & 0 & 0\\\\\n0 & 0 & 0 & 0 & \\hat{\\sigma}^2 & 0 & 0 & 0 & 0\\\\\n0 & 0 & 0 & 0 & 0 & \\hat{\\sigma}^2 & 0 & 0 & 0\\\\\n0 & 0 & 0 & 0 & 0 & 0 & \\hat{\\sigma}^2 & 0 & 0\\\\\n0 & 0 & 0 & 0 & 0 & 0 & 0 & \\hat{\\sigma}^2 & 0\\\\\n0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & \\hat{\\sigma}^2\n\\end{bmatrix}\n\\]\nA good estimation of the constant error variance \\(\\sigma^2\\) is to apply the standard formula to the residuals (i.e. method of moments):\n\\[\n\\hat{\\sigma^2}=\\frac{1}{n-k}\\sum{\\hat{\\epsilon_i}^2} \\equiv \\frac{\\epsilon^{\\intercal}\\epsilon }{n-k}\n\\]\n\n\n\n\n\n\nThus our “salmon sandwich” is:\n\n\n\n\\[\n\\underset{k \\times k}{(X^{\\intercal}X)}^{-1}\n\\underset{k \\times n}\n{X^{\\intercal}}\n\\left(\\frac{1}{n-k} \\times \\underset{1 \\times 1}{(\\epsilon^{\\intercal}\\epsilon)}\n\\times\\underset{n \\times n}{I} \\right)\n\\underset{n \\times k}{X}\n\\underset{k \\times k}{(X^{\\intercal}X)}^{-1}\n\\]\n\n\nIn this scenario, the only things that impact the standard error of the coefficient \\(\\beta_k\\) is:\n\nThe variance of all the residuals, \\(\\sigma^2\\)\nThe variance of the feature \\(V(X_k)\\)\n\n\n\nNote that under spherical errors, the sandwich can be simplified: \\[\n\\displaylines{\n\\begin{align}\nV[\\hat{\\beta}]\n& = (X^{\\intercal}X)^{-1}X^{\\intercal}E[\\epsilon\\epsilon^{\\intercal}]X(X^{\\intercal}X)^{-1}\n\\\\ & = (X^{\\intercal}X)^{-1}X^{\\intercal}\\sigma^2IX(X^{\\intercal}X)^{-1}\n\\\\ & = \\sigma^2(X^{\\intercal}X)^{-1}\\cancel{X^{\\intercal}X}\\cancel{(X^{\\intercal}X)^{-1}}\n\\\\ & = \\sigma^2(X^{\\intercal}X)^{-1}\n\\end{align}\n}\n\\]"
  },
  {
    "objectID": "posts/sandwich_estimators.html#cheese-clustered-errors",
    "href": "posts/sandwich_estimators.html#cheese-clustered-errors",
    "title": "Sandwiches: robust covariance error estimation",
    "section": "Cheese: Clustered Errors 🧀",
    "text": "Cheese: Clustered Errors 🧀\nCluster-robust errors are correct if:\n\nErrors vary for every individual: \\(V(\\epsilon_i)=\\sigma_i^2\\) for all observations i.e. still heteroskedastic\nAND errors within the same cluster \\(C_l\\) are serially correlated: \\(cov(\\epsilon_i,\\epsilon_{j}) \\neq 0 \\text{ if } \\epsilon_i,\\epsilon_j \\in C_l\\)\n\nNote - errors between clusters are assumed not to be serially correlated though i.e. \\(cov(\\epsilon_i,\\epsilon_{j}) =0 \\text{ if } \\epsilon_i \\in C_l,\\epsilon_j \\in C_{m \\neq l}\\)\n\n\nWhat does this look like for \\(\\epsilon\\epsilon^{\\intercal}\\)?\n\nThe diagonal of the matrix is the estimate of variance which is unique for each observation, \\(\\sigma_i^2\\)\nThe off-diagonals are also populated with the covariance - but only when they are both in the same cluster\n\nTake an example where observations 1, 2 and 3 are in cluster A, observations 4 and 5 are in cluster B, observations 6, 7 and 8 are in cluster C etc.\n\\[\n\\underset{n \\times n}{\\epsilon\\epsilon^{\\intercal}} \\sim \\begin{bmatrix}\n\\epsilon_1^2 & \\epsilon_1\\epsilon_2 & \\epsilon_1\\epsilon_3 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n\\epsilon_2\\epsilon_1 & \\epsilon_2^2 & \\epsilon_2\\epsilon_3 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n\\epsilon_3\\epsilon_1 & \\epsilon_3\\epsilon_2 & \\epsilon_3^2 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n0 & 0 & 0 & \\epsilon_4^2 & \\epsilon_4\\epsilon_5 & 0 & 0 & 0 & 0\\\\\n0 & 0 & 0 & \\epsilon_5\\epsilon_4 & \\epsilon_5^2 & 0 & 0 & 0 & 0\\\\\n0 & 0 & 0 & 0 & 0 & \\epsilon_6^2 & \\epsilon_6\\epsilon_7 & \\epsilon_6\\epsilon_8 & 0\\\\\n0 & 0 & 0 & 0 & 0 & \\epsilon_7\\epsilon_6 & \\epsilon_7^2 & \\epsilon_7\\epsilon_8 & 0\\\\\n0 & 0 & 0 & 0 & 0 & \\epsilon_8\\epsilon_6 & \\epsilon_8\\epsilon_7 & \\epsilon_8^2 & 0\\\\\n0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & \\epsilon_{..}^2\\\\\n\\end{bmatrix}\n\\]\nAdditionally, we have to do a finite-sample correction as well on the degrees of freedom, based on the number of clusters \\(n_c\\)\n\n💡 Thus our sandwich is: \\(\\underset{k \\times k}{(X^{\\intercal}X)}^{-1}\n\\underset{k \\times n}{X^{\\intercal}}\n\\left(\\frac{n-1}{n-k}\\frac{n_c}{n_c-1} \\times\n\\underset{n \\times n}{(\\epsilon \\epsilon^{\\intercal})}\n\\right)\n\\underset{n \\times k}{X}\n\\underset{k \\times k}{(X^{\\intercal}X)}^{-1}\\)\n\n\nIn this scenario, there are a few additional things that impact the standard error of coefficient \\(k\\):\n\nIf errors are correlated within clusters, this will increase the error.\nIf features are correlated within clusters, this will also increase the error (due to the off-diagonals in the error variance matrix)\nAnd if both the errors and feature correlations are the same sign, this will also increase the standard error.\n\nNote the sandwich formula above is equivalent to calculating mini sandwiches per cluster before summing. This is preferable to creating an enormous matrix from the error outer product and zeroing out covariances that aren’t the same cluster\ndef cluster_XeeX(cluster_index):\n    j = X[:,'cluster_id']== cluster_id # j is the indexes of the cluster\n    ec = e[j] # ec is a vector of errors for that cluster\n    sigma_sqc = ec.dot(ec.transpose()) # outerproduct of clustered errors\n    Xc = X[j,:] # Xc is a matrix of features for that cluster\n    XeeXc = Xc.transpose().dot(sigma_sqc).dot(Xc) # X'ee'X for that cluster \n        return(XeeXc) # shape is kxk\n\nXeeXc = np.sum([cluster_XeeX(i) for i in np.unique(cl)], axis = 0)\nn_cl = len(np.unique(cl)) # number of clusters\n\n       # finite-sample correction factor.    # sum XeeX across all clusters\nXeeX = ((n-1) / (n-k)) * (n_cl / (n_cl-1)) * np.sum(XeeXc, axis = 0)\nsandwich = var_X_inv.dot(XeeX).var_X_inv\n\n\n💡 Thus our sandwich can also be written: \\(\\underset{k \\times k}{(X^{\\intercal}X)^{-1}}\n\\left(\n\\frac{n-1}{n-k}\\frac{n_c}{n_c-1} \\times\n\\sum_{c=1}^{n_c}{\\underset{k \\times k}{(X_c^{\\intercal} \\epsilon_c \\epsilon_c^{\\intercal}X_c)}}\n\\right)\n\\underset{k \\times k}\n{(X^{\\intercal}X)^{-1}}\\)\n\nOne word of caution about averaging of clusters (requires a large number of clusters)"
  },
  {
    "objectID": "posts/sandwich_estimators.html#salmon-bagel-spherical-errors",
    "href": "posts/sandwich_estimators.html#salmon-bagel-spherical-errors",
    "title": "Sandwiches: robust covariance error estimation",
    "section": "Salmon bagel: Spherical Errors 🐟",
    "text": "Salmon bagel: Spherical Errors 🐟\nUsual OLS is efficient if the true model has “spherical errors”. What does this mean in practice?\n\nErrors are homoskedastic: \\(V(\\epsilon_i)=\\sigma^2\\) for all observations\nErrors are serially uncorrelated: \\(cov(\\epsilon_i,\\epsilon_{j\\neq i})=0\\)\n\nWhat does this look like for \\(E[\\epsilon\\epsilon^{\\intercal}]\\)?\n\nThe diagonal of the matrix is a constant value (scalar), \\(\\sigma^2\\)\nThe off-diagonals are all zero\n\n\\[\n\\hat{\\sigma}^2\\underset{n\\times n}{I} =\n\\begin{bmatrix}\n\\hat{\\sigma}^2 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n0 & \\hat{\\sigma}^2 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n0 & 0 & \\hat{\\sigma}^2 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n0 & 0 & 0 & \\hat{\\sigma}^2 & 0 & 0 & 0 & 0 & 0\\\\\n0 & 0 & 0 & 0 & \\hat{\\sigma}^2 & 0 & 0 & 0 & 0\\\\\n0 & 0 & 0 & 0 & 0 & \\hat{\\sigma}^2 & 0 & 0 & 0\\\\\n0 & 0 & 0 & 0 & 0 & 0 & \\hat{\\sigma}^2 & 0 & 0\\\\\n0 & 0 & 0 & 0 & 0 & 0 & 0 & \\hat{\\sigma}^2 & 0\\\\\n0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & \\hat{\\sigma}^2\n\\end{bmatrix}\n\\]\nA good estimation of the constant error variance \\(\\sigma^2\\) is to apply the standard formula to the residuals (i.e. method of moments):\n\\[\n\\hat{\\sigma^2}=\\frac{1}{n-k}\\sum{\\hat{\\epsilon_i}^2} \\equiv \\frac{\\epsilon^{\\intercal}\\epsilon }{n-k}\n\\]\n\n\n\n\n\n\nThus our “salmon sandwich” is:\n\n\n\n\\[\n\\underset{k \\times k}{(X^{\\intercal}X)}^{-1}\n\\underset{k \\times n}\n{X^{\\intercal}}\n\\left(\\frac{1}{n-k} \\times \\underset{1 \\times 1}{(\\epsilon^{\\intercal}\\epsilon)}\n\\times\\underset{n \\times n}{I} \\right)\n\\underset{n \\times k}{X}\n\\underset{k \\times k}{(X^{\\intercal}X)}^{-1}\n\\]\n\n\n\n\nNote that under spherical errors, the sandwich can be simplified: \\[\n\\displaylines{\n\\begin{align}\nV[\\hat{\\beta}]\n& = (X^{\\intercal}X)^{-1}X^{\\intercal}E[\\epsilon\\epsilon^{\\intercal}]X(X^{\\intercal}X)^{-1}\n\\\\ & = (X^{\\intercal}X)^{-1}X^{\\intercal}\\sigma^2IX(X^{\\intercal}X)^{-1}\n\\\\ & = \\sigma^2(X^{\\intercal}X)^{-1}\\cancel{X^{\\intercal}X}\\cancel{(X^{\\intercal}X)^{-1}}\n\\\\ & = \\sigma^2(X^{\\intercal}X)^{-1}\n\\end{align}\n}\n\\]\nIn this scenario, the only things that impact the standard error of the coefficient \\(\\beta_k\\) is:\n\nThe variance of all the residuals, \\(\\sigma^2\\)\nThe variance of the feature \\(V(X_k)\\)"
  },
  {
    "objectID": "posts/sandwich_estimators.html#ham-sarnie-heteroskedastic-errors",
    "href": "posts/sandwich_estimators.html#ham-sarnie-heteroskedastic-errors",
    "title": "Sandwiches: robust covariance error estimation",
    "section": "Ham sarnie: Heteroskedastic errors 🍖",
    "text": "Ham sarnie: Heteroskedastic errors 🍖\nHeteroskedastic correction is needed if:\n\nErrors vary for every individual: \\(V(\\epsilon_i)=\\sigma_i^2\\) for all observations\nBut they are still independent aka serially uncorrelated: \\(cov(\\epsilon_i,\\epsilon_{j\\neq i})=0\\)\n\nWhat does this look like for \\(E[\\epsilon\\epsilon^{\\intercal}]\\)?\n\nThe diagonal of the matrix is the estimate of variance which is unique for each observation, \\(\\sigma_i^2\\)\nThe off-diagonals are all zero\n\\[\n  \\underset{n \\times n}{\\sigma^2} = \\begin{bmatrix}\n  \\sigma_1^2 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n  0 & \\sigma_2^2 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n  0 & 0 & \\sigma_3^2 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n  0 & 0 & 0 & \\sigma_4^2 & 0 & 0 & 0 & 0 & 0\\\\\n  0 & 0 & 0 & 0 & \\sigma_5^2 & 0 & 0 & 0 & 0\\\\\n  0 & 0 & 0 & 0 & 0 & \\sigma_6^2 & 0 & 0 & 0\\\\\n  0 & 0 & 0 & 0 & 0 & 0 & \\sigma_7^2 & 0 & 0\\\\\n  0 & 0 & 0 & 0 & 0 & 0 & 0 & \\sigma_8^2 & 0\\\\\n  0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & \\sigma_{..}^2\\\\\n  \\end{bmatrix}\n  \\]\nA good estimation of the vector of heteroskedastic error variances \\(\\sigma^2\\) is again to apply the standard formula to the residuals for each individual:\n\n\\[\n\\hat{\\sigma_i^2}=\\frac{1}{n-k}\\sum{\\hat{\\epsilon_i}^2}\n\\]\n\n\n\n\n\n\nThus our “ham sandwich” is:\n\n\n\n\\[\n\\underset{k \\times k}{(X^{\\intercal}X)}^{-1}\n\\underset{k \\times n}{X^{\\intercal}}\n\\left(\\frac{1}{n-k} \\times \\underset{1 \\times n}{(\\epsilon\\odot \\epsilon)} ^{\\intercal}\n\\times\\underset{n \\times n}{I} \\right)\n\\underset{n \\times k}{X}\n\\underset{k \\times k}{(X^{\\intercal}X)}^{-1}\n\\]\n\n\nSimilar to homoskedastic errors, the things that impacts the standard error of coefficient \\(k\\) is:\n\nThe variance of all the individual errors, \\(\\sigma_i^2\\)\nThe variance of the feature \\(V(X_k)\\)"
  },
  {
    "objectID": "posts/sandwich_estimators.html#cheese-baguette-clustered-errors",
    "href": "posts/sandwich_estimators.html#cheese-baguette-clustered-errors",
    "title": "Sandwiches: robust covariance error estimation",
    "section": "Cheese baguette: Clustered Errors 🧀",
    "text": "Cheese baguette: Clustered Errors 🧀\nCluster-robust errors are correct if:\n\nErrors vary for every individual: \\(V(\\epsilon_i)=\\sigma_i^2\\) for all observations i.e. still heteroskedastic\nAND errors within the same cluster \\(C_l\\) are serially correlated: \\(cov(\\epsilon_i,\\epsilon_{j}) \\neq 0 \\text{ if } \\epsilon_i,\\epsilon_j \\in C_l\\)\n\nNote - errors between clusters are assumed not to be serially correlated though i.e. \\(cov(\\epsilon_i,\\epsilon_{j}) =0 \\text{ if } \\epsilon_i \\in C_l,\\epsilon_j \\in C_{m \\neq l}\\)\n\n\nWhat does this look like for \\(\\epsilon\\epsilon^{\\intercal}\\)?\n\nThe diagonal of the matrix is the estimate of variance which is unique for each observation, \\(\\sigma_i^2\\)\nThe off-diagonals are also populated with the covariance - but only when they are both in the same cluster\n\nTake an example where observations 1, 2 and 3 are in cluster A, observations 4 and 5 are in cluster B, observations 6, 7 and 8 are in cluster C etc.\n\\[\n\\underset{n \\times n}{\\epsilon\\epsilon^{\\intercal}} \\sim \\begin{bmatrix}\n\\epsilon_1^2 & \\epsilon_1\\epsilon_2 & \\epsilon_1\\epsilon_3 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n\\epsilon_2\\epsilon_1 & \\epsilon_2^2 & \\epsilon_2\\epsilon_3 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n\\epsilon_3\\epsilon_1 & \\epsilon_3\\epsilon_2 & \\epsilon_3^2 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n0 & 0 & 0 & \\epsilon_4^2 & \\epsilon_4\\epsilon_5 & 0 & 0 & 0 & 0\\\\\n0 & 0 & 0 & \\epsilon_5\\epsilon_4 & \\epsilon_5^2 & 0 & 0 & 0 & 0\\\\\n0 & 0 & 0 & 0 & 0 & \\epsilon_6^2 & \\epsilon_6\\epsilon_7 & \\epsilon_6\\epsilon_8 & 0\\\\\n0 & 0 & 0 & 0 & 0 & \\epsilon_7\\epsilon_6 & \\epsilon_7^2 & \\epsilon_7\\epsilon_8 & 0\\\\\n0 & 0 & 0 & 0 & 0 & \\epsilon_8\\epsilon_6 & \\epsilon_8\\epsilon_7 & \\epsilon_8^2 & 0\\\\\n0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & \\epsilon_{..}^2\\\\\n\\end{bmatrix}\n\\]\nAdditionally, we have to do a finite-sample correction as well on the degrees of freedom, based on the number of clusters \\(n_c\\)\n\n💡 Thus our sandwich is: \\(\\underset{k \\times k}{(X^{\\intercal}X)}^{-1}\n\\underset{k \\times n}{X^{\\intercal}}\n\\left(\\frac{n-1}{n-k}\\frac{n_c}{n_c-1} \\times\n\\underset{n \\times n}{(\\epsilon \\epsilon^{\\intercal})}\n\\right)\n\\underset{n \\times k}{X}\n\\underset{k \\times k}{(X^{\\intercal}X)}^{-1}\\)\n\n\nIn this scenario, there are a few additional things that impact the standard error of coefficient \\(k\\):\n\nIf errors are correlated within clusters, this will increase the error.\nIf features are correlated within clusters, this will also increase the error (due to the off-diagonals in the error variance matrix)\nAnd if both the errors and feature correlations are the same sign, this will also increase the standard error.\n\nNote the sandwich formula above is equivalent to calculating mini sandwiches per cluster before summing. This is preferable to creating an enormous matrix from the error outer product and zeroing out covariances that aren’t the same cluster\ndef cluster_XeeX(cluster_index):\n    j = X[:,'cluster_id']== cluster_id # j is the indexes of the cluster\n    ec = e[j] # ec is a vector of errors for that cluster\n    sigma_sqc = ec.dot(ec.transpose()) # outerproduct of clustered errors\n    Xc = X[j,:] # Xc is a matrix of features for that cluster\n    XeeXc = Xc.transpose().dot(sigma_sqc).dot(Xc) # X'ee'X for that cluster \n        return(XeeXc) # shape is kxk\n\nXeeXc = np.sum([cluster_XeeX(i) for i in np.unique(cl)], axis = 0)\nn_cl = len(np.unique(cl)) # number of clusters\n\n       # finite-sample correction factor.    # sum XeeX across all clusters\nXeeX = ((n-1) / (n-k)) * (n_cl / (n_cl-1)) * np.sum(XeeXc, axis = 0)\nsandwich = var_X_inv.dot(XeeX).var_X_inv\n\n\n💡 Thus our sandwich can also be written: \\(\\underset{k \\times k}{(X^{\\intercal}X)^{-1}}\n\\left(\n\\frac{n-1}{n-k}\\frac{n_c}{n_c-1} \\times\n\\sum_{c=1}^{n_c}{\\underset{k \\times k}{(X_c^{\\intercal} \\epsilon_c \\epsilon_c^{\\intercal}X_c)}}\n\\right)\n\\underset{k \\times k}\n{(X^{\\intercal}X)^{-1}}\\)\n\nOne word of caution about averaging of clusters (requires a large number of clusters)"
  },
  {
    "objectID": "posts/sandwich_estimators.html#cheese-roll-clustered-errors",
    "href": "posts/sandwich_estimators.html#cheese-roll-clustered-errors",
    "title": "Sandwiches: robust covariance error estimation",
    "section": "Cheese roll: Clustered Errors 🧀",
    "text": "Cheese roll: Clustered Errors 🧀\nCluster-robust errors are needed if:\n\nErrors vary for every individual: \\(V(\\epsilon_i)=\\sigma_i^2\\) for all observations i.e. still heteroskedastic\nAND errors within the same cluster \\(C_l\\) are serially correlated: \\(cov(\\epsilon_i,\\epsilon_{j}) \\neq 0 \\text{ if } \\epsilon_i,\\epsilon_j \\in C_l\\)\n\nNote - errors between clusters are assumed not to be serially correlated though i.e. \\(cov(\\epsilon_i,\\epsilon_{j}) =0 \\text{ if } \\epsilon_i \\in C_l,\\epsilon_j \\in C_{m \\neq l}\\)\n\n\nWhat does this look like for \\(E[\\epsilon\\epsilon^{\\intercal}]\\)?\n\nThe diagonal of the matrix is the estimate of variance which is unique for each observation, \\(\\sigma_i^2\\)\nThe off-diagonals are also populated with the covariance - but only when they are both in the same cluster\n\nHere is an example where observations 1, 2 and 3 are in cluster A, observations 4 and 5 are in cluster B, observations 6, 7 and 8 are in cluster C etc.\n\\[\n\\underset{n \\times n}{\\epsilon\\epsilon^{\\intercal}} \\sim \\begin{bmatrix}\n\\epsilon_1^2 & \\epsilon_1\\epsilon_2 & \\epsilon_1\\epsilon_3 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n\\epsilon_2\\epsilon_1 & \\epsilon_2^2 & \\epsilon_2\\epsilon_3 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n\\epsilon_3\\epsilon_1 & \\epsilon_3\\epsilon_2 & \\epsilon_3^2 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n0 & 0 & 0 & \\epsilon_4^2 & \\epsilon_4\\epsilon_5 & 0 & 0 & 0 & 0\\\\\n0 & 0 & 0 & \\epsilon_5\\epsilon_4 & \\epsilon_5^2 & 0 & 0 & 0 & 0\\\\\n0 & 0 & 0 & 0 & 0 & \\epsilon_6^2 & \\epsilon_6\\epsilon_7 & \\epsilon_6\\epsilon_8 & 0\\\\\n0 & 0 & 0 & 0 & 0 & \\epsilon_7\\epsilon_6 & \\epsilon_7^2 & \\epsilon_7\\epsilon_8 & 0\\\\\n0 & 0 & 0 & 0 & 0 & \\epsilon_8\\epsilon_6 & \\epsilon_8\\epsilon_7 & \\epsilon_8^2 & 0\\\\\n0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & \\epsilon_{..}^2\\\\\n\\end{bmatrix}\n\\]\nAdditionally, we have to do a finite-sample correction as well on the degrees of freedom, based on the number of clusters \\(n_c\\)\n\n\n\n\n\n\nThus our cheese sandwich is:\n\n\n\n\\(\\underset{k \\times k}{(X^{\\intercal}X)}^{-1}\n\\underset{k \\times n}{X^{\\intercal}}\n\\left(\\frac{n-1}{n-k}\\frac{n_c}{n_c-1} \\times\n\\underset{n \\times n}{(\\epsilon \\epsilon^{\\intercal})}\n\\right)\n\\underset{n \\times k}{X}\n\\underset{k \\times k}{(X^{\\intercal}X)}^{-1}\\)\n\n\nIn this scenario, there are a few additional things that impact the standard error of coefficient \\(k\\):\n\nIf errors are correlated within clusters, this will increase the error.\nIf features are correlated within clusters, this will also increase the error (due to the off-diagonals in the error variance matrix)\nAnd if both the errors and feature correlations are the same sign, this will also increase the standard error.\nAs well as the variance of the individual errors, \\(\\sigma_i^2\\), and the variance of each feature \\(V(X_k)\\), as before"
  },
  {
    "objectID": "posts/ols_coefs_multivariate.html",
    "href": "posts/ols_coefs_multivariate.html",
    "title": "Deriving OLS coefficients (multivariate)",
    "section": "",
    "text": "What we are exploring\n\n\n\nDeriving a unique, analytical solution to the set of coefficients that minimize the sum of squared residuals."
  },
  {
    "objectID": "posts/ols_coefs_multivariate.html#summary",
    "href": "posts/ols_coefs_multivariate.html#summary",
    "title": "Deriving OLS coefficients (multivariate)",
    "section": "Summary",
    "text": "Summary\nThe cost function for OLS is the sum of squared residuals, \\(\\hat{\\epsilon}^{\\intercal}\\hat{\\epsilon}\\). In order to fit a good linear model, we want to find optimum values for the estimated vector of coefficients, \\(\\hat{\\beta}^*\\), that minimizes this cost function.\nFirst we do partial differentiation of the cost function with respect to the coefficients. Finding the coefficient values where the partial differential is equal to zero reveals the stationary points of the cost function. For OLS in particular, we can find a unique solution for the choice of coefficients that can be found analytically. The hessian matrix then further proves that this is a global minima."
  },
  {
    "objectID": "posts/ols_coefs_multivariate.html#deriving-the-optimum-coefficients",
    "href": "posts/ols_coefs_multivariate.html#deriving-the-optimum-coefficients",
    "title": "Deriving OLS coefficients (multivariate)",
    "section": "Deriving the optimum coefficients",
    "text": "Deriving the optimum coefficients\n\n0. Defining the notation\nFor a sample \\(i\\), we observe an outcome \\(y_i\\). \\(y\\) is a vector of all \\(n\\) observed outcomes.\n\\[\n\\underset{n \\times 1} {y} =\n\\begin{bmatrix}\n    y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_{n-1} \\\\ y_n\n\\end{bmatrix}\n\\]\nWe also observe \\(k\\) features for every sample \\(i\\). \\(X\\) is a matrix of these observed features. Note the first column is usually all ones, to include an intercept to optimize (or “bias” term).\n\\[\n\\underset{n \\times k} {X} =\n\\begin{bmatrix}\n  1 & x_{11} & \\cdots & x_{1,k-1} & x_{1,k} \\\\\n  1 & x_{21} & \\cdots & x_{2,k-1} & x_{2,k} \\\\\n  1 & x_{31} & \\cdots & x_{3,k-1} & x_{3,k} \\\\\n  \\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\\n  1 & x_{n-2,1} & \\cdots & x_{n-2,k-1} & x_{n-2,k} \\\\\n  1 & x_{n-1,1} & \\cdots & x_{n-1,k-1} & x_{n-1,k} \\\\\n  1 & x_{n,1} & \\cdots & x_{n,k-1} & x_{n,k}\n\\end{bmatrix}\n\\]\nThe contribution of each feature to the prediction is estimated by the coefficients \\(\\hat{\\beta}\\).\n\\[\n\\underset{k \\times 1} {\\hat{\\beta}} =\n\\begin{bmatrix}\n  \\beta_0 \\\\\n  \\beta_1 \\\\\n  \\vdots \\\\\n  \\beta_{k-1} \\\\\n  \\beta_{k}\n\\end{bmatrix}\n\\]\nWe make predictions, \\(\\hat{y}\\), by calculating the dot product of the features \\(X\\) and the coefficients \\(\\hat{\\beta}\\).\n\\[\n\\hat{y} = X \\hat{\\beta}\n\\]\nwhich is shorthand for this:\n\\[\n\\displaylines{\n\\begin{align}\n\\begin{bmatrix}\n  \\hat{y}_1 \\\\ \\hat{y}_2 \\\\ \\hat{y}_3 \\\\ \\vdots \\\\ \\hat{y}_{n-2} \\\\ \\hat{y}_{n-1} \\\\ \\hat{y}_n\n\\end{bmatrix} & =\n\\begin{bmatrix}\n  1 & x_{11} & \\cdots & x_{1,k-1} & x_{1,k} \\\\\n  1 & x_{21} & \\cdots & x_{2,k-1} & x_{2,k} \\\\\n  1 & x_{31} & \\cdots & x_{3,k-1} & x_{3,k} \\\\\n  \\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\\n  1 & x_{n-2,1} & \\cdots & x_{n-2,k-1} & x_{n-2,k} \\\\\n  1 & x_{n-1,1} & \\cdots & x_{n-1,k-1} & x_{n-1,k} \\\\\n  1 & x_{n,1} & \\cdots & x_{n,k-1} & x_{n,k}\n\\end{bmatrix}\n\\begin{bmatrix}\n  \\hat{\\beta}_0 \\\\\n  \\hat{\\beta}_1 \\\\\n  \\vdots \\\\\n  \\hat{\\beta}_{k-1} \\\\\n  \\hat{\\beta}_{k}\n\\end{bmatrix}\n\\\\ \\\\ & =\n\\begin{bmatrix}\n  \\hat{\\beta}_0 + \\hat{\\beta}_{1}x_{1,1} + \\cdots + \\hat{\\beta}_{k-1}x_{1,k-1} + \\hat{\\beta}_{k}x_{1,k} \\\\\n  \\hat{\\beta}_0 + \\hat{\\beta}_{1}x_{2,1} + \\cdots + \\hat{\\beta}_{k-1}x_{2,k-1} + \\hat{\\beta}_{k}x_{2,k} \\\\\n  \\hat{\\beta}_0 + \\hat{\\beta}_{1}x_{3,1} + \\cdots + \\hat{\\beta}_{k-1}x_{3,k-1} + \\hat{\\beta}_{k}x_{3,k} \\\\\n  \\vdots \\\\\n  \\hat{\\beta}_0 + \\hat{\\beta}_{1}x_{n-2,1} + \\cdots + \\hat{\\beta}_{k-1}x_{2,k-1} + \\hat{\\beta}_{k}x_{n-2,k} \\\\\n  \\hat{\\beta}_0 + \\hat{\\beta}_{1}x_{n-1,1} + \\cdots + \\hat{\\beta}_{k-1}x_{2,k-1} + \\hat{\\beta}_{k}x_{n-1,k} \\\\\n  \\hat{\\beta}_0 + \\hat{\\beta}_{1}x_{n,1} + \\cdots + \\hat{\\beta}_{k-1}x_{2,k-1} + \\hat{\\beta}_{k}x_{n,k} \\\\\n\\end{bmatrix}\n\\end{align}\n}\n\\]\nThe residual is the difference between the true outcome and the model prediction.\n\\[\n\\hat{\\epsilon} = y_i -\\hat{y}_i\n\\]\nwhich is shorthand for this:\n\\[\n\\begin{bmatrix}\n  \\hat{\\epsilon_1} \\\\\n  \\hat{\\epsilon_2} \\\\\n  \\hat{\\epsilon_3} \\\\\n  \\vdots \\\\\n  \\hat{\\epsilon_{n-2}} \\\\\n  \\hat{\\epsilon_{n-1}} \\\\\n  \\hat{\\epsilon_{n}}\n\\end{bmatrix} =\n\\begin{bmatrix}\n    y_1 \\\\ y_2 \\\\ y_3 \\\\ \\vdots \\\\ y_{n-2} \\\\ y_{n-1} \\\\ y_n\n\\end{bmatrix} -\n\\begin{bmatrix}\n  \\hat{y}_1 \\\\ \\hat{y}_2 \\\\ \\hat{y}_3 \\\\ \\vdots \\\\ \\hat{y}_{n-2} \\\\ \\hat{y}_{n-1} \\\\ \\hat{y}_n\n\\end{bmatrix}\n\\]\nOur aim is to find the optimum vector of coefficients, \\(\\hat{\\beta}^*\\), that minimizes the sum of squared residuals:\n\\[\n\\min_{\\beta} \\left( \\epsilon^{\\intercal}\\epsilon \\right)\n\\]\n\n\n1. Expand the sum of squared residuals\nThe first step involves expanding the sum of squared residuals, and substituting in \\(X \\hat{\\beta}\\) for \\(\\hat{y}\\). \\[\n\\displaylines{\n\\begin{align}\n\\sum_{i=1}^n{\\hat{\\epsilon}_i^2} & = \\hat{\\epsilon}^{\\intercal}\\hat{\\epsilon}\n\\\\& =(y-X\\hat{\\beta})^{\\intercal}(y-X\\hat{\\beta})\n\\\\& = y^{\\intercal}y - y^{\\intercal}X\\hat{\\beta}-\n\\hat{\\beta}^{\\intercal} X^{\\intercal}y+\n\\hat{\\beta}^{\\intercal}X^{\\intercal}X\\hat{\\beta}\n\\\\& = y^{\\intercal}y - 2y^{\\intercal}X\\hat{\\beta}\n+\\hat{\\beta}^{\\intercal}X^{\\intercal}X\\hat{\\beta}\n\\end{align}\n}\n\\]\n\n\nNote we can simply add the two middle terms, since are both scalars:\n\\[\n\\displaylines{\ny^{\\intercal}X\\hat{\\beta} =\n\\hat{\\beta}^{\\intercal} X^{\\intercal}y \\\\\n\\because \\underset{1 \\times n}{y^{\\intercal}} \\times\n\\underset{n \\times k}{X} \\times\n\\hat{\\underset{k \\times 1}{\\beta}}\n=\n\\hat{\\underset{1 \\times k}{\\beta}^{\\intercal}} \\times\n\\underset{k \\times n}{X^{\\intercal}} \\times\n\\underset{n \\times 1}{y}\n}\n\\]\n\n\n2. Partially differentiate RSS with respect to beta\nThe second step involves partially differentiating the cost function with respect to its parameters, to understand how it changes as the coefficients vary.\n\\[\n\\displaylines{\n\\begin{align}\n\\frac{\\partial}{\\partial\\hat{\\beta}}\\hat{\\epsilon}^{\\intercal}\\hat{\\epsilon} & \\equiv\n\\begin{bmatrix}\n    \\frac{\\partial}{\\partial\\hat{\\beta}_1}\\hat{\\epsilon}^{\\intercal}\\hat{\\epsilon} \\\\\n    \\frac{\\partial}{\\partial\\hat{\\beta}_2}\\hat{\\epsilon}^{\\intercal}\\hat{\\epsilon} \\\\\n    \\vdots \\\\\n    \\frac{\\partial}{\\partial\\hat{\\beta}_k}\\hat{\\epsilon}^{\\intercal}\\hat{\\epsilon}\n\\end{bmatrix}\n\\\\ & = \\frac{d}{d\\hat{\\beta}}(\ny^{\\intercal}y - 2y^{\\intercal}X\\hat{\\beta}\n+\\hat{\\beta}^{\\intercal}X^{\\intercal}X\\hat{\\beta}) \\\\ & = 0 - 2X^{\\intercal}y +((X^{\\intercal}X)\\hat{\\beta} + (X^{\\intercal}X)^{\\intercal}\\hat{\\beta})\n\\\\ & = -2X^{\\intercal}y +  2((X^{\\intercal}X)\\hat{\\beta})\n\\end{align}\n}\n\\]\n\n\nTwo matrix differentiation rules used here for reference:\n\\[\n\\displaylines{\n\\frac{\\partial}{dx}(Ax) = A^{\\intercal}x \\\\\n\\frac{\\partial}{dx}(xAx) = Ax + A^{\\intercal}x\n}\n\\]\nAnd note \\(X^{\\intercal}X = (X^{\\intercal}X)^{\\intercal}\\) by definition, so we can add the two last terms.\n\n\n3. Find the coefficient values at the stationary point\nNow we find the choices of \\(\\beta\\) where the partial differential is equal to zero. These stationary points for the cost function are either at its maximum or minimum.\nFor OLS - we actually only find one unique solution!\n\\[\n\\displaylines{\n\\begin{align}\n\\cancel{2}X^{\\intercal}y +  \\cancel{2}((X^{\\intercal}X)\\hat{\\beta}) & = 0 \\\\\n\\therefore (X^{\\intercal}X)\\hat{\\beta} & = X^{\\intercal}y \\\\\n\\therefore \\hat{\\beta} & = (X^{\\intercal}X)^{-1}X^{\\intercal}y\n\\end{align}\n}\n\\]\n\n\nNote the need to invert \\(X^{\\intercal}X\\). This is only possible for a full rank matrix.\nThe first term is the (inverse) variance matrix of \\(X\\). This term normalizes the coefficient with respect to the magnitude of \\(X\\).\nThe second term is the covariance matrix between \\(X\\) and \\(y\\). This incorporates the linear relationship between the two in the coefficient.\nHence, the coefficient can be interpreted as the estimated change in \\(y\\) given a one unit change in \\(X\\).\n\n\n4. Check the stationary point is a global minimum (hessian matrix)\nFinally, we derive the hessian matrix, by double-differentiating the cost function with respect to the coefficients:\n\\[\n\\displaylines{\n\\frac{\\partial^2}{\\partial\\hat{\\beta}\\partial\\hat{\\beta}^{\\intercal}}\n\\left(\n  \\hat{\\epsilon}^{\\intercal}\\hat{\\epsilon}\n\\right)\n\\\\ = \\frac{\\partial}{\\partial \\hat{\\beta}^{\\intercal}} \\left( -2X^{\\intercal}y +  2((X^{\\intercal}X)\\hat{\\beta}) \\right)\n\\\\ = 2(X^{\\intercal}X)^{\\intercal}\n}\n\\]\nSince \\(X^{\\intercal}X\\) is clearly positive definite, the cost function is convex. Thus, we know our unique solution for \\(\\beta\\) where the partial differential is at zero is indeed a global minimum for the cost function."
  },
  {
    "objectID": "posts/ols_coefs_multivariate.html#final-reflections",
    "href": "posts/ols_coefs_multivariate.html#final-reflections",
    "title": "Deriving OLS coefficients (multivariate)",
    "section": "Final reflections",
    "text": "Final reflections\nUnlike logistic regression, or the multiple hidden-layer structure of neural networks, we can “jump” straight to the optimum coefficients for OLS. Why can we do this? Well chiefly its because OLS is a bit of a special case:\n\nThe minima is a global minima: The hessian matrix is positive definite, and hence the cost function is strictly convex. This means we know that when a choice of coefficients is found that ensure the partially differentiated cost function is equal zero, this minima is also a global one, not a local one.\nThere is only one solution for the optimum coefficient: We assume that the matrix is full rank (every feature provides additional predictive power) and that the number of predictors is smaller than the number of obervations. This also means that partially differentiating is okay to do!\nA closed-form solution can be found The predictions are generated from \\(X\\) using a simple, purely algebraic function, i.e. the sum-product of \\(X\\) by \\(\\beta\\). This means we can find an analytical solution to the optimal choice \\(\\beta^*\\). Note this often isn’t possible since non-linear activation functions (i.e. link functions) are often transcendental.\n\nWe will dive into this in another post.\nFin."
  },
  {
    "objectID": "posts/ols_sandwich_estimators.html",
    "href": "posts/ols_sandwich_estimators.html",
    "title": "Sandwiches: robust covariance error estimation",
    "section": "",
    "text": "What are we exploring?\n\n\n\nEstimating the correct coefficient variance when relaxing homoskedastic error assumptions"
  },
  {
    "objectID": "posts/ols_sandwich_estimators.html#introducing-sandwiches",
    "href": "posts/ols_sandwich_estimators.html#introducing-sandwiches",
    "title": "Sandwiches: robust covariance error estimation",
    "section": "Introducing sandwiches",
    "text": "Introducing sandwiches\nThe variance for the OLS coefficient estimator is equal to the following:\n\\[\n\\displaylines{\nV(\\hat{\\beta}) =\n(X^{\\intercal}X)^{-1}X^{\\intercal}E[\\epsilon\\epsilon^{\\intercal}]X(X^{\\intercal}X)^{-1}\n}\n\\]\nThis can be though to as a sandwich:\n\nThe “bread” either side: \\((X^{\\intercal}X)^{-1}X^{\\intercal}\\) on the left and its transpose \\(X(X^{\\intercal}X)^{-1}\\) on the right\nThe “meat” in the middle: what we assume for \\(E[\\epsilon\\epsilon^{\\intercal}]\\)\n\nNote that this is the same as the error variance, since \\(V[\\epsilon]=E[\\epsilon\\epsilon^{\\intercal}]-E[\\epsilon]E[\\epsilon^{\\intercal}]\\) and \\(E[\\epsilon] = 0\\)\n\n\nOur coefficient will only be efficient if these assumptions about the expected error are correct! We will explore what happens when the errors are assumed to be homoskedastic, heteroskedastic or clustered."
  },
  {
    "objectID": "posts/ols_sandwich_estimators.html#salmon-bagel-spherical-errors",
    "href": "posts/ols_sandwich_estimators.html#salmon-bagel-spherical-errors",
    "title": "Sandwiches: robust covariance error estimation",
    "section": "Salmon bagel: Spherical Errors 🐟",
    "text": "Salmon bagel: Spherical Errors 🐟\nUsual OLS is efficient if the true model has “spherical errors”. What does this mean in practice?\n\nErrors are homoskedastic: \\(V(\\epsilon_i)=\\sigma^2\\) for all observations\nErrors are serially uncorrelated: \\(cov(\\epsilon_i,\\epsilon_{j\\neq i})=0\\)\n\nWhat does this look like for \\(E[\\epsilon\\epsilon^{\\intercal}]\\)?\n\nThe diagonal of the matrix is a constant value (scalar), \\(\\sigma^2\\)\nThe off-diagonals are all zero\n\n\\[\n\\hat{\\sigma}^2\\underset{n\\times n}{I} =\n\\begin{bmatrix}\n\\hat{\\sigma}^2 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n0 & \\hat{\\sigma}^2 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n0 & 0 & \\hat{\\sigma}^2 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n0 & 0 & 0 & \\hat{\\sigma}^2 & 0 & 0 & 0 & 0 & 0\\\\\n0 & 0 & 0 & 0 & \\hat{\\sigma}^2 & 0 & 0 & 0 & 0\\\\\n0 & 0 & 0 & 0 & 0 & \\hat{\\sigma}^2 & 0 & 0 & 0\\\\\n0 & 0 & 0 & 0 & 0 & 0 & \\hat{\\sigma}^2 & 0 & 0\\\\\n0 & 0 & 0 & 0 & 0 & 0 & 0 & \\hat{\\sigma}^2 & 0\\\\\n0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & \\hat{\\sigma}^2\n\\end{bmatrix}\n\\]\nA good estimation of the constant error variance \\(\\sigma^2\\) is to apply the standard formula to the residuals (i.e. method of moments):\n\\[\n\\hat{\\sigma^2}=\\frac{1}{n-k}\\sum{\\hat{\\epsilon_i}^2} \\equiv \\frac{\\epsilon^{\\intercal}\\epsilon }{n-k}\n\\]\n\n\n\n\n\n\nThus our “salmon sandwich” is:\n\n\n\n\\[\n\\underset{k \\times k}{(X^{\\intercal}X)}^{-1}\n\\underset{k \\times n}\n{X^{\\intercal}}\n\\left(\\frac{1}{n-k} \\times \\underset{1 \\times 1}{(\\epsilon^{\\intercal}\\epsilon)}\n\\times\\underset{n \\times n}{I} \\right)\n\\underset{n \\times k}{X}\n\\underset{k \\times k}{(X^{\\intercal}X)}^{-1}\n\\]\n\n\n\n\nNote that under spherical errors, the sandwich can be simplified: \\[\n\\displaylines{\n\\begin{align}\nV[\\hat{\\beta}]\n& = (X^{\\intercal}X)^{-1}X^{\\intercal}E[\\epsilon\\epsilon^{\\intercal}]X(X^{\\intercal}X)^{-1}\n\\\\ & = (X^{\\intercal}X)^{-1}X^{\\intercal}\\sigma^2IX(X^{\\intercal}X)^{-1}\n\\\\ & = \\sigma^2(X^{\\intercal}X)^{-1}\\cancel{X^{\\intercal}X}\\cancel{(X^{\\intercal}X)^{-1}}\n\\\\ & = \\sigma^2(X^{\\intercal}X)^{-1}\n\\end{align}\n}\n\\]\nIn this scenario, the only things that impact the standard error of the coefficient \\(\\beta_k\\) is:\n\nThe variance of all the residuals, \\(\\sigma^2\\)\nThe variance of the feature \\(V(X_k)\\)"
  },
  {
    "objectID": "posts/ols_sandwich_estimators.html#ham-sarnie-heteroskedastic-errors",
    "href": "posts/ols_sandwich_estimators.html#ham-sarnie-heteroskedastic-errors",
    "title": "Sandwiches: robust covariance error estimation",
    "section": "Ham sarnie: Heteroskedastic errors 🍖",
    "text": "Ham sarnie: Heteroskedastic errors 🍖\nHeteroskedastic correction is needed if:\n\nErrors vary for every individual: \\(V(\\epsilon_i)=\\sigma_i^2\\) for all observations\nBut they are still independent aka serially uncorrelated: \\(cov(\\epsilon_i,\\epsilon_{j\\neq i})=0\\)\n\nWhat does this look like for \\(E[\\epsilon\\epsilon^{\\intercal}]\\)?\n\nThe diagonal of the matrix is the estimate of variance which is unique for each observation, \\(\\sigma_i^2\\)\nThe off-diagonals are all zero\n\\[\n  \\underset{n \\times n}{\\sigma^2} = \\begin{bmatrix}\n  \\sigma_1^2 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n  0 & \\sigma_2^2 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n  0 & 0 & \\sigma_3^2 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n  0 & 0 & 0 & \\sigma_4^2 & 0 & 0 & 0 & 0 & 0\\\\\n  0 & 0 & 0 & 0 & \\sigma_5^2 & 0 & 0 & 0 & 0\\\\\n  0 & 0 & 0 & 0 & 0 & \\sigma_6^2 & 0 & 0 & 0\\\\\n  0 & 0 & 0 & 0 & 0 & 0 & \\sigma_7^2 & 0 & 0\\\\\n  0 & 0 & 0 & 0 & 0 & 0 & 0 & \\sigma_8^2 & 0\\\\\n  0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & \\sigma_{..}^2\\\\\n  \\end{bmatrix}\n  \\]\nA good estimation of the vector of heteroskedastic error variances \\(\\sigma^2\\) is again to apply the standard formula to the residuals for each individual:\n\n\\[\n\\hat{\\sigma_i^2}=\\frac{1}{n-k}\\sum{\\hat{\\epsilon_i}^2}\n\\]\n\n\n\n\n\n\nThus our “ham sandwich” is:\n\n\n\n\\[\n\\underset{k \\times k}{(X^{\\intercal}X)}^{-1}\n\\underset{k \\times n}{X^{\\intercal}}\n\\left(\\frac{1}{n-k} \\times \\underset{1 \\times n}{(\\epsilon\\odot \\epsilon)} ^{\\intercal}\n\\times\\underset{n \\times n}{I} \\right)\n\\underset{n \\times k}{X}\n\\underset{k \\times k}{(X^{\\intercal}X)}^{-1}\n\\]\n\n\nSimilar to homoskedastic errors, the things that impacts the standard error of coefficient \\(k\\) is:\n\nThe variance of all the individual errors, \\(\\sigma_i^2\\)\nThe variance of the feature \\(V(X_k)\\)"
  },
  {
    "objectID": "posts/ols_sandwich_estimators.html#cheese-roll-clustered-errors",
    "href": "posts/ols_sandwich_estimators.html#cheese-roll-clustered-errors",
    "title": "Sandwiches: robust covariance error estimation",
    "section": "Cheese roll: Clustered Errors 🧀",
    "text": "Cheese roll: Clustered Errors 🧀\nCluster-robust errors are needed if:\n\nErrors vary for every individual: \\(V(\\epsilon_i)=\\sigma_i^2\\) for all observations i.e. still heteroskedastic\nAND errors within the same cluster \\(C_l\\) are serially correlated: \\(cov(\\epsilon_i,\\epsilon_{j}) \\neq 0 \\text{ if } \\epsilon_i,\\epsilon_j \\in C_l\\)\n\nNote - errors between clusters are assumed not to be serially correlated though i.e. \\(cov(\\epsilon_i,\\epsilon_{j}) =0 \\text{ if } \\epsilon_i \\in C_l,\\epsilon_j \\in C_{m \\neq l}\\)\n\n\nWhat does this look like for \\(E[\\epsilon\\epsilon^{\\intercal}]\\)?\n\nThe diagonal of the matrix is the estimate of variance which is unique for each observation, \\(\\sigma_i^2\\)\nThe off-diagonals are also populated with the covariance - but only when they are both in the same cluster\n\nHere is an example where observations 1, 2 and 3 are in cluster A, observations 4 and 5 are in cluster B, observations 6, 7 and 8 are in cluster C etc.\n\\[\n\\underset{n \\times n}{\\epsilon\\epsilon^{\\intercal}} \\sim \\begin{bmatrix}\n\\epsilon_1^2 & \\epsilon_1\\epsilon_2 & \\epsilon_1\\epsilon_3 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n\\epsilon_2\\epsilon_1 & \\epsilon_2^2 & \\epsilon_2\\epsilon_3 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n\\epsilon_3\\epsilon_1 & \\epsilon_3\\epsilon_2 & \\epsilon_3^2 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n0 & 0 & 0 & \\epsilon_4^2 & \\epsilon_4\\epsilon_5 & 0 & 0 & 0 & 0\\\\\n0 & 0 & 0 & \\epsilon_5\\epsilon_4 & \\epsilon_5^2 & 0 & 0 & 0 & 0\\\\\n0 & 0 & 0 & 0 & 0 & \\epsilon_6^2 & \\epsilon_6\\epsilon_7 & \\epsilon_6\\epsilon_8 & 0\\\\\n0 & 0 & 0 & 0 & 0 & \\epsilon_7\\epsilon_6 & \\epsilon_7^2 & \\epsilon_7\\epsilon_8 & 0\\\\\n0 & 0 & 0 & 0 & 0 & \\epsilon_8\\epsilon_6 & \\epsilon_8\\epsilon_7 & \\epsilon_8^2 & 0\\\\\n0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & \\epsilon_{..}^2\\\\\n\\end{bmatrix}\n\\]\nAdditionally, we have to do a finite-sample correction as well on the degrees of freedom, based on the number of clusters \\(n_c\\)\n\n\n\n\n\n\nThus our cheese sandwich is:\n\n\n\n\\(\\underset{k \\times k}{(X^{\\intercal}X)}^{-1}\n\\underset{k \\times n}{X^{\\intercal}}\n\\left(\\frac{n-1}{n-k}\\frac{n_c}{n_c-1} \\times\n\\underset{n \\times n}{(\\epsilon \\epsilon^{\\intercal})}\n\\right)\n\\underset{n \\times k}{X}\n\\underset{k \\times k}{(X^{\\intercal}X)}^{-1}\\)\n\n\nIn this scenario, there are a few additional things that impact the standard error of coefficient \\(k\\):\n\nIf errors are correlated within clusters, this will increase the error.\nIf features are correlated within clusters, this will also increase the error (due to the off-diagonals in the error variance matrix)\nAnd if both the errors and feature correlations are the same sign, this will also increase the standard error.\nAs well as the variance of the individual errors, \\(\\sigma_i^2\\), and the variance of each feature \\(V(X_k)\\), as before"
  },
  {
    "objectID": "posts/mle_ols_normal.html",
    "href": "posts/mle_ols_normal.html",
    "title": "OLS vs MLE with gaussian noise",
    "section": "",
    "text": "What are we exploring?\n\n\n\nWhy MLE finds the same coefficients as OLS when assuming model errors are generated from a mean-zero gaussian probabilistic process\n\n\n\nNormality of errors\nLet’s assume the errors follow a normal distribution with a mean of zero: \\[\n\\epsilon  = y - X\\beta \\sim \\mathcal{N}(0,\\sigma^2)\n\\]\n\nYou might already notice how similar this is to the Gauss-Markov requirements to ensure OLS coefficients are BLUE!\n\nThe expected error is zero, and consistent for all values of X, so we have “strict exogeneity”: \\(E[\\epsilon|X] = 0\\)\nThe error variance is uniform, again consistent for all values of X, so we have “spherical errors”: \\(E[\\epsilon\\epsilon^{\\intercal}|X] = 0\\)\n\n\n\nApplying the normal pdf\nFor any datapoint \\(i\\), we can formulate the likelihood of observing the outcome \\(y_i\\) as being generated from the normal probability density function applied to the squared error:\n\\[\n\\displaylines{\n\\begin{align}\np(y_i|\\beta,X_i,\\sigma^2)\n& = \\frac{1}{\\sigma\\sqrt{2\\pi}}\\exp{\\left\\{-\\frac{1}{2\\sigma^2}\\epsilon_i^2\\right\\}} \\\\\n& = \\frac{1}{\\sigma\\sqrt{2\\pi}}\\exp{\\left\\{-\\frac{1}{2\\sigma^2}(y_i-X_i\\beta)^2\\right\\}}\n\\end{align}\n}\n\\]\nMaximum likelihood estimation aims to find the set of coefficients that maximises the likelihood of observing the evidence we have. We thus aim to find the coefficients \\(\\beta\\) that maximise the likelihood of observing \\(y\\) across all \\(n\\) samples:\n\\[\n\\displaylines{\n\\begin{align}\np(y|\\beta,X,\\sigma^2)\n& = \\prod_{i=1}^{n}{\\frac{1}{\\sigma\\sqrt{2\\pi}}}\\exp{\\left\\{-\\frac{1}{2\\sigma^2}\\epsilon_i^2\\right\\}}\n\\end{align}\n}\n\\]\n\n\nTaking the negative log-likelihood\nIn practice, dealing with a cost function made up of a sum product is tricky - it is easier to take the log and deal with addition instead. Further, rather than maximise, it is common to “minimize” cost functions, so the negative log-likelihood is usually used.\n\n\nRecall that \\(\\log{\\left(ab\\right)} = \\log{\\left(a\\right)} + \\log{\\left(b\\right)}\\)\n\\[\n\\displaylines{\n\\begin{align}\n\\max_\\beta{p(y|\\beta,X,\\sigma^2)}\n= &\n\\max_\\beta{\\left[ \\prod{ \\frac{1}{\\sigma\\sqrt{2\\pi}}\\exp{\\left\\{-\\frac{1}{2\\sigma^2}\\epsilon_i^2\\right\\}} } \\right]}\n\\\\ \\\\ \\Rightarrow &\n\\min_\\beta{\\left[ -\\sum{\\log{\\left(\\frac{1}{\\sigma\\sqrt{2\\pi}}\\exp{\\left\\{-\\frac{1}{2\\sigma^2}\\epsilon_i^2\\right\\}}\\right)}} \\right]}\n\\end{align}\n}\n\\]\n\n\nSimplifying the cost function\nAnd now we can look to simplify this: \\[\n\\displaylines{\n\\begin{align}\n& \\min_\\beta{\\left[ -\\sum{\\log{\\left(\\frac{1}{\\sigma\\sqrt{2\\pi}}\\exp{\\left\\{-\\frac{1}{2\\sigma^2}\\epsilon_i^2\\right\\}}\\right)}} \\right]}\n\\\\ = & \\min_\\beta{\\left[ -\\sum{\\log{\\left(\\frac{1}{\\sigma\\sqrt{2\\pi}}\\right)}} -\\sum{\\log{\\left(\\exp{\\left\\{-\\frac{1}{2\\sigma^2}\\epsilon_i^2\\right\\}}\\right)}} \\right]}\n\\\\ = & \\min_\\beta{\\left[ -\\sum{\\log{((2\\pi\\sigma^2)^{-\\frac{1}{2}})}} - \\sum{\\left(-\\frac{1}{2\\sigma^2} \\epsilon_i^2\\right)} \\right]}\n\\\\ = & \\min_\\beta{\\left[ \\frac{1}{2}\\sum{\\log{(2\\pi\\sigma^2)}} + \\frac{1}{2\\sigma^2}\\sum{\\epsilon_i^2} \\right]}\n\\\\ = & \\min_\\beta{\\left[ \\frac{1}{2} \\left(n\\log{(2\\pi\\sigma^2)} + \\frac{1}{\\sigma^{2}} \\sum{\\epsilon_i^2}  \\right)\\right]}\n\\end{align}\n}\n\\]\n\n\nCoefficient point-estimate is the same as OLS\nWe minimise the cost function by finding the optimum coefficient values \\(\\beta^*\\) so that the partial differential is equal to zero.\nThe constant \\(\\log{(2\\pi\\sigma^2)}\\) doesn’t vary with respect to \\(\\beta\\), so it drops out. The fraction \\(\\frac{1}{2}\\) also drops out when finding where differential is set to zero.\nHence we are left finding that we are solving the same problem as usual least-squares!\n\\[\n\\displaylines{\n\\begin{align}\n\\therefore \\beta^* & =\\arg\\min_\\beta{\\left[ \\frac{1}{2} \\left(n\\log{(2\\pi\\sigma^2)} + \\frac{1}{\\sigma^{2}} \\sum{\\epsilon_i^2}  \\right)\\right]}\n\\\\ & =\\arg\\min_\\beta{\\left[ \\epsilon^T\\epsilon \\right]}\n\\end{align}\n}\n\\]\n\n\nError-variance estimate is the same as OLS\nOLS estimates the variance of the models errors using the residuals from the sample:\n\\[\n\\sigma^2 = \\frac{1}{n}\\hat{\\epsilon}^{\\intercal}\\hat{\\epsilon}\n\\]\nDo we see the same with MLE? Well so far we have only found the optimum \\(\\hat{\\beta}^*\\) to ensure the expected conditional error is zero, we haven’t touched our other parameter \\(\\sigma^2\\).\nNow lets instead find the estimate of \\(\\sigma\\) that minimizes the negative log-likelihood:\n\\[\n\\displaylines{\n\\begin{align}\n& \\min_{\\sigma^2}{\\left[ \\frac{1}{2} \\left(n\\log{(2\\pi\\sigma^2)} + \\frac{1}{\\sigma^{2}} \\sum{\\epsilon_i^2}  \\right)\\right]} \\\\\n\\Rightarrow &\n\\frac{\\partial}{\\partial\\sigma^2}\n\\left[ \\frac{1}{2} \\left(n\\log{(2\\pi\\sigma^2)} + \\frac{1}{\\sigma^{2}} \\sum{\\epsilon_i^2}  \\right)\\right]\n\\\\ & =\n\\frac{1}{2} \\left(n\\frac{2\\pi}{2\\pi\\sigma^2} + \\frac{-2}{\\sigma^4} \\sum{\\epsilon_i^2}  \\right)\n\\\\ & =\n\\frac{n}{2\\sigma^2} - \\frac{1}{2\\sigma^4} \\sum{\\epsilon_i^2} = 0 \\\\\n& \\therefore \\sigma^2 = \\frac{1}{n}\\sum{\\epsilon_i^2} = 0 \\\\\n\\end{align}\n}\n\\]\nand hence we can see that the estimation of OLS is the same too.\n\n\nFinal reflections\nOne advantage of using MLE is we can generate a probabilistic estimate for \\(y_i\\), rather than just a point-estimate (assuming we have fitted \\(\\hat{\\sigma}^2\\) as above).\n\nPoint estimate: \\(\\hat{y} = X\\hat{\\beta}\\)\nPosterior estimate: \\(P(\\hat{y_i}| X_i,\\hat{\\beta},\\sigma^2) = \\mathcal{N}(\\hat{y_i}| X_i\\hat{\\beta},\\sigma^2)\\),\n\nYou might already have started to see how probabilistic predictions and coefficients fit by MLE nicely fit into the bayesian paradigm. This opens up nice extensions,such as using priors as a form of regularization. This is for another post though!\nFin."
  },
  {
    "objectID": "posts/ols_blue.html",
    "href": "posts/ols_blue.html",
    "title": "BLUE coefficients: bias and efficiency",
    "section": "",
    "text": "What are we exploring?\n\n\n\nProving that the OLS coefficient is the best linear unbiased estimator."
  },
  {
    "objectID": "posts/ols_blue.html#are-the-ols-coefficients-blue",
    "href": "posts/ols_blue.html#are-the-ols-coefficients-blue",
    "title": "BLUE coefficients: bias and efficiency",
    "section": "Are the OLS coefficients “BLUE”?",
    "text": "Are the OLS coefficients “BLUE”?\nWe find a unique solution to the set of coefficients that minimize the sum of squared residuals analytically (see its derivation here):\n\\[\n\\hat{\\beta}^{OLS}=(X^{\\intercal}X)^{-1}X^{\\intercal}y\n\\]\nHowever, how do we know if these coefficients are the best ones we can estimate?\nFor the estimated coefficients to be the Best Linear Unbiased Estimator (i.e. “BLUE”):\n\nThe best estimator has to be unbiased: \\(E[\\hat{\\beta}^*] = \\beta\\)\nAnd among all possible linear, unbiased estimators, it must have the smallest variance: \\(V[\\hat{\\beta}^{*}] &lt; V[\\hat{\\beta}^{Z}]\\)\n\nWe want to ensure our OLS estimate is the best, i.e. that \\(\\hat{\\beta}^{OLS} = \\hat{\\beta}^{*}\\). To achieve this, first we need to confirm it is unbiased. Then given this is true, we can check that the coefficient is most efficient vs all other unbiased estimators.\n\n\n\n\n\n\nGauss Markov Assumptions\n\n\n\nAlong the way, we will outline the Gauss-Markov assumptions utilised that ensure the OLS coefficient is BLUE."
  },
  {
    "objectID": "posts/ols_blue.html#setting-the-scene",
    "href": "posts/ols_blue.html#setting-the-scene",
    "title": "BLUE coefficients: bias and efficiency",
    "section": "Setting the scene",
    "text": "Setting the scene\n\nThe true coefficient and model\n\\(\\beta\\) is the true (unobserved) relationship between all the relevant explanatory features, \\(X\\), and their associated observed outcomes, \\(y\\). In other words, if we observed the entire population of data, it is the relationship we would find.\nConcretely, we assume the outcome is a linear function of all its relevant features. This “true model” perfectly predicts the outcome, except for random noise \\(\\epsilon\\) that influences the observed outcome: \\(y = X\\beta + \\epsilon\\)\n\n\nThe estimated coefficient\n\\(\\hat{\\beta}\\) is our estimated coefficient for the true relationship \\(\\beta\\). In reality, we estimate \\(\\hat{\\beta}\\) from the small, finite sample of size \\(n\\) that is collected, not the whole population. Given any random sample could be collected, we can term the coefficient resulting from the optimum estimation proceedure to be \\(\\hat{\\beta}^*\\). We want to understand if \\(\\hat{\\beta}^{OLS} = \\hat{\\beta}^*\\).\n\n\nThe expected estimated coefficient\n\\(E[\\hat{\\beta}]\\) is the “expected” estimated coefficient. Imagine we repeat the action of estimating the coefficient \\(\\hat{\\beta}\\) many times, each time collecting a new sample (where each observation is sampled i.i.d), and recording the value for the estimated coefficient. \\(E[\\hat{\\beta}]\\) would then be the average of all of those estimated coefficients. If the OLS coefficient is unbiased, then the expected coefficient estimate should be equal to the true one, \\(E[\\hat{\\beta}^{OLS}]=\\beta\\).\n\n\nThe variance of the estimated coefficient\n\\(V[\\hat{\\beta}]\\) is the variance of the estimated coefficient. It determines how much we might expect our estimate \\(\\hat{\\beta}\\) to differ from the true \\(\\beta\\) for any sample drawn. Given the OLS coefficient has been shown to be unbiased, if it is BLUE we expect its variance to be lower than another other unbiased choice \\(\\hat{\\beta}^{Z}\\). Concretely, we want to find \\(V[\\hat{\\beta}^{OLS}] &lt; V[\\hat{\\beta}^{Z}]\\)."
  },
  {
    "objectID": "posts/ols_blue.html#bias",
    "href": "posts/ols_blue.html#bias",
    "title": "BLUE coefficients: bias and efficiency",
    "section": "Bias",
    "text": "Bias\nOften our small finite samples of size \\(n\\) are not a perfect reflection of the population they are drawn from. This “sampling error” means we might estimate a different relationship between \\(X\\) and \\(y\\) than the true relationship of the population, i.e. \\(\\hat{\\beta} \\neq \\beta\\).\nHowever, we should expect our estimated coefficient to be equal to the true value on average. This means we do not want to have a bias towards the estimate being systematically too small or too large, for example. In other words, if we repeated the whole proceedure thousands of times (each time taking new samples, and estimating a coefficient from the new sample) then the average of all the estimated coefficients values should be equal to the true value, i.e. \\(E[\\hat{\\beta}] = \\beta\\).\nRecall that we believe there is a true model that follows the form:\n\\[\ny = X\\beta + \\epsilon\n\\]\n\n\n\n\n\n\nGM1: Linearity \n\n\n\nThe formula above relies on the first Gauss-Markov assumption - that the dependent variable \\(y\\) is assumed to be a linear function of the variables \\(X\\). Note that implies that the proper functional form has been selected (i.e. the relationship is linear) and there are no omitted variables - a huge assumption!\n\n\nIf we substitute this into our estimated coefficient:\n\\[\n\\displaylines{\n\\begin{align}\n\\hat{\\beta}^{OLS} & = (X^{\\intercal}X)^{-1}X^{\\intercal}y\n\\\\ & = (X^{\\intercal}X)^{-1}X^{\\intercal}(X\\beta+\\epsilon)\n\\\\ & = (X^{\\intercal}X)^{-1}X^{\\intercal}X\\beta+(X^{\\intercal}X)^{-1}X^{\\intercal}\\epsilon\n\\\\ & = \\beta+(X^{\\intercal}X)^{-1}X^{\\intercal}\\epsilon\n\\end{align}\n}\n\\]\nwe show that the estimated coefficient \\(\\hat{\\beta}^{OLS}\\) will differ from the true value depending on the random error \\(\\epsilon\\) associated with the particular finite sample collected.\nNow let’s now take the expectation, to determine when the coefficient is unbiased. In other words, what is the “average” coefficient if we took the sample many times: \\[\n\\displaylines{\n\\begin{align}\nE[\\hat{\\beta}^{OLS}]\n& = \\beta +(X^{\\intercal}X)^{-1}X^{\\intercal}E[\\epsilon]\n\\\\ & = \\beta & \\iff E[\\epsilon] & = 0\n\\end{align}\n\\\\\n}\n\\]\nWe find that the coefficient is unbiased as long as the expected error is also zero.\n\n\n\n\n\n\nGM2: Strict Exogeneity \n\n\n\nThe second Gauss-Markov assumption is strict exogeneity, where the expected error is zero for all feature values: \\(E[\\epsilon|X] = 0\\). By definition, the weaker exogeneity statement of \\(E[\\epsilon] = 0\\) is implied by having the expected error conditional being equal to zero."
  },
  {
    "objectID": "posts/ols_blue.html#efficiency",
    "href": "posts/ols_blue.html#efficiency",
    "title": "BLUE coefficients: bias and efficiency",
    "section": "Efficiency",
    "text": "Efficiency\nTo be the “best”, the OLS estimator also needs to be efficient. This means that it has the lowest variance of all unbiased estimators. This section looks to prove this.\n\nCoefficient variance for OLS\nFirst, let’s derive the variance from the coefficients estimated using OLS, termed \\(V[\\hat{\\beta}^{OLS}]\\). As before, we substitute the true model \\(y = X\\beta + \\epsilon\\) into the coefficient estimated through OLS:\n\\[\n\\displaylines{\n\\begin{align}\n\\hat{\\beta}^{OLS}-\\beta\n& = \\beta +  ((X^{\\intercal}X)^{-1}X^{\\intercal}\\epsilon) - \\beta \\\\\n& = (X^{\\intercal}X)^{-1}X^{\\intercal}\\epsilon\n\\\\ \\\\\n\\therefore\n(\\hat{\\beta}^{OLS}-\\beta)(\\hat{\\beta}^{OLS}-\\beta)^{\\intercal}\n& = ((X^{\\intercal}X)^{-1}X^{\\intercal}\\epsilon)((X^{\\intercal}X)^{-1}X^{\\intercal}\\epsilon)^{\\intercal}\n\\\\ & =\n(X^{\\intercal}X)^{-1}X^{\\intercal}\\epsilon\\epsilon^{\\intercal}X(X^{\\intercal}X)^{-1} \\\\ \\\\\n\\therefore\nV(\\hat{\\beta}^{OLS}) & = E[(\\hat{\\beta}^{OLS}-\\beta)(\\hat{\\beta}^{OLS}-\\beta)^{\\intercal}]\n\\\\ & =\n(X^{\\intercal}X)^{-1}X^{\\intercal}E[\\epsilon\\epsilon^{\\intercal}]X(X^{\\intercal}X)^{-1}\n\\end{align}\n}\n\\]\nThis is sometimes called the sandwich estimator - post soon to follow on this!\n\n\nCoefficient variance assuming “spherical errors”\nWe can simplify this further by appling some assumptions to the estimated error variance \\(E[\\epsilon\\epsilon^{\\intercal}]\\):\n\\[\n\\displaylines{\n\\begin{align}\nE[\\epsilon \\epsilon^{\\intercal}] & =  \n\\begin{bmatrix}\nE[\\epsilon_1^2] & \\cdots & E[\\epsilon_1\\epsilon_n] \\\\\n\\vdots & \\ddots & \\vdots \\\\\nE[\\epsilon_n\\epsilon_1] & \\cdots & E[\\epsilon_n^2]\n\\end{bmatrix} \\\\ \\\\\n& = \\begin{bmatrix}\n\\frac{1}{n}\\sum_{i=1}^{n}{\\epsilon_i^2} & \\cdots & 0 \\\\\n\\vdots & \\ddots & \\vdots \\\\\n0 & \\cdots & \\frac{1}{n}\\sum_{i=1}^{n}{\\epsilon_i^2}\n\\end{bmatrix} \\\\ \\\\\n& = \\begin{bmatrix}\n\\hat{\\sigma}^2 & \\cdots & 0 \\\\\n\\vdots & \\ddots & \\vdots \\\\\n0 & \\cdots & \\hat{\\sigma}^2\n\\end{bmatrix} \\\\ \\\\\n& = \\hat{\\sigma}^2I\n\\end{align}\n}\n\\]\nHow can we jump to this result? Well we are making two assumptions:\n\nNo serial correlation: \\(\\rho_{\\epsilon_{i},\\epsilon_{i \\neq j}} = 0\\). No correlation between sample errors means that \\(E[\\epsilon_i \\epsilon_{j \\neq i}] = 0\\), and hence the off-diagonals of the error covariance matrix are zero.\nHomoskedasticity: the assumption of uniform error variance for all samples means that \\(V[\\epsilon_i^2] = V[\\epsilon_{j \\neq i}^2] = \\hat{\\sigma}^2\\). And our best approximation for \\(\\hat{\\sigma}^2\\) is simply taking the average squared error: \\(\\hat{\\sigma}^2 = \\frac{1}{n}\\sum_{i=1}^{n}{\\epsilon_i^2}\\)\n\n\n\n\n\n\n\nGM3: Spherical errors \n\n\n\nThe third Gauss-Markov assumption is spherical errors, \\(E[\\epsilon\\epsilon^{\\intercal}|X] = 0\\). This means that the outer product of the expected errors is a scalar matrix, which implies no serial correlation and homoskedasticity.\n\n\n\n\nIt is especially important to make the right assumptions about \\(E[\\epsilon\\epsilon^{\\intercal}|X]\\) as it impacts where our estimate of the standard errors is correct! We will dive into what happens to SE if we violate these assumptions in another post.\nSince we now see that \\(\\hat{\\sigma}^2\\) is a scalar matrix, we can thus simplfy the variance formula further:\n\\[\n\\displaylines{\n\\begin{align}\nV(\\hat{\\beta}^{OLS}) & =\n(X^{\\intercal}X)^{-1}X^{\\intercal}E[\\epsilon\\epsilon^{\\intercal}]X(X^{\\intercal}X)^{-1}\n\\\\ & =\n(X^{\\intercal}X)^{-1}X^{\\intercal} \\hat{\\sigma}^2I X(X^{\\intercal}X)^{-1}\n\\\\ & =\n\\hat{\\sigma}^2\n\\cancel{(X^{\\intercal}X)^{-1}}\n\\cancel{X^{\\intercal} X}\n(X^{\\intercal}X)^{-1}\n\\\\ & = \\hat{\\sigma}^2(X^{\\intercal}X)^{-1}\n\\end{align}\n}\n\\]\n\n\nFormulating an alternative unbiased coefficient\nNext step - lets formulate another estimator, \\(\\hat{\\beta}^{z}\\), which differs from \\(\\hat{\\beta}^{OLS}\\) by a non-zero matrix \\(A\\). See how they both differ below:\n\\[\n\\displaylines{\n\\begin{align}\n\\hat{\\beta}^{OLS} & =(X^{\\intercal}X)^{-1}X^{\\intercal}y \\\\\n\\hat{\\beta}^{Z} & =\\left((X^{\\intercal}X)^{-1}X^{\\intercal}+A\\right)y\n\\end{align}\n}\n\\]\nNow we need to ensure this new estimator is not biased. So by taking the expectation in the same was as for OLS…\n\\[\n\\displaylines{\n\\begin{align}\nE[\\hat{\\beta}^{Z}] & = E\\left[ \\left((X^{\\intercal}X)^{-1}X^{\\intercal}+A\\right)y \\right]\n\\\\ & = \\left((X^{\\intercal}X)^{-1}X^{\\intercal}+A\\right)(X\\beta+ \\cancel{E\\left[\\epsilon \\right]}) & \\because E[\\epsilon] = 0\n\\\\ & = (X^{\\intercal}X)^{-1}X^{\\intercal}X\\beta+AX\\beta\n\\\\ & = \\beta+AX\\beta\n\\end{align}\n}\n\\]\nSo the estimator is only unbiased iff \\(AX=0\\). This is important to note when comparing the variance between unbiased coefficients - see below!\n\n\nVariance of the alternative unbiased coefficient\nJust like before, we calculate the variance:\n\\[\n\\displaylines{\n\\begin{align}\nV[\\hat{\\beta}^{Z}]\n& = V\\left[ \\left((X^{\\intercal}X)^{-1}X^{\\intercal}+A\\right)y \\right]\n\\\\ & = \\left((X^{\\intercal}X)^{-1}X^{\\intercal}+A\\right) V[y] \\left((X^{\\intercal}X)^{-1}X^{\\intercal}+A\\right)^{\\intercal}\n\\\\ & = \\hat{\\sigma}^2 \\left((X^{\\intercal}X)^{-1}X^{\\intercal}+A\\right) \\left((X^{\\intercal}X)^{-1}X^{\\intercal}+A\\right)^{\\intercal}\n& \\because E[\\epsilon \\epsilon^{\\intercal}|X] = 0\n\\\\ & = \\hat{\\sigma}^2\\left((X^{\\intercal}X)^{-1}X^{\\intercal}+A\\right) \\left(X(X^{\\intercal}X)^{-1}+A^{\\intercal}\\right)\n\\\\ & = \\hat{\\sigma}^2 \\left(\n(X^{\\intercal}X)^{-1}X^{\\intercal} X(X^{\\intercal}X)^{-1} + AX(X^{\\intercal}X)^{-1} + (X^{\\intercal}X)^{-1}X^{\\intercal}A^{\\intercal} + AA^{\\intercal}\n\\right)\n\\\\ & = \\hat{\\sigma}^2\n(X^{\\intercal}X)^{-1} + \\hat{\\sigma}^2AA^{\\intercal}\n& \\because AX = 0\n\\\\ & = V[\\beta^{OLS}] + \\hat{\\sigma}^2AA^{\\intercal}\n\\end{align}\n}\n\\]\nNow since AA is surely a positive semi-definite matrix, then we know that \\(V[\\hat{\\beta}^{Z}] &gt; V[\\hat{\\beta}^{OLS}]\\).\nWe have shown that \\(\\hat{\\beta}^{OLS}\\) has the smallest variance among all unbiased estimators!"
  },
  {
    "objectID": "posts/ols_blue.html#summarising-the-gauss-markov-assumptions",
    "href": "posts/ols_blue.html#summarising-the-gauss-markov-assumptions",
    "title": "BLUE coefficients: bias and efficiency",
    "section": "Summarising the Gauss-markov assumptions",
    "text": "Summarising the Gauss-markov assumptions\nAlong the way, we showed where assumptions were needed to ensure the OLS coefficient estimation is BLUE.\nWe actually missed one out, but it is actually quite trivial to see from the OLS coefficient formula:\n\\[\n\\hat{\\beta}^{OLS}=(X^{\\intercal}X)^{-1}X^{\\intercal}y\n\\]\n\n\n\n\n\n\nGM4: Full rank \n\n\n\nThe matrix \\(X\\) must be of full rank \\(k\\), so that it is possible to invert the matrix \\(X^{\\intercal}X\\). This is equivalent to having no perfect multi-collinearity.\n\n\nWe have now collected our full set of Gauss-Markov assumptions required for the OLS coefficient to be BLUE:\n\nLinearity\nStrict Exogeneity\nSpherical Errors\nFull rank"
  },
  {
    "objectID": "posts/techniques_differentiation.html",
    "href": "posts/techniques_differentiation.html",
    "title": "Differentiation formulae",
    "section": "",
    "text": "What we are solving\n\n\n\nDeriving why \\(\\frac{d}{dx} \\left(x^{n} \\right) = nx^{n-1}\\)"
  },
  {
    "objectID": "posts/techniques_differentiation.html#formulating-an-approach-to-the-problem",
    "href": "posts/techniques_differentiation.html#formulating-an-approach-to-the-problem",
    "title": "Differentiation formulae",
    "section": "Formulating an approach to the problem",
    "text": "Formulating an approach to the problem\nThe gradient between two points, \\(x\\) and \\(a\\), is equal to the change in the y axis divided by the change in the x axis.\nFor example, if \\(y=f(x)=x^n\\), then we can approximate the gradient as follows:\n\\[\n\\frac{\\Delta y}{\\Delta x} =\n\\frac{f(x)-f(a)}{x-a} =\n\\frac{x^n-a^n}{x-a}\n\\]\nThis isn’t the same as saying a line which passes through both of these points has the same gradient though. There is a gap between this linear approximation and the exact curve.\nHowever, the smaller the movement across the x-axis, the closer the approximation is to the actual curve. For example, in Figure 1 below, the line drawn between \\(f(1)\\) and \\(f(2)\\) is closer to the gradient at \\(f(1)\\) of the true curve, compared to the line drawn between \\(f(1)\\) and \\(f(3)\\).\n\n\nCode\nimport plotly.graph_objects as go\nimport numpy as np\n\ndef interpolate(n,x1,x2,num=50):\n  x = np.linspace(x1,x2,num)\n  m = (x2**n - x1**n)/(x2 - x1)\n  c = x1**n - m*x1\n  y = m*x+c\n  return({'x':x,'y':y})\n\nn=3\nactual = {'x': np.arange(0,4,0.1)}\nactual['y'] = actual['x']**n\nlb, ub = 1, [3.5,3,2]\nlin0 = interpolate(n,lb,ub[0])\nlin1 = interpolate(n,lb,ub[1])\nlin2 = interpolate(n,lb,ub[2])\n\nfig = go.Figure(data = go.Scatter(mode='lines'))\nfig.add_trace( go.Scatter( x=actual['x'], y=actual['y'], line=dict(dash='solid'), name=f\"f(x)=x^{n}\" ))\nfig.add_trace( go.Scatter( x=lin0['x'], y=lin0['y'], name=f\"f({lb}) -&gt; f({ub[0]})\", line=dict(dash='dot') ))\nfig.add_trace( go.Scatter( x=lin1['x'], y=lin1['y'], name=f\"f({lb}) -&gt; f({ub[1]})\", line=dict(dash='dot') ))\nfig.add_trace( go.Scatter( x=lin2['x'], y=lin2['y'], name=f\"f({lb}) -&gt; f({ub[2]})\", line=dict(dash='dot') ))\nfig.show()\n\n\n\n\n                                                \n\n\nFigure 1: The smaller the change in x, the closer the linear approximation is to the true gradient of f(x)\n\n\n\n\nConsequently, we want to determine the gradient at the limit i.e. where \\(a \\rightarrow x\\), to get the true gradient."
  },
  {
    "objectID": "posts/techniques_differentiation.html#factoring-out-x-a",
    "href": "posts/techniques_differentiation.html#factoring-out-x-a",
    "title": "Differentiation formulae",
    "section": "Factoring out \\((x-a)\\)",
    "text": "Factoring out \\((x-a)\\)\nFirst though, let’s factor out the \\(x-a\\) term for simplicity. Let’s derive a generic formula for this:\n\nIf \\(n=2\\), then \\(x^2-a^2 = (x-a)(x+a)\\)\nIf \\(n=3\\), then \\(x^3-a^3 = (x-a)(x^2+xa+a^2)\\)\nIf \\(n=4\\), then \\(x^4-a^4 = (x-a)(x^3+x^2a+xa^2+a^3)\\)\nIf \\(n=5\\), then \\(x^5-a^5 = (x-a)(x^4+x^3a+x^2a^2+xa^3+a^4)\\)\nAnd so on. In fact for any \\(n\\), we can derive \\(x^n-a^n = (x-a)\\sum_{i=1}^n \\left( x^{n-i}a^{i-1} \\right)\\)\n\nAnd we can now sub that into our formula, and the \\(x-a\\) cancels out:\n\\[\n\\frac{x^n-a^n}{x-a} =\n\\frac{x-a}{x-a} \\sum_{i=1}^n \\left( x^{n-i}a^{i-1} \\right) =\n\\sum_{i=1}^n \\left( x^{n-i}a^{i-1} \\right)\n\\]"
  },
  {
    "objectID": "posts/techniques_differentiation.html#getting-our-result",
    "href": "posts/techniques_differentiation.html#getting-our-result",
    "title": "Differentiation formulae",
    "section": "Getting our result",
    "text": "Getting our result\nAnd now let’s calculate the result in the limit, where \\(x\\) approaches \\(a\\):\n\\[\n\\lim_{x \\rightarrow a} \\sum_{i=1}^n \\left( x^{n-i}a^{i-1} \\right) \\sim\n\\sum_{i=1}^n \\left( a^{n-i}a^{i-1} \\right) =\n\\sum_{i=1}^n \\left( a^{n-1} \\right) =\nna^{n-1}\n\\]\nHence \\(\\frac{d}{dx} \\left(x^{n} \\right) = nx^{n-1}\\).\nFin."
  },
  {
    "objectID": "posts/bayes_lasso.html",
    "href": "posts/bayes_lasso.html",
    "title": "Shrinkage priors for Lasso and Ridge",
    "section": "",
    "text": "What are we exploring?\n\n\n\nShowing Laplace priors in bayesian regression are equivalent to Lasso regularization\n\n\n\n\nBayesian Linear regression:\nWhen performing ordinary linear regression using maximum likelihood, we model the noise around \\(y\\) as being generated from a gaussian distributed process - conditional on the data \\(X\\) and estimated model parameters \\(\\beta\\) and \\(\\sigma\\):\n\\[\n\\displaylines{\n\\begin{align}\np(y|\\beta,X,\\sigma)\n& = \\prod_{i=1}^{n}{\\frac{1}{\\sigma\\sqrt{2\\pi}}}\\exp{\\left\\{-\\frac{1}{2\\sigma^2}\\epsilon_i^2\\right\\}} \\\\\n& \\sim \\prod_{i=1}^{n}{\\frac{1}{\\sigma\\sqrt{2\\pi}}}\\exp{\\left\\{-\\frac{1}{2\\sigma^2}\n\\left( y - X \\beta \\right)^2\n\\right\\}}\n\\end{align}\n}\n\\]\nWe can formulate our estimate of the coefficient in a bayesian way if we model \\(\\beta\\) as a random variable (rather than a fixed quantity as per frequenist thinking):\n\\[\n\\displaylines{\n\\begin{align}\np(\\beta|X,y,\\sigma)\n& =\n\\frac{\n  p(y|\\beta,X,\\sigma)\n  p(\\beta|X,\\sigma)\n  }\n  {p(y|X,\\sigma)}\n\\\\ \\\\ & =\n\\frac{\n  p(y|\\beta,X,\\sigma)\n  p(\\beta|X,\\sigma)\n  }\n{\\int{}{} p(y|X,\\beta,\\sigma)p(\\beta|X,\\sigma) \\mathrm{d}\\beta}\n\\end{align}\n}\n\\]\nWhere:\n\n\\(p(Y|\\beta,X,\\sigma)\\) is the likelihood function\n\\(p(Y|X,\\sigma)\\) is the evidence (the data we feed into the model)\n\\(p(\\beta|X,\\sigma)\\) is the prior for the coefficient\n\nNow in frequentist regression, we assume no prior at all: \\(\\beta\\) is fixed, so \\(p(\\beta|X,\\sigma)=p(\\beta)=1\\), regardless of the evidence observed. Thus we are just left with finding the coefficients that maximise \\(p(Y|\\beta,X,\\sigma)\\).\nBy taking the negative log likelihood, we find this is identical to finding the coefficient values that minimise the sum of squared residuals (see the derivation here):\n\\[\n\\displaylines{\n\\begin{align}\n\\beta^* & =\\arg\\min_\\beta{\\left[\n  \\sum_{i=1}^N{\\epsilon_i^2}\n\\right]}\n\\end{align}\n}\n\\]\nHowever, we could use other types of priors, with mass around zero, to apply regularization on our coefficients.\n\n\nUsing Laplace priors to shrink coefficients:\nRegularization aims to eliminate some of our predictors to create a more parsimonious model in a systematic way, and/or reduce their magnitude to prevent overfitting.\nPicking a prior for our coefficient that is concentrated at zero can help achieve this - for example, we could use a Laplace distribution, with a location parameter \\(\\mu\\) of zero as visualised below:\n\n\nCode\nlaplace_dist &lt;- function(x, mu = 0, gamma = 1) {\n  laplace_pdf &lt;- function(x,mu,gamma) {\n    return(\n      exp(-abs(x-mu)/gamma)/(2*gamma)\n    )\n  }\n  y = sapply(x, FUN = function(i) laplace_pdf(i,mu,gamma))\n  return(y)\n}\n\nx = seq(-10,10,0.1)\nplot(\n  x=x,\n  y=laplace_dist(x, gamma=1),\n  type='l',\n  main='Laplace(gamma=0.5)'\n  )\n\n\n\n\n\n\n\n\n\nNot only does the Laplace pdf increase when closer to zero, but it increases at an accelerating rate. Thus, we can imagine that the closer the likelihood estimate of the coefficient is to zero, the greater the influence of the prior.\n\nLaplace priors ~ Lasso Regression\nRecall that the probability density function of Laplace is \\[\nf(x|\\mu,\\gamma) =\n\\frac{1}{2\\gamma}\n\\exp{ \\left\\{\n  \\frac{x - \\mu}{\\gamma} \\right\n\\} }\n\\]\nThen the prior can be written as the maximum likelihood across the estimated coefficients each of the \\(K\\) features in the model, for a given penalty importance \\(\\lambda\\): \\[\n\\displaylines{\n\\begin{align}\np(\\beta|X,\\sigma) \\sim p(\\beta)\n& = \\prod_{k=1}^{K}{\n  \\frac{1}{2 \\gamma}\n  \\exp{\\left\\{ \\frac{ -\\mid \\beta - \\mu \\mid}{\\gamma} \\right\\}}\n}\n\\\\ \\\\ & = \\prod_{k=1}^{K}{\n  \\frac{1}{2 \\gamma}\n  \\exp{\\left\\{ \\frac{ -\\mid \\beta \\mid}{\\gamma} \\right\\}}\n}\n\\end{align}\n}\n\\]\n\n\nThis is how the prior is defined in Tibshirani (1996). Since the prior is assumed before any data is observed, intuitively \\(\\beta\\) should not need conditioning on \\(X\\).\nHowever, Park and Casella (2008) found that not conditioning on \\(\\sigma^2\\) can result in non-unimodal posterior, so in practice a non-informative scale-invariant marginal prior \\(\\pi(\\sigma^2) = 1/\\sigma\\) on \\(\\sigma^2\\) is used.\nNow if set this as the prior, we can derive the cost function that we aim to minimize when \\(X\\) is observed.\n\\[\n\\displaylines{\n\\begin{align}\n\\max_{\\beta} {\\left\\{ p(\\beta|y,X,\\sigma) \\right\\}}\n& = \\max_\\beta{\n  \\left[ p(y|\\beta,X,\\sigma) \\times p(\\beta)\\right]\n} \\\\\n& \\sim \\min_\\beta{\n  \\left\\{ -\\log{\n    \\left[ p(y|\\beta,X,\\sigma) \\times p(\\beta)\\right]\n    }\\right\\}\n} \\\\\n& = \\min_\\beta{\n  \\left\\{\n    -\\log{\\left[ p(y|\\beta,X,\\sigma) \\right]\n    -\\log{\\left[p(\\beta)\\right]}\n    }\\right\\}\n} \\\\\n& = \\min_\\beta \\left\\{\n\\frac{1}{2} \\left(\n  \\cancel{ n \\log{[2 \\pi \\sigma^2]} }\n  + \\frac{1}{\\sigma^2} \\sum_{i=1}^N{\\epsilon_i^2}\n\\right)\n- \\log{\\left[\n  \\prod_{k=1}^{K}{\n    \\frac{1}{2\\gamma}\n    \\exp{\\left\\{ \\frac{ -\\mid \\beta \\mid}{\\gamma} \\right \\}}\n  }\n\\right]}\n\\right\\} \\\\\n& = \\min_\\beta \\left\\{\n  \\frac{1}{2\\sigma^2} \\sum_{i=1}^N{\\epsilon_i^2}\n- \\sum_{k=1}^{K}{ \\log{ \\left[\n    \\frac{1}{2\\gamma}\n    \\exp{ \\left\\{ \\frac{ -\\mid \\beta \\mid}{\\gamma} \\right\\} }\n    \\right]\n    }\n  }\n\\right\\} \\\\\n& = \\min_\\beta \\left\\{\n  \\frac{1}{2\\sigma^2} \\sum_{i=1}^N{\\epsilon_i^2}\n- \\cancel{ K \\log{ \\left[ \\frac{1}{2\\gamma} \\right] } }\n- \\sum_{k=1}^{K}{\n    \\log{ \\left[ \\exp{\\left\\{ \\frac{ -\\mid \\beta \\mid}{\\gamma} \\right\\}} \\right] }\n  }  \n\\right\\} \\\\\n& = \\min_\\beta \\left\\{\n  \\frac{1}{2\\sigma^2} \\sum_{i=1}^N{\\epsilon_i^2}\n+ \\frac{1}{\\gamma} \\sum_{k=1}^{K}{ \\mid \\beta \\mid}  \n\\right\\} \\\\ \\\\\n\\therefore \\beta^*\n& =\\arg\\min_\\beta{\\left\\{\n  \\frac{1}{2\\sigma^2} \\sum_{i=1}^N{\\epsilon_i^2}\n+ \\frac{1}{\\gamma}\\sum_{k=1}^{K}{\\mid\\beta\\mid}\n  \\right\\}\n}\n\\end{align}\n}\n\\]\n\n\nNote that terms that do not vary with the choice of \\(\\beta\\) drop out\n\n\n\n\n\n\nKey point\n\n\n\nHence setting a Laplace prior on the coefficients is - almost - equivalent to running L1 regularization, where \\(1/\\gamma\\) is the parameter influencing the penalty size.\n\n\nAlthough there is sparsity in the mode of the prior, when combining this with the likelihood distribution, the sparsity will not continue into the posterior distribution. In practice, if the posterior of \\(\\beta\\) is sufficiently small, we would want to drop it - so a threshold value for the size at which coefficients are zero-oed out is set as a hyperparameter.\n\n\n\nUsing Gaussian priors to shrink coefficients:\nIn a similar way to before for Lasso, we set our coefficient priors to each have a Gaussian distribution, with a location parameter \\(\\mu=0\\):\n\n\nCode\nnormal_pdf &lt;- function(x, mu = 0, sigma = 2) {\n  z = (x-mu)/sigma\n  y = (2*pi*sigma^2)^(-1/2) * exp(-0.5*z^2)\n  return(y)\n}\n\nx = seq(-10,10,0.1)\nplot(\n  x=x,\n  y=normal_pdf(x),\n  type='l',\n  main='Normal(sigma=1)'\n  )\n\n\n\n\n\n\n\n\n\n\nGaussian priors ~ Ridge Regression\nThe gaussian prior can be written as the maximum likelihood across the estimated coefficients each of the \\(K\\) features in the model, for a given penalty importance \\(\\lambda\\):\n\\[\n\\displaylines{\n\\begin{align}\np(\\beta|X,\\sigma) \\sim p(\\beta)\n& = \\prod_{k=1}^{K}{\\frac{1}{\\sigma\\sqrt{2\\pi}}}\\exp{\\left\\{-\\frac{1}{2\\sigma^2}\\beta_k^2\\right\\}}\n\\end{align}\n}\n\\]\nNow if set this as the prior, we can derive the cost function that we aim to minimize when \\(X\\) is observed.\n\\[\n\\displaylines{\n\\begin{align}\n\\max_{\\beta} {\\left\\{ p(\\beta|y,X,\\sigma) \\right\\}}\n& \\sim \\min_\\beta{\n  \\left\\{\n    -\\log{\\left[ p(y|\\beta,X,\\sigma) \\right]\n    -\\log{\\left[p(\\beta)\\right]}\n    }\\right\\}\n} \\\\\n& = \\min_\\beta \\left\\{\n\\frac{1}{2} \\left(\n  \\cancel{ n \\log{[2 \\pi \\sigma^2]} }\n  + \\frac{1}{\\sigma^2} \\sum_{i=1}^N{\\epsilon_i^2}\n\\right)\n- \\log{\\left[\n  \\prod_{k=1}^{K}{\n    \\frac{1}{\\sigma\\sqrt{2\\pi}}\n    \\exp{\\left\\{-\\frac{1}{2\\sigma^2}\\beta_k^2\\right\\}}\n    }\n\\right]}\n\\right\\} \\\\\n& = \\min_\\beta \\left\\{\n  \\frac{1}{2\\sigma^2} \\sum_{i=1}^N{\\epsilon_i^2}\n- \\sum_{k=1}^{K}{ \\log{ \\left[\n    \\frac{1}{\\sigma\\sqrt{2\\pi}}\n    \\exp{\\left\\{-\\frac{1}{2\\sigma^2}\\beta_k^2\\right\\}}\n    \\right]\n    }\n  }\n\\right\\} \\\\\n& = \\min_\\beta \\left\\{\n  \\frac{1}{2\\sigma^2} \\sum_{i=1}^N{\\epsilon_i^2}\n- \\cancel{ K \\log{ \\left[ \\frac{1}{\\sigma\\sqrt{2\\pi}} \\right] } }\n- \\sum_{k=1}^{K}{\n    \\log{ \\left[ \\exp{\\left\\{ -\\frac{1}{2\\sigma^2}\\beta_k^2 \\right\\}} \\right] }\n  }  \n\\right\\} \\\\\n& = \\min_\\beta \\left\\{\n  \\frac{1}{2\\sigma^2} \\sum_{i=1}^N{\\epsilon_i^2}\n+ \\frac{1}{2\\sigma^2} \\sum_{k=1}^{K}{ \\beta^2 }  \n\\right\\} \\\\ \\\\\n\\therefore \\beta^*\n& =\\arg\\min_\\beta{\\left\\{\n  \\frac{1}{2\\sigma^2} \\sum_{i=1}^N{\\epsilon_i^2}\n+ \\frac{1}{2\\sigma^2} \\sum_{k=1}^{K}{ \\beta^2 }\n  \\right\\}\n}\n\\end{align}\n}\n\\]\n\n\n\n\n\n\nKey point\n\n\n\nHence setting a Gaussian prior on the coefficients is - almost - equivalent to running L1 regularization, where the variance of the prior - \\(\\sigma^2\\) is the parameter directly influencing the penalty size.\n\n\n\n\n\n\n\n\nReferences\n\nPark, Trevor, and George Casella. 2008. “The Bayesian Lasso.” Journal of the American Statistical Association 103 (482): 681–86. https://doi.org/10.1198/016214508000000337.\n\n\nTibshirani, Robert. 1996. “Regression Shrinkage and Selection via the Lasso.” Journal of the Royal Statistical Society: Series B (Methodological) 58 (1): 267–88. https://doi.org/10.1111/j.1467-9868.2011.00771.x."
  },
  {
    "objectID": "posts/bayes_regularization.html",
    "href": "posts/bayes_regularization.html",
    "title": "Shrinkage priors for Lasso and Ridge",
    "section": "",
    "text": "What are we exploring?\n\n\n\nShowing Laplace and Gaussian priors in Bayesian regression are equivalent to Lasso and Ridge regularization\n\n\n\n\nBayesian Linear regression:\nWhen performing ordinary linear regression using maximum likelihood, we model the noise around \\(y\\) as being generated from a gaussian distributed process - conditional on the data \\(X\\) and estimated model parameters \\(\\beta\\) and \\(\\sigma\\):\n\\[\n\\displaylines{\n\\begin{align}\np(y|\\beta,X,\\sigma)\n& = \\prod_{i=1}^{n}{\\frac{1}{\\sigma\\sqrt{2\\pi}}}\\exp{\\left\\{-\\frac{1}{2\\sigma^2}\\epsilon_i^2\\right\\}} \\\\\n& \\sim \\prod_{i=1}^{n}{\\frac{1}{\\sigma\\sqrt{2\\pi}}}\\exp{\\left\\{-\\frac{1}{2\\sigma^2}\n\\left( y - X \\beta \\right)^2\n\\right\\}}\n\\end{align}\n}\n\\]\nWe can formulate our estimate of the coefficient in a bayesian way if we model \\(\\beta\\) as a random variable (rather than a fixed quantity as per frequenist thinking):\n\\[\n\\displaylines{\n\\begin{align}\np(\\beta|X,y,\\sigma)\n& =\n\\frac{\n  p(y|\\beta,X,\\sigma)\n  p(\\beta|X,\\sigma)\n  }\n  {p(y|X,\\sigma)}\n\\\\ \\\\ & =\n\\frac{\n  p(y|\\beta,X,\\sigma)\n  p(\\beta|X,\\sigma)\n  }\n{\\int{}{} p(y|X,\\beta,\\sigma)p(\\beta|X,\\sigma) \\mathrm{d}\\beta}\n\\end{align}\n}\n\\]\nWhere:\n\n\\(p(Y|\\beta,X,\\sigma)\\) is the likelihood function\n\\(p(Y|X,\\sigma)\\) is the evidence (the data we feed into the model)\n\\(p(\\beta|X,\\sigma)\\) is the prior for the coefficient\n\nNow in frequentist regression, we assume no prior at all: \\(\\beta\\) is fixed, so \\(p(\\beta|X,\\sigma)=p(\\beta)=1\\), regardless of the evidence observed. Thus we are just left with finding the coefficients that maximise \\(p(Y|\\beta,X,\\sigma)\\).\nBy taking the negative log likelihood, we find this is identical to finding the coefficient values that minimise the sum of squared residuals (see the derivation here):\n\\[\n\\displaylines{\n\\begin{align}\n\\beta^* & =\\arg\\min_\\beta{\\left[\n  \\sum_{i=1}^N{\\epsilon_i^2}\n\\right]}\n\\end{align}\n}\n\\]\nHowever, we could use other types of priors, with mass around zero, to apply regularization on our coefficients.\n\n\nUsing Laplace priors to shrink coefficients:\nRegularization aims to eliminate some of our predictors to create a more parsimonious model in a systematic way, and/or reduce their magnitude to prevent overfitting.\nPicking a prior for our coefficient that is concentrated at zero can help achieve this - for example, we could use a Laplace distribution, with a location parameter \\(\\mu\\) of zero as visualised below:\n\n\nCode\nlaplace_dist &lt;- function(x, mu = 0, gamma = 1) {\n  laplace_pdf &lt;- function(x,mu,gamma) {\n    return(\n      exp(-abs(x-mu)/gamma)/(2*gamma)\n    )\n  }\n  y = sapply(x, FUN = function(i) laplace_pdf(i,mu,gamma))\n  return(y)\n}\n\nx = seq(-10,10,0.1)\nplot(\n  x=x,\n  y=laplace_dist(x, gamma=1),\n  type='l',\n  main='Laplace(gamma=0.5)'\n  )\n\n\n\n\n\n\n\n\n\nNot only does the Laplace pdf increase when closer to zero, but it increases at an accelerating rate. Thus, we can imagine that the closer the likelihood estimate of the coefficient is to zero, the greater the influence of the prior.\n\nLaplace priors ~ Lasso Regression\nRecall that the probability density function of Laplace is \\[\nf(x|\\mu,\\gamma) =\n\\frac{1}{2\\gamma}\n\\exp{ \\left\\{\n  \\frac{x - \\mu}{\\gamma} \\right\n\\} }\n\\]\nThen the prior can be written as the maximum likelihood across the estimated coefficients each of the \\(K\\) features in the model, for a given penalty importance \\(\\lambda\\): \\[\n\\displaylines{\n\\begin{align}\np(\\beta|X,\\sigma) \\sim p(\\beta)\n& = \\prod_{k=1}^{K}{\n  \\frac{1}{2 \\gamma}\n  \\exp{\\left\\{ \\frac{ -\\mid \\beta - \\mu \\mid}{\\gamma} \\right\\}}\n}\n\\\\ \\\\ & = \\prod_{k=1}^{K}{\n  \\frac{1}{2 \\gamma}\n  \\exp{\\left\\{ \\frac{ -\\mid \\beta \\mid}{\\gamma} \\right\\}}\n}\n\\end{align}\n}\n\\]\n\n\nThis is how the prior is defined in Tibshirani (1996). Since the prior is assumed before any data is observed, intuitively \\(\\beta\\) should not need conditioning on \\(X\\).\nHowever, Park and Casella (2008) found that not conditioning on \\(\\sigma^2\\) can result in non-unimodal posterior, so in practice a non-informative scale-invariant marginal prior \\(\\pi(\\sigma^2) = 1/\\sigma\\) on \\(\\sigma^2\\) is used.\nNow if set this as the prior, we can derive the cost function that we aim to minimize when \\(X\\) is observed.\n\\[\n\\displaylines{\n\\begin{align}\n\\max_{\\beta} {\\left\\{ p(\\beta|y,X,\\sigma) \\right\\}}\n& = \\max_\\beta{\n  \\left[ p(y|\\beta,X,\\sigma) \\times p(\\beta)\\right]\n} \\\\\n& \\sim \\min_\\beta{\n  \\left\\{ -\\log{\n    \\left[ p(y|\\beta,X,\\sigma) \\times p(\\beta)\\right]\n    }\\right\\}\n} \\\\\n& = \\min_\\beta{\n  \\left\\{\n    -\\log{\\left[ p(y|\\beta,X,\\sigma) \\right]\n    -\\log{\\left[p(\\beta)\\right]}\n    }\\right\\}\n} \\\\\n& = \\min_\\beta \\left\\{\n\\frac{1}{2} \\left(\n  \\cancel{ n \\log{[2 \\pi \\sigma^2]} }\n  + \\frac{1}{\\sigma^2} \\sum_{i=1}^N{\\epsilon_i^2}\n\\right)\n- \\log{\\left[\n  \\prod_{k=1}^{K}{\n    \\frac{1}{2\\gamma}\n    \\exp{\\left\\{ \\frac{ -\\mid \\beta \\mid}{\\gamma} \\right \\}}\n  }\n\\right]}\n\\right\\} \\\\\n& = \\min_\\beta \\left\\{\n  \\frac{1}{2\\sigma^2} \\sum_{i=1}^N{\\epsilon_i^2}\n- \\sum_{k=1}^{K}{ \\log{ \\left[\n    \\frac{1}{2\\gamma}\n    \\exp{ \\left\\{ \\frac{ -\\mid \\beta \\mid}{\\gamma} \\right\\} }\n    \\right]\n    }\n  }\n\\right\\} \\\\\n& = \\min_\\beta \\left\\{\n  \\frac{1}{2\\sigma^2} \\sum_{i=1}^N{\\epsilon_i^2}\n- \\cancel{ K \\log{ \\left[ \\frac{1}{2\\gamma} \\right] } }\n- \\sum_{k=1}^{K}{\n    \\log{ \\left[ \\exp{\\left\\{ \\frac{ -\\mid \\beta \\mid}{\\gamma} \\right\\}} \\right] }\n  }  \n\\right\\} \\\\\n& = \\min_\\beta \\left\\{\n  \\frac{1}{2\\sigma^2} \\sum_{i=1}^N{\\epsilon_i^2}\n+ \\frac{1}{\\gamma} \\sum_{k=1}^{K}{ \\mid \\beta \\mid}  \n\\right\\} \\\\ \\\\\n\\therefore \\beta^*\n& =\\arg\\min_\\beta{\\left\\{\n  \\frac{1}{2\\sigma^2} \\sum_{i=1}^N{\\epsilon_i^2}\n+ \\frac{1}{\\gamma}\\sum_{k=1}^{K}{\\mid\\beta\\mid}\n  \\right\\}\n}\n\\end{align}\n}\n\\]\n\n\nNote that terms that do not vary with the choice of \\(\\beta\\) drop out\n\n\n\n\n\n\nKey point\n\n\n\nHence setting a Laplace prior on the coefficients is - almost - equivalent to running L1 regularization, where \\(1/\\gamma\\) is the parameter influencing the penalty size.\n\n\nAlthough there is sparsity in the mode of the prior, when combining this with the likelihood distribution, the sparsity will not continue into the posterior distribution. In practice, if the posterior of \\(\\beta\\) is sufficiently small, we would want to drop it - so a threshold value for the size at which coefficients are zero-oed out is set as a hyperparameter.\n\n\n\nUsing Gaussian priors to shrink coefficients:\nIn a similar way to before for Lasso, we set our coefficient priors to each have a Gaussian distribution, with a location parameter \\(\\mu=0\\):\n\n\nCode\nnormal_pdf &lt;- function(x, mu = 0, sigma = 2) {\n  z = (x-mu)/sigma\n  y = (2*pi*sigma^2)^(-1/2) * exp(-0.5*z^2)\n  return(y)\n}\n\nx = seq(-10,10,0.1)\nplot(\n  x=x,\n  y=normal_pdf(x),\n  type='l',\n  main='Normal(sigma=1)'\n  )\n\n\n\n\n\n\n\n\n\n\nGaussian priors ~ Ridge Regression\nThe gaussian prior can be written as the maximum likelihood across the estimated coefficients each of the \\(K\\) features in the model, for a given penalty importance \\(\\lambda\\):\n\\[\n\\displaylines{\n\\begin{align}\np(\\beta|X,\\sigma) \\sim p(\\beta)\n& = \\prod_{k=1}^{K}{\\frac{1}{\\sigma\\sqrt{2\\pi}}}\\exp{\\left\\{-\\frac{1}{2\\sigma^2}\\beta_k^2\\right\\}}\n\\end{align}\n}\n\\]\nNow if set this as the prior, we can derive the cost function that we aim to minimize when \\(X\\) is observed.\n\\[\n\\displaylines{\n\\begin{align}\n\\max_{\\beta} {\\left\\{ p(\\beta|y,X,\\sigma) \\right\\}}\n& \\sim \\min_\\beta{\n  \\left\\{\n    -\\log{\\left[ p(y|\\beta,X,\\sigma) \\right]\n    -\\log{\\left[p(\\beta)\\right]}\n    }\\right\\}\n} \\\\\n& = \\min_\\beta \\left\\{\n\\frac{1}{2} \\left(\n  \\cancel{ n \\log{[2 \\pi \\sigma^2]} }\n  + \\frac{1}{\\sigma^2} \\sum_{i=1}^N{\\epsilon_i^2}\n\\right)\n- \\log{\\left[\n  \\prod_{k=1}^{K}{\n    \\frac{1}{\\sigma\\sqrt{2\\pi}}\n    \\exp{\\left\\{-\\frac{1}{2\\sigma^2}\\beta_k^2\\right\\}}\n    }\n\\right]}\n\\right\\} \\\\\n& = \\min_\\beta \\left\\{\n  \\frac{1}{2\\sigma^2} \\sum_{i=1}^N{\\epsilon_i^2}\n- \\sum_{k=1}^{K}{ \\log{ \\left[\n    \\frac{1}{\\sigma\\sqrt{2\\pi}}\n    \\exp{\\left\\{-\\frac{1}{2\\sigma^2}\\beta_k^2\\right\\}}\n    \\right]\n    }\n  }\n\\right\\} \\\\\n& = \\min_\\beta \\left\\{\n  \\frac{1}{2\\sigma^2} \\sum_{i=1}^N{\\epsilon_i^2}\n- \\cancel{ K \\log{ \\left[ \\frac{1}{\\sigma\\sqrt{2\\pi}} \\right] } }\n- \\sum_{k=1}^{K}{\n    \\log{ \\left[ \\exp{\\left\\{ -\\frac{1}{2\\sigma^2}\\beta_k^2 \\right\\}} \\right] }\n  }  \n\\right\\} \\\\\n& = \\min_\\beta \\left\\{\n  \\frac{1}{2\\sigma^2} \\sum_{i=1}^N{\\epsilon_i^2}\n+ \\frac{1}{2\\sigma^2} \\sum_{k=1}^{K}{ \\beta^2 }  \n\\right\\} \\\\ \\\\\n\\therefore \\beta^*\n& =\\arg\\min_\\beta{\\left\\{\n  \\frac{1}{2\\sigma^2} \\sum_{i=1}^N{\\epsilon_i^2}\n+ \\frac{1}{2\\sigma^2} \\sum_{k=1}^{K}{ \\beta^2 }\n  \\right\\}\n}\n\\end{align}\n}\n\\]\n\n\n\n\n\n\nKey point\n\n\n\nHence setting a Gaussian prior on the coefficients is - almost - equivalent to running L1 regularization, where the variance of the prior - \\(\\sigma^2\\) is the parameter directly influencing the penalty size.\n\n\n\n\n\n\n\n\nReferences\n\nPark, Trevor, and George Casella. 2008. “The Bayesian Lasso.” Journal of the American Statistical Association 103 (482): 681–86. https://doi.org/10.1198/016214508000000337.\n\n\nTibshirani, Robert. 1996. “Regression Shrinkage and Selection via the Lasso.” Journal of the Royal Statistical Society: Series B (Methodological) 58 (1): 267–88. https://doi.org/10.1111/j.1467-9868.2011.00771.x."
  },
  {
    "objectID": "posts/glm_logistic.html",
    "href": "posts/glm_logistic.html",
    "title": "Deriving Logistic Regression Coefficients",
    "section": "",
    "text": "What are we exploring?\n\n\n\nDeriving coefficients to predict the likelihood of a binary outcome using maximum likelihood estimation and the logit link funciton.\n\n\n\nThe bernoulli PMF\nImagine a basketball player is taking free throws.\nFor each trial \\(i\\) (a freethrow attempt), we observe the outcome \\(y_i\\) as being 1 if there is a success (i.e. the freethrow is made), or 0 if it is a failure (a miss).\nWe can denote \\(p\\) as the player’s fixed probability for success (the likelihood they make the free throw, e.g. 90%). It follows that the probability of failure as \\(1-p\\) (i.e. a 10% chance of missing).\nThis can be formalised as the following (the probability mass function of the bernoulli distribution):\n\\[\n\\displaylines{\n\\begin{align}\nP(y_i) =\n\\begin{cases}\n  p & \\text{if}\\ y_i=1 \\\\\n  1-p & \\text{if}\\ y_i=0\n\\end{cases}\n\\end{align}\n}\n\\]\nWhich is equivalent to the following:\n\\[\nP(y_i) = p^{y_i}(1-p)^{1-y_i}\n\\]\n\n\nSince \\(g(x)^0 = 1\\):\n\nif \\(y_i=1\\), \\(p^{1}(1-p)^{0} = p\\)\nif \\(y_i=0\\), \\(p^{0}(1-p)^{1} = 1-p\\)\n\nNow imagine that, rather than \\(p\\) being fixed for all attempts, there are some external variables that influence the shooter (for example, whether the game is at home or away). Let’s denote these relevant variables \\(X\\).\nWe can then denote the probability of success as the following:\n\\[\nP(y_i|X_i) = P(y_i=1|X_i)^{y_i}(1-P(y_i=1|X_i))^{1-y_i}\n\\]\n\n\nMaximum Likelihood\nImagine we observe the player make the first two freethrows but miss the third. If we guess the probability of them scoring is 80%, then the chance of what we observed occuring in that order is \\(80\\%\\times80\\%\\times20\\%=12.8\\%\\). They could also have got 2/3 by missing the first (scoring the last two) or missing the second (scoring first and third). So the overall probability is \\(3\\times8.1\\%=38.4\\%\\)\nWe might make a better guess - say that the probability of them scoring is 67%, since they made 2/3. Then the probability of what we observed occuring is \\(3\\times(67\\%\\times67\\%\\times33\\%)=44\\%\\).\nWhat we have done is estimated the maximum likelihood - the value of \\(\\hat{p}\\) that maximises the chance of observing the outcomes saw.\nHowever, in our problem,\n\nBut\n\n\\[\np(y|X\\beta) =\n\\underbrace{\n  \\prod_{y_i=0}^{N}{ p(y_i=1|X_i\\beta)^{y_i} }\n}_{y_i=1} \\times\n\\underbrace{\n  \\prod_{y_i=1}^{N}{ (1-p(y_i=1|X_i\\beta))^{1-y_i} }    \n}_{y_i=0}\n\\]\n\n\n\n\nLet’s split the outcomes between successes and failures. We thus derive the cost function:\n\n\nTaking the negative log-likelihood\nIn practice, dealing with a cost function made up of a sum product is tricky - it is easier to take the log and deal with addition instead. Further, rather than maximise, it is common to “minimize” cost functions, so the negative log-likelihood is usually used.\n\n\nRecall that \\(\\log{\\left(ab\\right)} = \\log{\\left(a\\right)} + \\log{\\left(b\\right)}\\)\n\\[\n\\displaylines{\n\\begin{align}\n& \\max_\\beta{p(y|\\beta,X)} \\\\\n= &\n\\max_\\beta{\\left\\{\n  \\prod_{i=1}^{N}{ p(y_i=1|X_i\\beta)^{y_i} }\n  \\times\n  \\prod_{i=1}^{N}{ (1-p(y_i=1|X_i\\beta))^{1-y_i} }\n  \\right\\}}\n\\\\ \\\\ \\Rightarrow &\n\\min_\\beta{\\left\\{ -\\log{ \\left[\n  \\prod_{i=1}^{N}{ p(y_i=1|X_i\\beta)^{y_i} }\n  \\times\n  \\prod_{i=1}^{N}{ (1-p(y_i=1|X_i\\beta))^{1-y_i} }\n  \\right] } \\right\\}}\n\\\\ \\\\ = &\n\\min_\\beta{\\left\\{\n  \\sum_{i=1}^{N}{ -\\log{ \\left[ p(y_i=1|X_i\\beta)^{y_i} \\right] } } +\n  \\sum_{i=1}^{N}{ -\\log{ \\left[ (1-p(y_i=1|X_i\\beta))^{1-y_i}\\right] } }\n  \\right\\}}  \n\\\\ \\\\ = &\n\\min_\\beta{\\left\\{\n  \\sum_{i=1}^{N}{ -y_i\\log{ \\left[ p(y_i=1|X_i\\beta) \\right] } } +\n  \\sum_{i=1}^{N}{ -(1-y_i)\\log{ \\left[ 1-p(y_i=1|X_i\\beta)\\right] } }\n  \\right\\}}  \n\\end{align}\n}\n\\]\n\n\n\nModelling the probability\n\nLogistic activation function\nWe might assume that the the log-odds - the logarithm of the probability of success divided by the probability of failure - is linearly related to its predictors, i.e.\n\\[\n\\text{logit}(E[Y_i|X_i]) = \\text{logit}(p_i) =\n\\ln{\\left(\\frac{p_i}{1-p_i}\\right)} = X_i \\beta\n\\]\nThis is called a “link function” - the link between the outcome, \\(y\\), and the linear predictors \\(X\\beta\\). This specific link function is called the “logit link function”.\nTo make predictions then for the probability of success, we need the inverse of the link function - sometimes called the “activation function” in the context of neural network.\nWe can derive the inverse of the logit link by rearranging it in terms of \\(p\\):\n\\[\n\\displaylines{\n\\begin{align}\n\\ln{\\left(\\frac{p}{1-p}\\right)} = X\\beta\n& \\Rightarrow \\frac{p}{1-p}\\ = \\exp{\\{X\\beta\\}}\n\\\\ \\\\\n& \\Rightarrow p = \\exp{\\{X\\beta\\}}(1-p)\n\\\\ \\\\\n& \\Rightarrow p - (1+\\exp{\\{X\\beta\\}}) = \\exp{\\{X\\beta\\}}\n\\\\ \\\\\n& \\Rightarrow p = \\frac{\\exp{\\{X\\beta\\}}}{1 + \\exp{\\{X\\beta\\}}}\n=  \\left( 1 + \\exp{\\{-X\\beta\\}} \\right)^{-1}\n\\end{align}\n}\n\\]\nWe can see that this activation function “squashes” all outputs \\(X\\beta \\in [-\\infty,\\infty]\\) between 0 and 1:\n\n\nCode\nlogistic_dist &lt;- function(x) {\n  return( (1+exp(-x))^-1 )\n}\nx = seq(-10,10,0.1)\nplot(\n  x=x,\n  y=logistic_dist(x),\n  type='l',col=\"blue\",,lty=1\n  )\nlines(\n  x=seq(-3,3,0.1),\n  y=(1/4)*seq(-3,3,0.1)+0.5,\n  type='l',col=\"red\",lty=2\n)\nlegend(\n  -10,1,\n  legend=c(\n    \"logistic activation\",\n    \"linear activation\"\n    ),\n  col=c(\"blue\",\"red\"),\n  lty=c(1,2)\n  )\n\n\n\n\n\n\n\n\n\n\n\nFor probabilities of between 0.3 to 0.7, we see that the logistic activation function maps very closely to that of a simply linear one. It is only at the more extreme probabilities that they diverge.\n\n\nA latent variable model\nEconomists often frame binary regression as a “latent variable model”. What this means is they frame the problem as though there is a hidden continuous variable we do not observe, \\(y_i^*\\), and if it exceeds a certain threshold (usually zero) then there is a success:\n\\[\n\\displaylines{\n\\begin{align}\ny_i^* & = X_i\\beta + \\epsilon_i\n\\\\ \\\\\ny_i & =\n\\begin{cases}\n  1 & \\text{if}\\ y_i^* &gt; 0 & \\text{i.e. } -\\epsilon_i &gt; X_i\\beta\\\\\n  0 & \\text{otherwise}\n\\end{cases}\n\\end{align}\n}\n\\]\nIn other words, the latent variable \\(y_i^*\\) is purely a function of its predictors \\(X_i\\), their relationship to \\(p_i\\) given by \\(\\beta\\), and additive noise \\(\\epsilon_i\\). If \\(y_i^*\\) is positive, we observe a success (where \\(y_i = 1\\)).\nWe can thus formulate \\(P(y_i=1|X)\\) as the likelihood that the addtive noise \\(\\epsilon_i\\) is less than \\(X\\beta\\), resulting in the probability \\(y_i^*\\) above zero:\n\\[\n\\displaylines{\n\\begin{align}\nP(y_i=1|X_i)\n& = P(y_i^* &gt; 0 | X_i)\n\\\\\n& = P(X_i\\beta + \\epsilon_i &gt; 0)\n\\\\\n& = P(\\epsilon_i &gt; -X_i\\beta )\n\\\\\n& = P(\\epsilon_i &lt; X_i\\beta ) & \\iff \\epsilon \\sim f(\\mu,s) \\text{ is symmetric}\n\\end{align}\n}\n\\]\nSo a good assumption for the probabilistic process that generates the errors \\(\\epsilon\\) is very important!\nThe error is often assumed to be generated from a logistic distribution, with location parameter \\(\\mu=0\\) and scale parameter \\(s=1\\):\n\\[\n\\epsilon \\sim \\text{Logistic}(0,1)\n\\]\nRecall the pdf of the logistic distribution is:\n\\[\n\\displaylines{\n\\begin{align}\nf(x,\\mu,s) & =\n\\frac{\\exp{\\left\\{ -(x-\\mu)/s \\right\\}}}\n{s(1+\\exp{\\left\\{ -(x-\\mu)/s \\right\\})^2}}\n\\\\ \\\\\n\\therefore f(x,0,1) & =\n\\frac{\\exp{\\left\\{ -x \\right\\}}}\n{(1+\\exp{\\left\\{ -x \\right\\})^2}}\n\\end{align}\n}\n\\]\nThen the CDF is the integral of the pdf: \\[\n\\displaylines{\n\\begin{align}\n\\int {f(x,0,1) \\,dx} & =\n\\int{\n  \\frac{e^{-x}}\n  {(1+e^{-x})^2}\n\\,dx}\n\\\\ \\\\\n\\text{let } u = 1+e^{-x} & \\therefore \\frac{du}{dx} = -e^{-x}\n\\\\\n& \\therefore dx = -\\frac{du}{e^x}\n\\\\ \\\\\n\\therefore\n\\int {f(x,0,1) \\,dx} & =\n\\int {\\frac{e^x}{u^2} \\times -\\frac{du}{e^x}}\n\\\\\n& = \\int {-u^{-2}\\,du}\n\\\\\n& = u^{-1} + c\n\\\\\n& = (1+\\exp{\\{-x\\}})^{-1} + c\n% \\therefore\n% P(y_i=1|X_i) & = P(X_i\\beta + \\epsilon_i &gt; 0)\n% \\\\ & =\n% [(1+\\exp{\\{-(X_i\\beta+\\epsilon_i)\\}})^{-1} + c] -\n% [(1+\\exp{\\{-0\\}})^{-1} + c]\n% \\\\ & =\n% [(1+\\exp{\\{-(X_i\\beta+\\epsilon_i)\\}})^{-1}] -\n% \\frac{1}{2}\n\\end{align}\n}\n\\]\nNote that this is the inverse of the logit function!\n\\[\n\\displaylines{\n\\begin{align}\n\\text{logit}(x)\n& = \\ln{\\left(\\frac{x}{1-x}\\right)}\n\\\\\n\\therefore \\text{let } y & = \\ln{\\left(\\frac{x}{1-x}\\right)}\n\\\\\ne^y & = \\frac{x}{1-x}\n\\\\\ne^y - xe^y & = x\n\\\\\ne^y & = x(1+e^y)\n\\\\\nx & = \\frac{e^y}{1+e^y}\n= (1+\\exp{\\{e^{-x}\\}})^{-1}\n\\\\ \\\\\n\\therefore\nP(\\epsilon_i &lt; X_i\\beta ) & = \\text{Logit}^{-1}(X\\beta)\n\\end{align}\n}\n\\]\n\n\n\nOptimal coefficients for the coefficient\n\nDeriving the gradient with respect to the coefficients\nWe minimise the cost function by finding the optimum coefficient values \\(\\beta^*\\) so that the partial differential is equal to zero.\n\\[\n\\displaylines{\n\\begin{align}\n& \\frac{\\partial}{\\partial \\beta_j}p(y|\\beta,X) \\\\ \\\\\n= &\n\\frac{\\partial}{\\partial \\beta_j} \\left(\n\\sum_{i=1}^{N}{ -y_i\\log{ \\left[ p(y_i=1|X_i\\beta) \\right] } } +\n  \\sum_{i=1}^{N}{ -(1-y_i)\\log{ \\left[ 1-p(y_i=1|X_i\\beta)\\right] } }\n\\right) \\\\ \\\\\n= &\n\\sum_{i=1}^{N}{ y_i\n  \\frac{\\frac{\\partial}{\\partial \\beta_j}p(y_i=1|X_i\\beta)}{p(y_i=1|X_i\\beta)}\n  } +\n  \\sum_{i=1}^{N}{ (1-y_i)\n    \\frac{\\frac{\\partial}{\\partial \\beta_j}\\left(1-p(y_i=1|X_i\\beta)\\right)}{1-p(y_i=1|X_i\\beta)}\n    }\n\\end{align}\n}\n\\]\nThis is as far as we can get, without now making some more assumptions. Let’s imagine that we can model the\nGiven that:\n\\[\n\\hat{p_i} = \\hat{p}(y_i=1|X_i \\hat{\\beta}) =\n\\frac{1}{1+\\exp{\\left\\{-X_i\\hat{\\beta}\\right\\}}}\n\\]\nThen the partial differential of the probability with respect to feature \\(j\\) is:\n\\[\n\\displaylines{\n\\begin{align}\n& \\frac{\\partial }{\\partial \\beta_j}\n\\hat{p}(y_i=1|X_i \\hat{\\beta_j})\n\\\\ = &\n\\frac{\\partial }{\\partial \\beta_j} (1+\\exp{\\{-X_i\\hat{\\beta}\\}})^{-1}\n\\\\ = &\n\\frac{\\partial }{\\partial \\beta_j} -1(1+\\exp{\\{-X_i\\hat{\\beta}\\}})^{-2}\n\\times x_{ij}\\exp{\\{-X_i\\hat{\\beta}\\}}\n\\\\ = &\nx_{ij} \\left( \\frac{\\exp{\\{-X_i\\hat{\\beta}\\}}}{(1+\\exp{\\{-X_i\\hat{\\beta}\\}})^{2}} \\right)\n\\\\ = &\nx_{ij} \\left( \\frac{1}{(1+\\exp{\\{-X_i\\hat{\\beta}\\}})} \\times \\frac{\\exp{\\{-X_i\\hat{\\beta}\\}}}{(1+\\exp{\\{-X_i\\hat{\\beta}\\}})} \\right)\n\\\\ = &\nx_{ij} \\left( \\frac{1}{(1+\\exp{\\{-X_i\\hat{\\beta}\\}})} \\times \\left( 1 - \\frac{1}{(1+\\exp{\\{-X_i\\hat{\\beta}\\}})} \\right) \\right)\n\\\\ \\\\ = &\nx_{ij} ( \\hat{p}(y_i=1|X_i \\hat{\\beta_j}) \\times (1-\\hat{p}(y_i=1|X_i \\hat{\\beta_j})) )\n\\\\ \\\\ = &\nx_{ij} ( \\hat{p_i} \\times (1-\\hat{p_i}) )\n\\end{align}\n}\n\\]\nAnd thus we can substitute this into our cost function:\n\\[\n\\displaylines{\n\\begin{align}\n& \\frac{\\partial}{\\partial \\beta_j}p(y|\\beta,X) \\\\ \\\\\n= &\n\\sum_{i=1}^{N}{ y_i\n  \\frac{\\frac{\\partial}{\\partial \\beta_j}p(y_i=1|X_i\\beta)}{p(y_i=1|X_i\\beta)}\n  } +\n\\sum_{i=1}^{N}{ (1-y_i)\n  \\frac{\\frac{\\partial}{\\partial \\beta_j}\\left(1-p(y_i=1|X_i\\beta)\\right)}{1-p(y_i=1|X_i\\beta)}\n  } \\\\ \\\\\n= &\n\\sum_{i=1}^{N}{ y_i\n  \\frac{x_{ij} ( \\hat{p_i} \\times (1-\\hat{p_i}) )}{\\hat{p_i}}\n  } +\n\\sum_{i=1}^{N}{ (1-y_i)\n  \\frac{x_{ij} ( \\hat{p_i} \\times (1-\\hat{p_i}) )}{(1-\\hat{p_i})}\n  } \\\\ \\\\\n= &\n\\sum_{i=1}^{N}{ y_i\n  \\frac{x_{ij} ( \\cancel{\\hat{p_i}} \\times (1-\\hat{p_i}) )}{\\cancel{\\hat{p_i}}}\n  } +\n\\sum_{i=1}^{N}{ (1-y_i)\n  \\frac{x_{ij} ( \\hat{p_i} \\times \\cancel{(1-\\hat{p_i})} )}{\\cancel{(1-\\hat{p_i})}}\n  } \\\\ \\\\  \n= &\n\\sum_{i=1}^{N}{ y_ix_{ij} (1-\\hat{p_i})} +\n\\sum_{i=1}^{N}{ (1-y_i)x_{ij} \\hat{p_i}} \\\\ \\\\\n= &\n\\sum_{i=1}^{N}{ x_{ij} \\left[\n    y_i(1-\\hat{p_i}) (1-y_i)(\\hat{p_i})\n   \\right]\n  }\n\\end{align}\n}\n\\]\nWhich is the coefficient from logistic regression.\nFin."
  },
  {
    "objectID": "posts/ols_finite_sample_correction.html",
    "href": "posts/ols_finite_sample_correction.html",
    "title": "Estimating population variance from a sample",
    "section": "",
    "text": "What are we exploring?\n\n\n\nWhy is sample variance divided by \\(n-1\\), but population variance by \\(N\\)."
  },
  {
    "objectID": "posts/ols_finite_sample_correction.html#intro-population-variance-vs.-variance-estimate-from-sample",
    "href": "posts/ols_finite_sample_correction.html#intro-population-variance-vs.-variance-estimate-from-sample",
    "title": "Estimating population variance from a sample",
    "section": "Intro: Population variance vs. variance estimate from sample",
    "text": "Intro: Population variance vs. variance estimate from sample\nIf we have an entire population of size \\(N\\), we can perfectly calculate the true population mean \\(\\mu\\). As a result, we calculate the variance statistic in an intuitive way:\n\\[\n\\displaylines{\n\\begin{align}\n\\mu &\n= \\frac{1}{N} \\sum_{i=1}^{N}{X_i}\n\\\\\n\\sigma^2 &\n= \\frac{1}{N}\\sum_{i=1}^{N}{(X_i-\\mu)^2}\n\\end{align}\n}\n\\]\nHowever, if we are creating an estimate of the population variance from a finite sample of size \\(n\\), then we need to do some bias correction, where we divide by \\(n-1\\) instead:\n\\[\n\\displaylines{\n\\begin{align}\n\\bar{x} &\n= \\frac{1}{n} \\sum_{i=1}^{n}{x_i}\n\\\\\n\\hat{\\sigma}^2 &\n= \\frac{1}{n-1}\\sum_{i=1}^{n}{(x_i-\\bar{x})^2}\n\\end{align}\n}\n\\]"
  },
  {
    "objectID": "posts/ols_finite_sample_correction.html#some-intuition-as-to-why",
    "href": "posts/ols_finite_sample_correction.html#some-intuition-as-to-why",
    "title": "Estimating population variance from a sample",
    "section": "Some intuition as to why:",
    "text": "Some intuition as to why:\nThe reason for this is because our sample mean \\(\\bar{x}\\) is almost always not going to be exactly the population mean \\(\\mu\\), i.e. \\(\\bar{x} \\neq \\mu\\).\nNow because \\(\\bar{x}\\) is calculated from the sample, the datapoints in that same sample will be closer to it than they would be \\(\\mu\\). So if using the squared distances between each sample and the sample mean to estimate the population variance, we need to take this into account!"
  },
  {
    "objectID": "posts/ols_finite_sample_correction.html#proving-the-difference",
    "href": "posts/ols_finite_sample_correction.html#proving-the-difference",
    "title": "Estimating population variance from a sample",
    "section": "Proving the difference:",
    "text": "Proving the difference:\nImagine there are \\(N\\) i.i.d random variables \\(X_1, X_2,...,X_n\\)1., generated from a population process with mean \\(\\mu\\) and \\(\\sigma\\). Then we might derive the following:\n\nThe mean \\(\\bar{X} = \\frac{1}{n}\\sum_{i=1}^{n}{(X_{i})}\\)\nThe uncorrected variance \\(s^2 = \\frac{1}{n} \\sum_{i=1}^{n}{(X_{i}-\\bar{X})^2}\\)\n\nLet’s now analyse the expected value for this uncorrected population variance:\n\\[\n\\displaylines{\n\\begin{align}\nE[s^2] & =\nE\\left[\n  \\frac{1}{n} \\sum_{i=1}^{n}{(X_i-\\bar{X})^2}\n\\right]\n\\\\ & =\nE\\left[\n  \\frac{1}{n} \\sum_{i=1}^{n}{\\bigg(\n    (X_i-\\mu)-(\\bar{X}-\\mu)\n  \\bigg)^2}\n\\right]\n\\\\ & =\nE\\left[\n  \\frac{1}{n} \\sum_{i=1}^{n}{\\bigg(\n    (X_i-\\mu)^2-2(X_i-\\mu)(\\bar{X}-\\mu)+(\\bar{X}-\\mu)^2\n  \\bigg)}\n\\right]\n\\\\ & =\nE\\left[\n  \\frac{1}{n} \\bigg(\\sum_{i=1}^{n}{ (X_i-\\mu)^2 } \\bigg)\n  - \\frac{2}{n} \\bigg((\\bar{X}-\\mu) \\sum_{i=1}^{n}{(X_i-\\mu)} \\bigg)\n  + \\frac{1}{n} \\bigg(n(\\bar{X}-\\mu)^2 \\bigg)\n\\right] & \\because \\bar{X}-\\mu \\text{ is constant}\n\\\\ & =\nE\\left[\n  \\frac{1}{n} \\bigg(\\sum_{i=1}^{n}{ (X_i-\\mu)^2 } \\bigg)\n  - \\frac{2}{n} \\bigg( (\\bar{X}-\\mu) [n(\\bar{X}-\\mu)] \\bigg)\n  + (\\bar{X}-\\mu)^2\n  \\bigg)\n\\right] & \\because \\frac{1}{n}\\sum_{i=1}^{n}{(X_i-\\mu)} = \\bar{X} - \\mu\n\\\\ & =\nE\\left[\n  \\frac{1}{n} \\bigg(\\sum_{i=1}^{n}{ (X_i-\\mu)^2 } \\bigg)\n  - (\\bar{X}-\\mu)^2\n  \\bigg)\n\\right]\n\\\\ & =\n\\underbrace{\nE\\left[\n  \\frac{1}{n} \\sum_{i=1}^{n}{ (X_i-\\mu)^2 }\n\\right]\n}_{\\text{True population variance}} -\n\\underbrace{\nE\\Bigg[\n  (\\bar{X}-\\mu)^2\n\\Bigg]\n}_{\\text{Sample vs pop. mean}}\n\\end{align}\n}\n\\]\nSo we can see that \\(E[s^2]\\) is too small by that extra term, \\(E[(\\bar{x}-\\mu)^2]\\).\nNote this is the expected variance of \\(\\bar{X}\\) i.e. \\(\\text{Var}[\\bar{X}]=E[(\\bar{x}-\\mu)^2]\\)"
  },
  {
    "objectID": "posts/ols_finite_sample_correction.html#exploring-the-variance-of-mean",
    "href": "posts/ols_finite_sample_correction.html#exploring-the-variance-of-mean",
    "title": "Estimating population variance from a sample",
    "section": "Exploring the variance of mean",
    "text": "Exploring the variance of mean\nIt can be shown that the variance of the sum of uncorrelated random variables is equal to the sum of their variances2:\n\\[\n\\text{Var}\\left( \\sum_{i=1}^{n}{X_i} \\right)\n= \\sum_{i=1}^{n}{\\text{Var}\\left(X_i\\right) }\n\\]\nNow since every \\(X_i\\) has the same variance \\(\\sigma^2\\), then we can derive the following:\n\\[\n\\displaylines{\n\\begin{align}\nE\\left[ (\\bar{X}-\\mu)^2 \\right]\n& =\n\\text{Var}\\left[\\bar{X}\\right]\n=\n\\text{Var}\\left[\\frac{1}{n}\\sum_{i=1}^{n}{X_i}\\right]\n\\\\ & =\n\\left( \\frac{1}{n} \\right)^2\n\\text{Var} \\left[ \\sum_{i=1}^{n}{X_i} \\right]\n\\\\ & =\n\\left( \\frac{1}{n} \\right)^2\n\\sum_{i=1}^{n}{ \\text{Var} \\big[X_i\\big] }\n\\\\ & =\n\\frac{1}{n^2}\n\\left( n \\times \\sigma^2 \\right)\n\\\\ & =\n\\frac{\\sigma^2}{n}\n\\end{align}\n}\n\\]\n\n\nSince \\(\\text{Var}(aX) = a^2\\text{Var}(X)\\)3"
  },
  {
    "objectID": "posts/ols_finite_sample_correction.html#bessels-correction",
    "href": "posts/ols_finite_sample_correction.html#bessels-correction",
    "title": "Estimating population variance from a sample",
    "section": "Bessel’s correction:",
    "text": "Bessel’s correction:\nSo if we sub this into our original equation: \\[\n\\displaylines{\n\\begin{align}\nE[s^2] & =\n\\underbrace{E \\left[ \\frac{1}{n} \\sum_{i=1}^{n}{ (x_i-\\mu)^2 } \\right]}_{\\sigma}\n-\n\\underbrace{E\\Bigg[ (\\bar{x}-\\mu)^2 \\Bigg]}_{\\sigma/n}\n\\\\ & =\n\\left(1-\\frac{1}{n} \\right)\\sigma^2\n\\\\ \\\\ & =\n\\frac{(n-1)}{n}\\sigma^2\n\\end{align}\n}\n\\]\nThen we can see that we need to make a correction - Bessel’s correction - to the statistic!\n\\[\n\\displaylines{\n\\begin{align}\n\\hat{\\sigma}^2\n& = \\frac{n}{n-1}\\left(s^2\\right) \\\\\n& = \\frac{1}{n-1} \\left( \\sum_{i=1}^{n}{x_i-\\bar{x}} \\right)\n\\end{align}\n}\n\\]\nFin."
  },
  {
    "objectID": "posts/ols_finite_sample_correction.html#footnotes",
    "href": "posts/ols_finite_sample_correction.html#footnotes",
    "title": "Estimating population variance from a sample",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nRandom variables can take on a range of possible values that are not yet realised (e.g. for a dice, \\(X \\in \\{1,2,3,4,5,6\\}\\)). These are often signified using capital letters. Samples have values that are already realised e.g. someone rolled a 3, then a 5 with the dice. We didn’t observe an infinite number of dice roles, but just two: \\(x_1=3,x_2=5\\) (\\(\\therefore n=2,\\bar{x}=4\\)). The population is usually denoted with capital letters too (since we usually don’t observe the entire population!), where e.g. for dice \\(E[X]=\\mu=3.5\\) ↩︎\nShowing \\(\\text{Var}\\big[ X+Y \\big] = \\text{Var}\\big[ X \\big] + \\text{Var}\\big[ Y \\big]\\): \\[\n  \\displaylines{\n  \\begin{align}\n  \\text{Var}\\big[ X+Y \\big]\n  & = E\\big[ (X+Y)^2 \\big] - \\big( E[X+Y] \\big)^2 \\\\\n  & = E\\big[ X^2+2XY+Y^2 \\big] - \\big( E[X]+E[Y] \\big)^2 \\\\\n  & = E\\big[ X^2 \\big] + 2E\\big[ XY \\big] + E\\big[Y^2 \\big] - \\bigg(E[X]^2 + 2E[X]E[Y] + E[Y]^2 \\bigg)  \\\\\n  & = \\bigg( E\\big[ X^2 \\big] - E[X]^2 \\bigg) + \\bigg( E\\big[Y^2 \\big]- E[Y]^2 \\bigg) \\\\\n  & = \\text{Var}\\big[ X \\big] + \\text{Var}\\big[ Y \\big]\n  \\end{align}\n  }\n  \\] ↩︎\nShowing \\(\\text{Var}\\big[ aX \\big] = a^2\\text{Var}\\big[ X \\big]\\): \\[\n  \\displaylines{\n  \\begin{align}\n  \\text{Var}\\big[ aX \\big]\n  & = E\\big[ (aX)^2 \\big] - \\big( E[aX] \\big)^2\n  = E\\big[ a^2X^2 \\big] - \\big( a E[X] \\big)^2 \\\\\n  & = a^2 E\\big[ X^2 \\big] - a^2 \\big(E[X]\\big)^2\n  = a^2 \\bigg( E\\big[ X^2 \\big] - \\big(E[X]\\big)^2 \\bigg) \\\\\n  & = a^2\\text{Var}\\big[ X \\big]\n  \\end{align}\n  }\n  \\]↩︎"
  },
  {
    "objectID": "posts/fundamentals_binom_pois.html",
    "href": "posts/fundamentals_binom_pois.html",
    "title": "Deriving the poisson distribution from binomial",
    "section": "",
    "text": "What are we exploring?\n\n\n\nHow extending the binomial distribution to an infinite number of trials derives the poisson distribution."
  },
  {
    "objectID": "posts/fundamentals_binom_pois.html#defining-a-binomial-problem",
    "href": "posts/fundamentals_binom_pois.html#defining-a-binomial-problem",
    "title": "Deriving the poisson distribution from binomial",
    "section": "Defining a binomial problem",
    "text": "Defining a binomial problem\nLet’s look at a metric for the number of failures of an autonomous driving system per 100,000 km driven.\nWe could model this using the binomial probability density, where we define each km driven as a trial, and an event being a failure occuring during a km driven:\n\\[\n\\displaylines{\n\\begin{align}\nP(x) = &\n{n \\choose x} p^x (1-p)^{n-x}\n\\\\\\\\ \\text{where } n & \\text{ is total miles driven}\n\\\\ \\text{and } x & \\text{ is number of failures}\n\\end{align}\n}\n\\]"
  },
  {
    "objectID": "posts/fundamentals_binom_pois.html#increasing-the-number-of-trials-to-infinity",
    "href": "posts/fundamentals_binom_pois.html#increasing-the-number-of-trials-to-infinity",
    "title": "Deriving the poisson distribution from binomial",
    "section": "Increasing the number of trials to infinity",
    "text": "Increasing the number of trials to infinity\nWhat happens if we test billions of miles? This is effectively the same as asking what happens if \\(n \\rightarrow \\infty\\).\nLet’s also define \\(\\lambda\\), the mean number of successes we expect over \\(n\\) trials (the “rate”). This simply means that \\(\\lambda = np\\)\n\\[\n\\displaylines{\n\\begin{align}\n\\lim_{n \\rightarrow \\infty} &\n{n \\choose x} p^x (1-p)^{n-x}\n\\\\\n= \\lim_{n \\rightarrow \\infty} &\n{n \\choose x} \\left( \\frac{\\lambda}{n} \\right)^x \\left( 1-\\frac{\\lambda}{n} \\right)^{(n-x)}\n& \\because \\lambda = np\n\\end{align}\n}\n\\]\nIt is helpful to simplify the expression before examining what happens when \\(n\\) tends to infinity. We can split it into four parts (i, ii, iii and iv):\n\\[\n\\displaylines{\n\\begin{align}\n\\lim_{n \\rightarrow \\infty} &\n{n \\choose x} \\left( \\frac{\\lambda}{n} \\right)^x \\left( 1-\\frac{\\lambda}{n} \\right)^{(n-x)}\n\\\\\n= \\lim_{n \\rightarrow \\infty} &\n\\frac{n!}{(n-x)! \\times x!} \\times \\left( \\frac{\\lambda}{n} \\right)^x \\times \\left( 1-\\frac{\\lambda}{n} \\right)^{n} \\times \\left( 1-\\frac{\\lambda}{n} \\right)^{-x}\n\\\\\n= \\lim_{n \\rightarrow \\infty} &\n\\underbrace{\n  \\frac{n!}{(n-x)! \\times n^x}\n}_{\\text{i}}\n\\times\n\\underbrace{\n  \\frac{\\lambda^x}{x!}\n}_{\\text{ii}}\n\\times\n\\underbrace{\n  \\left( 1-\\frac{\\lambda}{n} \\right)^{n}\n}_{\\text{iii}}\n\\times\n\\underbrace{\n   \\left( 1-\\frac{\\lambda}{n} \\right)^{-x}\n}_{\\text{iv}}\n\\end{align}\n}\n\\]\nWe now explore what happens if we extend \\(n\\) towards infinity for each term.\n\nTerm i\nWe can expand out the factorial to simplify it: \\[\n\\displaylines{\n\\begin{align}\n\\frac{n!}{(n-x)! \\times (n^x)}\n& =\n\\frac{\n  n \\times (n-1) \\times (n-2) \\times \\ldots \\times\n  (n-x+1) \\times \\cancel{(n-x)!}\n}{\n  \\cancel{(n-x)!} \\times (n^x)\n}\n\\\\\\\\ & =\n\\frac{n}{n} \\times\n\\frac{n-1}{n} \\times\n\\frac{n-2}{n} \\times\n\\ldots \\times\n\\frac{n-x-2}{n} \\times\n\\frac{n-x-1}{n}\n\\\\\\\\ & =\n1 \\times\n\\left( 1 - \\frac{1}{n} \\right) \\times\n\\left( 1 - \\frac{2}{n} \\right) \\times\n\\ldots \\times\n\\left( 1 - \\frac{x-2}{n} \\right) \\times\n\\left( 1 - \\frac{x-1}{n} \\right)\n\\\\\\\\ & =\n\\prod_{i=1}^{x-1}{\\left(1-\\frac{i}{n}\\right)}\n\\end{align}\n}\n\\]\nAs similarly to before \\(n \\rightarrow \\infty\\), then \\(\\frac{i}{n} \\rightarrow 0\\).\n\\[\n\\displaylines{\n\\begin{align}\n\\therefore\n\\lim_{n \\rightarrow \\infty}\n\\prod_{i=1}^{x-1}{\\left(1-\\frac{i}{n}\\right)}\n& = 1\n\\end{align}\n}\n\\]\n\n\nTerm ii\n\\[\n\\displaylines{\n\\begin{align}\n\\lim_{n \\rightarrow \\infty}\n\\frac{\\lambda^x}{x!}\n& =\n\\frac{\\lambda^x}{x!}\n& \\because\nn \\text{ does not blow up}\n\\end{align}\n}\n\\]\n\n\nTerm iii\nThis is a tricky one!\n\\[\n\\lim_{n \\rightarrow \\infty}{ \\left[\n    \\left(1 - \\frac{\\lambda}{n} \\right)^{n}\n  \\right]} = e^{-\\lambda}\n\\]\n(We go through this derivation in the footnotes for if you do not know it already)1\n\n\nTerm iv\nAs \\(n \\rightarrow \\infty\\), then \\(\\frac{\\lambda}{n} \\rightarrow 0\\), so:\n\\[\n\\lim_{n \\rightarrow \\infty}\\left( 1-\\frac{\\lambda}{n} \\right)^{-x} = (1-0)^{-x} = 1\n\\]"
  },
  {
    "objectID": "posts/fundamentals_binom_pois.html#tying-it-all-together",
    "href": "posts/fundamentals_binom_pois.html#tying-it-all-together",
    "title": "Deriving the poisson distribution from binomial",
    "section": "Tying it all together",
    "text": "Tying it all together\n\\[\n\\displaylines{\n\\begin{align}\n& \\lim_{n \\rightarrow \\infty}\n\\left[\n  \\frac{n!}{(n-x)! \\times n^x}\n\\times\n  \\frac{\\lambda^x}{x!}\n\\times\n  \\left( 1-\\frac{\\lambda}{n} \\right)^{n}\n\\times\n   \\left( 1-\\frac{\\lambda}{n} \\right)^{-x}\n\\right]\n\\\\ & =\n1 \\times\n  \\frac{\\lambda^x}{x!}\n\\times\n  e^{-\\lambda}\n\\times\n   1\n\\\\ & =\n\\frac{\\lambda^x e^{-\\lambda}}{x!}\n\\end{align}\n}\n\\]\nWhich is the poisson pdf!\nFin."
  },
  {
    "objectID": "posts/fundamentals_binom_pois.html#footnotes",
    "href": "posts/fundamentals_binom_pois.html#footnotes",
    "title": "Deriving the poisson distribution from binomial",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nShowing \\(\\lim_{n \\rightarrow \\infty}{ \\left[ \\left(1 + \\frac{x}{n} \\right)^{n} \\right]} = e^x\\): \\[\n\\displaylines{\n\\begin{align}\n\\text{Let } f(x) & = \\ln{(x)} \\\\\n\\therefore f'(x) & = \\frac{1}{x} \\\\\n\\text{and } f'(x) & =\n\\lim_{a \\rightarrow 0}{ \\left[\n  \\frac{f(x+a) - f(x)}{(x+a) - x}\n\\right] } \\\\\n& = \\lim_{a \\rightarrow 0}{ \\left[\n  \\frac{\\ln{(x+a)} - \\ln{(x)}}{a}\n\\right] }  \\\\\n& = \\lim_{a \\rightarrow 0}{ \\left[\n  \\frac{1}{a}  \\ln{\\left(\\frac{x+a}{x} \\right)}\n\\right] } \\\\\n& = \\lim_{a \\rightarrow 0}{ \\left[\n  \\ln{\\left(1 + \\frac{a}{x} \\right)^{\\frac{1}{a}}}\n\\right] } \\\\\n& = \\lim_{b \\rightarrow \\infty}{ \\left[\n  \\ln{\\left(1 + \\frac{1}{bx} \\right)^{b}}\n\\right] }\n& \\text{where } b = \\frac{1}{a}\n\\\\\n& = \\lim_{n \\rightarrow \\infty}{ \\left[\n  \\ln{\\left(1 + \\frac{x}{n} \\right)^{n}}\n\\right]^{1/x^2} }\n& \\text{where } n = bx^2\n\\\\\n& = \\frac{1}{x}\n\\\\\n\\therefore\ne^{1/x} & = \\lim_{n \\rightarrow \\infty}{ \\left[\n  \\left(1 + \\frac{x}{n} \\right)^{n}\n\\right]^{1/x^2} }\n\\\\\n\\therefore\ne^{x} & = \\lim_{n \\rightarrow \\infty}{ \\left[\n  \\left(1 + \\frac{x}{n} \\right)^{n}\n\\right]}\n\\end{align}\n}\n\\]↩︎"
  },
  {
    "objectID": "posts/glm_logit.html",
    "href": "posts/glm_logit.html",
    "title": "Logistic Regression",
    "section": "",
    "text": "What are we exploring?\n\n\n\nModelling bernoulli probabilities using maximum likelihood estimation and the logit link funciton."
  },
  {
    "objectID": "posts/glm_logit.html#a-latent-variable-model",
    "href": "posts/glm_logit.html#a-latent-variable-model",
    "title": "Logistic Regression",
    "section": "A latent variable model",
    "text": "A latent variable model\nImagine a basketball player is taking free throws.\nFor each trial \\(i\\) (a freethrow attempt), we observe the outcome \\(y_i\\) as being 1 if there is a success (i.e. the freethrow is made), or 0 if it is a failure (a miss).\nWhen the player takes the freethrow, there are exogenous features \\(X_i\\) that influence their likelihood of making it (e.g. whether they are home or away). Also there is some randomness, \\(\\varepsilon_i\\) that is unpredictable: a 99% accurate shooter still has a 1% chance of missing.\nEconomists often frame this problem as a “latent variable model”. What this means is they frame the problem as though there is a hidden continuous variable we do not observe, \\(y_i^*\\), and if it exceeds a certain threshold (usually zero) then there is a success (e.g. the freethrow is made):\n\\[\n\\displaylines{\n\\begin{align}\ny_i^* & = X_i\\beta + \\varepsilon_i\n\\\\ \\\\\ny_i & =\n\\begin{cases}\n  1 & \\text{if}\\ y_i^* &gt; 0 & \\text{i.e. } -\\varepsilon_i &gt; X_i\\beta\\\\\n  0 & \\text{otherwise}\n\\end{cases}\n\\end{align}\n}\n\\]\nIn other words, the latent variable \\(y_i^*\\) is purely a function of its predictors \\(X_i\\), their relationship to \\(y_i^*\\) given by \\(\\beta\\), and some additive noise \\(\\varepsilon_i\\). If \\(y_i^*\\) is positive, we observe a success (i.e. \\(y_i = 1\\)).\nWe can thus formulate the probability of success \\(P(y_i=1|X)\\) as the likelihood that the additive noise \\(\\varepsilon_i\\) is less than \\(X\\beta\\), resulting in the probability \\(y_i^*\\) above zero:\n\\[\n\\displaylines{\n\\begin{align}\np_i & = P(y_i=1|X_i) \\\\\n& = P(y_i^* &gt; 0 | X_i)\n\\\\\n& = P(X_i\\beta + \\varepsilon_i &gt; 0)\n\\\\\n& = P(\\varepsilon_i &gt; -X_i\\beta )\n\\\\\n& = P(\\varepsilon_i &lt; X_i\\beta ) & \\iff \\varepsilon_i \\sim f(\\mu,s) \\text{ is symmetric}\n\\end{align}\n}\n\\]\nSo a good assumption for the probabilistic process that generates the error \\(\\varepsilon_i\\) is very important! We will come back to this shortly, after discussing the Bernoulli probability mass function."
  },
  {
    "objectID": "posts/glm_logit.html#the-bernoulli-pmf",
    "href": "posts/glm_logit.html#the-bernoulli-pmf",
    "title": "Logistic Regression",
    "section": "The Bernoulli PMF",
    "text": "The Bernoulli PMF\nEven if a player has a 99% of making freethrows, they would be expected to miss 1 in 100. We can capture this through the probability mass function of the Bernoulli distribution:\n\\[\n\\displaylines{\n\\begin{align}\nP(y_i) =\n\\begin{cases}\n  p_i & \\text{if}\\ y_i=1 \\\\\n  1-p_i & \\text{if}\\ y_i=0\n\\end{cases}\n\\end{align}\n}\n\\]\nWhich is equivalent to the following:\n\\[\n\\displaylines{\n\\begin{align}\nP(y_i)\n& = p_i^{y_i}(1-p_i)^{1-y_i} \\\\ \\\\\n& = P(\\varepsilon_i &lt; X_i\\beta)^{y_i}\n(1-P(\\varepsilon_i &lt; X_i\\beta))^{1-y_i}\n\\end{align}\n}\n\\]\n\n\nSince \\(g(x)^0 = 1\\):\n\nif \\(y_i=1\\), \\(p^{1}(1-p)^{0} = p\\)\nif \\(y_i=0\\), \\(p^{0}(1-p)^{1} = 1-p\\)\n\nUntil now, we have just been looking at the likelihood of making a success for a single trial \\(i\\). But we want to find values for \\(\\beta\\) that optimize predictions across all trials, to learn the impact of \\(X\\) so we can better predict \\(y\\) next time - aka maximum likelihood estimation."
  },
  {
    "objectID": "posts/glm_logit.html#maximum-likelihood-estimation",
    "href": "posts/glm_logit.html#maximum-likelihood-estimation",
    "title": "Logistic Regression",
    "section": "Maximum Likelihood Estimation",
    "text": "Maximum Likelihood Estimation\nAssuming each trial is independent (a big assumption for free throws!) - the probability of making two then missing one is \\(p_i \\times p_i \\ \\times (1-p_i)\\). In other words - the combined probability is the multiplication of the individual probabilities.\nWe can generalize this to a sample size \\(N\\) as the following:\n\\[\n\\displaylines{\n\\begin{align}\np(y|X)\n& =\n\\prod_{i=1}^{N}{\n  p_i^{y_i} (1-p_i)^{1-y_i}\n  }\n\\\\ & =\n\\underbrace{\n  \\prod_{y_i=0}^{n_1}{ p_i^{y_i} }\n}_{y_i=1} \\times\n\\underbrace{\n  \\prod_{y_i=1}^{n_0}{ (1-p_i)^{1-y_i} }    \n}_{y_i=0}\n\\end{align}\n}\n\\]\nIn practice, it is common to minimize the negative log-likelihood, which is shown to be equivalent to maximising the likelihood directly (since the logarithm is a monotonic function):\n\\[\n\\displaylines{\n\\begin{align}\n& \\max_\\beta{p(y|X)} \\\\\n= &\n\\max_\\beta{\\left\\{\n  \\prod_{i=1}^{N}{ p_i^{y_i} }\n  \\times\n  \\prod_{i=1}^{N}{ (1-p_i)^{1-y_i} }\n  \\right\\}}\n\\\\ \\\\ \\Rightarrow &\n\\min_\\beta{\\left\\{ -\\log{ \\left[\n  \\prod_{i=1}^{N}{ p_i^{y_i} }\n  \\times\n  \\prod_{i=1}^{N}{ (1-p_i)^{1-y_i} }\n  \\right] } \\right\\}}\n\\\\ \\\\ = &\n\\min_\\beta{\\left\\{\n  \\sum_{i=1}^{N}{ -\\log{ \\left[ p_i^{y_i} \\right] } } +\n  \\sum_{i=1}^{N}{ -\\log{ \\left[ (1-p_i)^{1-y_i}\\right] } }\n  \\right\\}}  \n\\\\ \\\\ = &\n\\min_\\beta{\\left\\{\n  \\sum_{i=1}^{N}{ -y_i\\log{ \\left[ p_i \\right] } } +\n  \\sum_{i=1}^{N}{ -(1-y_i)\\log{ \\left[ 1-p_i\\right] } }\n  \\right\\}}\n\\\\ \\\\ = &\n\\min_\\beta{\\left\\{\n  \\sum_{i=1}^{N}{ -y_i\\log{ \\left[ P(\\varepsilon_i &lt; X \\beta) \\right] } } +\n  \\sum_{i=1}^{N}{ -(1-y_i)\\log{ \\left[ 1-P(\\varepsilon_i &lt; X \\beta)\\right] } }\n  \\right\\}}    \n\\end{align}\n}\n\\]\n\n\nRecall that \\(\\log{\\left(ab\\right)} = \\log{\\left(a\\right)} + \\log{\\left(b\\right)}\\)\nWe now - almost - have a well defined problem we can solve! We just need to make an assumption for the distribution of errors \\(\\varepsilon\\)."
  },
  {
    "objectID": "posts/glm_logit.html#assuming-errors-come-from-a-logistic-distribution",
    "href": "posts/glm_logit.html#assuming-errors-come-from-a-logistic-distribution",
    "title": "Logistic Regression",
    "section": "Assuming errors come from a logistic distribution",
    "text": "Assuming errors come from a logistic distribution\nThe errors are often assumed to be generated from a logistic distribution, with location parameter \\(\\mu=0\\) and scale parameter \\(s=1\\):\n\\[\n\\epsilon \\sim \\text{Logistic}(0,1)\n\\]\nWhy a logistic? Well because is highly similar to a normal distribution, but with fatter tails, so its seen as an approximation that is more robust. Furthermore, it has easier algebra to unpick - which we will show shortly.\n\n\nCode\nlogistic_func &lt;- function(x) {\n  return( exp(-x)*(1+exp(-x))^-2 )\n}\nnormal_func &lt;- function(x) {\n  return( (2*pi)^(-0.5) * exp(-0.5 * x^2) )\n}\nx = seq(-10,10,0.1)\nplot(\n  x=x,\n  y=normal_func(x),\n  type='l',col=\"blue\",lty=1\n  )\nlines(\n  x=x,\n  y=logistic_func(x),\n  type='l',col=\"red\",lty=1\n)\nlegend(\n  -10,0.4,\n  legend=c(\n    \"Normal PDF\",\n    \"Logistic function\"\n    ),\n  col=c(\"blue\",\"red\"),\n  lty=c(1,1)\n  )\n\n\n\n\n\n\n\n\n\nFor now, you might recall the pdf of the logistic distribution is:\n\\[\n\\displaylines{\n\\begin{align}\nf(x,\\mu,s) & =\n\\frac{e^{-(x-\\mu)/s)}}\n{s(1+e^{-(x-\\mu)/s)})^2}\n\\\\ \\\\\n\\therefore f(x,0,1) & =\n\\frac{e^{-x}}\n{(1+e^{-x})^2}\n\\end{align}\n}\n\\]\nThen we can obtain the CDF by taking the integral of the pdf:\n\\[\n\\displaylines{\n\\begin{align}\n\\int {f(x,0,1) \\,dx} & =\n\\int{\n  \\frac{e^{-x}}\n  {(1+e^{-x})^2}\n\\,dx}\n\\\\ \\\\\n\\text{let } u = 1+e^{-x} & \\therefore \\frac{du}{dx} = -e^{-x}\n\\\\\n& \\therefore dx = -\\frac{du}{e^{-x}}\n\\\\ \\\\ \\therefore\n\\int {f(x,0,1) \\,dx} & =\n\\int {\\frac{e^{-x}}{u^2} \\times -\\frac{du}{e^{-x}}}\n\\\\ &\n= \\int {-u^{-2}\\,du}\n\\\\\n& = u^{-1} + c\n\\\\\n& = (1+e^{-x})^{-1} + c\n\\end{align}\n}\n\\]\nAnd hence, we derive the “logisitic function”:\n\\[\n\\displaylines{\n\\begin{align}\np(y_i|X_i)\n& = p(\\varepsilon_i &lt; X_i\\beta) \\\\\n& = (1+e^{-X_i\\beta})^{-1} \\\\\n& = \\frac{1}{1+e^{-X_i\\beta}} \\\\ \\\\\n& = \\text{logistic}(X_i\\beta)\n\\end{align}\n}\n\\]\nSo we now have a mapping of \\(X\\) to \\(y\\), given by \\(\\beta\\) and the activation function: the “logistic function”.\nWe can see that this activation function “squashes” all outputs \\(X\\beta \\in [-\\infty,\\infty]\\) between 0 and 1:\n\n\nCode\nlogistic_dist &lt;- function(x) {\n  return( (1+exp(-x))^-1 )\n}\nx = seq(-10,10,0.1)\nplot(\n  x=x,\n  y=logistic_dist(x),\n  type='l',col=\"blue\",,lty=1\n  )\nlines(\n  x=seq(-3,3,0.1),\n  y=(1/4)*seq(-3,3,0.1)+0.5,\n  type='l',col=\"red\",lty=2\n)\nlegend(\n  -10,1,\n  legend=c(\n    \"logistic activation\",\n    \"linear activation\"\n    ),\n  col=c(\"blue\",\"red\"),\n  lty=c(1,2)\n  )\n\n\n\n\n\n\n\n\n\n\n\nFor probabilities of between 0.3 to 0.7, we see that the logistic activation function maps very closely to that of a simply linear one. It is only at the more extreme probabilities that they diverge."
  },
  {
    "objectID": "posts/glm_logit.html#linearity-in-terms-of-log-odds",
    "href": "posts/glm_logit.html#linearity-in-terms-of-log-odds",
    "title": "Logistic Regression",
    "section": "Linearity in terms of log-odds!",
    "text": "Linearity in terms of log-odds!\nTo further intuition, it can also be useful to rearrange the regression in terms of \\(X_i\\beta\\).\nBy doing this, we find that we are fitting a model where the “log-odds” are linearly related to its predictors:\n\n\nLog-odds means taking the logarithm of the probability of success divided by the probability of failure\n\\[\n\\displaylines{\n\\begin{align}\np(y_i|X_i) = p_i & = \\frac{1}{1+e^{-X_i\\beta}} \\\\\n\\therefore 1 + e^{-X_i\\beta} & = \\frac{1}{p_i} \\\\\n\\therefore e^{-X_i\\beta} & = \\frac{1}{p_i} - \\frac{p_i}{p_i} = \\frac{1-p_i}{p_i} \\\\\n\\therefore e^{X_i\\beta} & = \\frac{p_i}{1-p_i} \\\\\n\\therefore \\ln{\n  \\left\\{ \\frac{p_i}{1-p_i} \\right\\}\n  } & = X_i\\beta\n\\end{align}\n}\n\\]\nThis is a “link function” - the link between the outcome, \\(y\\), and the linear predictors \\(X\\) via \\(\\beta\\). This specific link function is called the “logit function”.\nAnd so it is now clear the inverse logit is the logistic function:\n\\[\n\\displaylines{\n\\begin{align}\n\\text{logit}(x) & = \\ln{\n  \\left\\{ \\frac{x}{1-x} \\right\\}\n  } \\\\\n\\text{logistic}(x)\n& = \\frac{1}{1+e^{-x}} \\\\\n& = \\text{logit}^{-1}(x) \\\\\n\\end{align}\n}  \n\\]"
  },
  {
    "objectID": "posts/glm_logit.html#optimising-the-coefficients",
    "href": "posts/glm_logit.html#optimising-the-coefficients",
    "title": "Logistic Regression",
    "section": "Optimising the coefficients",
    "text": "Optimising the coefficients\nWe minimise the cost function by finding the optimum coefficient values \\(\\beta^*\\) so that the partial differential is equal to zero.\n\\[\n\\displaylines{\n\\begin{align}\n& \\frac{\\partial}{\\partial \\beta_j}p(y|\\beta,X) \\\\ \\\\\n= &\n\\frac{\\partial}{\\partial \\beta_j} \\left(\n\\sum_{i=1}^{N}{ -y_i\\log{ \\left[ p_i \\right] } } +\n  \\sum_{i=1}^{N}{ -(1-y_i)\\log{ \\left[ 1-p_i\\right] } }\n\\right) \\\\ \\\\\n= &\n\\sum_{i=1}^{N}{ y_i\n  \\frac{\\partial p_i/\\partial \\beta_j}{p_i}\n  } +\n\\sum_{i=1}^{N}{ (1-y_i)\n  \\frac{(1-\\partial p_i)/\\partial \\beta_j}{1-p_i}\n  }\n\\end{align}\n}\n\\]\nThis is as far as we can get without modelling \\(P(\\varepsilon_i &lt; X \\beta)\\). So let’s look at substituting \\(p_i\\), \\(1-p_i\\), \\(\\partial p_i/\\partial \\beta\\) and \\(\\partial (1-p_i)/\\partial \\beta\\) into our first moment condition to derive the optimal coefficients:\n\\[\n\\displaylines{\n\\begin{align}\np_i & = P(\\varepsilon_i &lt; X \\beta) = (1+e^{-X\\beta})^{-1}\n\\\\ & = \\frac{1}{1+e^{-X\\beta}}\n\\\\\n\\therefore 1-p_i & = 1 - (1+e^{-X\\beta})^{-1} = \\frac{1+e^{-X\\beta}}{1+e^{-X\\beta}} - \\frac{1}{1+e^{-X\\beta}}\n\\\\ & = \\frac{e^{-X\\beta}}{1+e^{-X\\beta}}  \\\\\n\\therefore \\frac{\\partial p_i}{\\partial \\beta_j}\n& = -1(1+e^{-X\\beta})^{-2} \\times -x_je^{-X\\beta}\n\\\\ & = \\frac{-x_j \\times e^{-X\\beta}}{(1+e^{-X\\beta})^2}\n\\\\\n\\therefore \\frac{\\partial (1-p_i)}{\\partial \\beta_j} & = -1(1+e^{-X\\beta})^{-2} \\times x_j \\times e^{-X\\beta}\n\\\\ & = \\frac{x_j \\times e^{-X\\beta}}{(1+e^{-X\\beta})^2}\n\\end{align}\n}\n\\]\n\\[\n\\displaylines{\n\\begin{align}\n\\therefore \\frac{\\partial}{\\partial \\beta_j}p(y|\\beta,X) & =\n\\sum_{i=1}^{N}{ y_i\n  \\frac{\\partial p_i/\\partial \\beta_j}{p_i}\n  } +\n\\sum_{i=1}^{N}{ (1-y_i)\n  \\frac{\\partial(1-\\partial p_i)/ \\beta_j}{1-p_i}\n  } \\\\\n& =\n\\sum_{i=1}^{N}{ y_i\n  \\frac{\\frac{-x_j \\times e^{-X\\beta}}{(1+e^{-X\\beta})^2}}{\\frac{1}{1+e^{-X\\beta}}}\n  } +\n\\sum_{i=1}^{N}{ (1-y_i)\n  \\frac{\\frac{x_j \\times e^{-X\\beta}}{(1+e^{-X\\beta})^2}}{\\frac{e^{-X\\beta}}{1+e^{-X\\beta}}}\n  }\n\\\\\n& =\n\\sum_{i=1}^{N}{ -x_jy_i\n  \\frac{ e^{-X\\beta}}{1+e^{-X\\beta}}\n  } +\n\\sum_{i=1}^{N}{ x_j(1-y_i)\n  \\frac{1}{1+e^{-X\\beta}}\n  }    \n\\\\\n& =\n-x_j\\sum_{i=1}^{N}{\n  y_i \\times  \\left(1-p_i\\right) +\n  (1-y_i) \\times p_i\n  }    \n\\\\\n\\end{align}\n}\n\\]\nThus there is no closed form solution like OLS. However, given the cost function is convex, using an optimization like Newton Raphson will find the optimum coefficients.\nFin."
  }
]