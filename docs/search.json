[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Sciencing Data",
    "section": "",
    "text": "Generalized Linear Models from scratch\n\n\n\n\n\n\nGeneralized Linear Models\n\n\nMaximum Likelihood\n\n\n\n\n\n\n\n\n\nMay 28, 2024\n\n\nChris Kelly\n\n\n\n\n\n\n\n\n\n\n\n\nThe gaussian integral\n\n\n\n\n\n\nGaussian\n\n\nFundamentals\n\n\n\n\n\n\n\n\n\nMay 13, 2024\n\n\nChris Kelly\n\n\n\n\n\n\n\n\n\n\n\n\nDeriving the normal distribution\n\n\n\n\n\n\nGaussian\n\n\nFundamentals\n\n\n\n\n\n\n\n\n\nMay 11, 2024\n\n\nChris Kelly\n\n\n\n\n\n\n\n\n\n\n\n\nDeriving the poisson distribution from binomial\n\n\n\n\n\n\nBinomial\n\n\nPoisson\n\n\n\n\n\n\n\n\n\nFeb 28, 2024\n\n\nChris Kelly\n\n\n\n\n\n\n\n\n\n\n\n\nEstimating population variance from a sample\n\n\n\n\n\n\nFundamentals\n\n\nBias\n\n\n\n\n\n\n\n\n\nFeb 26, 2024\n\n\nChris Kelly\n\n\n\n\n\n\n\n\n\n\n\n\nLogistic Regression\n\n\n\n\n\n\nMaximum Likelihood\n\n\nGeneralized Linear Models\n\n\n\n\n\n\n\n\n\nFeb 25, 2024\n\n\nChris Kelly\n\n\n\n\n\n\n\n\n\n\n\n\nShrinkage priors for Lasso and Ridge\n\n\n\n\n\n\nBayes\n\n\nMaximum Likelihood\n\n\nRegularization\n\n\nLaplace\n\n\n\n\n\n\n\n\n\nFeb 24, 2024\n\n\nChris Kelly\n\n\n\n\n\n\n\n\n\n\n\n\nOLS vs MLE with gaussian noise\n\n\n\n\n\n\nLinear Models\n\n\nMaximum Likelihood\n\n\nGauss-Markov\n\n\n\n\n\n\n\n\n\nFeb 23, 2024\n\n\nChris Kelly\n\n\n\n\n\n\n\n\n\n\n\n\nSandwiches: robust covariance error estimation\n\n\n\n\n\n\nLinear Models\n\n\nGauss-Markov\n\n\nStandard errors\n\n\n\n\n\n\n\n\n\nFeb 22, 2024\n\n\nChris Kelly\n\n\n\n\n\n\n\n\n\n\n\n\nBLUE coefficients: bias and efficiency\n\n\n\n\n\n\nLinear Models\n\n\nOLS\n\n\nGauss-Markov\n\n\n\n\n\n\n\n\n\nFeb 21, 2024\n\n\nChris Kelly\n\n\n\n\n\n\n\n\n\n\n\n\nDeriving OLS coefficients (multivariate)\n\n\n\n\n\n\nLinear Models\n\n\nOLS\n\n\n\n\n\n\n\n\n\nFeb 20, 2024\n\n\nChris Kelly\n\n\n\n\n\n\n\n\n\n\n\n\nMean Squared Error vs Mean Absolute Error\n\n\n\n\n\n\nCost functions\n\n\n\n\n\n\n\n\n\nFeb 8, 2024\n\n\nChris Kelly\n\n\n\n\n\n\n\n\n\n\n\n\nDifferentiation formulae\n\n\n\n\n\n\nFundamentals\n\n\n\n\n\n\n\n\n\nFeb 8, 2024\n\n\nChris Kelly\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/blue_ols.html",
    "href": "posts/blue_ols.html",
    "title": "BLUE coefficients: bias and efficiency",
    "section": "",
    "text": "What are we exploring?\n\n\n\nProving that the OLS coefficient is the best linear unbiased estimator."
  },
  {
    "objectID": "posts/blue_ols.html#are-the-ols-coefficients-blue",
    "href": "posts/blue_ols.html#are-the-ols-coefficients-blue",
    "title": "BLUE coefficients: bias and efficiency",
    "section": "Are the OLS coefficients “BLUE”?",
    "text": "Are the OLS coefficients “BLUE”?\nWe find a unique solution to the set of coefficients that minimize the sum of squared residuals analytically (see its derivation here):\n\\[\n\\hat{\\beta}^{OLS}=(X^{\\intercal}X)^{-1}X^{\\intercal}y\n\\]\nHowever, how do we know if these coefficients are the best ones we can estimate?\nFor the estimated coefficients to be the Best Linear Unbiased Estimator (i.e. “BLUE”):\n\nThe best estimator has to be unbiased: \\(E[\\hat{\\beta}^*] = \\beta\\)\nAnd among all possible linear, unbiased estimators, it must have the smallest variance: \\(V[\\hat{\\beta}^{*}] &lt; V[\\hat{\\beta}^{Z}]\\)\n\nWe want to ensure our OLS estimate is the best, i.e. that \\(\\hat{\\beta}^{OLS} = \\hat{\\beta}^{*}\\). To achieve this, first we need to confirm it is unbiased. Then given this is true, we can check that the coefficient is most efficient vs all other unbiased estimators.\n\n\n\n\n\n\nGauss Markov Assumptions\n\n\n\nAlong the way, we will outline the Gauss-Markov assumptions utilised that ensure the OLS coefficient is BLUE."
  },
  {
    "objectID": "posts/blue_ols.html#setting-the-scene",
    "href": "posts/blue_ols.html#setting-the-scene",
    "title": "BLUE coefficients: bias and efficiency",
    "section": "Setting the scene",
    "text": "Setting the scene\n\nThe true coefficient and model\n\\(\\beta\\) is the true (unobserved) relationship between all the relevant explanatory features, \\(X\\), and their associated observed outcomes, \\(y\\). In other words, if we observed the entire population of data, it is the relationship we would find.\nConcretely, we assume the outcome is a linear function of all its relevant features. This “true model” perfectly predicts the outcome, except for random noise \\(\\epsilon\\) that influences the observed outcome: \\(y = X\\beta + \\epsilon\\)\n\n\nThe estimated coefficient\n\\(\\hat{\\beta}\\) is our estimated coefficient for the true relationship \\(\\beta\\). In reality, we estimate \\(\\hat{\\beta}\\) from the small, finite sample of size \\(n\\) that is collected, not the whole population. Given any random sample could be collected, we can term the coefficient resulting from the optimum estimation proceedure to be \\(\\hat{\\beta}^*\\). We want to understand if \\(\\hat{\\beta}^{OLS} = \\hat{\\beta}^*\\).\n\n\nThe expected estimated coefficient\n\\(E[\\hat{\\beta}]\\) is the “expected” estimated coefficient. Imagine we repeat the action of estimating the coefficient \\(\\hat{\\beta}\\) many times, each time collecting a new sample (where each observation is sampled i.i.d), and recording the value for the estimated coefficient. \\(E[\\hat{\\beta}]\\) would then be the average of all of those estimated coefficients. If the OLS coefficient is unbiased, then the expected coefficient estimate should be equal to the true one, \\(E[\\hat{\\beta}^{OLS}]=\\beta\\).\n\n\nThe variance of the estimated coefficient\n\\(V[\\hat{\\beta}]\\) is the variance of the estimated coefficient. It determines how much we might expect our estimate \\(\\hat{\\beta}\\) to differ from the true \\(\\beta\\) for any sample drawn. Given the OLS coefficient has been shown to be unbiased, if it is BLUE we expect its variance to be lower than another other unbiased choice \\(\\hat{\\beta}^{Z}\\). Concretely, we want to find \\(V[\\hat{\\beta}^{OLS}] &lt; V[\\hat{\\beta}^{Z}]\\)."
  },
  {
    "objectID": "posts/blue_ols.html#bias",
    "href": "posts/blue_ols.html#bias",
    "title": "BLUE coefficients: bias and efficiency",
    "section": "Bias",
    "text": "Bias\nOften our small finite samples of size \\(n\\) are not a perfect reflection of the population they are drawn from. This “sampling error” means we might estimate a different relationship between \\(X\\) and \\(y\\) than the true relationship of the population, i.e. \\(\\hat{\\beta} \\neq \\beta\\).\nHowever, we should expect our estimated coefficient to be equal to the true value on average. This means we do not want to have a bias towards the estimate being systematically too small or too large, for example. In other words, if we repeated the whole proceedure thousands of times (each time taking new samples, and estimating a coefficient from the new sample) then the average of all the estimated coefficients values should be equal to the true value, i.e. \\(E[\\hat{\\beta}] = \\beta\\).\nRecall that we believe there is a true model that follows the form:\n\\[\ny = X\\beta + \\epsilon\n\\]\n\n\n\n\n\n\nGM1: Linearity \n\n\n\nThe formula above relies on the first Gauss-Markov assumption - that the dependent variable \\(y\\) is assumed to be a linear function of the variables \\(X\\). Note that implies that the proper functional form has been selected (i.e. the relationship is linear) and there are no omitted variables - a huge assumption!\n\n\nIf we substitute this into our estimated coefficient:\n\\[\n\\displaylines{\n\\begin{align}\n\\hat{\\beta}^{OLS} & = (X^{\\intercal}X)^{-1}X^{\\intercal}y\n\\\\ & = (X^{\\intercal}X)^{-1}X^{\\intercal}(X\\beta+\\epsilon)\n\\\\ & = (X^{\\intercal}X)^{-1}X^{\\intercal}X\\beta+(X^{\\intercal}X)^{-1}X^{\\intercal}\\epsilon\n\\\\ & = \\beta+(X^{\\intercal}X)^{-1}X^{\\intercal}\\epsilon\n\\end{align}\n}\n\\]\nwe show that the estimated coefficient \\(\\hat{\\beta}^{OLS}\\) will differ from the true value depending on the random error \\(\\epsilon\\) associated with the particular finite sample collected.\nNow let’s now take the expectation, to determine when the coefficient is unbiased. In other words, what is the “average” coefficient if we took the sample many times: \\[\n\\displaylines{\n\\begin{align}\nE[\\hat{\\beta}^{OLS}]\n& = \\beta +(X^{\\intercal}X)^{-1}X^{\\intercal}E[\\epsilon]\n\\\\ & = \\beta & \\iff E[\\epsilon] & = 0\n\\end{align}\n\\\\\n}\n\\]\nWe find that the coefficient is unbiased as long as the expected error is also zero.\n\n\n\n\n\n\nGM2: Strict Exogeneity \n\n\n\nThe second Gauss-Markov assumption is strict exogeneity, where the expected error is zero for all feature values: \\(E[\\epsilon|X] = 0\\). By definition, the weaker exogeneity statement of \\(E[\\epsilon] = 0\\) is implied by having the expected error conditional being equal to zero."
  },
  {
    "objectID": "posts/blue_ols.html#efficiency",
    "href": "posts/blue_ols.html#efficiency",
    "title": "BLUE coefficients: bias and efficiency",
    "section": "Efficiency",
    "text": "Efficiency\nTo be the “best”, the OLS estimator also needs to be efficient. This means that it has the lowest variance of all unbiased estimators. This section looks to prove this.\n\nCoefficient variance for OLS\nFirst, let’s derive the variance from the coefficients estimated using OLS, termed \\(V[\\hat{\\beta}^{OLS}]\\). As before, we substitute the true model \\(y = X\\beta + \\epsilon\\) into the coefficient estimated through OLS:\n\\[\n\\displaylines{\n\\begin{align}\n\\hat{\\beta}^{OLS}-\\beta\n& = \\beta +  ((X^{\\intercal}X)^{-1}X^{\\intercal}\\epsilon) - \\beta \\\\\n& = (X^{\\intercal}X)^{-1}X^{\\intercal}\\epsilon\n\\\\ \\\\\n\\therefore\n(\\hat{\\beta}^{OLS}-\\beta)(\\hat{\\beta}^{OLS}-\\beta)^{\\intercal}\n& = ((X^{\\intercal}X)^{-1}X^{\\intercal}\\epsilon)((X^{\\intercal}X)^{-1}X^{\\intercal}\\epsilon)^{\\intercal}\n\\\\ & =\n(X^{\\intercal}X)^{-1}X^{\\intercal}\\epsilon\\epsilon^{\\intercal}X(X^{\\intercal}X)^{-1} \\\\ \\\\\n\\therefore\nV(\\hat{\\beta}^{OLS}) & = E[(\\hat{\\beta}^{OLS}-\\beta)(\\hat{\\beta}^{OLS}-\\beta)^{\\intercal}]\n\\\\ & =\n(X^{\\intercal}X)^{-1}X^{\\intercal}E[\\epsilon\\epsilon^{\\intercal}]X(X^{\\intercal}X)^{-1}\n\\end{align}\n}\n\\]\nThis is sometimes called the sandwich estimator - post soon to follow on this!\n\n\nCoefficient variance assuming “spherical errors”\nWe can simplify this further by appling some assumptions to the estimated error variance \\(E[\\epsilon\\epsilon^{\\intercal}]\\):\n\\[\n\\displaylines{\n\\begin{align}\nE[\\epsilon \\epsilon^{\\intercal}] & =  \n\\begin{bmatrix}\nE[\\epsilon_1^2] & \\cdots & E[\\epsilon_1\\epsilon_n] \\\\\n\\vdots & \\ddots & \\vdots \\\\\nE[\\epsilon_n\\epsilon_1] & \\cdots & E[\\epsilon_n^2]\n\\end{bmatrix} \\\\ \\\\\n& = \\begin{bmatrix}\n\\frac{1}{n}\\sum_{i=1}^{n}{\\epsilon_i^2} & \\cdots & 0 \\\\\n\\vdots & \\ddots & \\vdots \\\\\n0 & \\cdots & \\frac{1}{n}\\sum_{i=1}^{n}{\\epsilon_i^2}\n\\end{bmatrix} \\\\ \\\\\n& = \\begin{bmatrix}\n\\hat{\\sigma}^2 & \\cdots & 0 \\\\\n\\vdots & \\ddots & \\vdots \\\\\n0 & \\cdots & \\hat{\\sigma}^2\n\\end{bmatrix} \\\\ \\\\\n& = \\hat{\\sigma}^2I\n\\end{align}\n}\n\\]\nHow can we jump to this result? Well we are making two assumptions:\n\nNo serial correlation: \\(\\rho_{\\epsilon_{i},\\epsilon_{i \\neq j}} = 0\\). No correlation between sample errors means that \\(E[\\epsilon_i \\epsilon_{j \\neq i}] = 0\\), and hence the off-diagonals of the error covariance matrix are zero.\nHomoskedasticity: the assumption of uniform error variance for all samples means that \\(V[\\epsilon_i^2] = V[\\epsilon_{j \\neq i}^2] = \\hat{\\sigma}^2\\). And our best approximation for \\(\\hat{\\sigma}^2\\) is simply taking the average squared error: \\(\\hat{\\sigma}^2 = \\frac{1}{n}\\sum_{i=1}^{n}{\\epsilon_i^2}\\)\n\n\n\n\n\n\n\nGM3: Spherical errors \n\n\n\nThe third Gauss-Markov assumption is spherical errors, \\(E[\\epsilon\\epsilon^{\\intercal}|X] = 0\\). This means that the outer product of the expected errors is a scalar matrix, which implies no serial correlation and homoskedasticity.\n\n\n\n\nIt is especially important to make the right assumptions about \\(E[\\epsilon\\epsilon^{\\intercal}|X]\\) as it impacts where our estimate of the standard errors is correct! We will dive into what happens to SE if we violate these assumptions in another post.\nSince we now see that \\(\\hat{\\sigma}^2\\) is a scalar matrix, we can thus simplfy the variance formula further:\n\\[\n\\displaylines{\n\\begin{align}\nV(\\hat{\\beta}^{OLS}) & =\n(X^{\\intercal}X)^{-1}X^{\\intercal}E[\\epsilon\\epsilon^{\\intercal}]X(X^{\\intercal}X)^{-1}\n\\\\ & =\n(X^{\\intercal}X)^{-1}X^{\\intercal} \\hat{\\sigma}^2I X(X^{\\intercal}X)^{-1}\n\\\\ & =\n\\hat{\\sigma}^2\n\\cancel{(X^{\\intercal}X)^{-1}}\n\\cancel{X^{\\intercal} X}\n(X^{\\intercal}X)^{-1}\n\\\\ & = \\hat{\\sigma}^2(X^{\\intercal}X)^{-1}\n\\end{align}\n}\n\\]\n\n\nFormulating an alternative unbiased coefficient\nNext step - lets formulate another estimator, \\(\\hat{\\beta}^{z}\\), which differs from \\(\\hat{\\beta}^{OLS}\\) by a non-zero matrix \\(A\\). See how they both differ below:\n\\[\n\\displaylines{\n\\begin{align}\n\\hat{\\beta}^{OLS} & =(X^{\\intercal}X)^{-1}X^{\\intercal}y \\\\\n\\hat{\\beta}^{Z} & =\\left((X^{\\intercal}X)^{-1}X^{\\intercal}+A\\right)y\n\\end{align}\n}\n\\]\nNow we need to ensure this new estimator is not biased. So by taking the expectation in the same was as for OLS…\n\\[\n\\displaylines{\n\\begin{align}\nE[\\hat{\\beta}^{Z}] & = E\\left[ \\left((X^{\\intercal}X)^{-1}X^{\\intercal}+A\\right)y \\right]\n\\\\ & = \\left((X^{\\intercal}X)^{-1}X^{\\intercal}+A\\right)(X\\beta+ \\cancel{E\\left[\\epsilon \\right]}) & \\because E[\\epsilon] = 0\n\\\\ & = (X^{\\intercal}X)^{-1}X^{\\intercal}X\\beta+AX\\beta\n\\\\ & = \\beta+AX\\beta\n\\end{align}\n}\n\\]\nSo the estimator is only unbiased iff \\(AX=0\\). This is important to note when comparing the variance between unbiased coefficients - see below!\n\n\nVariance of the alternative unbiased coefficient\nJust like before, we calculate the variance:\n\\[\n\\displaylines{\n\\begin{align}\nV[\\hat{\\beta}^{Z}]\n& = V\\left[ \\left((X^{\\intercal}X)^{-1}X^{\\intercal}+A\\right)y \\right]\n\\\\ & = \\left((X^{\\intercal}X)^{-1}X^{\\intercal}+A\\right) V[y] \\left((X^{\\intercal}X)^{-1}X^{\\intercal}+A\\right)^{\\intercal}\n\\\\ & = \\hat{\\sigma}^2 \\left((X^{\\intercal}X)^{-1}X^{\\intercal}+A\\right) \\left((X^{\\intercal}X)^{-1}X^{\\intercal}+A\\right)^{\\intercal}\n& \\because E[\\epsilon \\epsilon^{\\intercal}|X] = 0\n\\\\ & = \\hat{\\sigma}^2\\left((X^{\\intercal}X)^{-1}X^{\\intercal}+A\\right) \\left(X(X^{\\intercal}X)^{-1}+A^{\\intercal}\\right)\n\\\\ & = \\hat{\\sigma}^2 \\left(\n(X^{\\intercal}X)^{-1}X^{\\intercal} X(X^{\\intercal}X)^{-1} + AX(X^{\\intercal}X)^{-1} + (X^{\\intercal}X)^{-1}X^{\\intercal}A^{\\intercal} + AA^{\\intercal}\n\\right)\n\\\\ & = \\hat{\\sigma}^2\n(X^{\\intercal}X)^{-1} + \\hat{\\sigma}^2AA^{\\intercal}\n& \\because AX = 0\n\\\\ & = V[\\beta^{OLS}] + \\hat{\\sigma}^2AA^{\\intercal}\n\\end{align}\n}\n\\]\nNow since AA is surely a positive semi-definite matrix, then we know that \\(V[\\hat{\\beta}^{Z}] &gt; V[\\hat{\\beta}^{OLS}]\\).\nWe have shown that \\(\\hat{\\beta}^{OLS}\\) has the smallest variance among all unbiased estimators!"
  },
  {
    "objectID": "posts/blue_ols.html#summarising-the-gauss-markov-assumptions",
    "href": "posts/blue_ols.html#summarising-the-gauss-markov-assumptions",
    "title": "BLUE coefficients: bias and efficiency",
    "section": "Summarising the Gauss-markov assumptions",
    "text": "Summarising the Gauss-markov assumptions\nAlong the way, we showed where assumptions were needed to ensure the OLS coefficient estimation is BLUE.\nWe actually missed one out, but it is actually quite trivial to see from the OLS coefficient formula:\n\\[\n\\hat{\\beta}^{OLS}=(X^{\\intercal}X)^{-1}X^{\\intercal}y\n\\]\n\n\n\n\n\n\nGM4: Full rank \n\n\n\nThe matrix \\(X\\) must be of full rank \\(k\\), so that it is possible to invert the matrix \\(X^{\\intercal}X\\). This is equivalent to having no perfect multi-collinearity.\n\n\nWe have now collected our full set of Gauss-Markov assumptions required for the OLS coefficient to be BLUE:\n\nLinearity\nStrict Exogeneity\nSpherical Errors\nFull rank"
  },
  {
    "objectID": "posts/mse_mean_mae_median.html",
    "href": "posts/mse_mean_mae_median.html",
    "title": "Mean Squared Error vs Mean Absolute Error",
    "section": "",
    "text": "What we are solving\n\n\n\nMinimizing MAE tends predictions towards the sample median, whereas for MSE predictions tend towards the sample mean.\n\n\nIn the absence of informative features, an ML algorithm minimizing the sum of squared errors will tend towards predicting the mean of the sample. However, minimizing the sum of absolute errors will tend towards predicting the median of the sample.\nThis post dives into why this is the case.\n\nMinimizing residual sum-of-squares\nLet’s define the residual for sample \\(i\\) as \\(\\epsilon_i\\). We now want to find the prediction \\(\\hat{y}\\) that minimizes the sum of all squared residuals (i.e. where the gradient is zero):\n\\[\n\\displaylines{\n\\begin{align}\n\\min_\\hat{y}{\\left[\\sum_{i=1}^N{\\epsilon_i^2}\\right]}\n\\Rightarrow &\n\\frac{\\partial}{\\partial \\hat{y}} \\sum_{i=1}^N{\\epsilon_i^2} \\\\ = &\n\\frac{\\partial \\left( \\sum_{i=1}^N{\\epsilon_i^2} \\right) }{\\partial\\epsilon}\n\\left( \\frac{\\partial\\epsilon}{\\partial \\hat{y} } \\right) \\\\ = &\n\\sum_{i=1}^N 2\\epsilon_i \\left( \\frac{\\partial\\epsilon_i}{\\partial \\hat{y} }\\right) = 0\n\\end{align}\n}\n\\]\nWe can now substitue in \\(\\epsilon_i = y - \\hat{y}\\):\n\\[\n\\displaylines{\n\\begin{align}\n\\sum_{i=1}^N 2\\epsilon_i \\left( \\frac{\\partial\\epsilon_i}{\\partial \\hat{y} }\\right) &\n= \\sum_{i=1}^N2( y_i- \\hat{y})\\left(\\frac{\\partial( y_i-\\hat{y})}{\\partial \\hat{y}}\\right) \\\\  &\n= \\sum_{i=1}^N2( y_i- \\hat{y} )(-1) \\\\ &\n= \\sum_{i=1}^N2( y_i) - 2ny = 0 \\\\ &\n\\therefore n \\hat{y} = \\sum_{i=1}^N( y_i) \\\\ &\n\\therefore \\hat{y} = \\frac{\\sum_{i=1}^N{y_i}}{n} = \\bar{y}\n\\end{align}\n}\n\\]\nThus we can see that the prediction that minimizes the sum of squared residuals, is simply the mean.\n\n\nMinimize sum of absolute residuals\nWe now do the same think again, but this time look to minimize the sum of all absolute residuals instead.\n\\[\n\\displaylines{\n\\begin{align}\n\\min_\\hat{y}{\\left[\\sum_{i=1}^N{\\mid \\epsilon_i \\mid}\\right]}\n\\Rightarrow &\n\\frac{\\partial}{\\partial \\hat{y}} \\sum_{i=1}^N{\\left(\\epsilon_i^2\\right)^{1/2}} \\\\ = &\n\\frac{\\partial \\sum_{i=1}^N{ \\left(\\epsilon_i^2\\right)^{1/2} } }{\\partial\\epsilon_i^2}\n\\times \\frac{\\partial\\epsilon_i^2}{\\partial \\epsilon_i }\n\\times \\frac{\\partial\\epsilon_i}{\\partial \\hat{y} }  \\\\ = &\n\\frac{1}{2} \\sum_{i=1}^N{ \\left(\\epsilon_i^2\\right)^{-1/2} }\n\\times 2 \\epsilon_i\n\\times \\frac{\\partial\\epsilon_i}{\\partial \\hat{y} }  \\\\ = &\n\\sum_{i=1}^N{ \\left(\\epsilon_i^2\\right)^{-1/2} } \\times \\epsilon_i\n\\times \\frac{\\partial\\epsilon_i}{\\partial \\hat{y} }  \\\\ = &\n\\sum_{i=1}^N \\frac{\\epsilon_i}{\\mid \\epsilon_i \\mid} \\left( \\frac{\\partial\\epsilon_i}{\\partial \\hat{y} }\\right) = 0\n\\end{align}\n}\n\\]\nAnd similarly to before, we can now substitute in \\(\\epsilon_i = y - \\hat{y}\\):\n\\[\n\\displaylines{\n\\begin{align}\n\\sum_{i=1}^N \\frac{\\epsilon_i}{\\mid \\epsilon_i \\mid} \\left( \\frac{\\partial\\epsilon_i}{\\partial \\hat{y} }\\right) &\n= \\sum_{i=1}^N \\frac{ y_i- \\hat{y} }{\\mid y_i- \\hat{y} \\mid}\\left(\\frac{\\partial( y_i-\\hat{y})}{\\partial \\hat{y}}\\right) \\\\  &\n= \\sum_{i=1}^N \\frac{ y_i- \\hat{y} }{\\mid y_i- \\hat{y} \\mid}(-1) = 0\n\\end{align}\n}\n\\]\nNow \\(f(x) = \\frac{ x }{\\mid x \\mid}\\) is an cool transformation, keeping its sign but getting rid of the magnitude of the size of \\(x\\), i.e.:\n\n\\(f(x &lt; 0) = -1\\)\n\\(f(x &gt; 0) = 1\\)\n\nSo to ensure that \\(\\sum f(\\epsilon_i)=0\\), we need to pick a value for \\(\\hat{y}\\) that means half of the errors are \\(&lt;0\\) and half of the errors are \\(&gt;0\\).\nSo that means half the errors must be negative, and half are positive. So \\(\\hat{y}\\) has to be the median value!\nFin."
  },
  {
    "objectID": "posts/ols_coef_derivation.html",
    "href": "posts/ols_coef_derivation.html",
    "title": "Deriving OLS coefficients (matrix algebra)",
    "section": "",
    "text": "What we are exploring\n\n\n\nDeriving a unique, analytical solution to the set of coefficients that minimize the sum of squared residuals."
  },
  {
    "objectID": "posts/ols_coef_derivation.html#summary",
    "href": "posts/ols_coef_derivation.html#summary",
    "title": "Deriving OLS coefficients (matrix algebra)",
    "section": "Summary",
    "text": "Summary\nThe cost function for OLS is the sum of squared residuals, \\(\\hat{\\epsilon}^{\\intercal}\\hat{\\epsilon}\\). In order to fit a good linear model, we want to find optimum values for the estimated vector of coefficients, \\(\\hat{\\beta}^*\\), that minimizes this cost function.\nFirst we do partial differentiation of the cost function with respect to the coefficients. Finding the coefficient values where the partial differential is equal to zero reveals the stationary points of the cost function. For OLS in particular, we can find a unique solution for the choice of coefficients that can be found analytically. The hessian matrix then further proves that this is a global minima."
  },
  {
    "objectID": "posts/ols_coef_derivation.html#deriving-the-optimum-coefficients",
    "href": "posts/ols_coef_derivation.html#deriving-the-optimum-coefficients",
    "title": "Deriving OLS coefficients (matrix algebra)",
    "section": "Deriving the optimum coefficients",
    "text": "Deriving the optimum coefficients\n\n0. Defining the notation\nFor a sample \\(i\\), we observe an outcome \\(y_i\\). \\(y\\) is a vector of all \\(n\\) observed outcomes.\n\\[\n\\underset{n \\times 1} {y} =\n\\begin{bmatrix}\n    y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_{n-1} \\\\ y_n\n\\end{bmatrix}\n\\]\nWe also observe \\(k\\) features for every sample \\(i\\). \\(X\\) is a matrix of these observed features. Note the first column is usually all ones, to include an intercept to optimize (or “bias” term).\n\\[\n\\underset{n \\times k} {X} =\n\\begin{bmatrix}\n  1 & x_{11} & \\cdots & x_{1,k-1} & x_{1,k} \\\\\n  1 & x_{21} & \\cdots & x_{2,k-1} & x_{2,k} \\\\\n  1 & x_{31} & \\cdots & x_{3,k-1} & x_{3,k} \\\\\n  \\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\\n  1 & x_{n-2,1} & \\cdots & x_{n-2,k-1} & x_{n-2,k} \\\\\n  1 & x_{n-1,1} & \\cdots & x_{n-1,k-1} & x_{n-1,k} \\\\\n  1 & x_{n,1} & \\cdots & x_{n,k-1} & x_{n,k}\n\\end{bmatrix}\n\\]\nThe contribution of each feature to the prediction is estimated by the coefficients \\(\\hat{\\beta}\\).\n\\[\n\\underset{k \\times 1} {\\hat{\\beta}} =\n\\begin{bmatrix}\n  \\beta_0 \\\\\n  \\beta_1 \\\\\n  \\vdots \\\\\n  \\beta_{k-1} \\\\\n  \\beta_{k}\n\\end{bmatrix}\n\\]\nWe make predictions, \\(\\hat{y}\\), by calculating the dot product of the features \\(X\\) and the coefficients \\(\\hat{\\beta}\\).\n\\[\n\\hat{y} = X \\hat{\\beta}\n\\]\nwhich is shorthand for this:\n\\[\n\\displaylines{\n\\begin{align}\n\\begin{bmatrix}\n  \\hat{y}_1 \\\\ \\hat{y}_2 \\\\ \\hat{y}_3 \\\\ \\vdots \\\\ \\hat{y}_{n-2} \\\\ \\hat{y}_{n-1} \\\\ \\hat{y}_n\n\\end{bmatrix} & =\n\\begin{bmatrix}\n  1 & x_{11} & \\cdots & x_{1,k-1} & x_{1,k} \\\\\n  1 & x_{21} & \\cdots & x_{2,k-1} & x_{2,k} \\\\\n  1 & x_{31} & \\cdots & x_{3,k-1} & x_{3,k} \\\\\n  \\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\\n  1 & x_{n-2,1} & \\cdots & x_{n-2,k-1} & x_{n-2,k} \\\\\n  1 & x_{n-1,1} & \\cdots & x_{n-1,k-1} & x_{n-1,k} \\\\\n  1 & x_{n,1} & \\cdots & x_{n,k-1} & x_{n,k}\n\\end{bmatrix}\n\\begin{bmatrix}\n  \\hat{\\beta}_0 \\\\\n  \\hat{\\beta}_1 \\\\\n  \\vdots \\\\\n  \\hat{\\beta}_{k-1} \\\\\n  \\hat{\\beta}_{k}\n\\end{bmatrix}\n\\\\ \\\\ & =\n\\begin{bmatrix}\n  \\hat{\\beta}_0 + \\hat{\\beta}_{1}x_{1,1} + \\cdots + \\hat{\\beta}_{k-1}x_{1,k-1} + \\hat{\\beta}_{k}x_{1,k} \\\\\n  \\hat{\\beta}_0 + \\hat{\\beta}_{1}x_{2,1} + \\cdots + \\hat{\\beta}_{k-1}x_{2,k-1} + \\hat{\\beta}_{k}x_{2,k} \\\\\n  \\hat{\\beta}_0 + \\hat{\\beta}_{1}x_{3,1} + \\cdots + \\hat{\\beta}_{k-1}x_{3,k-1} + \\hat{\\beta}_{k}x_{3,k} \\\\\n  \\vdots \\\\\n  \\hat{\\beta}_0 + \\hat{\\beta}_{1}x_{n-2,1} + \\cdots + \\hat{\\beta}_{k-1}x_{2,k-1} + \\hat{\\beta}_{k}x_{n-2,k} \\\\\n  \\hat{\\beta}_0 + \\hat{\\beta}_{1}x_{n-1,1} + \\cdots + \\hat{\\beta}_{k-1}x_{2,k-1} + \\hat{\\beta}_{k}x_{n-1,k} \\\\\n  \\hat{\\beta}_0 + \\hat{\\beta}_{1}x_{n,1} + \\cdots + \\hat{\\beta}_{k-1}x_{2,k-1} + \\hat{\\beta}_{k}x_{n,k} \\\\\n\\end{bmatrix}\n\\end{align}\n}\n\\]\nThe residual is the difference between the true outcome and the model prediction.\n\\[\n\\hat{\\epsilon} = y_i -\\hat{y}_i\n\\]\nwhich is shorthand for this:\n\\[\n\\begin{bmatrix}\n  \\hat{\\epsilon_1} \\\\\n  \\hat{\\epsilon_2} \\\\\n  \\hat{\\epsilon_3} \\\\\n  \\vdots \\\\\n  \\hat{\\epsilon_{n-2}} \\\\\n  \\hat{\\epsilon_{n-1}} \\\\\n  \\hat{\\epsilon_{n}}\n\\end{bmatrix} =\n\\begin{bmatrix}\n    y_1 \\\\ y_2 \\\\ y_3 \\\\ \\vdots \\\\ y_{n-2} \\\\ y_{n-1} \\\\ y_n\n\\end{bmatrix} -\n\\begin{bmatrix}\n  \\hat{y}_1 \\\\ \\hat{y}_2 \\\\ \\hat{y}_3 \\\\ \\vdots \\\\ \\hat{y}_{n-2} \\\\ \\hat{y}_{n-1} \\\\ \\hat{y}_n\n\\end{bmatrix}\n\\]\nOur aim is to find the optimum vector of coefficients, \\(\\hat{\\beta}^*\\), that minimizes the sum of squared residuals:\n\\[\n\\min_{\\beta} \\left( \\epsilon^{\\intercal}\\epsilon \\right)\n\\]\n\n\n1. Expand the sum of squared residuals\nThe first step involves expanding the sum of squared residuals, and substituting in \\(X \\hat{\\beta}\\) for \\(\\hat{y}\\). \\[\n\\displaylines{\n\\begin{align}\n\\sum_{i=1}^n{\\hat{\\epsilon}_i^2} & = \\hat{\\epsilon}^{\\intercal}\\hat{\\epsilon}\n\\\\& =(y-X\\hat{\\beta})^{\\intercal}(y-X\\hat{\\beta})\n\\\\& = y^{\\intercal}y - y^{\\intercal}X\\hat{\\beta}-\n\\hat{\\beta}^{\\intercal} X^{\\intercal}y+\n\\hat{\\beta}^{\\intercal}X^{\\intercal}X\\hat{\\beta}\n\\\\& = y^{\\intercal}y - 2y^{\\intercal}X\\hat{\\beta}\n+\\hat{\\beta}^{\\intercal}X^{\\intercal}X\\hat{\\beta}\n\\end{align}\n}\n\\]\n\n\nNote we can simply add the two middle terms, since are both scalars:\n\\[\n\\displaylines{\ny^{\\intercal}X\\hat{\\beta} =\n\\hat{\\beta}^{\\intercal} X^{\\intercal}y \\\\\n\\because \\underset{1 \\times n}{y^{\\intercal}} \\times\n\\underset{n \\times k}{X} \\times\n\\hat{\\underset{k \\times 1}{\\beta}}\n=\n\\hat{\\underset{1 \\times k}{\\beta}^{\\intercal}} \\times\n\\underset{k \\times n}{X^{\\intercal}} \\times\n\\underset{n \\times 1}{y}\n}\n\\]\n\n\n2. Partially differentiate RSS with respect to beta\nThe second step involves partially differentiating the cost function with respect to its parameters, to understand how it changes as the coefficients vary.\n\\[\n\\displaylines{\n\\begin{align}\n\\frac{\\partial}{\\partial\\hat{\\beta}}\\hat{\\epsilon}^{\\intercal}\\hat{\\epsilon} & \\equiv\n\\begin{bmatrix}\n    \\frac{\\partial}{\\partial\\hat{\\beta}_1}\\hat{\\epsilon}^{\\intercal}\\hat{\\epsilon} \\\\\n    \\frac{\\partial}{\\partial\\hat{\\beta}_2}\\hat{\\epsilon}^{\\intercal}\\hat{\\epsilon} \\\\\n    \\vdots \\\\\n    \\frac{\\partial}{\\partial\\hat{\\beta}_k}\\hat{\\epsilon}^{\\intercal}\\hat{\\epsilon}\n\\end{bmatrix}\n\\\\ & = \\frac{d}{d\\hat{\\beta}}(\ny^{\\intercal}y - 2y^{\\intercal}X\\hat{\\beta}\n+\\hat{\\beta}^{\\intercal}X^{\\intercal}X\\hat{\\beta}) \\\\ & = 0 - 2X^{\\intercal}y +((X^{\\intercal}X)\\hat{\\beta} + (X^{\\intercal}X)^{\\intercal}\\hat{\\beta})\n\\\\ & = -2X^{\\intercal}y +  2((X^{\\intercal}X)\\hat{\\beta})\n\\end{align}\n}\n\\]\n\n\nTwo matrix differentiation rules used here for reference:\n\\[\n\\displaylines{\n\\frac{\\partial}{dx}(Ax) = A^{\\intercal}x \\\\\n\\frac{\\partial}{dx}(xAx) = Ax + A^{\\intercal}x\n}\n\\]\nAnd note \\(X^{\\intercal}X = (X^{\\intercal}X)^{\\intercal}\\) by definition, so we can add the two last terms.\n\n\n3. Find the coefficient values at the stationary point\nNow we find the choices of \\(\\beta\\) where the partial differential is equal to zero. These stationary points for the cost function are either at its maximum or minimum.\nFor OLS - we actually only find one unique solution!\n\\[\n\\displaylines{\n\\begin{align}\n\\cancel{2}X^{\\intercal}y +  \\cancel{2}((X^{\\intercal}X)\\hat{\\beta}) & = 0 \\\\\n\\therefore (X^{\\intercal}X)\\hat{\\beta} & = X^{\\intercal}y \\\\\n\\therefore \\hat{\\beta} & = (X^{\\intercal}X)^{-1}X^{\\intercal}y\n\\end{align}\n}\n\\]\n\n\nNote the need to invert \\(X^{\\intercal}X\\). This is only possible for a full rank matrix.\nThe first term is the (inverse) variance matrix of \\(X\\). This term normalizes the coefficient with respect to the magnitude of \\(X\\).\nThe second term is the covariance matrix between \\(X\\) and \\(y\\). This incorporates the linear relationship between the two in the coefficient.\nHence, the coefficient can be interpreted as the estimated change in \\(y\\) given a one unit change in \\(X\\).\n\n\n4. Check the stationary point is a global minimum (hessian matrix)\nFinally, we derive the hessian matrix, by double-differentiating the cost function with respect to the coefficients:\n\\[\n\\displaylines{\n\\frac{\\partial^2}{\\partial\\hat{\\beta}\\partial\\hat{\\beta}^{\\intercal}}\n\\left(\n  \\hat{\\epsilon}^{\\intercal}\\hat{\\epsilon}\n\\right)\n\\\\ = \\frac{\\partial}{\\partial \\hat{\\beta}^{\\intercal}} \\left( -2X^{\\intercal}y +  2((X^{\\intercal}X)\\hat{\\beta}) \\right)\n\\\\ = 2(X^{\\intercal}X)^{\\intercal}\n}\n\\]\nSince \\(X^{\\intercal}X\\) is clearly positive definite, the cost function is convex. Thus, we know our unique solution for \\(\\beta\\) where the partial differential is at zero is indeed a global minimum for the cost function."
  },
  {
    "objectID": "posts/ols_coef_derivation.html#final-reflections",
    "href": "posts/ols_coef_derivation.html#final-reflections",
    "title": "Deriving OLS coefficients (matrix algebra)",
    "section": "Final reflections",
    "text": "Final reflections\nUnlike logistic regression, or the multiple hidden-layer structure of neural networks, we can “jump” straight to the optimum coefficients for OLS. Why can we do this? Well chiefly its because OLS is a bit of a special case:\n\nThe minima is a global minima: The hessian matrix is positive definite, and hence the cost function is strictly convex. This means we know that when a choice of coefficients is found that ensure the partially differentiated cost function is equal zero, this minima is also a global one, not a local one.\nThere is only one solution for the optimum coefficient: We assume that the matrix is full rank (every feature provides additional predictive power) and that the number of predictors is smaller than the number of obervations. This also means that partially differentiating is okay to do!\nA closed-form solution can be found The predictions are generated from \\(X\\) using a simple, purely algebraic function, i.e. the sum-product of \\(X\\) by \\(\\beta\\). This means we can find an analytical solution to the optimal choice \\(\\beta^*\\). Note this often isn’t possible since non-linear activation functions (i.e. link functions) are often transcendental.\n\nWe will dive into this in another post.\nFin."
  },
  {
    "objectID": "posts/differentiation.html",
    "href": "posts/differentiation.html",
    "title": "Differentiation formulae",
    "section": "",
    "text": "What we are solving\n\n\n\nDeriving why \\(\\frac{d}{dx} \\left(x^{n} \\right) = nx^{n-1}\\)"
  },
  {
    "objectID": "posts/differentiation.html#formulating-an-approach-to-the-problem",
    "href": "posts/differentiation.html#formulating-an-approach-to-the-problem",
    "title": "Differentiation formulae",
    "section": "Formulating an approach to the problem",
    "text": "Formulating an approach to the problem\nThe gradient between two points, \\(x\\) and \\(a\\), is equal to the change in the y axis divided by the change in the x axis.\nFor example, if \\(y=f(x)=x^n\\), then we can approximate the gradient as follows:\n\\[\n\\frac{\\Delta y}{\\Delta x} =\n\\frac{f(x)-f(a)}{x-a} =\n\\frac{x^n-a^n}{x-a}\n\\]\nThis isn’t the same as saying a line which passes through both of these points has the same gradient though. There is a gap between this linear approximation and the exact curve.\nHowever, the smaller the movement across the x-axis, the closer the approximation is to the actual curve. For example, in Figure 1 below, the line drawn between \\(f(1)\\) and \\(f(2)\\) is closer to the gradient at \\(f(1)\\) of the true curve, compared to the line drawn between \\(f(1)\\) and \\(f(3)\\).\n\n\nCode\nimport plotly.graph_objects as go\nimport numpy as np\n\ndef interpolate(n,x1,x2,num=50):\n  x = np.linspace(x1,x2,num)\n  m = (x2**n - x1**n)/(x2 - x1)\n  c = x1**n - m*x1\n  y = m*x+c\n  return({'x':x,'y':y})\n\nn=3\nactual = {'x': np.arange(0,4,0.1)}\nactual['y'] = actual['x']**n\nlb, ub = 1, [3.5,3,2]\nlin0 = interpolate(n,lb,ub[0])\nlin1 = interpolate(n,lb,ub[1])\nlin2 = interpolate(n,lb,ub[2])\n\nfig = go.Figure(data = go.Scatter(mode='lines'))\nfig.add_trace( go.Scatter( x=actual['x'], y=actual['y'], line=dict(dash='solid'), name=f\"f(x)=x^{n}\" ))\nfig.add_trace( go.Scatter( x=lin0['x'], y=lin0['y'], name=f\"f({lb}) -&gt; f({ub[0]})\", line=dict(dash='dot') ))\nfig.add_trace( go.Scatter( x=lin1['x'], y=lin1['y'], name=f\"f({lb}) -&gt; f({ub[1]})\", line=dict(dash='dot') ))\nfig.add_trace( go.Scatter( x=lin2['x'], y=lin2['y'], name=f\"f({lb}) -&gt; f({ub[2]})\", line=dict(dash='dot') ))\nfig.show()\n\n\n\n\n                                                \n\n\nFigure 1: The smaller the change in x, the closer the linear approximation is to the true gradient of f(x)\n\n\n\n\nConsequently, we want to determine the gradient at the limit i.e. where \\(a \\rightarrow x\\), to get the true gradient."
  },
  {
    "objectID": "posts/differentiation.html#factoring-out-x-a",
    "href": "posts/differentiation.html#factoring-out-x-a",
    "title": "Differentiation formulae",
    "section": "Factoring out \\((x-a)\\)",
    "text": "Factoring out \\((x-a)\\)\nFirst though, let’s factor out the \\(x-a\\) term for simplicity. Let’s derive a generic formula for this:\n\nIf \\(n=2\\), then \\(x^2-a^2 = (x-a)(x+a)\\)\nIf \\(n=3\\), then \\(x^3-a^3 = (x-a)(x^2+xa+a^2)\\)\nIf \\(n=4\\), then \\(x^4-a^4 = (x-a)(x^3+x^2a+xa^2+a^3)\\)\nIf \\(n=5\\), then \\(x^5-a^5 = (x-a)(x^4+x^3a+x^2a^2+xa^3+a^4)\\)\nAnd so on. In fact for any \\(n\\), we can derive \\(x^n-a^n = (x-a)\\sum_{i=1}^n \\left( x^{n-i}a^{i-1} \\right)\\)\n\nAnd we can now sub that into our formula, and the \\(x-a\\) cancels out:\n\\[\n\\frac{x^n-a^n}{x-a} =\n\\frac{x-a}{x-a} \\sum_{i=1}^n \\left( x^{n-i}a^{i-1} \\right) =\n\\sum_{i=1}^n \\left( x^{n-i}a^{i-1} \\right)\n\\]"
  },
  {
    "objectID": "posts/differentiation.html#getting-our-result",
    "href": "posts/differentiation.html#getting-our-result",
    "title": "Differentiation formulae",
    "section": "Getting our result",
    "text": "Getting our result\nAnd now let’s calculate the result in the limit, where \\(x\\) approaches \\(a\\):\n\\[\n\\lim_{x \\rightarrow a} \\sum_{i=1}^n \\left( x^{n-i}a^{i-1} \\right) \\sim\n\\sum_{i=1}^n \\left( a^{n-i}a^{i-1} \\right) =\n\\sum_{i=1}^n \\left( a^{n-1} \\right) =\nna^{n-1}\n\\]\nHence \\(\\frac{d}{dx} \\left(x^{n} \\right) = nx^{n-1}\\).\nFin."
  },
  {
    "objectID": "posts/sandwich_estimators.html",
    "href": "posts/sandwich_estimators.html",
    "title": "Sandwiches: robust covariance error estimation",
    "section": "",
    "text": "What are we exploring?\n\n\n\nEstimating the correct coefficient variance under different error assumptions"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I’ve taught myself Data Science over the last few years, and I’ve taken lots of notes to solidify my understanding along the way. I thought it was worthwhile to share them!\nAll notes and code are my own, or references have been added.\nAll images are generated using Microsoft Copilot in Bing."
  },
  {
    "objectID": "posts/sandwich_estimators.html#introducing-sandwiches",
    "href": "posts/sandwich_estimators.html#introducing-sandwiches",
    "title": "Sandwiches: robust covariance error estimation",
    "section": "Introducing sandwiches",
    "text": "Introducing sandwiches\nThe variance for the OLS coefficient estimator is equal to the following:\n\\[\n\\displaylines{\nV(\\hat{\\beta}) =\n(X^{\\intercal}X)^{-1}X^{\\intercal}E[\\epsilon\\epsilon^{\\intercal}]X(X^{\\intercal}X)^{-1}\n}\n\\]\nThis can be though to as a sandwich:\n\nThe “bread” either side: \\((X^{\\intercal}X)^{-1}X^{\\intercal}\\) on the left and its transpose \\(X(X^{\\intercal}X)^{-1}\\) on the right\nThe “meat” in the middle: what we assume for \\(E[\\epsilon\\epsilon^{\\intercal}]\\)\n\nNote that this is the same as the error variance, since \\(V[\\epsilon]=E[\\epsilon\\epsilon^{\\intercal}]-E[\\epsilon]E[\\epsilon^{\\intercal}]\\) and \\(E[\\epsilon] = 0\\)\n\n\nOur coefficient will only be efficient if these assumptions about the expected error are correct! We will explore what happens when the errors are assumed to be homoskedastic, heteroskedastic or clustered."
  },
  {
    "objectID": "posts/sandwich_estimators.html#cucumber-filling-constant-error-variance",
    "href": "posts/sandwich_estimators.html#cucumber-filling-constant-error-variance",
    "title": "Sandwiches: robust covariance error estimation",
    "section": "Cucumber filling: Constant Error Variance 🥒",
    "text": "Cucumber filling: Constant Error Variance 🥒\nUsual OLS is efficient if the true model has “spherical errors”. What does this mean in practice?\n\nErrors are homoskedastic: \\(V(\\epsilon_i)=\\sigma^2\\) for all observations\nErrors are serially uncorrelated: \\(cov(\\epsilon_i,\\epsilon_{j\\neq i})=0\\)\n\nWhat does this look like for \\(E[\\epsilon\\epsilon^{\\intercal}]\\)?\n\nThe diagonal of the matrix is a constant value (scalar), \\(\\sigma^2\\)\nThe off-diagonals are all zero\n\n\\[\n\\hat{\\sigma}^2\\underset{n\\times n}{I} =\n\\begin{bmatrix}\n\\hat{\\sigma}^2 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n0 & \\hat{\\sigma}^2 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n0 & 0 & \\hat{\\sigma}^2 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n0 & 0 & 0 & \\hat{\\sigma}^2 & 0 & 0 & 0 & 0 & 0\\\\\n0 & 0 & 0 & 0 & \\hat{\\sigma}^2 & 0 & 0 & 0 & 0\\\\\n0 & 0 & 0 & 0 & 0 & \\hat{\\sigma}^2 & 0 & 0 & 0\\\\\n0 & 0 & 0 & 0 & 0 & 0 & \\hat{\\sigma}^2 & 0 & 0\\\\\n0 & 0 & 0 & 0 & 0 & 0 & 0 & \\hat{\\sigma}^2 & 0\\\\\n0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & \\hat{\\sigma}^2\n\\end{bmatrix}\n\\]\nA good estimation of the constant error variance \\(\\sigma^2\\) is to apply the standard formula to the residuals (i.e. method of moments):\n\\[\n\\hat{\\sigma^2}=\\frac{1}{n-k}\\sum{\\hat{\\epsilon_i}^2} \\equiv \\frac{\\epsilon^{\\intercal}\\epsilon }{n-k}\n\\]\n\n\n\n\n\n\nThus our sandwich is:\n\n\n\n\\[\n\\underset{k \\times k}{(X^{\\intercal}X)}^{-1}\n\\underset{k \\times n}\n{X^{\\intercal}}\n\\left(\\frac{1}{n-k} \\times \\underset{1 \\times 1}{(\\epsilon^{\\intercal}\\epsilon)}\n\\times\\underset{n \\times n}{I} \\right)\n\\underset{n \\times k}{X}\n\\underset{k \\times k}{(X^{\\intercal}X)}^{-1}\n\\]\n\n\nIn this scenario, the only things that impact the standard error of the coefficient \\(\\beta_k\\) is:\n\nThe variance of all the residuals, \\(\\sigma^2\\)\nThe variance of the feature \\(V(X_k)\\)\n\n\n\nNote that under spherical errors, the sandwich can be simplified: \\[\n\\displaylines{\n\\begin{align}\nV[\\hat{\\beta}]\n& = (X^{\\intercal}X)^{-1}X^{\\intercal}E[\\epsilon\\epsilon^{\\intercal}]X(X^{\\intercal}X)^{-1}\n\\\\ & = (X^{\\intercal}X)^{-1}X^{\\intercal}\\sigma^2IX(X^{\\intercal}X)^{-1}\n\\\\ & = \\sigma^2(X^{\\intercal}X)^{-1}\\cancel{X^{\\intercal}X}\\cancel{(X^{\\intercal}X)^{-1}}\n\\\\ & = \\sigma^2(X^{\\intercal}X)^{-1}\n\\end{align}\n}\n\\]\n\\[\n1\n\\]"
  },
  {
    "objectID": "posts/sandwich_estimators.html#cucumber-filling-constant-error-variance-1",
    "href": "posts/sandwich_estimators.html#cucumber-filling-constant-error-variance-1",
    "title": "Sandwiches: robust covariance error estimation",
    "section": "🥒 Cucumber filling: Constant Error Variance",
    "text": "🥒 Cucumber filling: Constant Error Variance\nAs shown before, usual OLS is efficient if the true model has “spherical errors. What does this mean in practice?\n\nErrors are homoskedastic: \\(V(\\epsilon_i)=\\sigma^2\\) for all observations\nErrors are serially uncorrelated: \\(cov(\\epsilon_i,\\epsilon_{j\\neq i})=0\\)\n\nWhat does this look like for \\(E[\\epsilon\\epsilon^{\\intercal}]\\)?\n\nThe diagonal of the matrix is a constant value (scalar), \\(\\sigma^2\\)\nThe off-diagonals are all zero\n\n\\[\n\\sigma^2\\underset{n\\times n}{I} = \\begin{bmatrix}\n\\sigma^2 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n0 & \\sigma^2 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n0 & 0 & \\sigma^2 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n0 & 0 & 0 & \\sigma^2 & 0 & 0 & 0 & 0 & 0\\\\\n0 & 0 & 0 & 0 & \\sigma^2 & 0 & 0 & 0 & 0\\\\\n0 & 0 & 0 & 0 & 0 & \\sigma^2 & 0 & 0 & 0\\\\\n0 & 0 & 0 & 0 & 0 & 0 & \\sigma^2 & 0 & 0\\\\\n0 & 0 & 0 & 0 & 0 & 0 & 0 & \\sigma^2 & 0\\\\\n0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & \\sigma^2\n\\end{bmatrix}\n\\]\n\nA good estimation of the constant error variance \\(\\sigma^2\\) is the standard formula (i.e. method of moments):\n\n\\[\n\\hat{\\sigma^2}=\\frac{1}{n-k}\\sum{\\hat{\\epsilon_i}^2} \\equiv \\frac{\\epsilon^{\\intercal}\\epsilon }{n-k}\n\\]\n\n💡 Thus our sandwich is: \\(\\underset{k \\times k}{(X^{\\intercal}X)}^{-1}\n\\underset{k \\times n}\n{X^{\\intercal}}\n\\left(\\frac{1}{n-k} \\times \\underset{1 \\times 1}{(\\epsilon^{\\intercal}\\epsilon)}\n\\times\\underset{n \\times n}{I} \\right)\n\\underset{n \\times k}{X}\n\\underset{k \\times k}{(X^{\\intercal}X)}^{-1}\\)\n\nIn this scenario, the only things that impact the standard error of coefficient \\(k\\) is:\n\nThe variance of all the errors, \\(\\sigma^2\\)\nThe variance of the feature \\(V(X_k)\\)\nFinally, note that under homoskedasticity, the sandwich can be simplified:\n\\[\n  V[\\hat{\\beta}] = (X^{\\intercal}X)^{-1}X^{\\intercal}E[\\epsilon\\epsilon^{\\intercal}]X(X^{\\intercal}X)^{-1} \\\\\n  = (X^{\\intercal}X)^{-1}X^{\\intercal}\\sigma^2IX(X^{\\intercal}X)^{-1} \\\\\n  = \\sigma^2(X^{\\intercal}X)^{-1}\\cancel{X^{\\intercal}X}\\cancel{(X^{\\intercal}X)^{-1}}\n  \\]"
  },
  {
    "objectID": "posts/sandwich_estimators.html#cucumber-constant-error-variance",
    "href": "posts/sandwich_estimators.html#cucumber-constant-error-variance",
    "title": "Sandwiches: robust covariance error estimation",
    "section": "Cucumber: Constant Error Variance 🥒",
    "text": "Cucumber: Constant Error Variance 🥒\nUsual OLS is efficient if the true model has “spherical errors”. What does this mean in practice?\n\nErrors are homoskedastic: \\(V(\\epsilon_i)=\\sigma^2\\) for all observations\nErrors are serially uncorrelated: \\(cov(\\epsilon_i,\\epsilon_{j\\neq i})=0\\)\n\nWhat does this look like for \\(E[\\epsilon\\epsilon^{\\intercal}]\\)?\n\nThe diagonal of the matrix is a constant value (scalar), \\(\\sigma^2\\)\nThe off-diagonals are all zero\n\n\\[\n\\hat{\\sigma}^2\\underset{n\\times n}{I} =\n\\begin{bmatrix}\n\\hat{\\sigma}^2 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n0 & \\hat{\\sigma}^2 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n0 & 0 & \\hat{\\sigma}^2 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n0 & 0 & 0 & \\hat{\\sigma}^2 & 0 & 0 & 0 & 0 & 0\\\\\n0 & 0 & 0 & 0 & \\hat{\\sigma}^2 & 0 & 0 & 0 & 0\\\\\n0 & 0 & 0 & 0 & 0 & \\hat{\\sigma}^2 & 0 & 0 & 0\\\\\n0 & 0 & 0 & 0 & 0 & 0 & \\hat{\\sigma}^2 & 0 & 0\\\\\n0 & 0 & 0 & 0 & 0 & 0 & 0 & \\hat{\\sigma}^2 & 0\\\\\n0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & \\hat{\\sigma}^2\n\\end{bmatrix}\n\\]\nA good estimation of the constant error variance \\(\\sigma^2\\) is to apply the standard formula to the residuals (i.e. method of moments):\n\\[\n\\hat{\\sigma^2}=\\frac{1}{n-k}\\sum{\\hat{\\epsilon_i}^2} \\equiv \\frac{\\epsilon^{\\intercal}\\epsilon }{n-k}\n\\]\n\n\n\n\n\n\nThus our “cucumber sandwich” is:\n\n\n\n\\[\n\\underset{k \\times k}{(X^{\\intercal}X)}^{-1}\n\\underset{k \\times n}\n{X^{\\intercal}}\n\\left(\\frac{1}{n-k} \\times \\underset{1 \\times 1}{(\\epsilon^{\\intercal}\\epsilon)}\n\\times\\underset{n \\times n}{I} \\right)\n\\underset{n \\times k}{X}\n\\underset{k \\times k}{(X^{\\intercal}X)}^{-1}\n\\]\n\n\nIn this scenario, the only things that impact the standard error of the coefficient \\(\\beta_k\\) is:\n\nThe variance of all the residuals, \\(\\sigma^2\\)\nThe variance of the feature \\(V(X_k)\\)\n\n\n\nNote that under spherical errors, the sandwich can be simplified: \\[\n\\displaylines{\n\\begin{align}\nV[\\hat{\\beta}]\n& = (X^{\\intercal}X)^{-1}X^{\\intercal}E[\\epsilon\\epsilon^{\\intercal}]X(X^{\\intercal}X)^{-1}\n\\\\ & = (X^{\\intercal}X)^{-1}X^{\\intercal}\\sigma^2IX(X^{\\intercal}X)^{-1}\n\\\\ & = \\sigma^2(X^{\\intercal}X)^{-1}\\cancel{X^{\\intercal}X}\\cancel{(X^{\\intercal}X)^{-1}}\n\\\\ & = \\sigma^2(X^{\\intercal}X)^{-1}\n\\end{align}\n}\n\\]"
  },
  {
    "objectID": "posts/sandwich_estimators.html#ham-heteroskedastic-errors",
    "href": "posts/sandwich_estimators.html#ham-heteroskedastic-errors",
    "title": "Sandwiches: robust covariance error estimation",
    "section": "Ham: Heteroskedastic errors 🍖",
    "text": "Ham: Heteroskedastic errors 🍖\nHeteroskedastic correction is needed if:\n\nErrors vary for every individual: \\(V(\\epsilon_i)=\\sigma_i^2\\) for all observations\nBut they are still independent aka serially uncorrelated: \\(cov(\\epsilon_i,\\epsilon_{j\\neq i})=0\\)\n\nWhat does this look like for \\(E[\\epsilon\\epsilon^{\\intercal}]\\)?\n\nThe diagonal of the matrix is the estimate of variance which is unique for each observation, \\(\\sigma_i^2\\)\nThe off-diagonals are all zero\n\\[\n  \\underset{n \\times n}{\\sigma^2} = \\begin{bmatrix}\n  \\sigma_1^2 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n  0 & \\sigma_2^2 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n  0 & 0 & \\sigma_3^2 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n  0 & 0 & 0 & \\sigma_4^2 & 0 & 0 & 0 & 0 & 0\\\\\n  0 & 0 & 0 & 0 & \\sigma_5^2 & 0 & 0 & 0 & 0\\\\\n  0 & 0 & 0 & 0 & 0 & \\sigma_6^2 & 0 & 0 & 0\\\\\n  0 & 0 & 0 & 0 & 0 & 0 & \\sigma_7^2 & 0 & 0\\\\\n  0 & 0 & 0 & 0 & 0 & 0 & 0 & \\sigma_8^2 & 0\\\\\n  0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & \\sigma_{..}^2\\\\\n  \\end{bmatrix}\n  \\]\nA good estimation of the vector of heteroskedastic error variances \\(\\sigma^2\\) is again to apply the standard formula to the residuals for each individual:\n\n\\[\n\\hat{\\sigma_i^2}=\\frac{1}{n-k}\\sum{\\hat{\\epsilon_i}^2}\n\\]\n\n\n\n\n\n\nThus our “ham sandwich” is:\n\n\n\n\\[\n\\underset{k \\times k}{(X^{\\intercal}X)}^{-1}\n\\underset{k \\times n}{X^{\\intercal}}\n\\left(\\frac{1}{n-k} \\times \\underset{1 \\times n}{(\\epsilon\\odot \\epsilon)} ^{\\intercal}\n\\times\\underset{n \\times n}{I} \\right)\n\\underset{n \\times k}{X}\n\\underset{k \\times k}{(X^{\\intercal}X)}^{-1}\n\\]\n\n\nSimilar to homoskedastic errors, the things that impacts the standard error of coefficient \\(k\\) is:\n\nThe variance of all the individual errors, \\(\\sigma_i^2\\)\nThe variance of the feature \\(V(X_k)\\)"
  },
  {
    "objectID": "posts/sandwich_estimators.html#salmon-spherical-errors",
    "href": "posts/sandwich_estimators.html#salmon-spherical-errors",
    "title": "Sandwiches: robust covariance error estimation",
    "section": "Salmon: Spherical Errors 🐟",
    "text": "Salmon: Spherical Errors 🐟\nUsual OLS is efficient if the true model has “spherical errors”. What does this mean in practice?\n\nErrors are homoskedastic: \\(V(\\epsilon_i)=\\sigma^2\\) for all observations\nErrors are serially uncorrelated: \\(cov(\\epsilon_i,\\epsilon_{j\\neq i})=0\\)\n\nWhat does this look like for \\(E[\\epsilon\\epsilon^{\\intercal}]\\)?\n\nThe diagonal of the matrix is a constant value (scalar), \\(\\sigma^2\\)\nThe off-diagonals are all zero\n\n\\[\n\\hat{\\sigma}^2\\underset{n\\times n}{I} =\n\\begin{bmatrix}\n\\hat{\\sigma}^2 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n0 & \\hat{\\sigma}^2 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n0 & 0 & \\hat{\\sigma}^2 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n0 & 0 & 0 & \\hat{\\sigma}^2 & 0 & 0 & 0 & 0 & 0\\\\\n0 & 0 & 0 & 0 & \\hat{\\sigma}^2 & 0 & 0 & 0 & 0\\\\\n0 & 0 & 0 & 0 & 0 & \\hat{\\sigma}^2 & 0 & 0 & 0\\\\\n0 & 0 & 0 & 0 & 0 & 0 & \\hat{\\sigma}^2 & 0 & 0\\\\\n0 & 0 & 0 & 0 & 0 & 0 & 0 & \\hat{\\sigma}^2 & 0\\\\\n0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & \\hat{\\sigma}^2\n\\end{bmatrix}\n\\]\nA good estimation of the constant error variance \\(\\sigma^2\\) is to apply the standard formula to the residuals (i.e. method of moments):\n\\[\n\\hat{\\sigma^2}=\\frac{1}{n-k}\\sum{\\hat{\\epsilon_i}^2} \\equiv \\frac{\\epsilon^{\\intercal}\\epsilon }{n-k}\n\\]\n\n\n\n\n\n\nThus our “salmon sandwich” is:\n\n\n\n\\[\n\\underset{k \\times k}{(X^{\\intercal}X)}^{-1}\n\\underset{k \\times n}\n{X^{\\intercal}}\n\\left(\\frac{1}{n-k} \\times \\underset{1 \\times 1}{(\\epsilon^{\\intercal}\\epsilon)}\n\\times\\underset{n \\times n}{I} \\right)\n\\underset{n \\times k}{X}\n\\underset{k \\times k}{(X^{\\intercal}X)}^{-1}\n\\]\n\n\nIn this scenario, the only things that impact the standard error of the coefficient \\(\\beta_k\\) is:\n\nThe variance of all the residuals, \\(\\sigma^2\\)\nThe variance of the feature \\(V(X_k)\\)\n\n\n\nNote that under spherical errors, the sandwich can be simplified: \\[\n\\displaylines{\n\\begin{align}\nV[\\hat{\\beta}]\n& = (X^{\\intercal}X)^{-1}X^{\\intercal}E[\\epsilon\\epsilon^{\\intercal}]X(X^{\\intercal}X)^{-1}\n\\\\ & = (X^{\\intercal}X)^{-1}X^{\\intercal}\\sigma^2IX(X^{\\intercal}X)^{-1}\n\\\\ & = \\sigma^2(X^{\\intercal}X)^{-1}\\cancel{X^{\\intercal}X}\\cancel{(X^{\\intercal}X)^{-1}}\n\\\\ & = \\sigma^2(X^{\\intercal}X)^{-1}\n\\end{align}\n}\n\\]"
  },
  {
    "objectID": "posts/sandwich_estimators.html#cheese-clustered-errors",
    "href": "posts/sandwich_estimators.html#cheese-clustered-errors",
    "title": "Sandwiches: robust covariance error estimation",
    "section": "Cheese: Clustered Errors 🧀",
    "text": "Cheese: Clustered Errors 🧀\nCluster-robust errors are correct if:\n\nErrors vary for every individual: \\(V(\\epsilon_i)=\\sigma_i^2\\) for all observations i.e. still heteroskedastic\nAND errors within the same cluster \\(C_l\\) are serially correlated: \\(cov(\\epsilon_i,\\epsilon_{j}) \\neq 0 \\text{ if } \\epsilon_i,\\epsilon_j \\in C_l\\)\n\nNote - errors between clusters are assumed not to be serially correlated though i.e. \\(cov(\\epsilon_i,\\epsilon_{j}) =0 \\text{ if } \\epsilon_i \\in C_l,\\epsilon_j \\in C_{m \\neq l}\\)\n\n\nWhat does this look like for \\(\\epsilon\\epsilon^{\\intercal}\\)?\n\nThe diagonal of the matrix is the estimate of variance which is unique for each observation, \\(\\sigma_i^2\\)\nThe off-diagonals are also populated with the covariance - but only when they are both in the same cluster\n\nTake an example where observations 1, 2 and 3 are in cluster A, observations 4 and 5 are in cluster B, observations 6, 7 and 8 are in cluster C etc.\n\\[\n\\underset{n \\times n}{\\epsilon\\epsilon^{\\intercal}} \\sim \\begin{bmatrix}\n\\epsilon_1^2 & \\epsilon_1\\epsilon_2 & \\epsilon_1\\epsilon_3 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n\\epsilon_2\\epsilon_1 & \\epsilon_2^2 & \\epsilon_2\\epsilon_3 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n\\epsilon_3\\epsilon_1 & \\epsilon_3\\epsilon_2 & \\epsilon_3^2 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n0 & 0 & 0 & \\epsilon_4^2 & \\epsilon_4\\epsilon_5 & 0 & 0 & 0 & 0\\\\\n0 & 0 & 0 & \\epsilon_5\\epsilon_4 & \\epsilon_5^2 & 0 & 0 & 0 & 0\\\\\n0 & 0 & 0 & 0 & 0 & \\epsilon_6^2 & \\epsilon_6\\epsilon_7 & \\epsilon_6\\epsilon_8 & 0\\\\\n0 & 0 & 0 & 0 & 0 & \\epsilon_7\\epsilon_6 & \\epsilon_7^2 & \\epsilon_7\\epsilon_8 & 0\\\\\n0 & 0 & 0 & 0 & 0 & \\epsilon_8\\epsilon_6 & \\epsilon_8\\epsilon_7 & \\epsilon_8^2 & 0\\\\\n0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & \\epsilon_{..}^2\\\\\n\\end{bmatrix}\n\\]\nAdditionally, we have to do a finite-sample correction as well on the degrees of freedom, based on the number of clusters \\(n_c\\)\n\n💡 Thus our sandwich is: \\(\\underset{k \\times k}{(X^{\\intercal}X)}^{-1}\n\\underset{k \\times n}{X^{\\intercal}}\n\\left(\\frac{n-1}{n-k}\\frac{n_c}{n_c-1} \\times\n\\underset{n \\times n}{(\\epsilon \\epsilon^{\\intercal})}\n\\right)\n\\underset{n \\times k}{X}\n\\underset{k \\times k}{(X^{\\intercal}X)}^{-1}\\)\n\n\nIn this scenario, there are a few additional things that impact the standard error of coefficient \\(k\\):\n\nIf errors are correlated within clusters, this will increase the error.\nIf features are correlated within clusters, this will also increase the error (due to the off-diagonals in the error variance matrix)\nAnd if both the errors and feature correlations are the same sign, this will also increase the standard error.\n\nNote the sandwich formula above is equivalent to calculating mini sandwiches per cluster before summing. This is preferable to creating an enormous matrix from the error outer product and zeroing out covariances that aren’t the same cluster\ndef cluster_XeeX(cluster_index):\n    j = X[:,'cluster_id']== cluster_id # j is the indexes of the cluster\n    ec = e[j] # ec is a vector of errors for that cluster\n    sigma_sqc = ec.dot(ec.transpose()) # outerproduct of clustered errors\n    Xc = X[j,:] # Xc is a matrix of features for that cluster\n    XeeXc = Xc.transpose().dot(sigma_sqc).dot(Xc) # X'ee'X for that cluster \n        return(XeeXc) # shape is kxk\n\nXeeXc = np.sum([cluster_XeeX(i) for i in np.unique(cl)], axis = 0)\nn_cl = len(np.unique(cl)) # number of clusters\n\n       # finite-sample correction factor.    # sum XeeX across all clusters\nXeeX = ((n-1) / (n-k)) * (n_cl / (n_cl-1)) * np.sum(XeeXc, axis = 0)\nsandwich = var_X_inv.dot(XeeX).var_X_inv\n\n\n💡 Thus our sandwich can also be written: \\(\\underset{k \\times k}{(X^{\\intercal}X)^{-1}}\n\\left(\n\\frac{n-1}{n-k}\\frac{n_c}{n_c-1} \\times\n\\sum_{c=1}^{n_c}{\\underset{k \\times k}{(X_c^{\\intercal} \\epsilon_c \\epsilon_c^{\\intercal}X_c)}}\n\\right)\n\\underset{k \\times k}\n{(X^{\\intercal}X)^{-1}}\\)\n\nOne word of caution about averaging of clusters (requires a large number of clusters)"
  },
  {
    "objectID": "posts/sandwich_estimators.html#salmon-bagel-spherical-errors",
    "href": "posts/sandwich_estimators.html#salmon-bagel-spherical-errors",
    "title": "Sandwiches: robust covariance error estimation",
    "section": "Salmon bagel: Spherical Errors 🐟",
    "text": "Salmon bagel: Spherical Errors 🐟\nUsual OLS is efficient if the true model has “spherical errors”. What does this mean in practice?\n\nErrors are homoskedastic: \\(V(\\epsilon_i)=\\sigma^2\\) for all observations\nErrors are serially uncorrelated: \\(cov(\\epsilon_i,\\epsilon_{j\\neq i})=0\\)\n\nWhat does this look like for \\(E[\\epsilon\\epsilon^{\\intercal}]\\)?\n\nThe diagonal of the matrix is a constant value (scalar), \\(\\sigma^2\\)\nThe off-diagonals are all zero\n\n\\[\n\\hat{\\sigma}^2\\underset{n\\times n}{I} =\n\\begin{bmatrix}\n\\hat{\\sigma}^2 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n0 & \\hat{\\sigma}^2 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n0 & 0 & \\hat{\\sigma}^2 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n0 & 0 & 0 & \\hat{\\sigma}^2 & 0 & 0 & 0 & 0 & 0\\\\\n0 & 0 & 0 & 0 & \\hat{\\sigma}^2 & 0 & 0 & 0 & 0\\\\\n0 & 0 & 0 & 0 & 0 & \\hat{\\sigma}^2 & 0 & 0 & 0\\\\\n0 & 0 & 0 & 0 & 0 & 0 & \\hat{\\sigma}^2 & 0 & 0\\\\\n0 & 0 & 0 & 0 & 0 & 0 & 0 & \\hat{\\sigma}^2 & 0\\\\\n0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & \\hat{\\sigma}^2\n\\end{bmatrix}\n\\]\nA good estimation of the constant error variance \\(\\sigma^2\\) is to apply the standard formula to the residuals (i.e. method of moments):\n\\[\n\\hat{\\sigma^2}=\\frac{1}{n-k}\\sum{\\hat{\\epsilon_i}^2} \\equiv \\frac{\\epsilon^{\\intercal}\\epsilon }{n-k}\n\\]\n\n\n\n\n\n\nThus our “salmon sandwich” is:\n\n\n\n\\[\n\\underset{k \\times k}{(X^{\\intercal}X)}^{-1}\n\\underset{k \\times n}\n{X^{\\intercal}}\n\\left(\\frac{1}{n-k} \\times \\underset{1 \\times 1}{(\\epsilon^{\\intercal}\\epsilon)}\n\\times\\underset{n \\times n}{I} \\right)\n\\underset{n \\times k}{X}\n\\underset{k \\times k}{(X^{\\intercal}X)}^{-1}\n\\]\n\n\n\n\nNote that under spherical errors, the sandwich can be simplified: \\[\n\\displaylines{\n\\begin{align}\nV[\\hat{\\beta}]\n& = (X^{\\intercal}X)^{-1}X^{\\intercal}E[\\epsilon\\epsilon^{\\intercal}]X(X^{\\intercal}X)^{-1}\n\\\\ & = (X^{\\intercal}X)^{-1}X^{\\intercal}\\sigma^2IX(X^{\\intercal}X)^{-1}\n\\\\ & = \\sigma^2(X^{\\intercal}X)^{-1}\\cancel{X^{\\intercal}X}\\cancel{(X^{\\intercal}X)^{-1}}\n\\\\ & = \\sigma^2(X^{\\intercal}X)^{-1}\n\\end{align}\n}\n\\]\nIn this scenario, the only things that impact the standard error of the coefficient \\(\\beta_k\\) is:\n\nThe variance of all the residuals, \\(\\sigma^2\\)\nThe variance of the feature \\(V(X_k)\\)"
  },
  {
    "objectID": "posts/sandwich_estimators.html#ham-sarnie-heteroskedastic-errors",
    "href": "posts/sandwich_estimators.html#ham-sarnie-heteroskedastic-errors",
    "title": "Sandwiches: robust covariance error estimation",
    "section": "Ham sarnie: Heteroskedastic errors 🍖",
    "text": "Ham sarnie: Heteroskedastic errors 🍖\nHeteroskedastic correction is needed if:\n\nErrors vary for every individual: \\(V(\\epsilon_i)=\\sigma_i^2\\) for all observations\nBut they are still independent aka serially uncorrelated: \\(cov(\\epsilon_i,\\epsilon_{j\\neq i})=0\\)\n\nWhat does this look like for \\(E[\\epsilon\\epsilon^{\\intercal}]\\)?\n\nThe diagonal of the matrix is the estimate of variance which is unique for each observation, \\(\\sigma_i^2\\)\nThe off-diagonals are all zero\n\\[\n  \\underset{n \\times n}{\\sigma^2} = \\begin{bmatrix}\n  \\sigma_1^2 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n  0 & \\sigma_2^2 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n  0 & 0 & \\sigma_3^2 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n  0 & 0 & 0 & \\sigma_4^2 & 0 & 0 & 0 & 0 & 0\\\\\n  0 & 0 & 0 & 0 & \\sigma_5^2 & 0 & 0 & 0 & 0\\\\\n  0 & 0 & 0 & 0 & 0 & \\sigma_6^2 & 0 & 0 & 0\\\\\n  0 & 0 & 0 & 0 & 0 & 0 & \\sigma_7^2 & 0 & 0\\\\\n  0 & 0 & 0 & 0 & 0 & 0 & 0 & \\sigma_8^2 & 0\\\\\n  0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & \\sigma_{..}^2\\\\\n  \\end{bmatrix}\n  \\]\nA good estimation of the vector of heteroskedastic error variances \\(\\sigma^2\\) is again to apply the standard formula to the residuals for each individual:\n\n\\[\n\\hat{\\sigma_i^2}=\\frac{1}{n-k}\\sum{\\hat{\\epsilon_i}^2}\n\\]\n\n\n\n\n\n\nThus our “ham sandwich” is:\n\n\n\n\\[\n\\underset{k \\times k}{(X^{\\intercal}X)}^{-1}\n\\underset{k \\times n}{X^{\\intercal}}\n\\left(\\frac{1}{n-k} \\times \\underset{1 \\times n}{(\\epsilon\\odot \\epsilon)} ^{\\intercal}\n\\times\\underset{n \\times n}{I} \\right)\n\\underset{n \\times k}{X}\n\\underset{k \\times k}{(X^{\\intercal}X)}^{-1}\n\\]\n\n\nSimilar to homoskedastic errors, the things that impacts the standard error of coefficient \\(k\\) is:\n\nThe variance of all the individual errors, \\(\\sigma_i^2\\)\nThe variance of the feature \\(V(X_k)\\)"
  },
  {
    "objectID": "posts/sandwich_estimators.html#cheese-baguette-clustered-errors",
    "href": "posts/sandwich_estimators.html#cheese-baguette-clustered-errors",
    "title": "Sandwiches: robust covariance error estimation",
    "section": "Cheese baguette: Clustered Errors 🧀",
    "text": "Cheese baguette: Clustered Errors 🧀\nCluster-robust errors are correct if:\n\nErrors vary for every individual: \\(V(\\epsilon_i)=\\sigma_i^2\\) for all observations i.e. still heteroskedastic\nAND errors within the same cluster \\(C_l\\) are serially correlated: \\(cov(\\epsilon_i,\\epsilon_{j}) \\neq 0 \\text{ if } \\epsilon_i,\\epsilon_j \\in C_l\\)\n\nNote - errors between clusters are assumed not to be serially correlated though i.e. \\(cov(\\epsilon_i,\\epsilon_{j}) =0 \\text{ if } \\epsilon_i \\in C_l,\\epsilon_j \\in C_{m \\neq l}\\)\n\n\nWhat does this look like for \\(\\epsilon\\epsilon^{\\intercal}\\)?\n\nThe diagonal of the matrix is the estimate of variance which is unique for each observation, \\(\\sigma_i^2\\)\nThe off-diagonals are also populated with the covariance - but only when they are both in the same cluster\n\nTake an example where observations 1, 2 and 3 are in cluster A, observations 4 and 5 are in cluster B, observations 6, 7 and 8 are in cluster C etc.\n\\[\n\\underset{n \\times n}{\\epsilon\\epsilon^{\\intercal}} \\sim \\begin{bmatrix}\n\\epsilon_1^2 & \\epsilon_1\\epsilon_2 & \\epsilon_1\\epsilon_3 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n\\epsilon_2\\epsilon_1 & \\epsilon_2^2 & \\epsilon_2\\epsilon_3 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n\\epsilon_3\\epsilon_1 & \\epsilon_3\\epsilon_2 & \\epsilon_3^2 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n0 & 0 & 0 & \\epsilon_4^2 & \\epsilon_4\\epsilon_5 & 0 & 0 & 0 & 0\\\\\n0 & 0 & 0 & \\epsilon_5\\epsilon_4 & \\epsilon_5^2 & 0 & 0 & 0 & 0\\\\\n0 & 0 & 0 & 0 & 0 & \\epsilon_6^2 & \\epsilon_6\\epsilon_7 & \\epsilon_6\\epsilon_8 & 0\\\\\n0 & 0 & 0 & 0 & 0 & \\epsilon_7\\epsilon_6 & \\epsilon_7^2 & \\epsilon_7\\epsilon_8 & 0\\\\\n0 & 0 & 0 & 0 & 0 & \\epsilon_8\\epsilon_6 & \\epsilon_8\\epsilon_7 & \\epsilon_8^2 & 0\\\\\n0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & \\epsilon_{..}^2\\\\\n\\end{bmatrix}\n\\]\nAdditionally, we have to do a finite-sample correction as well on the degrees of freedom, based on the number of clusters \\(n_c\\)\n\n💡 Thus our sandwich is: \\(\\underset{k \\times k}{(X^{\\intercal}X)}^{-1}\n\\underset{k \\times n}{X^{\\intercal}}\n\\left(\\frac{n-1}{n-k}\\frac{n_c}{n_c-1} \\times\n\\underset{n \\times n}{(\\epsilon \\epsilon^{\\intercal})}\n\\right)\n\\underset{n \\times k}{X}\n\\underset{k \\times k}{(X^{\\intercal}X)}^{-1}\\)\n\n\nIn this scenario, there are a few additional things that impact the standard error of coefficient \\(k\\):\n\nIf errors are correlated within clusters, this will increase the error.\nIf features are correlated within clusters, this will also increase the error (due to the off-diagonals in the error variance matrix)\nAnd if both the errors and feature correlations are the same sign, this will also increase the standard error.\n\nNote the sandwich formula above is equivalent to calculating mini sandwiches per cluster before summing. This is preferable to creating an enormous matrix from the error outer product and zeroing out covariances that aren’t the same cluster\ndef cluster_XeeX(cluster_index):\n    j = X[:,'cluster_id']== cluster_id # j is the indexes of the cluster\n    ec = e[j] # ec is a vector of errors for that cluster\n    sigma_sqc = ec.dot(ec.transpose()) # outerproduct of clustered errors\n    Xc = X[j,:] # Xc is a matrix of features for that cluster\n    XeeXc = Xc.transpose().dot(sigma_sqc).dot(Xc) # X'ee'X for that cluster \n        return(XeeXc) # shape is kxk\n\nXeeXc = np.sum([cluster_XeeX(i) for i in np.unique(cl)], axis = 0)\nn_cl = len(np.unique(cl)) # number of clusters\n\n       # finite-sample correction factor.    # sum XeeX across all clusters\nXeeX = ((n-1) / (n-k)) * (n_cl / (n_cl-1)) * np.sum(XeeXc, axis = 0)\nsandwich = var_X_inv.dot(XeeX).var_X_inv\n\n\n💡 Thus our sandwich can also be written: \\(\\underset{k \\times k}{(X^{\\intercal}X)^{-1}}\n\\left(\n\\frac{n-1}{n-k}\\frac{n_c}{n_c-1} \\times\n\\sum_{c=1}^{n_c}{\\underset{k \\times k}{(X_c^{\\intercal} \\epsilon_c \\epsilon_c^{\\intercal}X_c)}}\n\\right)\n\\underset{k \\times k}\n{(X^{\\intercal}X)^{-1}}\\)\n\nOne word of caution about averaging of clusters (requires a large number of clusters)"
  },
  {
    "objectID": "posts/sandwich_estimators.html#cheese-roll-clustered-errors",
    "href": "posts/sandwich_estimators.html#cheese-roll-clustered-errors",
    "title": "Sandwiches: robust covariance error estimation",
    "section": "Cheese roll: Clustered Errors 🧀",
    "text": "Cheese roll: Clustered Errors 🧀\nCluster-robust errors are needed if:\n\nErrors vary for every individual: \\(V(\\epsilon_i)=\\sigma_i^2\\) for all observations i.e. still heteroskedastic\nAND errors within the same cluster \\(C_l\\) are serially correlated: \\(cov(\\epsilon_i,\\epsilon_{j}) \\neq 0 \\text{ if } \\epsilon_i,\\epsilon_j \\in C_l\\)\n\nNote - errors between clusters are assumed not to be serially correlated though i.e. \\(cov(\\epsilon_i,\\epsilon_{j}) =0 \\text{ if } \\epsilon_i \\in C_l,\\epsilon_j \\in C_{m \\neq l}\\)\n\n\nWhat does this look like for \\(E[\\epsilon\\epsilon^{\\intercal}]\\)?\n\nThe diagonal of the matrix is the estimate of variance which is unique for each observation, \\(\\sigma_i^2\\)\nThe off-diagonals are also populated with the covariance - but only when they are both in the same cluster\n\nHere is an example where observations 1, 2 and 3 are in cluster A, observations 4 and 5 are in cluster B, observations 6, 7 and 8 are in cluster C etc.\n\\[\n\\underset{n \\times n}{\\epsilon\\epsilon^{\\intercal}} \\sim \\begin{bmatrix}\n\\epsilon_1^2 & \\epsilon_1\\epsilon_2 & \\epsilon_1\\epsilon_3 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n\\epsilon_2\\epsilon_1 & \\epsilon_2^2 & \\epsilon_2\\epsilon_3 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n\\epsilon_3\\epsilon_1 & \\epsilon_3\\epsilon_2 & \\epsilon_3^2 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n0 & 0 & 0 & \\epsilon_4^2 & \\epsilon_4\\epsilon_5 & 0 & 0 & 0 & 0\\\\\n0 & 0 & 0 & \\epsilon_5\\epsilon_4 & \\epsilon_5^2 & 0 & 0 & 0 & 0\\\\\n0 & 0 & 0 & 0 & 0 & \\epsilon_6^2 & \\epsilon_6\\epsilon_7 & \\epsilon_6\\epsilon_8 & 0\\\\\n0 & 0 & 0 & 0 & 0 & \\epsilon_7\\epsilon_6 & \\epsilon_7^2 & \\epsilon_7\\epsilon_8 & 0\\\\\n0 & 0 & 0 & 0 & 0 & \\epsilon_8\\epsilon_6 & \\epsilon_8\\epsilon_7 & \\epsilon_8^2 & 0\\\\\n0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & \\epsilon_{..}^2\\\\\n\\end{bmatrix}\n\\]\nAdditionally, we have to do a finite-sample correction as well on the degrees of freedom, based on the number of clusters \\(n_c\\)\n\n\n\n\n\n\nThus our cheese sandwich is:\n\n\n\n\\(\\underset{k \\times k}{(X^{\\intercal}X)}^{-1}\n\\underset{k \\times n}{X^{\\intercal}}\n\\left(\\frac{n-1}{n-k}\\frac{n_c}{n_c-1} \\times\n\\underset{n \\times n}{(\\epsilon \\epsilon^{\\intercal})}\n\\right)\n\\underset{n \\times k}{X}\n\\underset{k \\times k}{(X^{\\intercal}X)}^{-1}\\)\n\n\nIn this scenario, there are a few additional things that impact the standard error of coefficient \\(k\\):\n\nIf errors are correlated within clusters, this will increase the error.\nIf features are correlated within clusters, this will also increase the error (due to the off-diagonals in the error variance matrix)\nAnd if both the errors and feature correlations are the same sign, this will also increase the standard error.\nAs well as the variance of the individual errors, \\(\\sigma_i^2\\), and the variance of each feature \\(V(X_k)\\), as before"
  },
  {
    "objectID": "posts/ols_coefs_multivariate.html",
    "href": "posts/ols_coefs_multivariate.html",
    "title": "Deriving OLS coefficients (multivariate)",
    "section": "",
    "text": "What we are exploring\n\n\n\nDeriving a unique, analytical solution to the set of coefficients that minimize the sum of squared residuals."
  },
  {
    "objectID": "posts/ols_coefs_multivariate.html#summary",
    "href": "posts/ols_coefs_multivariate.html#summary",
    "title": "Deriving OLS coefficients (multivariate)",
    "section": "Summary",
    "text": "Summary\nThe cost function for OLS is the sum of squared residuals, \\(\\hat{\\epsilon}^{\\intercal}\\hat{\\epsilon}\\). In order to fit a good linear model, we want to find optimum values for the estimated vector of coefficients, \\(\\hat{\\beta}^*\\), that minimizes this cost function.\nFirst we do partial differentiation of the cost function with respect to the coefficients. Finding the coefficient values where the partial differential is equal to zero reveals the stationary points of the cost function. For OLS in particular, we can find a unique solution for the choice of coefficients that can be found analytically. The hessian matrix then further proves that this is a global minima."
  },
  {
    "objectID": "posts/ols_coefs_multivariate.html#deriving-the-optimum-coefficients",
    "href": "posts/ols_coefs_multivariate.html#deriving-the-optimum-coefficients",
    "title": "Deriving OLS coefficients (multivariate)",
    "section": "Deriving the optimum coefficients",
    "text": "Deriving the optimum coefficients\n\n0. Defining the notation\nFor a sample \\(i\\), we observe an outcome \\(y_i\\). \\(y\\) is a vector of all \\(n\\) observed outcomes.\n\\[\n\\underset{n \\times 1} {y} =\n\\begin{bmatrix}\n    y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_{n-1} \\\\ y_n\n\\end{bmatrix}\n\\]\nWe also observe \\(k\\) features for every sample \\(i\\). \\(X\\) is a matrix of these observed features. Note the first column is usually all ones, to include an intercept to optimize (or “bias” term).\n\\[\n\\underset{n \\times k} {X} =\n\\begin{bmatrix}\n  1 & x_{11} & \\cdots & x_{1,k-1} & x_{1,k} \\\\\n  1 & x_{21} & \\cdots & x_{2,k-1} & x_{2,k} \\\\\n  1 & x_{31} & \\cdots & x_{3,k-1} & x_{3,k} \\\\\n  \\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\\n  1 & x_{n-2,1} & \\cdots & x_{n-2,k-1} & x_{n-2,k} \\\\\n  1 & x_{n-1,1} & \\cdots & x_{n-1,k-1} & x_{n-1,k} \\\\\n  1 & x_{n,1} & \\cdots & x_{n,k-1} & x_{n,k}\n\\end{bmatrix}\n\\]\nThe contribution of each feature to the prediction is estimated by the coefficients \\(\\hat{\\beta}\\).\n\\[\n\\underset{k \\times 1} {\\hat{\\beta}} =\n\\begin{bmatrix}\n  \\beta_0 \\\\\n  \\beta_1 \\\\\n  \\vdots \\\\\n  \\beta_{k-1} \\\\\n  \\beta_{k}\n\\end{bmatrix}\n\\]\nWe make predictions, \\(\\hat{y}\\), by calculating the dot product of the features \\(X\\) and the coefficients \\(\\hat{\\beta}\\).\n\\[\n\\hat{y} = X \\hat{\\beta}\n\\]\nwhich is shorthand for this:\n\\[\n\\displaylines{\n\\begin{align}\n\\begin{bmatrix}\n  \\hat{y}_1 \\\\ \\hat{y}_2 \\\\ \\hat{y}_3 \\\\ \\vdots \\\\ \\hat{y}_{n-2} \\\\ \\hat{y}_{n-1} \\\\ \\hat{y}_n\n\\end{bmatrix} & =\n\\begin{bmatrix}\n  1 & x_{11} & \\cdots & x_{1,k-1} & x_{1,k} \\\\\n  1 & x_{21} & \\cdots & x_{2,k-1} & x_{2,k} \\\\\n  1 & x_{31} & \\cdots & x_{3,k-1} & x_{3,k} \\\\\n  \\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\\n  1 & x_{n-2,1} & \\cdots & x_{n-2,k-1} & x_{n-2,k} \\\\\n  1 & x_{n-1,1} & \\cdots & x_{n-1,k-1} & x_{n-1,k} \\\\\n  1 & x_{n,1} & \\cdots & x_{n,k-1} & x_{n,k}\n\\end{bmatrix}\n\\begin{bmatrix}\n  \\hat{\\beta}_0 \\\\\n  \\hat{\\beta}_1 \\\\\n  \\vdots \\\\\n  \\hat{\\beta}_{k-1} \\\\\n  \\hat{\\beta}_{k}\n\\end{bmatrix}\n\\\\ \\\\ & =\n\\begin{bmatrix}\n  \\hat{\\beta}_0 + \\hat{\\beta}_{1}x_{1,1} + \\cdots + \\hat{\\beta}_{k-1}x_{1,k-1} + \\hat{\\beta}_{k}x_{1,k} \\\\\n  \\hat{\\beta}_0 + \\hat{\\beta}_{1}x_{2,1} + \\cdots + \\hat{\\beta}_{k-1}x_{2,k-1} + \\hat{\\beta}_{k}x_{2,k} \\\\\n  \\hat{\\beta}_0 + \\hat{\\beta}_{1}x_{3,1} + \\cdots + \\hat{\\beta}_{k-1}x_{3,k-1} + \\hat{\\beta}_{k}x_{3,k} \\\\\n  \\vdots \\\\\n  \\hat{\\beta}_0 + \\hat{\\beta}_{1}x_{n-2,1} + \\cdots + \\hat{\\beta}_{k-1}x_{2,k-1} + \\hat{\\beta}_{k}x_{n-2,k} \\\\\n  \\hat{\\beta}_0 + \\hat{\\beta}_{1}x_{n-1,1} + \\cdots + \\hat{\\beta}_{k-1}x_{2,k-1} + \\hat{\\beta}_{k}x_{n-1,k} \\\\\n  \\hat{\\beta}_0 + \\hat{\\beta}_{1}x_{n,1} + \\cdots + \\hat{\\beta}_{k-1}x_{2,k-1} + \\hat{\\beta}_{k}x_{n,k} \\\\\n\\end{bmatrix}\n\\end{align}\n}\n\\]\nThe residual is the difference between the true outcome and the model prediction.\n\\[\n\\hat{\\epsilon} = y_i -\\hat{y}_i\n\\]\nwhich is shorthand for this:\n\\[\n\\begin{bmatrix}\n  \\hat{\\epsilon_1} \\\\\n  \\hat{\\epsilon_2} \\\\\n  \\hat{\\epsilon_3} \\\\\n  \\vdots \\\\\n  \\hat{\\epsilon_{n-2}} \\\\\n  \\hat{\\epsilon_{n-1}} \\\\\n  \\hat{\\epsilon_{n}}\n\\end{bmatrix} =\n\\begin{bmatrix}\n    y_1 \\\\ y_2 \\\\ y_3 \\\\ \\vdots \\\\ y_{n-2} \\\\ y_{n-1} \\\\ y_n\n\\end{bmatrix} -\n\\begin{bmatrix}\n  \\hat{y}_1 \\\\ \\hat{y}_2 \\\\ \\hat{y}_3 \\\\ \\vdots \\\\ \\hat{y}_{n-2} \\\\ \\hat{y}_{n-1} \\\\ \\hat{y}_n\n\\end{bmatrix}\n\\]\nOur aim is to find the optimum vector of coefficients, \\(\\hat{\\beta}^*\\), that minimizes the sum of squared residuals:\n\\[\n\\min_{\\beta} \\left( \\epsilon^{\\intercal}\\epsilon \\right)\n\\]\n\n\n1. Expand the sum of squared residuals\nThe first step involves expanding the sum of squared residuals, and substituting in \\(X \\hat{\\beta}\\) for \\(\\hat{y}\\). \\[\n\\displaylines{\n\\begin{align}\n\\sum_{i=1}^n{\\hat{\\epsilon}_i^2} & = \\hat{\\epsilon}^{\\intercal}\\hat{\\epsilon}\n\\\\& =(y-X\\hat{\\beta})^{\\intercal}(y-X\\hat{\\beta})\n\\\\& = y^{\\intercal}y - y^{\\intercal}X\\hat{\\beta}-\n\\hat{\\beta}^{\\intercal} X^{\\intercal}y+\n\\hat{\\beta}^{\\intercal}X^{\\intercal}X\\hat{\\beta}\n\\\\& = y^{\\intercal}y - 2y^{\\intercal}X\\hat{\\beta}\n+\\hat{\\beta}^{\\intercal}X^{\\intercal}X\\hat{\\beta}\n\\end{align}\n}\n\\]\n\n\nNote we can simply add the two middle terms, since are both scalars:\n\\[\n\\displaylines{\ny^{\\intercal}X\\hat{\\beta} =\n\\hat{\\beta}^{\\intercal} X^{\\intercal}y \\\\\n\\because \\underset{1 \\times n}{y^{\\intercal}} \\times\n\\underset{n \\times k}{X} \\times\n\\hat{\\underset{k \\times 1}{\\beta}}\n=\n\\hat{\\underset{1 \\times k}{\\beta}^{\\intercal}} \\times\n\\underset{k \\times n}{X^{\\intercal}} \\times\n\\underset{n \\times 1}{y}\n}\n\\]\n\n\n2. Partially differentiate RSS with respect to beta\nThe second step involves partially differentiating the cost function with respect to its parameters, to understand how it changes as the coefficients vary.\n\\[\n\\displaylines{\n\\begin{align}\n\\frac{\\partial}{\\partial\\hat{\\beta}}\\hat{\\epsilon}^{\\intercal}\\hat{\\epsilon} & \\equiv\n\\begin{bmatrix}\n    \\frac{\\partial}{\\partial\\hat{\\beta}_1}\\hat{\\epsilon}^{\\intercal}\\hat{\\epsilon} \\\\\n    \\frac{\\partial}{\\partial\\hat{\\beta}_2}\\hat{\\epsilon}^{\\intercal}\\hat{\\epsilon} \\\\\n    \\vdots \\\\\n    \\frac{\\partial}{\\partial\\hat{\\beta}_k}\\hat{\\epsilon}^{\\intercal}\\hat{\\epsilon}\n\\end{bmatrix}\n\\\\ & = \\frac{d}{d\\hat{\\beta}}(\ny^{\\intercal}y - 2y^{\\intercal}X\\hat{\\beta}\n+\\hat{\\beta}^{\\intercal}X^{\\intercal}X\\hat{\\beta}) \\\\ & = 0 - 2X^{\\intercal}y +((X^{\\intercal}X)\\hat{\\beta} + (X^{\\intercal}X)^{\\intercal}\\hat{\\beta})\n\\\\ & = -2X^{\\intercal}y +  2((X^{\\intercal}X)\\hat{\\beta})\n\\end{align}\n}\n\\]\n\n\nTwo matrix differentiation rules used here for reference:\n\\[\n\\displaylines{\n\\frac{\\partial}{dx}(Ax) = A^{\\intercal}x \\\\\n\\frac{\\partial}{dx}(xAx) = Ax + A^{\\intercal}x\n}\n\\]\nAnd note \\(X^{\\intercal}X = (X^{\\intercal}X)^{\\intercal}\\) by definition, so we can add the two last terms.\n\n\n3. Find the coefficient values at the stationary point\nNow we find the choices of \\(\\beta\\) where the partial differential is equal to zero. These stationary points for the cost function are either at its maximum or minimum.\nFor OLS - we actually only find one unique solution!\n\\[\n\\displaylines{\n\\begin{align}\n\\cancel{2}X^{\\intercal}y +  \\cancel{2}((X^{\\intercal}X)\\hat{\\beta}) & = 0 \\\\\n\\therefore (X^{\\intercal}X)\\hat{\\beta} & = X^{\\intercal}y \\\\\n\\therefore \\hat{\\beta} & = (X^{\\intercal}X)^{-1}X^{\\intercal}y\n\\end{align}\n}\n\\]\n\n\nNote the need to invert \\(X^{\\intercal}X\\). This is only possible for a full rank matrix.\nThe first term is the (inverse) variance matrix of \\(X\\). This term normalizes the coefficient with respect to the magnitude of \\(X\\).\nThe second term is the covariance matrix between \\(X\\) and \\(y\\). This incorporates the linear relationship between the two in the coefficient.\nHence, the coefficient can be interpreted as the estimated change in \\(y\\) given a one unit change in \\(X\\).\n\n\n4. Check the stationary point is a global minimum (hessian matrix)\nFinally, we derive the hessian matrix, by double-differentiating the cost function with respect to the coefficients:\n\\[\n\\displaylines{\n\\frac{\\partial^2}{\\partial\\hat{\\beta}\\partial\\hat{\\beta}^{\\intercal}}\n\\left(\n  \\hat{\\epsilon}^{\\intercal}\\hat{\\epsilon}\n\\right)\n\\\\ = \\frac{\\partial}{\\partial \\hat{\\beta}^{\\intercal}} \\left( -2X^{\\intercal}y +  2((X^{\\intercal}X)\\hat{\\beta}) \\right)\n\\\\ = 2(X^{\\intercal}X)^{\\intercal}\n}\n\\]\nSince \\(X^{\\intercal}X\\) is clearly positive definite, the cost function is convex. Thus, we know our unique solution for \\(\\beta\\) where the partial differential is at zero is indeed a global minimum for the cost function."
  },
  {
    "objectID": "posts/ols_coefs_multivariate.html#final-reflections",
    "href": "posts/ols_coefs_multivariate.html#final-reflections",
    "title": "Deriving OLS coefficients (multivariate)",
    "section": "Final reflections",
    "text": "Final reflections\nUnlike logistic regression, or the multiple hidden-layer structure of neural networks, we can “jump” straight to the optimum coefficients for OLS. Why can we do this? Well chiefly its because OLS is a bit of a special case:\n\nThe minima is a global minima: The hessian matrix is positive definite, and hence the cost function is strictly convex. This means we know that when a choice of coefficients is found that ensure the partially differentiated cost function is equal zero, this minima is also a global one, not a local one.\nThere is only one solution for the optimum coefficient: We assume that the matrix is full rank (every feature provides additional predictive power) and that the number of predictors is smaller than the number of obervations. This also means that partially differentiating is okay to do!\nA closed-form solution can be found The predictions are generated from \\(X\\) using a simple, purely algebraic function, i.e. the sum-product of \\(X\\) by \\(\\beta\\). This means we can find an analytical solution to the optimal choice \\(\\beta^*\\). Note this often isn’t possible since non-linear activation functions (i.e. link functions) are often transcendental.\n\nWe will dive into this in another post.\nFin."
  },
  {
    "objectID": "posts/ols_sandwich_estimators.html",
    "href": "posts/ols_sandwich_estimators.html",
    "title": "Sandwiches: robust covariance error estimation",
    "section": "",
    "text": "What are we exploring?\n\n\n\nEstimating the correct coefficient variance when relaxing homoskedastic error assumptions"
  },
  {
    "objectID": "posts/ols_sandwich_estimators.html#introducing-sandwiches",
    "href": "posts/ols_sandwich_estimators.html#introducing-sandwiches",
    "title": "Sandwiches: robust covariance error estimation",
    "section": "Introducing sandwiches",
    "text": "Introducing sandwiches\nThe variance for the OLS coefficient estimator is equal to the following:\n\\[\n\\displaylines{\nV(\\hat{\\beta}) =\n(X^{\\intercal}X)^{-1}X^{\\intercal}E[\\epsilon\\epsilon^{\\intercal}]X(X^{\\intercal}X)^{-1}\n}\n\\]\nThis can be though to as a sandwich:\n\nThe “bread” either side: \\((X^{\\intercal}X)^{-1}X^{\\intercal}\\) on the left and its transpose \\(X(X^{\\intercal}X)^{-1}\\) on the right\nThe “meat” in the middle: what we assume for \\(E[\\epsilon\\epsilon^{\\intercal}]\\)\n\nNote that this is the same as the error variance, since \\(V[\\epsilon]=E[\\epsilon\\epsilon^{\\intercal}]-E[\\epsilon]E[\\epsilon^{\\intercal}]\\) and \\(E[\\epsilon] = 0\\)\n\n\nOur coefficient will only be efficient if these assumptions about the expected error are correct! We will explore what happens when the errors are assumed to be homoskedastic, heteroskedastic or clustered."
  },
  {
    "objectID": "posts/ols_sandwich_estimators.html#salmon-bagel-spherical-errors",
    "href": "posts/ols_sandwich_estimators.html#salmon-bagel-spherical-errors",
    "title": "Sandwiches: robust covariance error estimation",
    "section": "Salmon bagel: Spherical Errors 🐟",
    "text": "Salmon bagel: Spherical Errors 🐟\nUsual OLS is efficient if the true model has “spherical errors”. What does this mean in practice?\n\nErrors are homoskedastic: \\(V(\\epsilon_i)=\\sigma^2\\) for all observations\nErrors are serially uncorrelated: \\(cov(\\epsilon_i,\\epsilon_{j\\neq i})=0\\)\n\nWhat does this look like for \\(E[\\epsilon\\epsilon^{\\intercal}]\\)?\n\nThe diagonal of the matrix is a constant value (scalar), \\(\\sigma^2\\)\nThe off-diagonals are all zero\n\n\\[\n\\hat{\\sigma}^2\\underset{n\\times n}{I} =\n\\begin{bmatrix}\n\\hat{\\sigma}^2 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n0 & \\hat{\\sigma}^2 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n0 & 0 & \\hat{\\sigma}^2 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n0 & 0 & 0 & \\hat{\\sigma}^2 & 0 & 0 & 0 & 0 & 0\\\\\n0 & 0 & 0 & 0 & \\hat{\\sigma}^2 & 0 & 0 & 0 & 0\\\\\n0 & 0 & 0 & 0 & 0 & \\hat{\\sigma}^2 & 0 & 0 & 0\\\\\n0 & 0 & 0 & 0 & 0 & 0 & \\hat{\\sigma}^2 & 0 & 0\\\\\n0 & 0 & 0 & 0 & 0 & 0 & 0 & \\hat{\\sigma}^2 & 0\\\\\n0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & \\hat{\\sigma}^2\n\\end{bmatrix}\n\\]\nA good estimation of the constant error variance \\(\\sigma^2\\) is to apply the standard formula to the residuals (i.e. method of moments):\n\\[\n\\hat{\\sigma^2}=\\frac{1}{n-k}\\sum{\\hat{\\epsilon_i}^2} \\equiv \\frac{\\epsilon^{\\intercal}\\epsilon }{n-k}\n\\]\n\n\n\n\n\n\nThus our “salmon sandwich” is:\n\n\n\n\\[\n\\underset{k \\times k}{(X^{\\intercal}X)}^{-1}\n\\underset{k \\times n}\n{X^{\\intercal}}\n\\left(\\frac{1}{n-k} \\times \\underset{1 \\times 1}{(\\epsilon^{\\intercal}\\epsilon)}\n\\times\\underset{n \\times n}{I} \\right)\n\\underset{n \\times k}{X}\n\\underset{k \\times k}{(X^{\\intercal}X)}^{-1}\n\\]\n\n\n\n\nNote that under spherical errors, the sandwich can be simplified: \\[\n\\displaylines{\n\\begin{align}\nV[\\hat{\\beta}]\n& = (X^{\\intercal}X)^{-1}X^{\\intercal}E[\\epsilon\\epsilon^{\\intercal}]X(X^{\\intercal}X)^{-1}\n\\\\ & = (X^{\\intercal}X)^{-1}X^{\\intercal}\\sigma^2IX(X^{\\intercal}X)^{-1}\n\\\\ & = \\sigma^2(X^{\\intercal}X)^{-1}\\cancel{X^{\\intercal}X}\\cancel{(X^{\\intercal}X)^{-1}}\n\\\\ & = \\sigma^2(X^{\\intercal}X)^{-1}\n\\end{align}\n}\n\\]\nIn this scenario, the only things that impact the standard error of the coefficient \\(\\beta_k\\) is:\n\nThe variance of all the residuals, \\(\\sigma^2\\)\nThe variance of the feature \\(V(X_k)\\)"
  },
  {
    "objectID": "posts/ols_sandwich_estimators.html#ham-sarnie-heteroskedastic-errors",
    "href": "posts/ols_sandwich_estimators.html#ham-sarnie-heteroskedastic-errors",
    "title": "Sandwiches: robust covariance error estimation",
    "section": "Ham sarnie: Heteroskedastic errors 🍖",
    "text": "Ham sarnie: Heteroskedastic errors 🍖\nHeteroskedastic correction is needed if:\n\nErrors vary for every individual: \\(V(\\epsilon_i)=\\sigma_i^2\\) for all observations\nBut they are still independent aka serially uncorrelated: \\(cov(\\epsilon_i,\\epsilon_{j\\neq i})=0\\)\n\nWhat does this look like for \\(E[\\epsilon\\epsilon^{\\intercal}]\\)?\n\nThe diagonal of the matrix is the estimate of variance which is unique for each observation, \\(\\sigma_i^2\\)\nThe off-diagonals are all zero\n\\[\n  \\underset{n \\times n}{\\sigma^2} = \\begin{bmatrix}\n  \\sigma_1^2 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n  0 & \\sigma_2^2 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n  0 & 0 & \\sigma_3^2 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n  0 & 0 & 0 & \\sigma_4^2 & 0 & 0 & 0 & 0 & 0\\\\\n  0 & 0 & 0 & 0 & \\sigma_5^2 & 0 & 0 & 0 & 0\\\\\n  0 & 0 & 0 & 0 & 0 & \\sigma_6^2 & 0 & 0 & 0\\\\\n  0 & 0 & 0 & 0 & 0 & 0 & \\sigma_7^2 & 0 & 0\\\\\n  0 & 0 & 0 & 0 & 0 & 0 & 0 & \\sigma_8^2 & 0\\\\\n  0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & \\sigma_{..}^2\\\\\n  \\end{bmatrix}\n  \\]\nA good estimation of the vector of heteroskedastic error variances \\(\\sigma^2\\) is again to apply the standard formula to the residuals for each individual:\n\n\\[\n\\hat{\\sigma_i^2}=\\frac{1}{n-k}\\sum{\\hat{\\epsilon_i}^2}\n\\]\n\n\n\n\n\n\nThus our “ham sandwich” is:\n\n\n\n\\[\n\\underset{k \\times k}{(X^{\\intercal}X)}^{-1}\n\\underset{k \\times n}{X^{\\intercal}}\n\\left(\\frac{1}{n-k} \\times \\underset{1 \\times n}{(\\epsilon\\odot \\epsilon)} ^{\\intercal}\n\\times\\underset{n \\times n}{I} \\right)\n\\underset{n \\times k}{X}\n\\underset{k \\times k}{(X^{\\intercal}X)}^{-1}\n\\]\n\n\nSimilar to homoskedastic errors, the things that impacts the standard error of coefficient \\(k\\) is:\n\nThe variance of all the individual errors, \\(\\sigma_i^2\\)\nThe variance of the feature \\(V(X_k)\\)"
  },
  {
    "objectID": "posts/ols_sandwich_estimators.html#cheese-roll-clustered-errors",
    "href": "posts/ols_sandwich_estimators.html#cheese-roll-clustered-errors",
    "title": "Sandwiches: robust covariance error estimation",
    "section": "Cheese roll: Clustered Errors 🧀",
    "text": "Cheese roll: Clustered Errors 🧀\nCluster-robust errors are needed if:\n\nErrors vary for every individual: \\(V(\\epsilon_i)=\\sigma_i^2\\) for all observations i.e. still heteroskedastic\nAND errors within the same cluster \\(C_l\\) are serially correlated: \\(cov(\\epsilon_i,\\epsilon_{j}) \\neq 0 \\text{ if } \\epsilon_i,\\epsilon_j \\in C_l\\)\n\nNote - errors between clusters are assumed not to be serially correlated though i.e. \\(cov(\\epsilon_i,\\epsilon_{j}) =0 \\text{ if } \\epsilon_i \\in C_l,\\epsilon_j \\in C_{m \\neq l}\\)\n\n\nWhat does this look like for \\(E[\\epsilon\\epsilon^{\\intercal}]\\)?\n\nThe diagonal of the matrix is the estimate of variance which is unique for each observation, \\(\\sigma_i^2\\)\nThe off-diagonals are also populated with the covariance - but only when they are both in the same cluster\n\nHere is an example where observations 1, 2 and 3 are in cluster A, observations 4 and 5 are in cluster B, observations 6, 7 and 8 are in cluster C etc.\n\\[\n\\underset{n \\times n}{\\epsilon\\epsilon^{\\intercal}} \\sim \\begin{bmatrix}\n\\epsilon_1^2 & \\epsilon_1\\epsilon_2 & \\epsilon_1\\epsilon_3 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n\\epsilon_2\\epsilon_1 & \\epsilon_2^2 & \\epsilon_2\\epsilon_3 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n\\epsilon_3\\epsilon_1 & \\epsilon_3\\epsilon_2 & \\epsilon_3^2 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n0 & 0 & 0 & \\epsilon_4^2 & \\epsilon_4\\epsilon_5 & 0 & 0 & 0 & 0\\\\\n0 & 0 & 0 & \\epsilon_5\\epsilon_4 & \\epsilon_5^2 & 0 & 0 & 0 & 0\\\\\n0 & 0 & 0 & 0 & 0 & \\epsilon_6^2 & \\epsilon_6\\epsilon_7 & \\epsilon_6\\epsilon_8 & 0\\\\\n0 & 0 & 0 & 0 & 0 & \\epsilon_7\\epsilon_6 & \\epsilon_7^2 & \\epsilon_7\\epsilon_8 & 0\\\\\n0 & 0 & 0 & 0 & 0 & \\epsilon_8\\epsilon_6 & \\epsilon_8\\epsilon_7 & \\epsilon_8^2 & 0\\\\\n0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & \\epsilon_{..}^2\\\\\n\\end{bmatrix}\n\\]\nAdditionally, we have to do a finite-sample correction as well on the degrees of freedom, based on the number of clusters \\(n_c\\)\n\n\n\n\n\n\nThus our cheese sandwich is:\n\n\n\n\\(\\underset{k \\times k}{(X^{\\intercal}X)}^{-1}\n\\underset{k \\times n}{X^{\\intercal}}\n\\left(\\frac{n-1}{n-k}\\frac{n_c}{n_c-1} \\times\n\\underset{n \\times n}{(\\epsilon \\epsilon^{\\intercal})}\n\\right)\n\\underset{n \\times k}{X}\n\\underset{k \\times k}{(X^{\\intercal}X)}^{-1}\\)\n\n\nIn this scenario, there are a few additional things that impact the standard error of coefficient \\(k\\):\n\nIf errors are correlated within clusters, this will increase the error.\nIf features are correlated within clusters, this will also increase the error (due to the off-diagonals in the error variance matrix)\nAnd if both the errors and feature correlations are the same sign, this will also increase the standard error.\nAs well as the variance of the individual errors, \\(\\sigma_i^2\\), and the variance of each feature \\(V(X_k)\\), as before"
  },
  {
    "objectID": "posts/mle_ols_normal.html",
    "href": "posts/mle_ols_normal.html",
    "title": "OLS vs MLE with gaussian noise",
    "section": "",
    "text": "What are we exploring?\n\n\n\nWhy MLE finds the same coefficients as OLS when assuming model errors are generated from a mean-zero gaussian probabilistic process\n\n\n\nNormality of errors\nLet’s assume the errors follow a normal distribution with a mean of zero: \\[\n\\epsilon  = y - X\\beta \\sim \\mathcal{N}(0,\\sigma^2)\n\\]\n\nYou might already notice how similar this is to the Gauss-Markov requirements to ensure OLS coefficients are BLUE!\n\nThe expected error is zero, and consistent for all values of X, so we have “strict exogeneity”: \\(E[\\epsilon|X] = 0\\)\nThe error variance is uniform, again consistent for all values of X, so we have “spherical errors”: \\(E[\\epsilon\\epsilon^{\\intercal}|X] = 0\\)\n\n\n\nApplying the normal pdf\nFor any datapoint \\(i\\), we can formulate the likelihood of observing the outcome \\(y_i\\) as being generated from the normal probability density function applied to the squared error:\n\\[\n\\displaylines{\n\\begin{align}\np(y_i|\\beta,X_i,\\sigma^2)\n& = \\frac{1}{\\sigma\\sqrt{2\\pi}}\\exp{\\left\\{-\\frac{1}{2\\sigma^2}\\epsilon_i^2\\right\\}} \\\\\n& = \\frac{1}{\\sigma\\sqrt{2\\pi}}\\exp{\\left\\{-\\frac{1}{2\\sigma^2}(y_i-X_i\\beta)^2\\right\\}}\n\\end{align}\n}\n\\]\nMaximum likelihood estimation aims to find the set of coefficients that maximises the likelihood of observing the evidence we have. We thus aim to find the coefficients \\(\\beta\\) that maximise the likelihood of observing \\(y\\) across all \\(n\\) samples:\n\\[\n\\displaylines{\n\\begin{align}\np(y|\\beta,X,\\sigma^2)\n& = \\prod_{i=1}^{n}{\\frac{1}{\\sigma\\sqrt{2\\pi}}}\\exp{\\left\\{-\\frac{1}{2\\sigma^2}\\epsilon_i^2\\right\\}}\n\\end{align}\n}\n\\]\n\n\nTaking the negative log-likelihood\nIn practice, dealing with a cost function made up of a sum product is tricky - it is easier to take the log and deal with addition instead. Further, rather than maximise, it is common to “minimize” cost functions, so the negative log-likelihood is usually used.\n\n\nRecall that \\(\\log{\\left(ab\\right)} = \\log{\\left(a\\right)} + \\log{\\left(b\\right)}\\)\n\\[\n\\displaylines{\n\\begin{align}\n\\max_\\beta{p(y|\\beta,X,\\sigma^2)}\n= &\n\\max_\\beta{\\left[ \\prod{ \\frac{1}{\\sigma\\sqrt{2\\pi}}\\exp{\\left\\{-\\frac{1}{2\\sigma^2}\\epsilon_i^2\\right\\}} } \\right]}\n\\\\ \\\\ \\Rightarrow &\n\\min_\\beta{\\left[ -\\sum{\\log{\\left(\\frac{1}{\\sigma\\sqrt{2\\pi}}\\exp{\\left\\{-\\frac{1}{2\\sigma^2}\\epsilon_i^2\\right\\}}\\right)}} \\right]}\n\\end{align}\n}\n\\]\n\n\nSimplifying the cost function\nAnd now we can look to simplify this: \\[\n\\displaylines{\n\\begin{align}\n& \\min_\\beta{\\left[ -\\sum{\\log{\\left(\\frac{1}{\\sigma\\sqrt{2\\pi}}\\exp{\\left\\{-\\frac{1}{2\\sigma^2}\\epsilon_i^2\\right\\}}\\right)}} \\right]}\n\\\\ = & \\min_\\beta{\\left[ -\\sum{\\log{\\left(\\frac{1}{\\sigma\\sqrt{2\\pi}}\\right)}} -\\sum{\\log{\\left(\\exp{\\left\\{-\\frac{1}{2\\sigma^2}\\epsilon_i^2\\right\\}}\\right)}} \\right]}\n\\\\ = & \\min_\\beta{\\left[ -\\sum{\\log{((2\\pi\\sigma^2)^{-\\frac{1}{2}})}} - \\sum{\\left(-\\frac{1}{2\\sigma^2} \\epsilon_i^2\\right)} \\right]}\n\\\\ = & \\min_\\beta{\\left[ \\frac{1}{2}\\sum{\\log{(2\\pi\\sigma^2)}} + \\frac{1}{2\\sigma^2}\\sum{\\epsilon_i^2} \\right]}\n\\\\ = & \\min_\\beta{\\left[ \\frac{1}{2} \\left(n\\log{(2\\pi\\sigma^2)} + \\frac{1}{\\sigma^{2}} \\sum{\\epsilon_i^2}  \\right)\\right]}\n\\end{align}\n}\n\\]\n\n\nCoefficient point-estimate is the same as OLS\nWe minimise the cost function by finding the optimum coefficient values \\(\\beta^*\\) so that the partial differential is equal to zero.\nThe constant \\(\\log{(2\\pi\\sigma^2)}\\) doesn’t vary with respect to \\(\\beta\\), so it drops out. The fraction \\(\\frac{1}{2}\\) also drops out when finding where differential is set to zero.\nHence we are left finding that we are solving the same problem as usual least-squares!\n\\[\n\\displaylines{\n\\begin{align}\n\\therefore \\beta^* & =\\arg\\min_\\beta{\\left[ \\frac{1}{2} \\left(n\\log{(2\\pi\\sigma^2)} + \\frac{1}{\\sigma^{2}} \\sum{\\epsilon_i^2}  \\right)\\right]}\n\\\\ & =\\arg\\min_\\beta{\\left[ \\epsilon^T\\epsilon \\right]}\n\\end{align}\n}\n\\]\n\n\nError-variance estimate is the same as OLS\nOLS estimates the variance of the models errors using the residuals from the sample:\n\\[\n\\sigma^2 = \\frac{1}{n}\\hat{\\epsilon}^{\\intercal}\\hat{\\epsilon}\n\\]\nDo we see the same with MLE? Well so far we have only found the optimum \\(\\hat{\\beta}^*\\) to ensure the expected conditional error is zero, we haven’t touched our other parameter \\(\\sigma^2\\).\nNow lets instead find the estimate of \\(\\sigma\\) that minimizes the negative log-likelihood:\n\\[\n\\displaylines{\n\\begin{align}\n& \\min_{\\sigma^2}{\\left[ \\frac{1}{2} \\left(n\\log{(2\\pi\\sigma^2)} + \\frac{1}{\\sigma^{2}} \\sum{\\epsilon_i^2}  \\right)\\right]} \\\\\n\\Rightarrow &\n\\frac{\\partial}{\\partial\\sigma^2}\n\\left[ \\frac{1}{2} \\left(n\\log{(2\\pi\\sigma^2)} + \\frac{1}{\\sigma^{2}} \\sum{\\epsilon_i^2}  \\right)\\right]\n\\\\ & =\n\\frac{1}{2} \\left(n\\frac{2\\pi}{2\\pi\\sigma^2} + \\frac{-2}{\\sigma^4} \\sum{\\epsilon_i^2}  \\right)\n\\\\ & =\n\\frac{n}{2\\sigma^2} - \\frac{1}{2\\sigma^4} \\sum{\\epsilon_i^2} = 0 \\\\\n& \\therefore \\sigma^2 = \\frac{1}{n}\\sum{\\epsilon_i^2} = 0 \\\\\n\\end{align}\n}\n\\]\nand hence we can see that the estimation of OLS is the same too.\n\n\nFinal reflections\nOne advantage of using MLE is we can generate a probabilistic estimate for \\(y_i\\), rather than just a point-estimate (assuming we have fitted \\(\\hat{\\sigma}^2\\) as above).\n\nPoint estimate: \\(\\hat{y} = X\\hat{\\beta}\\)\nPosterior estimate: \\(P(\\hat{y_i}| X_i,\\hat{\\beta},\\sigma^2) = \\mathcal{N}(\\hat{y_i}| X_i\\hat{\\beta},\\sigma^2)\\),\n\nYou might already have started to see how probabilistic predictions and coefficients fit by MLE nicely fit into the bayesian paradigm. This opens up nice extensions,such as using priors as a form of regularization. This is for another post though!\nFin."
  },
  {
    "objectID": "posts/ols_blue.html",
    "href": "posts/ols_blue.html",
    "title": "BLUE coefficients: bias and efficiency",
    "section": "",
    "text": "What are we exploring?\n\n\n\nProving that the OLS coefficient is the best linear unbiased estimator."
  },
  {
    "objectID": "posts/ols_blue.html#are-the-ols-coefficients-blue",
    "href": "posts/ols_blue.html#are-the-ols-coefficients-blue",
    "title": "BLUE coefficients: bias and efficiency",
    "section": "Are the OLS coefficients “BLUE”?",
    "text": "Are the OLS coefficients “BLUE”?\nWe find a unique solution to the set of coefficients that minimize the sum of squared residuals analytically (see its derivation here):\n\\[\n\\hat{\\beta}^{OLS}=(X^{\\intercal}X)^{-1}X^{\\intercal}y\n\\]\nHowever, how do we know if these coefficients are the best ones we can estimate?\nFor the estimated coefficients to be the Best Linear Unbiased Estimator (i.e. “BLUE”):\n\nThe best estimator has to be unbiased: \\(E[\\hat{\\beta}^*] = \\beta\\)\nAnd among all possible linear, unbiased estimators, it must have the smallest variance: \\(V[\\hat{\\beta}^{*}] &lt; V[\\hat{\\beta}^{Z}]\\)\n\nWe want to ensure our OLS estimate is the best, i.e. that \\(\\hat{\\beta}^{OLS} = \\hat{\\beta}^{*}\\). To achieve this, first we need to confirm it is unbiased. Then given this is true, we can check that the coefficient is most efficient vs all other unbiased estimators.\n\n\n\n\n\n\nGauss Markov Assumptions\n\n\n\nAlong the way, we will outline the Gauss-Markov assumptions utilised that ensure the OLS coefficient is BLUE."
  },
  {
    "objectID": "posts/ols_blue.html#setting-the-scene",
    "href": "posts/ols_blue.html#setting-the-scene",
    "title": "BLUE coefficients: bias and efficiency",
    "section": "Setting the scene",
    "text": "Setting the scene\n\nThe true coefficient and model\n\\(\\beta\\) is the true (unobserved) relationship between all the relevant explanatory features, \\(X\\), and their associated observed outcomes, \\(y\\). In other words, if we observed the entire population of data, it is the relationship we would find.\nConcretely, we assume the outcome is a linear function of all its relevant features. This “true model” perfectly predicts the outcome, except for random noise \\(\\epsilon\\) that influences the observed outcome: \\(y = X\\beta + \\epsilon\\)\n\n\nThe estimated coefficient\n\\(\\hat{\\beta}\\) is our estimated coefficient for the true relationship \\(\\beta\\). In reality, we estimate \\(\\hat{\\beta}\\) from the small, finite sample of size \\(n\\) that is collected, not the whole population. Given any random sample could be collected, we can term the coefficient resulting from the optimum estimation proceedure to be \\(\\hat{\\beta}^*\\). We want to understand if \\(\\hat{\\beta}^{OLS} = \\hat{\\beta}^*\\).\n\n\nThe expected estimated coefficient\n\\(E[\\hat{\\beta}]\\) is the “expected” estimated coefficient. Imagine we repeat the action of estimating the coefficient \\(\\hat{\\beta}\\) many times, each time collecting a new sample (where each observation is sampled i.i.d), and recording the value for the estimated coefficient. \\(E[\\hat{\\beta}]\\) would then be the average of all of those estimated coefficients. If the OLS coefficient is unbiased, then the expected coefficient estimate should be equal to the true one, \\(E[\\hat{\\beta}^{OLS}]=\\beta\\).\n\n\nThe variance of the estimated coefficient\n\\(V[\\hat{\\beta}]\\) is the variance of the estimated coefficient. It determines how much we might expect our estimate \\(\\hat{\\beta}\\) to differ from the true \\(\\beta\\) for any sample drawn. Given the OLS coefficient has been shown to be unbiased, if it is BLUE we expect its variance to be lower than another other unbiased choice \\(\\hat{\\beta}^{Z}\\). Concretely, we want to find \\(V[\\hat{\\beta}^{OLS}] &lt; V[\\hat{\\beta}^{Z}]\\)."
  },
  {
    "objectID": "posts/ols_blue.html#bias",
    "href": "posts/ols_blue.html#bias",
    "title": "BLUE coefficients: bias and efficiency",
    "section": "Bias",
    "text": "Bias\nOften our small finite samples of size \\(n\\) are not a perfect reflection of the population they are drawn from. This “sampling error” means we might estimate a different relationship between \\(X\\) and \\(y\\) than the true relationship of the population, i.e. \\(\\hat{\\beta} \\neq \\beta\\).\nHowever, we should expect our estimated coefficient to be equal to the true value on average. This means we do not want to have a bias towards the estimate being systematically too small or too large, for example. In other words, if we repeated the whole proceedure thousands of times (each time taking new samples, and estimating a coefficient from the new sample) then the average of all the estimated coefficients values should be equal to the true value, i.e. \\(E[\\hat{\\beta}] = \\beta\\).\nRecall that we believe there is a true model that follows the form:\n\\[\ny = X\\beta + \\epsilon\n\\]\n\n\n\n\n\n\nGM1: Linearity \n\n\n\nThe formula above relies on the first Gauss-Markov assumption - that the dependent variable \\(y\\) is assumed to be a linear function of the variables \\(X\\). Note that implies that the proper functional form has been selected (i.e. the relationship is linear) and there are no omitted variables - a huge assumption!\n\n\nIf we substitute this into our estimated coefficient:\n\\[\n\\displaylines{\n\\begin{align}\n\\hat{\\beta}^{OLS} & = (X^{\\intercal}X)^{-1}X^{\\intercal}y\n\\\\ & = (X^{\\intercal}X)^{-1}X^{\\intercal}(X\\beta+\\epsilon)\n\\\\ & = (X^{\\intercal}X)^{-1}X^{\\intercal}X\\beta+(X^{\\intercal}X)^{-1}X^{\\intercal}\\epsilon\n\\\\ & = \\beta+(X^{\\intercal}X)^{-1}X^{\\intercal}\\epsilon\n\\end{align}\n}\n\\]\nwe show that the estimated coefficient \\(\\hat{\\beta}^{OLS}\\) will differ from the true value depending on the random error \\(\\epsilon\\) associated with the particular finite sample collected.\nNow let’s now take the expectation, to determine when the coefficient is unbiased. In other words, what is the “average” coefficient if we took the sample many times: \\[\n\\displaylines{\n\\begin{align}\nE[\\hat{\\beta}^{OLS}]\n& = \\beta +(X^{\\intercal}X)^{-1}X^{\\intercal}E[\\epsilon]\n\\\\ & = \\beta & \\iff E[\\epsilon] & = 0\n\\end{align}\n\\\\\n}\n\\]\nWe find that the coefficient is unbiased as long as the expected error is also zero.\n\n\n\n\n\n\nGM2: Strict Exogeneity \n\n\n\nThe second Gauss-Markov assumption is strict exogeneity, where the expected error is zero for all feature values: \\(E[\\epsilon|X] = 0\\). By definition, the weaker exogeneity statement of \\(E[\\epsilon] = 0\\) is implied by having the expected error conditional being equal to zero."
  },
  {
    "objectID": "posts/ols_blue.html#efficiency",
    "href": "posts/ols_blue.html#efficiency",
    "title": "BLUE coefficients: bias and efficiency",
    "section": "Efficiency",
    "text": "Efficiency\nTo be the “best”, the OLS estimator also needs to be efficient. This means that it has the lowest variance of all unbiased estimators. This section looks to prove this.\n\nCoefficient variance for OLS\nFirst, let’s derive the variance from the coefficients estimated using OLS, termed \\(V[\\hat{\\beta}^{OLS}]\\). As before, we substitute the true model \\(y = X\\beta + \\epsilon\\) into the coefficient estimated through OLS:\n\\[\n\\displaylines{\n\\begin{align}\n\\hat{\\beta}^{OLS}-\\beta\n& = \\beta +  ((X^{\\intercal}X)^{-1}X^{\\intercal}\\epsilon) - \\beta \\\\\n& = (X^{\\intercal}X)^{-1}X^{\\intercal}\\epsilon\n\\\\ \\\\\n\\therefore\n(\\hat{\\beta}^{OLS}-\\beta)(\\hat{\\beta}^{OLS}-\\beta)^{\\intercal}\n& = ((X^{\\intercal}X)^{-1}X^{\\intercal}\\epsilon)((X^{\\intercal}X)^{-1}X^{\\intercal}\\epsilon)^{\\intercal}\n\\\\ & =\n(X^{\\intercal}X)^{-1}X^{\\intercal}\\epsilon\\epsilon^{\\intercal}X(X^{\\intercal}X)^{-1} \\\\ \\\\\n\\therefore\nV(\\hat{\\beta}^{OLS}) & = E[(\\hat{\\beta}^{OLS}-\\beta)(\\hat{\\beta}^{OLS}-\\beta)^{\\intercal}]\n\\\\ & =\n(X^{\\intercal}X)^{-1}X^{\\intercal}E[\\epsilon\\epsilon^{\\intercal}]X(X^{\\intercal}X)^{-1}\n\\end{align}\n}\n\\]\nThis is sometimes called the sandwich estimator - post soon to follow on this!\n\n\nCoefficient variance assuming “spherical errors”\nWe can simplify this further by appling some assumptions to the estimated error variance \\(E[\\epsilon\\epsilon^{\\intercal}]\\):\n\\[\n\\displaylines{\n\\begin{align}\nE[\\epsilon \\epsilon^{\\intercal}] & =  \n\\begin{bmatrix}\nE[\\epsilon_1^2] & \\cdots & E[\\epsilon_1\\epsilon_n] \\\\\n\\vdots & \\ddots & \\vdots \\\\\nE[\\epsilon_n\\epsilon_1] & \\cdots & E[\\epsilon_n^2]\n\\end{bmatrix} \\\\ \\\\\n& = \\begin{bmatrix}\n\\frac{1}{n}\\sum_{i=1}^{n}{\\epsilon_i^2} & \\cdots & 0 \\\\\n\\vdots & \\ddots & \\vdots \\\\\n0 & \\cdots & \\frac{1}{n}\\sum_{i=1}^{n}{\\epsilon_i^2}\n\\end{bmatrix} \\\\ \\\\\n& = \\begin{bmatrix}\n\\hat{\\sigma}^2 & \\cdots & 0 \\\\\n\\vdots & \\ddots & \\vdots \\\\\n0 & \\cdots & \\hat{\\sigma}^2\n\\end{bmatrix} \\\\ \\\\\n& = \\hat{\\sigma}^2I\n\\end{align}\n}\n\\]\nHow can we jump to this result? Well we are making two assumptions:\n\nNo serial correlation: \\(\\rho_{\\epsilon_{i},\\epsilon_{i \\neq j}} = 0\\). No correlation between sample errors means that \\(E[\\epsilon_i \\epsilon_{j \\neq i}] = 0\\), and hence the off-diagonals of the error covariance matrix are zero.\nHomoskedasticity: the assumption of uniform error variance for all samples means that \\(V[\\epsilon_i^2] = V[\\epsilon_{j \\neq i}^2] = \\hat{\\sigma}^2\\). And our best approximation for \\(\\hat{\\sigma}^2\\) is simply taking the average squared error: \\(\\hat{\\sigma}^2 = \\frac{1}{n}\\sum_{i=1}^{n}{\\epsilon_i^2}\\)\n\n\n\n\n\n\n\nGM3: Spherical errors \n\n\n\nThe third Gauss-Markov assumption is spherical errors, \\(E[\\epsilon\\epsilon^{\\intercal}|X] = 0\\). This means that the outer product of the expected errors is a scalar matrix, which implies no serial correlation and homoskedasticity.\n\n\n\n\nIt is especially important to make the right assumptions about \\(E[\\epsilon\\epsilon^{\\intercal}|X]\\) as it impacts where our estimate of the standard errors is correct! We will dive into what happens to SE if we violate these assumptions in another post.\nSince we now see that \\(\\hat{\\sigma}^2\\) is a scalar matrix, we can thus simplfy the variance formula further:\n\\[\n\\displaylines{\n\\begin{align}\nV(\\hat{\\beta}^{OLS}) & =\n(X^{\\intercal}X)^{-1}X^{\\intercal}E[\\epsilon\\epsilon^{\\intercal}]X(X^{\\intercal}X)^{-1}\n\\\\ & =\n(X^{\\intercal}X)^{-1}X^{\\intercal} \\hat{\\sigma}^2I X(X^{\\intercal}X)^{-1}\n\\\\ & =\n\\hat{\\sigma}^2\n\\cancel{(X^{\\intercal}X)^{-1}}\n\\cancel{X^{\\intercal} X}\n(X^{\\intercal}X)^{-1}\n\\\\ & = \\hat{\\sigma}^2(X^{\\intercal}X)^{-1}\n\\end{align}\n}\n\\]\n\n\nFormulating an alternative unbiased coefficient\nNext step - lets formulate another estimator, \\(\\hat{\\beta}^{z}\\), which differs from \\(\\hat{\\beta}^{OLS}\\) by a non-zero matrix \\(A\\). See how they both differ below:\n\\[\n\\displaylines{\n\\begin{align}\n\\hat{\\beta}^{OLS} & =(X^{\\intercal}X)^{-1}X^{\\intercal}y \\\\\n\\hat{\\beta}^{Z} & =\\left((X^{\\intercal}X)^{-1}X^{\\intercal}+A\\right)y\n\\end{align}\n}\n\\]\nNow we need to ensure this new estimator is not biased. So by taking the expectation in the same was as for OLS…\n\\[\n\\displaylines{\n\\begin{align}\nE[\\hat{\\beta}^{Z}] & = E\\left[ \\left((X^{\\intercal}X)^{-1}X^{\\intercal}+A\\right)y \\right]\n\\\\ & = \\left((X^{\\intercal}X)^{-1}X^{\\intercal}+A\\right)(X\\beta+ \\cancel{E\\left[\\epsilon \\right]}) & \\because E[\\epsilon] = 0\n\\\\ & = (X^{\\intercal}X)^{-1}X^{\\intercal}X\\beta+AX\\beta\n\\\\ & = \\beta+AX\\beta\n\\end{align}\n}\n\\]\nSo the estimator is only unbiased iff \\(AX=0\\). This is important to note when comparing the variance between unbiased coefficients - see below!\n\n\nVariance of the alternative unbiased coefficient\nJust like before, we calculate the variance:\n\\[\n\\displaylines{\n\\begin{align}\nV[\\hat{\\beta}^{Z}]\n& = V\\left[ \\left((X^{\\intercal}X)^{-1}X^{\\intercal}+A\\right)y \\right]\n\\\\ & = \\left((X^{\\intercal}X)^{-1}X^{\\intercal}+A\\right) V[y] \\left((X^{\\intercal}X)^{-1}X^{\\intercal}+A\\right)^{\\intercal}\n\\\\ & = \\hat{\\sigma}^2 \\left((X^{\\intercal}X)^{-1}X^{\\intercal}+A\\right) \\left((X^{\\intercal}X)^{-1}X^{\\intercal}+A\\right)^{\\intercal}\n& \\because E[\\epsilon \\epsilon^{\\intercal}|X] = 0\n\\\\ & = \\hat{\\sigma}^2\\left((X^{\\intercal}X)^{-1}X^{\\intercal}+A\\right) \\left(X(X^{\\intercal}X)^{-1}+A^{\\intercal}\\right)\n\\\\ & = \\hat{\\sigma}^2 \\left(\n(X^{\\intercal}X)^{-1}X^{\\intercal} X(X^{\\intercal}X)^{-1} + AX(X^{\\intercal}X)^{-1} + (X^{\\intercal}X)^{-1}X^{\\intercal}A^{\\intercal} + AA^{\\intercal}\n\\right)\n\\\\ & = \\hat{\\sigma}^2\n(X^{\\intercal}X)^{-1} + \\hat{\\sigma}^2AA^{\\intercal}\n& \\because AX = 0\n\\\\ & = V[\\beta^{OLS}] + \\hat{\\sigma}^2AA^{\\intercal}\n\\end{align}\n}\n\\]\nNow since AA is surely a positive semi-definite matrix, then we know that \\(V[\\hat{\\beta}^{Z}] &gt; V[\\hat{\\beta}^{OLS}]\\).\nWe have shown that \\(\\hat{\\beta}^{OLS}\\) has the smallest variance among all unbiased estimators!"
  },
  {
    "objectID": "posts/ols_blue.html#summarising-the-gauss-markov-assumptions",
    "href": "posts/ols_blue.html#summarising-the-gauss-markov-assumptions",
    "title": "BLUE coefficients: bias and efficiency",
    "section": "Summarising the Gauss-markov assumptions",
    "text": "Summarising the Gauss-markov assumptions\nAlong the way, we showed where assumptions were needed to ensure the OLS coefficient estimation is BLUE.\nWe actually missed one out, but it is actually quite trivial to see from the OLS coefficient formula:\n\\[\n\\hat{\\beta}^{OLS}=(X^{\\intercal}X)^{-1}X^{\\intercal}y\n\\]\n\n\n\n\n\n\nGM4: Full rank \n\n\n\nThe matrix \\(X\\) must be of full rank \\(k\\), so that it is possible to invert the matrix \\(X^{\\intercal}X\\). This is equivalent to having no perfect multi-collinearity.\n\n\nWe have now collected our full set of Gauss-Markov assumptions required for the OLS coefficient to be BLUE:\n\nLinearity\nStrict Exogeneity\nSpherical Errors\nFull rank"
  },
  {
    "objectID": "posts/techniques_differentiation.html",
    "href": "posts/techniques_differentiation.html",
    "title": "Differentiation formulae",
    "section": "",
    "text": "What we are solving\n\n\n\nDeriving why \\(\\frac{d}{dx} \\left(x^{n} \\right) = nx^{n-1}\\)"
  },
  {
    "objectID": "posts/techniques_differentiation.html#formulating-an-approach-to-the-problem",
    "href": "posts/techniques_differentiation.html#formulating-an-approach-to-the-problem",
    "title": "Differentiation formulae",
    "section": "Formulating an approach to the problem",
    "text": "Formulating an approach to the problem\nThe gradient between two points, \\(x\\) and \\(a\\), is equal to the change in the y axis divided by the change in the x axis.\nFor example, if \\(y=f(x)=x^n\\), then we can approximate the gradient as follows:\n\\[\n\\frac{\\Delta y}{\\Delta x} =\n\\frac{f(x)-f(a)}{x-a} =\n\\frac{x^n-a^n}{x-a}\n\\]\nThis isn’t the same as saying a line which passes through both of these points has the same gradient though. There is a gap between this linear approximation and the exact curve.\nHowever, the smaller the movement across the x-axis, the closer the approximation is to the actual curve. For example, in Figure 1 below, the line drawn between \\(f(1)\\) and \\(f(2)\\) is closer to the gradient at \\(f(1)\\) of the true curve, compared to the line drawn between \\(f(1)\\) and \\(f(3)\\).\n\n\nCode\nimport plotly.graph_objects as go\nimport numpy as np\n\ndef interpolate(n,x1,x2,num=50):\n  x = np.linspace(x1,x2,num)\n  m = (x2**n - x1**n)/(x2 - x1)\n  c = x1**n - m*x1\n  y = m*x+c\n  return({'x':x,'y':y})\n\nn=3\nactual = {'x': np.arange(0,4,0.1)}\nactual['y'] = actual['x']**n\nlb, ub = 1, [3.5,3,2]\nlin0 = interpolate(n,lb,ub[0])\nlin1 = interpolate(n,lb,ub[1])\nlin2 = interpolate(n,lb,ub[2])\n\nfig = go.Figure(data = go.Scatter(mode='lines'))\nfig.add_trace( go.Scatter( x=actual['x'], y=actual['y'], line=dict(dash='solid'), name=f\"f(x)=x^{n}\" ))\nfig.add_trace( go.Scatter( x=lin0['x'], y=lin0['y'], name=f\"f({lb}) -&gt; f({ub[0]})\", line=dict(dash='dot') ))\nfig.add_trace( go.Scatter( x=lin1['x'], y=lin1['y'], name=f\"f({lb}) -&gt; f({ub[1]})\", line=dict(dash='dot') ))\nfig.add_trace( go.Scatter( x=lin2['x'], y=lin2['y'], name=f\"f({lb}) -&gt; f({ub[2]})\", line=dict(dash='dot') ))\nfig.show()\n\n\n\n\n                                                \n\n\nFigure 1: The smaller the change in x, the closer the linear approximation is to the true gradient of f(x)\n\n\n\n\nConsequently, we want to determine the gradient at the limit i.e. where \\(a \\rightarrow x\\), to get the true gradient."
  },
  {
    "objectID": "posts/techniques_differentiation.html#factoring-out-x-a",
    "href": "posts/techniques_differentiation.html#factoring-out-x-a",
    "title": "Differentiation formulae",
    "section": "Factoring out \\((x-a)\\)",
    "text": "Factoring out \\((x-a)\\)\nFirst though, let’s factor out the \\(x-a\\) term for simplicity. Let’s derive a generic formula for this:\n\nIf \\(n=2\\), then \\(x^2-a^2 = (x-a)(x+a)\\)\nIf \\(n=3\\), then \\(x^3-a^3 = (x-a)(x^2+xa+a^2)\\)\nIf \\(n=4\\), then \\(x^4-a^4 = (x-a)(x^3+x^2a+xa^2+a^3)\\)\nIf \\(n=5\\), then \\(x^5-a^5 = (x-a)(x^4+x^3a+x^2a^2+xa^3+a^4)\\)\nAnd so on. In fact for any \\(n\\), we can derive \\(x^n-a^n = (x-a)\\sum_{i=1}^n \\left( x^{n-i}a^{i-1} \\right)\\)\n\nAnd we can now sub that into our formula, and the \\(x-a\\) cancels out:\n\\[\n\\frac{x^n-a^n}{x-a} =\n\\frac{x-a}{x-a} \\sum_{i=1}^n \\left( x^{n-i}a^{i-1} \\right) =\n\\sum_{i=1}^n \\left( x^{n-i}a^{i-1} \\right)\n\\]"
  },
  {
    "objectID": "posts/techniques_differentiation.html#getting-our-result",
    "href": "posts/techniques_differentiation.html#getting-our-result",
    "title": "Differentiation formulae",
    "section": "Getting our result",
    "text": "Getting our result\nAnd now let’s calculate the result in the limit, where \\(x\\) approaches \\(a\\):\n\\[\n\\lim_{x \\rightarrow a} \\sum_{i=1}^n \\left( x^{n-i}a^{i-1} \\right) \\sim\n\\sum_{i=1}^n \\left( a^{n-i}a^{i-1} \\right) =\n\\sum_{i=1}^n \\left( a^{n-1} \\right) =\nna^{n-1}\n\\]\nHence \\(\\frac{d}{dx} \\left(x^{n} \\right) = nx^{n-1}\\).\nFin."
  },
  {
    "objectID": "posts/bayes_lasso.html",
    "href": "posts/bayes_lasso.html",
    "title": "Shrinkage priors for Lasso and Ridge",
    "section": "",
    "text": "What are we exploring?\n\n\n\nShowing Laplace priors in bayesian regression are equivalent to Lasso regularization\n\n\n\n\nBayesian Linear regression:\nWhen performing ordinary linear regression using maximum likelihood, we model the noise around \\(y\\) as being generated from a gaussian distributed process - conditional on the data \\(X\\) and estimated model parameters \\(\\beta\\) and \\(\\sigma\\):\n\\[\n\\displaylines{\n\\begin{align}\np(y|\\beta,X,\\sigma)\n& = \\prod_{i=1}^{n}{\\frac{1}{\\sigma\\sqrt{2\\pi}}}\\exp{\\left\\{-\\frac{1}{2\\sigma^2}\\epsilon_i^2\\right\\}} \\\\\n& \\sim \\prod_{i=1}^{n}{\\frac{1}{\\sigma\\sqrt{2\\pi}}}\\exp{\\left\\{-\\frac{1}{2\\sigma^2}\n\\left( y - X \\beta \\right)^2\n\\right\\}}\n\\end{align}\n}\n\\]\nWe can formulate our estimate of the coefficient in a bayesian way if we model \\(\\beta\\) as a random variable (rather than a fixed quantity as per frequenist thinking):\n\\[\n\\displaylines{\n\\begin{align}\np(\\beta|X,y,\\sigma)\n& =\n\\frac{\n  p(y|\\beta,X,\\sigma)\n  p(\\beta|X,\\sigma)\n  }\n  {p(y|X,\\sigma)}\n\\\\ \\\\ & =\n\\frac{\n  p(y|\\beta,X,\\sigma)\n  p(\\beta|X,\\sigma)\n  }\n{\\int{}{} p(y|X,\\beta,\\sigma)p(\\beta|X,\\sigma) \\mathrm{d}\\beta}\n\\end{align}\n}\n\\]\nWhere:\n\n\\(p(Y|\\beta,X,\\sigma)\\) is the likelihood function\n\\(p(Y|X,\\sigma)\\) is the evidence (the data we feed into the model)\n\\(p(\\beta|X,\\sigma)\\) is the prior for the coefficient\n\nNow in frequentist regression, we assume no prior at all: \\(\\beta\\) is fixed, so \\(p(\\beta|X,\\sigma)=p(\\beta)=1\\), regardless of the evidence observed. Thus we are just left with finding the coefficients that maximise \\(p(Y|\\beta,X,\\sigma)\\).\nBy taking the negative log likelihood, we find this is identical to finding the coefficient values that minimise the sum of squared residuals (see the derivation here):\n\\[\n\\displaylines{\n\\begin{align}\n\\beta^* & =\\arg\\min_\\beta{\\left[\n  \\sum_{i=1}^N{\\epsilon_i^2}\n\\right]}\n\\end{align}\n}\n\\]\nHowever, we could use other types of priors, with mass around zero, to apply regularization on our coefficients.\n\n\nUsing Laplace priors to shrink coefficients:\nRegularization aims to eliminate some of our predictors to create a more parsimonious model in a systematic way, and/or reduce their magnitude to prevent overfitting.\nPicking a prior for our coefficient that is concentrated at zero can help achieve this - for example, we could use a Laplace distribution, with a location parameter \\(\\mu\\) of zero as visualised below:\n\n\nCode\nlaplace_dist &lt;- function(x, mu = 0, gamma = 1) {\n  laplace_pdf &lt;- function(x,mu,gamma) {\n    return(\n      exp(-abs(x-mu)/gamma)/(2*gamma)\n    )\n  }\n  y = sapply(x, FUN = function(i) laplace_pdf(i,mu,gamma))\n  return(y)\n}\n\nx = seq(-10,10,0.1)\nplot(\n  x=x,\n  y=laplace_dist(x, gamma=1),\n  type='l',\n  main='Laplace(gamma=0.5)'\n  )\n\n\n\n\n\n\n\n\n\nNot only does the Laplace pdf increase when closer to zero, but it increases at an accelerating rate. Thus, we can imagine that the closer the likelihood estimate of the coefficient is to zero, the greater the influence of the prior.\n\nLaplace priors ~ Lasso Regression\nRecall that the probability density function of Laplace is \\[\nf(x|\\mu,\\gamma) =\n\\frac{1}{2\\gamma}\n\\exp{ \\left\\{\n  \\frac{x - \\mu}{\\gamma} \\right\n\\} }\n\\]\nThen the prior can be written as the maximum likelihood across the estimated coefficients each of the \\(K\\) features in the model, for a given penalty importance \\(\\lambda\\): \\[\n\\displaylines{\n\\begin{align}\np(\\beta|X,\\sigma) \\sim p(\\beta)\n& = \\prod_{k=1}^{K}{\n  \\frac{1}{2 \\gamma}\n  \\exp{\\left\\{ \\frac{ -\\mid \\beta - \\mu \\mid}{\\gamma} \\right\\}}\n}\n\\\\ \\\\ & = \\prod_{k=1}^{K}{\n  \\frac{1}{2 \\gamma}\n  \\exp{\\left\\{ \\frac{ -\\mid \\beta \\mid}{\\gamma} \\right\\}}\n}\n\\end{align}\n}\n\\]\n\n\nThis is how the prior is defined in Tibshirani (1996). Since the prior is assumed before any data is observed, intuitively \\(\\beta\\) should not need conditioning on \\(X\\).\nHowever, Park and Casella (2008) found that not conditioning on \\(\\sigma^2\\) can result in non-unimodal posterior, so in practice a non-informative scale-invariant marginal prior \\(\\pi(\\sigma^2) = 1/\\sigma\\) on \\(\\sigma^2\\) is used.\nNow if set this as the prior, we can derive the cost function that we aim to minimize when \\(X\\) is observed.\n\\[\n\\displaylines{\n\\begin{align}\n\\max_{\\beta} {\\left\\{ p(\\beta|y,X,\\sigma) \\right\\}}\n& = \\max_\\beta{\n  \\left[ p(y|\\beta,X,\\sigma) \\times p(\\beta)\\right]\n} \\\\\n& \\sim \\min_\\beta{\n  \\left\\{ -\\log{\n    \\left[ p(y|\\beta,X,\\sigma) \\times p(\\beta)\\right]\n    }\\right\\}\n} \\\\\n& = \\min_\\beta{\n  \\left\\{\n    -\\log{\\left[ p(y|\\beta,X,\\sigma) \\right]\n    -\\log{\\left[p(\\beta)\\right]}\n    }\\right\\}\n} \\\\\n& = \\min_\\beta \\left\\{\n\\frac{1}{2} \\left(\n  \\cancel{ n \\log{[2 \\pi \\sigma^2]} }\n  + \\frac{1}{\\sigma^2} \\sum_{i=1}^N{\\epsilon_i^2}\n\\right)\n- \\log{\\left[\n  \\prod_{k=1}^{K}{\n    \\frac{1}{2\\gamma}\n    \\exp{\\left\\{ \\frac{ -\\mid \\beta \\mid}{\\gamma} \\right \\}}\n  }\n\\right]}\n\\right\\} \\\\\n& = \\min_\\beta \\left\\{\n  \\frac{1}{2\\sigma^2} \\sum_{i=1}^N{\\epsilon_i^2}\n- \\sum_{k=1}^{K}{ \\log{ \\left[\n    \\frac{1}{2\\gamma}\n    \\exp{ \\left\\{ \\frac{ -\\mid \\beta \\mid}{\\gamma} \\right\\} }\n    \\right]\n    }\n  }\n\\right\\} \\\\\n& = \\min_\\beta \\left\\{\n  \\frac{1}{2\\sigma^2} \\sum_{i=1}^N{\\epsilon_i^2}\n- \\cancel{ K \\log{ \\left[ \\frac{1}{2\\gamma} \\right] } }\n- \\sum_{k=1}^{K}{\n    \\log{ \\left[ \\exp{\\left\\{ \\frac{ -\\mid \\beta \\mid}{\\gamma} \\right\\}} \\right] }\n  }  \n\\right\\} \\\\\n& = \\min_\\beta \\left\\{\n  \\frac{1}{2\\sigma^2} \\sum_{i=1}^N{\\epsilon_i^2}\n+ \\frac{1}{\\gamma} \\sum_{k=1}^{K}{ \\mid \\beta \\mid}  \n\\right\\} \\\\ \\\\\n\\therefore \\beta^*\n& =\\arg\\min_\\beta{\\left\\{\n  \\frac{1}{2\\sigma^2} \\sum_{i=1}^N{\\epsilon_i^2}\n+ \\frac{1}{\\gamma}\\sum_{k=1}^{K}{\\mid\\beta\\mid}\n  \\right\\}\n}\n\\end{align}\n}\n\\]\n\n\nNote that terms that do not vary with the choice of \\(\\beta\\) drop out\n\n\n\n\n\n\nKey point\n\n\n\nHence setting a Laplace prior on the coefficients is - almost - equivalent to running L1 regularization, where \\(1/\\gamma\\) is the parameter influencing the penalty size.\n\n\nAlthough there is sparsity in the mode of the prior, when combining this with the likelihood distribution, the sparsity will not continue into the posterior distribution. In practice, if the posterior of \\(\\beta\\) is sufficiently small, we would want to drop it - so a threshold value for the size at which coefficients are zero-oed out is set as a hyperparameter.\n\n\n\nUsing Gaussian priors to shrink coefficients:\nIn a similar way to before for Lasso, we set our coefficient priors to each have a Gaussian distribution, with a location parameter \\(\\mu=0\\):\n\n\nCode\nnormal_pdf &lt;- function(x, mu = 0, sigma = 2) {\n  z = (x-mu)/sigma\n  y = (2*pi*sigma^2)^(-1/2) * exp(-0.5*z^2)\n  return(y)\n}\n\nx = seq(-10,10,0.1)\nplot(\n  x=x,\n  y=normal_pdf(x),\n  type='l',\n  main='Normal(sigma=1)'\n  )\n\n\n\n\n\n\n\n\n\n\nGaussian priors ~ Ridge Regression\nThe gaussian prior can be written as the maximum likelihood across the estimated coefficients each of the \\(K\\) features in the model, for a given penalty importance \\(\\lambda\\):\n\\[\n\\displaylines{\n\\begin{align}\np(\\beta|X,\\sigma) \\sim p(\\beta)\n& = \\prod_{k=1}^{K}{\\frac{1}{\\sigma\\sqrt{2\\pi}}}\\exp{\\left\\{-\\frac{1}{2\\sigma^2}\\beta_k^2\\right\\}}\n\\end{align}\n}\n\\]\nNow if set this as the prior, we can derive the cost function that we aim to minimize when \\(X\\) is observed.\n\\[\n\\displaylines{\n\\begin{align}\n\\max_{\\beta} {\\left\\{ p(\\beta|y,X,\\sigma) \\right\\}}\n& \\sim \\min_\\beta{\n  \\left\\{\n    -\\log{\\left[ p(y|\\beta,X,\\sigma) \\right]\n    -\\log{\\left[p(\\beta)\\right]}\n    }\\right\\}\n} \\\\\n& = \\min_\\beta \\left\\{\n\\frac{1}{2} \\left(\n  \\cancel{ n \\log{[2 \\pi \\sigma^2]} }\n  + \\frac{1}{\\sigma^2} \\sum_{i=1}^N{\\epsilon_i^2}\n\\right)\n- \\log{\\left[\n  \\prod_{k=1}^{K}{\n    \\frac{1}{\\sigma\\sqrt{2\\pi}}\n    \\exp{\\left\\{-\\frac{1}{2\\sigma^2}\\beta_k^2\\right\\}}\n    }\n\\right]}\n\\right\\} \\\\\n& = \\min_\\beta \\left\\{\n  \\frac{1}{2\\sigma^2} \\sum_{i=1}^N{\\epsilon_i^2}\n- \\sum_{k=1}^{K}{ \\log{ \\left[\n    \\frac{1}{\\sigma\\sqrt{2\\pi}}\n    \\exp{\\left\\{-\\frac{1}{2\\sigma^2}\\beta_k^2\\right\\}}\n    \\right]\n    }\n  }\n\\right\\} \\\\\n& = \\min_\\beta \\left\\{\n  \\frac{1}{2\\sigma^2} \\sum_{i=1}^N{\\epsilon_i^2}\n- \\cancel{ K \\log{ \\left[ \\frac{1}{\\sigma\\sqrt{2\\pi}} \\right] } }\n- \\sum_{k=1}^{K}{\n    \\log{ \\left[ \\exp{\\left\\{ -\\frac{1}{2\\sigma^2}\\beta_k^2 \\right\\}} \\right] }\n  }  \n\\right\\} \\\\\n& = \\min_\\beta \\left\\{\n  \\frac{1}{2\\sigma^2} \\sum_{i=1}^N{\\epsilon_i^2}\n+ \\frac{1}{2\\sigma^2} \\sum_{k=1}^{K}{ \\beta^2 }  \n\\right\\} \\\\ \\\\\n\\therefore \\beta^*\n& =\\arg\\min_\\beta{\\left\\{\n  \\frac{1}{2\\sigma^2} \\sum_{i=1}^N{\\epsilon_i^2}\n+ \\frac{1}{2\\sigma^2} \\sum_{k=1}^{K}{ \\beta^2 }\n  \\right\\}\n}\n\\end{align}\n}\n\\]\n\n\n\n\n\n\nKey point\n\n\n\nHence setting a Gaussian prior on the coefficients is - almost - equivalent to running L1 regularization, where the variance of the prior - \\(\\sigma^2\\) is the parameter directly influencing the penalty size.\n\n\n\n\n\n\n\n\nReferences\n\nPark, Trevor, and George Casella. 2008. “The Bayesian Lasso.” Journal of the American Statistical Association 103 (482): 681–86. https://doi.org/10.1198/016214508000000337.\n\n\nTibshirani, Robert. 1996. “Regression Shrinkage and Selection via the Lasso.” Journal of the Royal Statistical Society: Series B (Methodological) 58 (1): 267–88. https://doi.org/10.1111/j.1467-9868.2011.00771.x."
  },
  {
    "objectID": "posts/bayes_regularization.html",
    "href": "posts/bayes_regularization.html",
    "title": "Shrinkage priors for Lasso and Ridge",
    "section": "",
    "text": "What are we exploring?\n\n\n\nShowing Laplace and Gaussian priors in Bayesian regression are equivalent to Lasso and Ridge regularization\n\n\n\n\nBayesian Linear regression:\nWhen performing ordinary linear regression using maximum likelihood, we model the noise around \\(y\\) as being generated from a gaussian distributed process - conditional on the data \\(X\\) and estimated model parameters \\(\\beta\\) and \\(\\sigma\\):\n\\[\n\\displaylines{\n\\begin{align}\np(y|\\beta,X,\\sigma)\n& = \\prod_{i=1}^{n}{\\frac{1}{\\sigma\\sqrt{2\\pi}}}\\exp{\\left\\{-\\frac{1}{2\\sigma^2}\\epsilon_i^2\\right\\}} \\\\\n& \\sim \\prod_{i=1}^{n}{\\frac{1}{\\sigma\\sqrt{2\\pi}}}\\exp{\\left\\{-\\frac{1}{2\\sigma^2}\n\\left( y - X \\beta \\right)^2\n\\right\\}}\n\\end{align}\n}\n\\]\nWe can formulate our estimate of the coefficient in a bayesian way if we model \\(\\beta\\) as a random variable (rather than a fixed quantity as per frequenist thinking):\n\\[\n\\displaylines{\n\\begin{align}\np(\\beta|X,y,\\sigma)\n& =\n\\frac{\n  p(y|\\beta,X,\\sigma)\n  p(\\beta|X,\\sigma)\n  }\n  {p(y|X,\\sigma)}\n\\\\ \\\\ & =\n\\frac{\n  p(y|\\beta,X,\\sigma)\n  p(\\beta|X,\\sigma)\n  }\n{\\int{}{} p(y|X,\\beta,\\sigma)p(\\beta|X,\\sigma) \\mathrm{d}\\beta}\n\\end{align}\n}\n\\]\nWhere:\n\n\\(p(Y|\\beta,X,\\sigma)\\) is the likelihood function\n\\(p(Y|X,\\sigma)\\) is the evidence (the data we feed into the model)\n\\(p(\\beta|X,\\sigma)\\) is the prior for the coefficient\n\nNow in frequentist regression, we assume no prior at all: \\(\\beta\\) is fixed, so \\(p(\\beta|X,\\sigma)=p(\\beta)=1\\), regardless of the evidence observed. Thus we are just left with finding the coefficients that maximise \\(p(Y|\\beta,X,\\sigma)\\).\nBy taking the negative log likelihood, we find this is identical to finding the coefficient values that minimise the sum of squared residuals (see the derivation here):\n\\[\n\\displaylines{\n\\begin{align}\n\\beta^* & =\\arg\\min_\\beta{\\left[\n  \\sum_{i=1}^N{\\epsilon_i^2}\n\\right]}\n\\end{align}\n}\n\\]\nHowever, we could use other types of priors, with mass around zero, to apply regularization on our coefficients.\n\n\nUsing Laplace priors to shrink coefficients:\nRegularization aims to eliminate some of our predictors to create a more parsimonious model in a systematic way, and/or reduce their magnitude to prevent overfitting.\nPicking a prior for our coefficient that is concentrated at zero can help achieve this - for example, we could use a Laplace distribution, with a location parameter \\(\\mu\\) of zero as visualised below:\n\n\nCode\nlaplace_dist &lt;- function(x, mu = 0, gamma = 1) {\n  laplace_pdf &lt;- function(x,mu,gamma) {\n    return(\n      exp(-abs(x-mu)/gamma)/(2*gamma)\n    )\n  }\n  y = sapply(x, FUN = function(i) laplace_pdf(i,mu,gamma))\n  return(y)\n}\n\nx = seq(-10,10,0.1)\nplot(\n  x=x,\n  y=laplace_dist(x, gamma=1),\n  type='l',\n  main='Laplace(gamma=0.5)'\n  )\n\n\n\n\n\n\n\n\n\nNot only does the Laplace pdf increase when closer to zero, but it increases at an accelerating rate. Thus, we can imagine that the closer the likelihood estimate of the coefficient is to zero, the greater the influence of the prior.\n\nLaplace priors ~ Lasso Regression\nRecall that the probability density function of Laplace is \\[\nf(x|\\mu,\\gamma) =\n\\frac{1}{2\\gamma}\n\\exp{ \\left\\{\n  \\frac{x - \\mu}{\\gamma} \\right\n\\} }\n\\]\nThen the prior can be written as the maximum likelihood across the estimated coefficients each of the \\(K\\) features in the model, for a given penalty importance \\(\\lambda\\): \\[\n\\displaylines{\n\\begin{align}\np(\\beta|X,\\sigma) \\sim p(\\beta)\n& = \\prod_{k=1}^{K}{\n  \\frac{1}{2 \\gamma}\n  \\exp{\\left\\{ \\frac{ -\\mid \\beta - \\mu \\mid}{\\gamma} \\right\\}}\n}\n\\\\ \\\\ & = \\prod_{k=1}^{K}{\n  \\frac{1}{2 \\gamma}\n  \\exp{\\left\\{ \\frac{ -\\mid \\beta \\mid}{\\gamma} \\right\\}}\n}\n\\end{align}\n}\n\\]\n\n\nThis is how the prior is defined in Tibshirani (1996). Since the prior is assumed before any data is observed, intuitively \\(\\beta\\) should not need conditioning on \\(X\\).\nHowever, Park and Casella (2008) found that not conditioning on \\(\\sigma^2\\) can result in non-unimodal posterior, so in practice a non-informative scale-invariant marginal prior \\(\\pi(\\sigma^2) = 1/\\sigma\\) on \\(\\sigma^2\\) is used.\nNow if set this as the prior, we can derive the cost function that we aim to minimize when \\(X\\) is observed.\n\\[\n\\displaylines{\n\\begin{align}\n\\max_{\\beta} {\\left\\{ p(\\beta|y,X,\\sigma) \\right\\}}\n& = \\max_\\beta{\n  \\left[ p(y|\\beta,X,\\sigma) \\times p(\\beta)\\right]\n} \\\\\n& \\sim \\min_\\beta{\n  \\left\\{ -\\log{\n    \\left[ p(y|\\beta,X,\\sigma) \\times p(\\beta)\\right]\n    }\\right\\}\n} \\\\\n& = \\min_\\beta{\n  \\left\\{\n    -\\log{\\left[ p(y|\\beta,X,\\sigma) \\right]\n    -\\log{\\left[p(\\beta)\\right]}\n    }\\right\\}\n} \\\\\n& = \\min_\\beta \\left\\{\n\\frac{1}{2} \\left(\n  \\cancel{ n \\log{[2 \\pi \\sigma^2]} }\n  + \\frac{1}{\\sigma^2} \\sum_{i=1}^N{\\epsilon_i^2}\n\\right)\n- \\log{\\left[\n  \\prod_{k=1}^{K}{\n    \\frac{1}{2\\gamma}\n    \\exp{\\left\\{ \\frac{ -\\mid \\beta \\mid}{\\gamma} \\right \\}}\n  }\n\\right]}\n\\right\\} \\\\\n& = \\min_\\beta \\left\\{\n  \\frac{1}{2\\sigma^2} \\sum_{i=1}^N{\\epsilon_i^2}\n- \\sum_{k=1}^{K}{ \\log{ \\left[\n    \\frac{1}{2\\gamma}\n    \\exp{ \\left\\{ \\frac{ -\\mid \\beta \\mid}{\\gamma} \\right\\} }\n    \\right]\n    }\n  }\n\\right\\} \\\\\n& = \\min_\\beta \\left\\{\n  \\frac{1}{2\\sigma^2} \\sum_{i=1}^N{\\epsilon_i^2}\n- \\cancel{ K \\log{ \\left[ \\frac{1}{2\\gamma} \\right] } }\n- \\sum_{k=1}^{K}{\n    \\log{ \\left[ \\exp{\\left\\{ \\frac{ -\\mid \\beta \\mid}{\\gamma} \\right\\}} \\right] }\n  }  \n\\right\\} \\\\\n& = \\min_\\beta \\left\\{\n  \\frac{1}{2\\sigma^2} \\sum_{i=1}^N{\\epsilon_i^2}\n+ \\frac{1}{\\gamma} \\sum_{k=1}^{K}{ \\mid \\beta \\mid}  \n\\right\\} \\\\ \\\\\n\\therefore \\beta^*\n& =\\arg\\min_\\beta{\\left\\{\n  \\frac{1}{2\\sigma^2} \\sum_{i=1}^N{\\epsilon_i^2}\n+ \\frac{1}{\\gamma}\\sum_{k=1}^{K}{\\mid\\beta\\mid}\n  \\right\\}\n}\n\\end{align}\n}\n\\]\n\n\nNote that terms that do not vary with the choice of \\(\\beta\\) drop out\n\n\n\n\n\n\nKey point\n\n\n\nHence setting a Laplace prior on the coefficients is - almost - equivalent to running L1 regularization, where \\(1/\\gamma\\) is the parameter influencing the penalty size.\n\n\nAlthough there is sparsity in the mode of the prior, when combining this with the likelihood distribution, the sparsity will not continue into the posterior distribution. In practice, if the posterior of \\(\\beta\\) is sufficiently small, we would want to drop it - so a threshold value for the size at which coefficients are zero-oed out is set as a hyperparameter.\n\n\n\nUsing Gaussian priors to shrink coefficients:\nIn a similar way to before for Lasso, we set our coefficient priors to each have a Gaussian distribution, with a location parameter \\(\\mu=0\\):\n\n\nCode\nnormal_pdf &lt;- function(x, mu = 0, sigma = 2) {\n  z = (x-mu)/sigma\n  y = (2*pi*sigma^2)^(-1/2) * exp(-0.5*z^2)\n  return(y)\n}\n\nx = seq(-10,10,0.1)\nplot(\n  x=x,\n  y=normal_pdf(x),\n  type='l',\n  main='Normal(sigma=1)'\n  )\n\n\n\n\n\n\n\n\n\n\nGaussian priors ~ Ridge Regression\nThe gaussian prior can be written as the maximum likelihood across the estimated coefficients each of the \\(K\\) features in the model, for a given penalty importance \\(\\lambda\\):\n\\[\n\\displaylines{\n\\begin{align}\np(\\beta|X,\\sigma) \\sim p(\\beta)\n& = \\prod_{k=1}^{K}{\\frac{1}{\\sigma\\sqrt{2\\pi}}}\\exp{\\left\\{-\\frac{1}{2\\sigma^2}\\beta_k^2\\right\\}}\n\\end{align}\n}\n\\]\nNow if set this as the prior, we can derive the cost function that we aim to minimize when \\(X\\) is observed.\n\\[\n\\displaylines{\n\\begin{align}\n\\max_{\\beta} {\\left\\{ p(\\beta|y,X,\\sigma) \\right\\}}\n& \\sim \\min_\\beta{\n  \\left\\{\n    -\\log{\\left[ p(y|\\beta,X,\\sigma) \\right]\n    -\\log{\\left[p(\\beta)\\right]}\n    }\\right\\}\n} \\\\\n& = \\min_\\beta \\left\\{\n\\frac{1}{2} \\left(\n  \\cancel{ n \\log{[2 \\pi \\sigma^2]} }\n  + \\frac{1}{\\sigma^2} \\sum_{i=1}^N{\\epsilon_i^2}\n\\right)\n- \\log{\\left[\n  \\prod_{k=1}^{K}{\n    \\frac{1}{\\sigma\\sqrt{2\\pi}}\n    \\exp{\\left\\{-\\frac{1}{2\\sigma^2}\\beta_k^2\\right\\}}\n    }\n\\right]}\n\\right\\} \\\\\n& = \\min_\\beta \\left\\{\n  \\frac{1}{2\\sigma^2} \\sum_{i=1}^N{\\epsilon_i^2}\n- \\sum_{k=1}^{K}{ \\log{ \\left[\n    \\frac{1}{\\sigma\\sqrt{2\\pi}}\n    \\exp{\\left\\{-\\frac{1}{2\\sigma^2}\\beta_k^2\\right\\}}\n    \\right]\n    }\n  }\n\\right\\} \\\\\n& = \\min_\\beta \\left\\{\n  \\frac{1}{2\\sigma^2} \\sum_{i=1}^N{\\epsilon_i^2}\n- \\cancel{ K \\log{ \\left[ \\frac{1}{\\sigma\\sqrt{2\\pi}} \\right] } }\n- \\sum_{k=1}^{K}{\n    \\log{ \\left[ \\exp{\\left\\{ -\\frac{1}{2\\sigma^2}\\beta_k^2 \\right\\}} \\right] }\n  }  \n\\right\\} \\\\\n& = \\min_\\beta \\left\\{\n  \\frac{1}{2\\sigma^2} \\sum_{i=1}^N{\\epsilon_i^2}\n+ \\frac{1}{2\\sigma^2} \\sum_{k=1}^{K}{ \\beta^2 }  \n\\right\\} \\\\ \\\\\n\\therefore \\beta^*\n& =\\arg\\min_\\beta{\\left\\{\n  \\frac{1}{2\\sigma^2} \\sum_{i=1}^N{\\epsilon_i^2}\n+ \\frac{1}{2\\sigma^2} \\sum_{k=1}^{K}{ \\beta^2 }\n  \\right\\}\n}\n\\end{align}\n}\n\\]\n\n\n\n\n\n\nKey point\n\n\n\nHence setting a Gaussian prior on the coefficients is - almost - equivalent to running L1 regularization, where the variance of the prior - \\(\\sigma^2\\) is the parameter directly influencing the penalty size.\n\n\n\n\n\n\n\n\nReferences\n\nPark, Trevor, and George Casella. 2008. “The Bayesian Lasso.” Journal of the American Statistical Association 103 (482): 681–86. https://doi.org/10.1198/016214508000000337.\n\n\nTibshirani, Robert. 1996. “Regression Shrinkage and Selection via the Lasso.” Journal of the Royal Statistical Society: Series B (Methodological) 58 (1): 267–88. https://doi.org/10.1111/j.1467-9868.2011.00771.x."
  },
  {
    "objectID": "posts/ols_finite_sample_correction.html",
    "href": "posts/ols_finite_sample_correction.html",
    "title": "Estimating population variance from a sample",
    "section": "",
    "text": "What are we exploring?\n\n\n\nWhy is sample variance divided by \\(n-1\\), but population variance by \\(N\\)."
  },
  {
    "objectID": "posts/ols_finite_sample_correction.html#intro-population-variance-vs.-variance-estimate-from-sample",
    "href": "posts/ols_finite_sample_correction.html#intro-population-variance-vs.-variance-estimate-from-sample",
    "title": "Estimating population variance from a sample",
    "section": "Intro: Population variance vs. variance estimate from sample",
    "text": "Intro: Population variance vs. variance estimate from sample\nIf we have an entire population of size \\(N\\), we can perfectly calculate the true population mean \\(\\mu\\). As a result, we calculate the variance statistic in an intuitive way:\n\\[\n\\displaylines{\n\\begin{align}\n\\mu &\n= \\frac{1}{N} \\sum_{i=1}^{N}{X_i}\n\\\\\n\\sigma^2 &\n= \\frac{1}{N}\\sum_{i=1}^{N}{(X_i-\\mu)^2}\n\\end{align}\n}\n\\]\nHowever, if we are creating an estimate of the population variance from a finite sample of size \\(n\\), then we need to do some bias correction, where we divide by \\(n-1\\) instead:\n\\[\n\\displaylines{\n\\begin{align}\n\\bar{x} &\n= \\frac{1}{n} \\sum_{i=1}^{n}{x_i}\n\\\\\n\\hat{\\sigma}^2 &\n= \\frac{1}{n-1}\\sum_{i=1}^{n}{(x_i-\\bar{x})^2}\n\\end{align}\n}\n\\]"
  },
  {
    "objectID": "posts/ols_finite_sample_correction.html#some-intuition-as-to-why",
    "href": "posts/ols_finite_sample_correction.html#some-intuition-as-to-why",
    "title": "Estimating population variance from a sample",
    "section": "Some intuition as to why:",
    "text": "Some intuition as to why:\nThe reason for this is because our sample mean \\(\\bar{x}\\) is almost always not going to be exactly the population mean \\(\\mu\\), i.e. \\(\\bar{x} \\neq \\mu\\).\nNow because \\(\\bar{x}\\) is calculated from the sample, the datapoints in that same sample will be closer to it than they would be \\(\\mu\\). So if using the squared distances between each sample and the sample mean to estimate the population variance, we need to take this into account!"
  },
  {
    "objectID": "posts/ols_finite_sample_correction.html#proving-the-difference",
    "href": "posts/ols_finite_sample_correction.html#proving-the-difference",
    "title": "Estimating population variance from a sample",
    "section": "Proving the difference:",
    "text": "Proving the difference:\nImagine there are \\(N\\) i.i.d random variables \\(X_1, X_2,...,X_N\\)1., of sample sizes \\(n_1, n_2, ..., n_N\\), generated from a population process with mean \\(\\mu\\) and \\(\\sigma\\). Then we might derive the following:\n\nThe mean \\(\\bar{X_i} = \\frac{1}{n_i}\\sum_{j=1}^{n_i}{(X_{i,j})}\\)\nThe variance of sample \\(i\\) is \\(s_i^2 = \\frac{1}{n_i} \\sum_{j=1}^{n_i}{(X_{i,j}-\\bar{X_i})^2}\\)\n\nLet’s now compare how the expected sample variance, \\(\\mathbb{E}[s_i^2]\\) differs from the true population variance, \\(\\mathbb{V}[X]=\\frac{1}{N} \\sum_{i=1}^{N}{ (X_i-\\mu)^2 }\\):\n\\[\n\\displaylines{\n\\begin{align}\n\\mathbb{E}[s^2] & =\n\\mathbb{E}\\left[\n  \\frac{1}{n_i} \\sum_{i=1}^{n_i}{(X_i-\\bar{X})^2}\n\\right]\n\\\\ & =\n\\mathbb{E}\\left[\n  \\frac{1}{n} \\sum_{i=1}^{n}{\\bigg(\n    (X_i-\\mu)-(\\bar{X}-\\mu)\n  \\bigg)^2}\n\\right]\n\\\\ & =\n\\mathbb{E}\\left[\n  \\frac{1}{n} \\sum_{i=1}^{n}{\\bigg(\n    (X_i-\\mu)^2-2(X_i-\\mu)(\\bar{X}-\\mu)+(\\bar{X}-\\mu)^2\n  \\bigg)}\n\\right]\n\\\\ & =\n\\mathbb{E}\\left[\n  \\frac{1}{n} \\bigg(\\sum_{i=1}^{n}{ (X_i-\\mu)^2 } \\bigg)\n  - \\frac{2}{n} \\bigg((\\bar{X}-\\mu) \\sum_{i=1}^{n}{(X_i-\\mu)} \\bigg)\n  + \\frac{1}{n} \\bigg(n(\\bar{X}-\\mu)^2 \\bigg)\n\\right] & \\because \\bar{X}-\\mu \\text{ is constant}\n\\\\ & =\n\\mathbb{E}\\left[\n  \\frac{1}{n} \\bigg(\\sum_{i=1}^{n}{ (X_i-\\mu)^2 } \\bigg)\n  - \\frac{2}{n} \\bigg( (\\bar{X}-\\mu) [n(\\bar{X}-\\mu)] \\bigg)\n  + (\\bar{X}-\\mu)^2\n  \\bigg)\n\\right] & \\because \\frac{1}{n}\\sum_{i=1}^{n}{(X_i-\\mu)} = \\bar{X} - \\mu\n\\\\ & =\n\\mathbb{E}\\left[\n  \\frac{1}{n} \\bigg(\\sum_{i=1}^{n}{ (X_i-\\mu)^2 } \\bigg)\n  - (\\bar{X}-\\mu)^2\n  \\bigg)\n\\right]\n\\\\ & =\n\\underbrace{\n\\mathbb{E}\\left[\n  \\frac{1}{n} \\sum_{i=1}^{n}{ (X_i-\\mu)^2 }\n\\right]\n}_{\\text{True population variance}} -\n\\underbrace{\n\\mathbb{E}\\Bigg[\n  (\\bar{X}-\\mu)^2\n\\Bigg]\n}_{\\text{Sample vs pop. mean}}\n\\end{align}\n}\n\\]\nSo we can see that \\(E[s^2]\\) is too small by that extra term, \\(E[(\\bar{x}-\\mu)^2]\\).\nNote this is the expected variance of \\(\\bar{X}\\) i.e. \\(\\text{Var}[\\bar{X}]=E[(\\bar{x}-\\mu)^2]\\)"
  },
  {
    "objectID": "posts/ols_finite_sample_correction.html#exploring-the-variance-of-mean",
    "href": "posts/ols_finite_sample_correction.html#exploring-the-variance-of-mean",
    "title": "Estimating population variance from a sample",
    "section": "Exploring the variance of mean",
    "text": "Exploring the variance of mean\nIt can be shown that the variance of the sum of uncorrelated random variables is equal to the sum of their variances2:\n\\[\n\\text{Var}\\left( \\sum_{i=1}^{n}{X_i} \\right)\n= \\sum_{i=1}^{n}{\\text{Var}\\left(X_i\\right) }\n\\]\nNow since every \\(X_i\\) has the same variance \\(\\sigma^2\\), then we can derive the following:\n\\[\n\\displaylines{\n\\begin{align}\nE\\left[ (\\bar{X}-\\mu)^2 \\right]\n& =\n\\text{Var}\\left[\\bar{X}\\right]\n=\n\\text{Var}\\left[\\frac{1}{n}\\sum_{i=1}^{n}{X_i}\\right]\n\\\\ & =\n\\left( \\frac{1}{n} \\right)^2\n\\text{Var} \\left[ \\sum_{i=1}^{n}{X_i} \\right]\n\\\\ & =\n\\left( \\frac{1}{n} \\right)^2\n\\sum_{i=1}^{n}{ \\text{Var} \\big[X_i\\big] }\n\\\\ & =\n\\frac{1}{n^2}\n\\left( n \\times \\sigma^2 \\right)\n\\\\ & =\n\\frac{\\sigma^2}{n}\n\\end{align}\n}\n\\]\n\n\nSince \\(\\text{Var}(aX) = a^2\\text{Var}(X)\\)3"
  },
  {
    "objectID": "posts/ols_finite_sample_correction.html#bessels-correction",
    "href": "posts/ols_finite_sample_correction.html#bessels-correction",
    "title": "Estimating population variance from a sample",
    "section": "Bessel’s correction:",
    "text": "Bessel’s correction:\nSo if we sub this into our original equation: \\[\n\\displaylines{\n\\begin{align}\nE[s^2] & =\n\\underbrace{E \\left[ \\frac{1}{n} \\sum_{i=1}^{n}{ (x_i-\\mu)^2 } \\right]}_{\\sigma}\n-\n\\underbrace{E\\Bigg[ (\\bar{x}-\\mu)^2 \\Bigg]}_{\\sigma/n}\n\\\\ & =\n\\left(1-\\frac{1}{n} \\right)\\sigma^2\n\\\\ \\\\ & =\n\\frac{(n-1)}{n}\\sigma^2\n\\end{align}\n}\n\\]\nThen we can see that we need to make a correction of \\(n/(n-1)\\), aka Bessel’s correction, to the statistic!\n\\[\n\\displaylines{\n\\begin{align}\n\\hat{\\sigma}^2\n& = \\frac{n}{n-1}\\left(s^2\\right) \\\\\n& = \\frac{1}{n-1} \\left( \\sum_{i=1}^{n}{x_i-\\bar{x}} \\right)\n\\end{align}\n}\n\\]\nFin."
  },
  {
    "objectID": "posts/ols_finite_sample_correction.html#footnotes",
    "href": "posts/ols_finite_sample_correction.html#footnotes",
    "title": "Estimating population variance from a sample",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nRandom variables can take on a range of possible values that are not yet realised (e.g. for a dice, \\(X \\in \\{1,2,3,4,5,6\\}\\)). These are often signified using capital letters. Samples have values that are already realised e.g. someone rolled a 3, then a 5 with the dice. We didn’t observe an infinite number of dice roles, but just two: \\(x_1=3,x_2=5\\) (\\(\\therefore n=2,\\bar{x}=4\\)). The population is usually denoted with capital letters too (since we usually don’t observe the entire population!), where e.g. for dice \\(E[X]=\\mu=3.5\\) ↩︎\nShowing \\(\\text{Var}\\big[ X+Y \\big] = \\text{Var}\\big[ X \\big] + \\text{Var}\\big[ Y \\big]\\): \\[\n  \\displaylines{\n  \\begin{align}\n  \\text{Var}\\big[ X+Y \\big]\n  & = E\\big[ (X+Y)^2 \\big] - \\big( E[X+Y] \\big)^2 \\\\\n  & = E\\big[ X^2+2XY+Y^2 \\big] - \\big( E[X]+E[Y] \\big)^2 \\\\\n  & = E\\big[ X^2 \\big] + 2E\\big[ XY \\big] + E\\big[Y^2 \\big] - \\bigg(E[X]^2 + 2E[X]E[Y] + E[Y]^2 \\bigg)  \\\\\n  & = \\bigg( E\\big[ X^2 \\big] - E[X]^2 \\bigg) + \\bigg( E\\big[Y^2 \\big]- E[Y]^2 \\bigg) +\n  \\bigg( E\\big[XY]- E[X]E[Y] \\bigg) \\\\\n  & = \\text{Var}\\big[ X \\big] + \\text{Var}\\big[ Y \\big] + \\cancel{\\text{Cov}\\big[ X,Y \\big]}\n  & \\because X \\text{ and } Y \\text{ are independent}\\\\\n  & = \\text{Var}\\big[ X \\big] + \\text{Var}\\big[ Y \\big]\n  \\end{align}\n  }\n  \\] ↩︎\nShowing \\(\\text{Var}\\big[ aX \\big] = a^2\\text{Var}\\big[ X \\big]\\): \\[\n  \\displaylines{\n  \\begin{align}\n  \\text{Var}\\big[ aX \\big]\n  & = E\\big[ (aX)^2 \\big] - \\big( E[aX] \\big)^2 \\\\\n  & = E\\big[ a^2X^2 \\big] - \\big( a E[X] \\big)^2 \\\\\n  & = a^2 E\\big[ X^2 \\big] - a^2 \\big(E[X]\\big)^2 \\\\\n  & = a^2 \\bigg( E\\big[ X^2 \\big] - \\big(E[X]\\big)^2 \\bigg) \\\\\n  & = a^2\\text{Var}\\big[ X \\big]\n  \\end{align}\n  }\n  \\]↩︎"
  },
  {
    "objectID": "posts/fundamentals_binom_pois.html",
    "href": "posts/fundamentals_binom_pois.html",
    "title": "Deriving the poisson distribution from binomial",
    "section": "",
    "text": "What are we exploring?\n\n\n\nHow extending the binomial distribution to an infinite number of trials derives the poisson distribution."
  },
  {
    "objectID": "posts/fundamentals_binom_pois.html#defining-a-binomial-problem",
    "href": "posts/fundamentals_binom_pois.html#defining-a-binomial-problem",
    "title": "Deriving the poisson distribution from binomial",
    "section": "Defining a binomial problem",
    "text": "Defining a binomial problem\nLet’s look at a metric for the number of failures of an autonomous driving system per 100,000 km driven.\nWe could model this using the binomial probability density, where we define each km driven as a trial, and an event being a failure occuring during a km driven:\n\\[\n\\displaylines{\n\\begin{align}\nP(x) = &\n{n \\choose x} p^x (1-p)^{n-x}\n\\\\\\\\ \\text{where } n & \\text{ is total miles driven}\n\\\\ \\text{and } x & \\text{ is number of failures}\n\\end{align}\n}\n\\]"
  },
  {
    "objectID": "posts/fundamentals_binom_pois.html#increasing-the-number-of-trials-to-infinity",
    "href": "posts/fundamentals_binom_pois.html#increasing-the-number-of-trials-to-infinity",
    "title": "Deriving the poisson distribution from binomial",
    "section": "Increasing the number of trials to infinity",
    "text": "Increasing the number of trials to infinity\nWhat happens if we test billions of miles? This is effectively the same as asking what happens if \\(n \\rightarrow \\infty\\).\nLet’s also define \\(\\lambda\\), the mean number of successes we expect over \\(n\\) trials (the “rate”). This simply means that \\(\\lambda = np\\)\n\\[\n\\displaylines{\n\\begin{align}\n\\lim_{n \\rightarrow \\infty} &\n{n \\choose x} p^x (1-p)^{n-x}\n\\\\\n= \\lim_{n \\rightarrow \\infty} &\n{n \\choose x} \\left( \\frac{\\lambda}{n} \\right)^x \\left( 1-\\frac{\\lambda}{n} \\right)^{(n-x)}\n& \\because \\lambda = np\n\\end{align}\n}\n\\]\nIt is helpful to simplify the expression before examining what happens when \\(n\\) tends to infinity. We can split it into four parts (i, ii, iii and iv):\n\\[\n\\displaylines{\n\\begin{align}\n\\lim_{n \\rightarrow \\infty} &\n{n \\choose x} \\left( \\frac{\\lambda}{n} \\right)^x \\left( 1-\\frac{\\lambda}{n} \\right)^{(n-x)}\n\\\\\n= \\lim_{n \\rightarrow \\infty} &\n\\frac{n!}{(n-x)! \\times x!} \\times \\left( \\frac{\\lambda}{n} \\right)^x \\times \\left( 1-\\frac{\\lambda}{n} \\right)^{n} \\times \\left( 1-\\frac{\\lambda}{n} \\right)^{-x}\n\\\\\n= \\lim_{n \\rightarrow \\infty} &\n\\underbrace{\n  \\frac{n!}{(n-x)! \\times n^x}\n}_{\\text{i}}\n\\times\n\\underbrace{\n  \\frac{\\lambda^x}{x!}\n}_{\\text{ii}}\n\\times\n\\underbrace{\n  \\left( 1-\\frac{\\lambda}{n} \\right)^{n}\n}_{\\text{iii}}\n\\times\n\\underbrace{\n   \\left( 1-\\frac{\\lambda}{n} \\right)^{-x}\n}_{\\text{iv}}\n\\end{align}\n}\n\\]\nWe now explore what happens if we extend \\(n\\) towards infinity for each term.\n\nTerm i\nWe can expand out the factorial to simplify it: \\[\n\\displaylines{\n\\begin{align}\n\\frac{n!}{(n-x)! \\times (n^x)}\n& =\n\\frac{\n  n \\times (n-1) \\times (n-2) \\times \\ldots \\times\n  (n-x+1) \\times \\cancel{(n-x)!}\n}{\n  \\cancel{(n-x)!} \\times (n^x)\n}\n\\\\\\\\ & =\n\\frac{n}{n} \\times\n\\frac{n-1}{n} \\times\n\\frac{n-2}{n} \\times\n\\ldots \\times\n\\frac{n-x-2}{n} \\times\n\\frac{n-x-1}{n}\n\\\\\\\\ & =\n1 \\times\n\\left( 1 - \\frac{1}{n} \\right) \\times\n\\left( 1 - \\frac{2}{n} \\right) \\times\n\\ldots \\times\n\\left( 1 - \\frac{x-2}{n} \\right) \\times\n\\left( 1 - \\frac{x-1}{n} \\right)\n\\\\\\\\ & =\n\\prod_{i=1}^{x-1}{\\left(1-\\frac{i}{n}\\right)}\n\\end{align}\n}\n\\]\nAs similarly to before \\(n \\rightarrow \\infty\\), then \\(\\frac{i}{n} \\rightarrow 0\\).\n\\[\n\\displaylines{\n\\begin{align}\n\\therefore\n\\lim_{n \\rightarrow \\infty}\n\\prod_{i=1}^{x-1}{\\left(1-\\frac{i}{n}\\right)}\n& = 1\n\\end{align}\n}\n\\]\n\n\nTerm ii\n\\[\n\\displaylines{\n\\begin{align}\n\\lim_{n \\rightarrow \\infty}\n\\frac{\\lambda^x}{x!}\n& =\n\\frac{\\lambda^x}{x!}\n& \\because\nn \\text{ does not blow up}\n\\end{align}\n}\n\\]\n\n\nTerm iii\nThis is a tricky one!\n\\[\n\\lim_{n \\rightarrow \\infty}{ \\left[\n    \\left(1 - \\frac{\\lambda}{n} \\right)^{n}\n  \\right]} = e^{-\\lambda}\n\\]\n(We go through this derivation in the footnotes for if you do not know it already)1\n\n\nTerm iv\nAs \\(n \\rightarrow \\infty\\), then \\(\\frac{\\lambda}{n} \\rightarrow 0\\), so:\n\\[\n\\lim_{n \\rightarrow \\infty}\\left( 1-\\frac{\\lambda}{n} \\right)^{-x} = (1-0)^{-x} = 1\n\\]"
  },
  {
    "objectID": "posts/fundamentals_binom_pois.html#tying-it-all-together",
    "href": "posts/fundamentals_binom_pois.html#tying-it-all-together",
    "title": "Deriving the poisson distribution from binomial",
    "section": "Tying it all together",
    "text": "Tying it all together\n\\[\n\\displaylines{\n\\begin{align}\n& \\lim_{n \\rightarrow \\infty}\n\\left[\n  \\frac{n!}{(n-x)! \\times n^x}\n\\times\n  \\frac{\\lambda^x}{x!}\n\\times\n  \\left( 1-\\frac{\\lambda}{n} \\right)^{n}\n\\times\n   \\left( 1-\\frac{\\lambda}{n} \\right)^{-x}\n\\right]\n\\\\ & =\n1 \\times\n  \\frac{\\lambda^x}{x!}\n\\times\n  e^{-\\lambda}\n\\times\n   1\n\\\\ & =\n\\frac{\\lambda^x e^{-\\lambda}}{x!}\n\\end{align}\n}\n\\]\nWhich is the poisson pdf!\nFin."
  },
  {
    "objectID": "posts/fundamentals_binom_pois.html#footnotes",
    "href": "posts/fundamentals_binom_pois.html#footnotes",
    "title": "Deriving the poisson distribution from binomial",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nShowing \\(\\lim_{n \\rightarrow \\infty}{ \\left[ \\left(1 + \\frac{x}{n} \\right)^{n} \\right]} = e^x\\): \\[\n\\displaylines{\n\\begin{align}\n\\text{Let } f(x) & = \\ln{(x)} \\\\\n\\therefore f'(x) & = \\frac{1}{x} \\\\\n\\text{and } f'(x) & =\n\\lim_{a \\rightarrow 0}{ \\left[\n  \\frac{f(x+a) - f(x)}{(x+a) - x}\n\\right] } \\\\\n& = \\lim_{a \\rightarrow 0}{ \\left[\n  \\frac{\\ln{(x+a)} - \\ln{(x)}}{a}\n\\right] }  \\\\\n& = \\lim_{a \\rightarrow 0}{ \\left[\n  \\frac{1}{a}  \\ln{\\left(\\frac{x+a}{x} \\right)}\n\\right] } \\\\\n& = \\lim_{a \\rightarrow 0}{ \\left[\n  \\ln{\\left(1 + \\frac{a}{x} \\right)^{\\frac{1}{a}}}\n\\right] } \\\\\n& = \\lim_{b \\rightarrow \\infty}{ \\left[\n  \\ln{\\left(1 + \\frac{1}{bx} \\right)^{b}}\n\\right] }\n& \\text{where } b = \\frac{1}{a}\n\\\\\n& = \\lim_{n \\rightarrow \\infty}{ \\left[\n  \\ln{\\left(1 + \\frac{x}{n} \\right)^{n}}\n\\right]^{1/x^2} }\n& \\text{where } n = bx^2\n\\\\\n& = \\ln{\\left(\n  \\lim_{n \\rightarrow \\infty}{\\left[1 + \\frac{x}{n} \\right]^{n}}\n\\right)^{1/x^2}\n}\n& \\because \\ln(x) \\text{ is continuous)}\n\\\\\n& = \\frac{1}{x}\n\\\\\n\\therefore\ne^{1/x} & = \\lim_{n \\rightarrow \\infty}{ \\left[\n  \\left(1 + \\frac{x}{n} \\right)^{n}\n\\right]^{1/x^2} }\n\\\\\n\\therefore\ne^{x} & = \\lim_{n \\rightarrow \\infty}{ \\left[\n  \\left(1 + \\frac{x}{n} \\right)^{n}\n\\right]}\n\\end{align}\n}\n\\]↩︎"
  },
  {
    "objectID": "posts/glm_logit.html",
    "href": "posts/glm_logit.html",
    "title": "Logistic Regression",
    "section": "",
    "text": "What are we exploring?\n\n\n\nModelling bernoulli probabilities using maximum likelihood estimation and the logit link funciton."
  },
  {
    "objectID": "posts/glm_logit.html#a-latent-variable-model",
    "href": "posts/glm_logit.html#a-latent-variable-model",
    "title": "Logistic Regression",
    "section": "A latent variable model",
    "text": "A latent variable model\nImagine a basketball player is taking free throws.\nFor each trial \\(i\\) (a freethrow attempt), we observe the outcome \\(y_i\\) as being 1 if there is a success (i.e. the freethrow is made), or 0 if it is a failure (a miss).\nWhen the player takes the freethrow, there are exogenous features \\(X_i\\) that influence their likelihood of making it (e.g. whether they are home or away). Also there is some randomness, \\(\\varepsilon_i\\) that is unpredictable: a 99% accurate shooter still has a 1% chance of missing.\nEconomists often frame this problem as a “latent variable model”. What this means is they frame the problem as though there is a hidden continuous variable we do not observe, \\(y_i^*\\), and if it exceeds a certain threshold (usually zero) then there is a success (e.g. the freethrow is made):\n\\[\n\\displaylines{\n\\begin{align}\ny_i^* & = X_i\\beta + \\varepsilon_i\n\\\\ \\\\\ny_i & =\n\\begin{cases}\n  1 & \\text{if}\\ y_i^* &gt; 0 & \\text{i.e. } -\\varepsilon_i &gt; X_i\\beta\\\\\n  0 & \\text{otherwise}\n\\end{cases}\n\\end{align}\n}\n\\]\nIn other words, the latent variable \\(y_i^*\\) is purely a function of its predictors \\(X_i\\), their relationship to \\(y_i^*\\) given by \\(\\beta\\), and some additive noise \\(\\varepsilon_i\\). If \\(y_i^*\\) is positive, we observe a success (i.e. \\(y_i = 1\\)).\nWe can thus formulate the probability of success \\(P(y_i=1|X)\\) as the likelihood that the additive noise \\(\\varepsilon_i\\) is less than \\(X\\beta\\), resulting in the probability \\(y_i^*\\) above zero:\n\\[\n\\displaylines{\n\\begin{align}\np_i & = P(y_i=1|X_i) \\\\\n& = P(y_i^* &gt; 0 | X_i)\n\\\\\n& = P(X_i\\beta + \\varepsilon_i &gt; 0)\n\\\\\n& = P(\\varepsilon_i &gt; -X_i\\beta )\n\\\\\n& = P(\\varepsilon_i &lt; X_i\\beta ) & \\iff \\varepsilon_i \\sim f(\\mu,s) \\text{ is symmetric}\n\\end{align}\n}\n\\]\nSo a good assumption for the probabilistic process that generates the error \\(\\varepsilon_i\\) is very important! We will come back to this shortly, after discussing the Bernoulli probability mass function."
  },
  {
    "objectID": "posts/glm_logit.html#the-bernoulli-pmf",
    "href": "posts/glm_logit.html#the-bernoulli-pmf",
    "title": "Logistic Regression",
    "section": "The Bernoulli PMF",
    "text": "The Bernoulli PMF\nEven if a player has a 99% of making freethrows, they would be expected to miss 1 in 100. We can capture this through the probability mass function of the Bernoulli distribution:\n\\[\n\\displaylines{\n\\begin{align}\nP(y_i) =\n\\begin{cases}\n  p_i & \\text{if}\\ y_i=1 \\\\\n  1-p_i & \\text{if}\\ y_i=0\n\\end{cases}\n\\end{align}\n}\n\\]\nWhich is equivalent to the following:\n\\[\n\\displaylines{\n\\begin{align}\nP(y_i)\n& = p_i^{y_i}(1-p_i)^{1-y_i} \\\\ \\\\\n& = P(\\varepsilon_i &lt; X_i\\beta)^{y_i}\n(1-P(\\varepsilon_i &lt; X_i\\beta))^{1-y_i}\n\\end{align}\n}\n\\]\n\n\nSince \\(g(x)^0 = 1\\):\n\nif \\(y_i=1\\), \\(p^{1}(1-p)^{0} = p\\)\nif \\(y_i=0\\), \\(p^{0}(1-p)^{1} = 1-p\\)\n\nUntil now, we have just been looking at the likelihood of making a success for a single trial \\(i\\). But we want to find values for \\(\\beta\\) that optimize predictions across all trials, to learn the impact of \\(X\\) so we can better predict \\(y\\) next time - aka maximum likelihood estimation."
  },
  {
    "objectID": "posts/glm_logit.html#maximum-likelihood-estimation",
    "href": "posts/glm_logit.html#maximum-likelihood-estimation",
    "title": "Logistic Regression",
    "section": "Maximum Likelihood Estimation",
    "text": "Maximum Likelihood Estimation\nAssuming each trial is independent (a big assumption for free throws!) - the probability of making two then missing one is \\(p_i \\times p_i \\ \\times (1-p_i)\\). In other words - the combined probability is the multiplication of the individual probabilities.\nWe can generalize this to a sample size \\(N\\) as the following:\n\\[\n\\displaylines{\n\\begin{align}\np(y|X)\n& =\n\\prod_{i=1}^{N}{\n  p_i^{y_i} (1-p_i)^{1-y_i}\n  }\n\\\\ & =\n\\underbrace{\n  \\prod_{y_i=0}^{n_1}{ p_i^{y_i} }\n}_{y_i=1} \\times\n\\underbrace{\n  \\prod_{y_i=1}^{n_0}{ (1-p_i)^{1-y_i} }    \n}_{y_i=0}\n\\end{align}\n}\n\\]\nIn practice, it is common to minimize the negative log-likelihood, which is shown to be equivalent to maximising the likelihood directly (since the logarithm is a monotonic function):\n\\[\n\\displaylines{\n\\begin{align}\n& \\max_\\beta{p(y|X)} \\\\\n= &\n\\max_\\beta{\\left\\{\n  \\prod_{i=1}^{N}{ p_i^{y_i} }\n  \\times\n  \\prod_{i=1}^{N}{ (1-p_i)^{1-y_i} }\n  \\right\\}}\n\\\\ \\\\ \\Rightarrow &\n\\min_\\beta{\\left\\{ -\\log{ \\left[\n  \\prod_{i=1}^{N}{ p_i^{y_i} }\n  \\times\n  \\prod_{i=1}^{N}{ (1-p_i)^{1-y_i} }\n  \\right] } \\right\\}}\n\\\\ \\\\ = &\n\\min_\\beta{\\left\\{\n  \\sum_{i=1}^{N}{ -\\log{ \\left[ p_i^{y_i} \\right] } } +\n  \\sum_{i=1}^{N}{ -\\log{ \\left[ (1-p_i)^{1-y_i}\\right] } }\n  \\right\\}}  \n\\\\ \\\\ = &\n\\min_\\beta{\\left\\{\n  \\sum_{i=1}^{N}{ -y_i\\log{ \\left[ p_i \\right] } } +\n  \\sum_{i=1}^{N}{ -(1-y_i)\\log{ \\left[ 1-p_i\\right] } }\n  \\right\\}}\n\\\\ \\\\ = &\n\\min_\\beta{\\left\\{\n  \\sum_{i=1}^{N}{ -y_i\\log{ \\left[ P(\\varepsilon_i &lt; X \\beta) \\right] } } +\n  \\sum_{i=1}^{N}{ -(1-y_i)\\log{ \\left[ 1-P(\\varepsilon_i &lt; X \\beta)\\right] } }\n  \\right\\}}    \n\\end{align}\n}\n\\]\n\n\nRecall that \\(\\log{\\left(ab\\right)} = \\log{\\left(a\\right)} + \\log{\\left(b\\right)}\\)\nWe now - almost - have a well defined problem we can solve! We just need to make an assumption for the distribution of errors \\(\\varepsilon\\)."
  },
  {
    "objectID": "posts/glm_logit.html#assuming-errors-come-from-a-logistic-distribution",
    "href": "posts/glm_logit.html#assuming-errors-come-from-a-logistic-distribution",
    "title": "Logistic Regression",
    "section": "Assuming errors come from a logistic distribution",
    "text": "Assuming errors come from a logistic distribution\nThe errors are often assumed to be generated from a logistic distribution, with location parameter \\(\\mu=0\\) and scale parameter \\(s=1\\):\n\\[\n\\epsilon \\sim \\text{Logistic}(0,1)\n\\]\nWhy a logistic? Well because is highly similar to a normal distribution, but with fatter tails, so its seen as an approximation that is more robust. Furthermore, it has easier algebra to unpick - which we will show shortly.\n\n\nCode\nlogistic_func &lt;- function(x) {\n  return( exp(-x)*(1+exp(-x))^-2 )\n}\nnormal_func &lt;- function(x) {\n  return( (2*pi)^(-0.5) * exp(-0.5 * x^2) )\n}\nx = seq(-10,10,0.1)\nplot(\n  x=x,\n  y=normal_func(x),\n  type='l',col=\"blue\",lty=1\n  )\nlines(\n  x=x,\n  y=logistic_func(x),\n  type='l',col=\"red\",lty=1\n)\nlegend(\n  -10,0.4,\n  legend=c(\n    \"Normal PDF\",\n    \"Logistic function\"\n    ),\n  col=c(\"blue\",\"red\"),\n  lty=c(1,1)\n  )\n\n\n\n\n\n\n\n\n\nFor now, you might recall the pdf of the logistic distribution is:\n\\[\n\\displaylines{\n\\begin{align}\nf(x,\\mu,s) & =\n\\frac{e^{-(x-\\mu)/s)}}\n{s(1+e^{-(x-\\mu)/s)})^2}\n\\\\ \\\\\n\\therefore f(x,0,1) & =\n\\frac{e^{-x}}\n{(1+e^{-x})^2}\n\\end{align}\n}\n\\]\nThen we can obtain the CDF by taking the integral of the pdf:\n\\[\n\\displaylines{\n\\begin{align}\n\\int {f(x,0,1) \\,dx} & =\n\\int{\n  \\frac{e^{-x}}\n  {(1+e^{-x})^2}\n\\,dx}\n\\\\ \\\\\n\\text{let } u = 1+e^{-x} & \\therefore \\frac{du}{dx} = -e^{-x}\n\\\\\n& \\therefore dx = -\\frac{du}{e^{-x}}\n\\\\ \\\\ \\therefore\n\\int {f(x,0,1) \\,dx} & =\n\\int {\\frac{e^{-x}}{u^2} \\times -\\frac{du}{e^{-x}}}\n\\\\ &\n= \\int {-u^{-2}\\,du}\n\\\\\n& = u^{-1} + c\n\\\\\n& = (1+e^{-x})^{-1} + c\n\\end{align}\n}\n\\]\nAnd hence, we derive the “logisitic function”:\n\\[\n\\displaylines{\n\\begin{align}\np(y_i|X_i)\n& = p(\\varepsilon_i &lt; X_i\\beta) \\\\\n& = (1+e^{-X_i\\beta})^{-1} \\\\\n& = \\frac{1}{1+e^{-X_i\\beta}} \\\\ \\\\\n& = \\text{logistic}(X_i\\beta)\n\\end{align}\n}\n\\]\nSo we now have a mapping of \\(X\\) to \\(y\\), given by \\(\\beta\\) and the activation function: the “logistic function”.\nWe can see that this activation function “squashes” all outputs \\(X\\beta \\in [-\\infty,\\infty]\\) between 0 and 1:\n\n\nCode\nlogistic_dist &lt;- function(x) {\n  return( (1+exp(-x))^-1 )\n}\nx = seq(-10,10,0.1)\nplot(\n  x=x,\n  y=logistic_dist(x),\n  type='l',col=\"blue\",,lty=1\n  )\nlines(\n  x=seq(-3,3,0.1),\n  y=(1/4)*seq(-3,3,0.1)+0.5,\n  type='l',col=\"red\",lty=2\n)\nlegend(\n  -10,1,\n  legend=c(\n    \"logistic activation\",\n    \"linear activation\"\n    ),\n  col=c(\"blue\",\"red\"),\n  lty=c(1,2)\n  )\n\n\n\n\n\n\n\n\n\n\n\nFor probabilities of between 0.3 to 0.7, we see that the logistic activation function maps very closely to that of a simply linear one. It is only at the more extreme probabilities that they diverge."
  },
  {
    "objectID": "posts/glm_logit.html#linearity-in-terms-of-log-odds",
    "href": "posts/glm_logit.html#linearity-in-terms-of-log-odds",
    "title": "Logistic Regression",
    "section": "Linearity in terms of log-odds!",
    "text": "Linearity in terms of log-odds!\nTo further intuition, it can also be useful to rearrange the regression in terms of \\(X_i\\beta\\).\nBy doing this, we find that we are fitting a model where the “log-odds” are linearly related to its predictors:\n\n\nLog-odds means taking the logarithm of the probability of success divided by the probability of failure\n\\[\n\\displaylines{\n\\begin{align}\np(y_i|X_i) = p_i & = \\frac{1}{1+e^{-X_i\\beta}} \\\\\n\\therefore 1 + e^{-X_i\\beta} & = \\frac{1}{p_i} \\\\\n\\therefore e^{-X_i\\beta} & = \\frac{1}{p_i} - \\frac{p_i}{p_i} = \\frac{1-p_i}{p_i} \\\\\n\\therefore e^{X_i\\beta} & = \\frac{p_i}{1-p_i} \\\\\n\\therefore \\ln{\n  \\left\\{ \\frac{p_i}{1-p_i} \\right\\}\n  } & = X_i\\beta\n\\end{align}\n}\n\\]\nThis is a “link function” - the link between the outcome, \\(y\\), and the linear predictors \\(X\\) via \\(\\beta\\). This specific link function is called the “logit function”.\nAnd so it is now clear the inverse logit is the logistic function:\n\\[\n\\displaylines{\n\\begin{align}\n\\text{logit}(x) & = \\ln{\n  \\left\\{ \\frac{x}{1-x} \\right\\}\n  } \\\\\n\\text{logistic}(x)\n& = \\frac{1}{1+e^{-x}} \\\\\n& = \\text{logit}^{-1}(x) \\\\\n\\end{align}\n}  \n\\]"
  },
  {
    "objectID": "posts/glm_logit.html#optimising-the-coefficients",
    "href": "posts/glm_logit.html#optimising-the-coefficients",
    "title": "Logistic Regression",
    "section": "Optimising the coefficients",
    "text": "Optimising the coefficients\nWe minimise the cost function by finding the optimum coefficient values \\(\\beta^*\\) so that the partial differential is equal to zero.\n\\[\n\\displaylines{\n\\begin{align}\n& \\frac{\\partial}{\\partial \\beta_j}p(y|\\beta,X) \\\\ \\\\\n= &\n\\frac{\\partial}{\\partial \\beta_j} \\left(\n\\sum_{i=1}^{N}{ -y_i\\log{ \\left[ p_i \\right] } } +\n  \\sum_{i=1}^{N}{ -(1-y_i)\\log{ \\left[ 1-p_i\\right] } }\n\\right) \\\\ \\\\\n= &\n\\sum_{i=1}^{N}{ y_i\n  \\frac{\\partial p_i/\\partial \\beta_j}{p_i}\n  } +\n\\sum_{i=1}^{N}{ (1-y_i)\n  \\frac{(1-\\partial p_i)/\\partial \\beta_j}{1-p_i}\n  }\n\\end{align}\n}\n\\]\nThis is as far as we can get without modelling \\(P(\\varepsilon_i &lt; X \\beta)\\). So let’s look at substituting \\(p_i\\), \\(1-p_i\\), \\(\\partial p_i/\\partial \\beta\\) and \\(\\partial (1-p_i)/\\partial \\beta\\) into our first moment condition to derive the optimal coefficients:\n\\[\n\\displaylines{\n\\begin{align}\np_i & = P(\\varepsilon_i &lt; X \\beta) = (1+e^{-X\\beta})^{-1}\n\\\\ & = \\frac{1}{1+e^{-X\\beta}}\n\\\\\n\\therefore 1-p_i & = 1 - (1+e^{-X\\beta})^{-1} = \\frac{1+e^{-X\\beta}}{1+e^{-X\\beta}} - \\frac{1}{1+e^{-X\\beta}}\n\\\\ & = \\frac{e^{-X\\beta}}{1+e^{-X\\beta}}  \\\\\n\\therefore \\frac{\\partial p_i}{\\partial \\beta_j}\n& = -1(1+e^{-X\\beta})^{-2} \\times -x_je^{-X\\beta}\n\\\\ & = \\frac{-x_j \\times e^{-X\\beta}}{(1+e^{-X\\beta})^2}\n\\\\\n\\therefore \\frac{\\partial (1-p_i)}{\\partial \\beta_j} & = -1(1+e^{-X\\beta})^{-2} \\times x_j \\times e^{-X\\beta}\n\\\\ & = \\frac{x_j \\times e^{-X\\beta}}{(1+e^{-X\\beta})^2}\n\\end{align}\n}\n\\]\n\\[\n\\displaylines{\n\\begin{align}\n\\therefore \\frac{\\partial}{\\partial \\beta_j}p(y|\\beta,X) & =\n\\sum_{i=1}^{N}{ y_i\n  \\frac{\\partial p_i/\\partial \\beta_j}{p_i}\n  } +\n\\sum_{i=1}^{N}{ (1-y_i)\n  \\frac{\\partial(1-\\partial p_i)/ \\beta_j}{1-p_i}\n  } \\\\\n& =\n\\sum_{i=1}^{N}{ y_i\n  \\frac{\\frac{-x_j \\times e^{-X\\beta}}{(1+e^{-X\\beta})^2}}{\\frac{1}{1+e^{-X\\beta}}}\n  } +\n\\sum_{i=1}^{N}{ (1-y_i)\n  \\frac{\\frac{x_j \\times e^{-X\\beta}}{(1+e^{-X\\beta})^2}}{\\frac{e^{-X\\beta}}{1+e^{-X\\beta}}}\n  }\n\\\\\n& =\n\\sum_{i=1}^{N}{ -x_jy_i\n  \\frac{ e^{-X\\beta}}{1+e^{-X\\beta}}\n  } +\n\\sum_{i=1}^{N}{ x_j(1-y_i)\n  \\frac{1}{1+e^{-X\\beta}}\n  }    \n\\\\\n& =\n-x_j\\sum_{i=1}^{N}{\n  y_i \\times  \\left(1-p_i\\right) +\n  (1-y_i) \\times p_i\n  }    \n\\\\\n\\end{align}\n}\n\\]\nThus there is no closed form solution like OLS. However, given the cost function is convex, using an optimization like Newton Raphson will find the optimum coefficients.\nFin."
  },
  {
    "objectID": "posts/fundamentals_gaussian_dist.html",
    "href": "posts/fundamentals_gaussian_dist.html",
    "title": "Deriving the normal distribution",
    "section": "",
    "text": "What are we exploring?\n\n\n\nWhere does the gaussian distribution come form? We derive if from first principles i.e. proving the Herschel-Maxwell theorem."
  },
  {
    "objectID": "posts/fundamentals_gaussian_dist.html#setting-the-scene",
    "href": "posts/fundamentals_gaussian_dist.html#setting-the-scene",
    "title": "Deriving the normal distribution",
    "section": "Setting the scene",
    "text": "Setting the scene\nImagine we are throwing darts, aiming at a circular target. The closer you get to the centre, the more points you get.\nImagine you miss the centre by 5cm. It doesn’t matter whether you were 5cm too high or low, 5cm too far left or right, or a combination of the two (e.g. 3cm too far left, 4cm too far right, which by pythagoras is also \\(\\sqrt{3^2+4^2}=5\\text{cm}\\) away from the centre) - hitting those regions are all equally likely.\n\n\nCode\nimport numpy as np\nimport plotly.graph_objects as go\n\nfig = go.Figure()\n\n# Define the points\npoints = [(5,0), (0,5), (0,-5), (3,4), (-4,-3)]\n\n# Add the points to the plot\nfig.add_trace(\n  go.Scatter(\n    x=[p[0] for p in points], \n    y=[p[1] for p in points], \n    mode='markers', \n    marker = dict(symbol='cross',color='black', size=20)\n  )\n)\n\n# Add lines to the plot\nfor point in points:\n    fig.add_trace(\n      go.Scatter(\n        x=[0, point[0]/2], \n        y=[0, point[1]/2], \n        mode='lines', \n        line=dict(color='#636363'),\n      )\n    )\n\n# Add arrows to the plot\nfor point in points:\n    fig.add_annotation(\n        x=point[0]  ,\n        y=point[1],\n        ax=0,\n        ay=0,\n        xref='x',\n        yref='y',\n        axref='x',\n        ayref='y',\n        showarrow=True,\n        arrowhead=3,\n        arrowsize=1,\n        arrowwidth=2,\n        arrowcolor='#636363'\n    )\n\n# Define the circle\nr = np.sqrt(5**2)\ntheta = np.linspace(0, 2*np.pi, 100)\nx = r * np.cos(theta)\ny = r * np.sin(theta)\n\n# Add the circle to the plot\nfig.add_trace(\n  go.Scatter(\n    x=x,\n    y=y, \n    mode='lines', \n    line=dict(dash='dot',color='#636363')\n  )\n)\n\n# Update axes\nlayout_options = dict(\n  range=[-6, 6],\n  dtick=1, \n  gridwidth=1,\n)\nfig.update_xaxes(**layout_options)\nfig.update_yaxes(**layout_options)\nfig.update_layout(\n  showlegend=False,\n  title={\n    'text': 'All shots 5cm from the centre are equally likely to happen',\n    'x': 0.5,\n    'xanchor': 'center',\n  },\n  scene = dict(aspectratio=dict(x=1, y=1),aspectmode=\"manual\"),\n  autosize=False,\n  width=600,\n  height=600,\n)\nfig.show()\n\n\n                                                \n\n\n\nNow having said that, the distribution of horizontal distances from the centre are completely independent of the distribution of vertical distances. We are just trying to aim for the middle: getting less or more accurate horizontally doesn’t affect how accurate we are vertically.\nLet’s denote the horizontal and vertical distances from the centre \\(X\\) and \\(Y\\). Now here is the magic - after taking many shots, we would find that the distribution of vertical distances from the centre of the target would follow a normal distribution!\nWe would also find the horizontal distances form the centre of the target would follow a normal distribution too!\n\n\nCode\nfrom math import pi, sqrt, exp\n\nsigma=3\n\ndef _normal_pdf(x,mu=0.0,sigma=1.0):\n    result = ( \n      1/(sigma * sqrt(2*pi))\n    ) * ( exp(\n      -0.5 * ((x-mu)/sigma)**2\n    ) )\n    return result\n\ndef normal_pdf(x,mu=0.0,sigma=1.0):\n    result = [_normal_pdf(i,mu,sigma) for i in x]\n    return np.array(result)\n\n# Define the range of values for x and y\nX = np.linspace(-10,10,100)\nzeros = np.zeros(len(X))\nones = np.ones(len(X))\nP_X = normal_pdf(X,sigma=sigma)\nX2 = np.linspace(-8,8,100)\nY2 = np.linspace(-6,6,100)\n\nP_X5 = _normal_pdf(5,sigma=sigma)\nP_X05 = np.linspace(0,P_X5,100)\n\nfig = go.Figure()\n\nline_style = dict(color='black',dash='dot')\n\nfig.add_trace(\n  go.Scatter3d(x=X,y=zeros,z=P_X,mode='lines',line=line_style)\n)\n\nfig.add_trace(\n  go.Scatter3d(x=zeros,y=X,z=P_X,mode='lines',line=line_style)\n)\n\nfig.add_trace(\n  go.Scatter3d(x=X2,y=Y2,z=P_X,mode='lines',line=line_style)\n)\n\nfig.add_trace(\n  go.Scatter3d(x=Y2,y=X2,z=P_X,mode='lines',line=line_style)\n)\n\nmarker_style = dict(symbol='cross',color='black')\n\nfig.add_trace(\n  go.Scatter3d(x=ones*5,y=zeros,z=zeros, marker=marker_style)\n)\n\nfig.add_trace(\n  go.Scatter3d(x=zeros,y=ones*5,z=zeros, marker=marker_style)\n)\n\nfig.add_trace(\n  go.Scatter3d(x=zeros,y=ones*-5,z=zeros, marker=marker_style)\n)\n\nfig.add_trace(\n  go.Scatter3d(x=ones*3,y=ones*4,z=zeros, marker=marker_style)\n)\n\nfig.add_trace(\n  go.Scatter3d(x=ones*-4,y=ones*-3,z=zeros, marker=marker_style)\n)\n\nfig.add_trace(\n  go.Scatter3d(x=ones*5,y=zeros,z=P_X05, mode='lines',line=line_style)\n)\n\nfig.add_trace(\n  go.Scatter3d(x=zeros,y=ones*5,z=P_X05, mode='lines',line=line_style)\n)\n\nfig.add_trace(\n  go.Scatter3d(x=zeros,y=ones*-5,z=P_X05, mode='lines',line=line_style)\n)\n\nfig.add_trace(\n  go.Scatter3d(x=ones*3,y=ones*4,z=P_X05, mode='lines',line=line_style)\n)\n\nfig.add_trace(\n  go.Scatter3d(x=ones*-4,y=ones*-3,z=P_X05, mode='lines',line=line_style)\n)\n\nfig.add_trace(\n  go.Scatter3d(x=x,y=y,z=ones*P_X5, mode='lines',line=line_style)\n)\n\ncamera = dict(\n    up=dict(x=0, y=0, z=1),\n    center=dict(x=0, y=0, z=0),\n    eye=dict(x=0.5, y=-1.2, z=1)\n)\n\nfig.update_layout(\n  showlegend=False, scene_camera=camera, \n  scene=dict(\n    xaxis=dict(nticks=20),\n    yaxis=dict(nticks=20),\n    aspectratio=dict(x=1, y=1,z=1),\n    aspectmode=\"manual\",\n    ),\n)\n\nfig.show()\n\n\n                                                \n\n\n\\[\nP_X(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\frac{x^2}{2\\sigma^2}}\n\\text{;  }\nP_Y(y) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\frac{y^2}{2\\sigma^2}}\n\\]\nSeems like very, very few assumptions needed to know that the horizontal and vertical distances will follow a normal distribution? That’s true! But it works. This is the Herschel-Maxwell theorem.\n\n\n\n\n\n\nHerschel-Maxwell theorem\n\n\n\nGiven two random variables, \\(X\\) and \\(Y\\), with a joint distribution function of \\(P_{X,Y}(dX,dY)\\):\n\n\\(P_{X,Y}\\) is invariant to real rotations\n\\(X, Y\\) are independent\n\nThen both \\(X\\) and \\(Y\\) are normally distributed.\n\n\nAnd this is the theorem that we will prove in this post."
  },
  {
    "objectID": "posts/fundamentals_gaussian_dist.html#first-step---combining-the-two-assumptions",
    "href": "posts/fundamentals_gaussian_dist.html#first-step---combining-the-two-assumptions",
    "title": "Deriving the normal distribution",
    "section": "First step - combining the two assumptions",
    "text": "First step - combining the two assumptions\nThe horizontal and vertical distances from the centre are two random variables, \\(X\\) and \\(Y\\), that are independently and identically generated from a distribution with identical population parameters (mean of zero and finite, constant variance).\n\n\nWe do not expect a bias in aiming too high or too low. Hence the expected values of \\(X\\) and \\(Y\\) (their means) are zero: \\(\\mathbb{E}[X]=0\\text{; }\\mathbb{E}[Y]=0\\)\nAlso, we want to hit closer to the centre. Hence the probability density must diminish with large values of \\(|x|\\), as it is more likely we get closer - so the variance of these distances is finite. Also, by making the variance constant, we assume that we do not get better over time. \\(\\mathbb{V}[X]=c_X\\text{; }\\mathbb{V}[Y]=c_Y\\)\nLet’s now derive the normal distribution from first principles. We rewrite the Herschel-Maxwell theorem above as our starting point:\n\nThe probability of hitting a point on the target is radially symmetric\nThe horizontal and vertical distances from the centre are independent (unrelated)\n\nWhat does this really mean? Well (1) means there is “radial symmetry”. In other words, the likelihood of a shot ending in some region is only based on how close it is to the centre: it doesn’t matter how far the distance is along the horizontal or vertical axis (\\(X\\) or \\(Y\\)), just the combined distance (i.e. the radius from the centre).\n\\[\n\\displaylines{\n\\begin{align}\n\\text{Let } & x^2 + y^2 = x'^2 + y'^2 \\\\\n\\text{Where } & x \\neq x', y \\neq y'\\\\\n\\text{Then: } \\\\\n& P_{X,Y}(X = x, Y = y) = P_{X,Y}(X = x', Y = y') \\\\\n\\end{align}\n}\n\\]\nConcretely - the joint probability between \\(X\\) and \\(Y\\) for two shots landing in any region equidistant from the origin is the same. It depends only on the distance from the centre, not the direction from the centre. So the probability density function can be simplified to \\(\\omega\\), as a function of the radius. And since the radius is the hypotenuse of a right-angled triangle, we know from pythagoras that \\(r = \\sqrt{x^2+y^2}\\):\n\\[\nP_{X,Y}(X = x,Y = y) = \\omega (\\sqrt{x^2+y^2})\n\\]\nThe second bullet point (2) - is that there is no correlation between whether you shoot too high/low and whether you shoot too far left/right. This assumption of independence is a crucial, and commonly used assumption in statistics.\n\\[\nP_{X,Y}(X=x,Y=y) = P_X(X=x) \\times P_Y(Y=y)\n\\]\nAnd hence we can combine the two:\n\\[\n\\omega (\\sqrt{x^2+y^2}) = P_X(x) \\times P_Y(y)\n\\]"
  },
  {
    "objectID": "posts/fundamentals_gaussian_dist.html#simplifying-to-a-single-pdf",
    "href": "posts/fundamentals_gaussian_dist.html#simplifying-to-a-single-pdf",
    "title": "Deriving the normal distribution",
    "section": "Simplifying to a single pdf",
    "text": "Simplifying to a single pdf\nNow you might be thinking at this point - why start with two distributions? It would be easier to derive the normal distribution from just one variable, \\(X\\). However, initially providing a second independent variable \\(Y\\) (which we now remove) provides a shortcut to derive the normal distribution for \\(X\\), so it is a useful step.\nSince \\(Y\\) is i.i.d, then the probability density \\(P_Y(Y=0)\\) is constant regardless of \\(X\\) (i.e. \\(P_Y(Y=0) = \\gamma\\) where \\(\\gamma\\) is some constant). We can similarly denote the constant \\(P_X(0) = \\lambda\\).\nThus, for any distance \\(z\\):\n\\[\n\\displaylines{\n\\begin{align}\n\\omega(\\sqrt{x^2+y^2})\n& = P_X(x) \\times P_Y(y)\n\\\\\n\\therefore \\omega(\\sqrt{x^2+0^2})\n& = P_X(x) \\times P_Y(0)\n\\\\\n\\equiv \\omega(x) & = P_X(x) \\times \\gamma\n\\\\\n\\therefore \\omega(\\sqrt{0^2+y^2})\n& = P_X(0) \\times P_Y(y)\n\\\\\n\\equiv \\omega(y) & = \\lambda \\times P_Y(y)\n\\\\\n\\\\\n\\Rightarrow \\omega(z) & = \\gamma P_X(z) \\\\\n\\Rightarrow P_Y(z)\n& = \\frac{\\omega(z)}{\\lambda}\n= \\frac{\\gamma}{\\lambda}P_X(z)\n\\end{align}\n}\n\\]\nIn other words - we can represent the joint pdf of \\(X\\) and \\(Y\\) solely in terms of \\(P_X(z)\\): \\[\n\\displaylines{\n\\begin{align}\n\\omega (\\sqrt{x^2+y^2}) & = P_X(x) \\times P_Y(y) \\\\\n\\equiv\n\\gamma P_X(\\sqrt{x^2+y^2}) & = P_X(x) \\times\n\\frac{\\gamma}{\\lambda}P_X(y)\n\\end{align}\n}\n\\]\nAnd we can simplify this further, using a helper function \\(h(z)\\): \\[\n\\displaylines{\n\\begin{align}\n\\gamma P_X(\\sqrt{x^2+y^2}) & = P_X(x) \\times\n\\frac{\\gamma}{\\lambda}P_X(y) \\\\\n\\Rightarrow P_X(\\sqrt{x^2+y^2}) & = P_X(x) \\times\n\\frac{P_X(y)}{\\lambda} & \\div \\gamma \\\\\n\\Rightarrow \\frac{P_X(\\sqrt{x^2+y^2})}{\\lambda} & = \\frac{P_X(x)}{\\lambda} \\times\n\\frac{P_X(y)}{\\lambda} & \\div \\lambda \\\\\n\\equiv h(x^2+y^2) & = h(x^2) \\times h(y^2) & \\text{where } h(z) = \\frac{P_X(\\sqrt{z})}{\\lambda} \\\\\n\\end{align}\n}\n\\]\nWe now have everything in terms of one function, \\(h(z)\\). We now look to find a form for \\(h(z)\\) that satisfies this criteria."
  },
  {
    "objectID": "posts/fundamentals_gaussian_dist.html#satisfying-the-equality-using-exponentials",
    "href": "posts/fundamentals_gaussian_dist.html#satisfying-the-equality-using-exponentials",
    "title": "Deriving the normal distribution",
    "section": "Satisfying the equality using exponentials",
    "text": "Satisfying the equality using exponentials\nAn exponential function is a good candidate to ensure that multiplying each pdf together is the same as raising it to the sum of their powers:\n\n\nNote that for any base \\(b\\), we can reformulate it in terms of the natural number \\(e\\)1, i.e. setting \\(h(z^2) = e^{kz^2}\\)\n\\[\n\\displaylines{\n\\begin{align}\nb^{(x^2+y^2)} & = b^{x^2} \\times b^{y^2}\n\\\\ \\equiv\ne^{kx^2+ky^2} & = e^{kx^2} \\times e^{ky^2}\n& \\text{where } k = \\ln{[b]}\n\\\\\n\\therefore h(z) & = e^{kx^2}\n\\end{align}\n}\n\\]\nAnd thus: \\[\nP_X(x) = \\lambda h(x^2) = \\lambda e^{kx^2}\n\\]\nWe now have a pdf that is an exponential form - we are getting closer!"
  },
  {
    "objectID": "posts/fundamentals_gaussian_dist.html#ensuring-finite-variance",
    "href": "posts/fundamentals_gaussian_dist.html#ensuring-finite-variance",
    "title": "Deriving the normal distribution",
    "section": "Ensuring finite variance",
    "text": "Ensuring finite variance\nSo far we have that: \\[\nP_X(x) = \\lambda e^{kx^2}\n\\]\nWe can determine that \\(k\\) must be a negative value. The reasoning behind this:\n\nThe variance is finite, so the probability density must diminish with large values of \\(|x|\\). Thus, \\(kx^2 &lt; 0\\)\n\\(x^2\\) is positive for all real \\(x\\), so \\(k\\) has to be negative to ensure \\(kx^2 &lt; 0\\)\n\nSo going forward, let’s substitute \\(k=-\\left(m^2\\right)\\), to ensure it is a negative number for any real value of \\(m\\).\n\\[\nP_X(x) = \\lambda e^{kx^2} = \\lambda e^{-m^2x^2}\n\\]\nFurthermore - we know that the integral between \\(-\\infty\\) and \\(+\\infty\\) must equal one, since this is the full range of values that \\(x\\) could take, and the probabilities must sum to one.\n\\[\n\\lambda \\int_{-\\infty}^{\\infty}{e^{-m^2x^2} \\, dx} = 1\n\\]\nAnd we can use both of these deductions to find a value for \\(m\\):\n\\[\n\\displaylines{\n\\begin{align}\n\\text{Let } u & = mx \\\\\n\\Rightarrow dx & = \\frac{du}{m} \\text{ } & \\because \\frac{du}{dx} = m\n\\\\ \\\\\n\\therefore\n\\lambda \\int_{-\\infty}^{+\\infty}{e^{-m^2x^2} \\,dx}\n& = \\lambda \\int_{-\\infty}^{+\\infty}{e^{-u^2} \\,\\frac{du}{m}} \\\\\n& = \\frac{\\lambda}{m} \\int_{-\\infty}^{+\\infty}{e^{-u^2} \\,du}\n\\\\\n& = \\frac{\\lambda}{m} \\left[ -\\frac{1}{2u} \\times e^{-u^2} \\right]_{-\\infty}^{+\\infty}\n\\\\\n& = \\frac{\\lambda}{m} \\sqrt{\\pi}\n& \\because \\int_{-\\infty}^{\\infty}{e^{-z^2}}{dz} = \\sqrt{\\pi}\n\\\\\n& = 1\n\\\\ \\\\\n\\therefore m & = \\lambda \\sqrt{\\pi}\n\\end{align}\n}\n\\]\n\n\nThe integral \\(\\int_{-\\infty}^{\\infty}{e^{-z^2}}{dz} = \\sqrt{\\pi}\\) is a special case: of the gaussian integral. See this page if you are interested in its derivation.\nAnd thus:\n\\[\n\\displaylines{\n\\begin{align}\nP_X(x) = \\lambda e^{-m^2x^2} = \\lambda e^{-\\pi \\lambda^2x^2}\n\\end{align}\n}\n\\]"
  },
  {
    "objectID": "posts/fundamentals_gaussian_dist.html#reformulating-to-include-a-standard-deviation-term",
    "href": "posts/fundamentals_gaussian_dist.html#reformulating-to-include-a-standard-deviation-term",
    "title": "Deriving the normal distribution",
    "section": "Reformulating to include a standard deviation term",
    "text": "Reformulating to include a standard deviation term\nSo we are closer to deriving our standard normal probability density function, but we have a \\(\\lambda\\) constant instead of the constant variance \\(\\sigma^2\\). It might not be surprising then that it turns out this variance is a function of \\(\\lambda\\).\nTo help see this, consider the point \\(x=0\\), where we find that the height of the pdf is \\(\\lambda\\) (i.e. \\(P_X(0) = \\lambda e^{-\\pi \\lambda^20^2} = \\lambda\\)). So the higher the value of \\(\\lambda\\), the larger the probability of getting closer to the centre (since the cdf must equal one), and hence the smaller the variance. In other words - the variance has to be a function of \\(\\lambda\\).\nTo understand the relationship between \\(\\lambda\\) and \\(\\sigma^2\\), let’s plug \\(P_X(x) = \\lambda e^{-\\pi \\lambda^2x^2}\\) into the standard calculation for the variance of any pdf:\n\\[\n\\displaylines{\n\\begin{align}\n\\sigma^2\n& = \\int_{-\\infty}^{\\infty}{(x-\\mu)^2 \\times P_X(x) \\, dx} \\\\\n& = \\int_{-\\infty}^{\\infty}{(x-\\mu)^2 \\times \\lambda e^{-\\pi \\lambda^2x^2} \\, dx} \\\\\n& = \\lambda \\int_{-\\infty}^{\\infty}{x^2 \\times e^{-\\pi \\lambda^2x^2} \\, dx} & \\because \\mu = 0 \\\\\n\\end{align}\n}\n\\]\n\nThen we can integrate by parts, i.e. using the form \\(\\int{u \\, dv} = uv - \\int{v \\, du}\\):\n\\[\n\\displaylines{\n\\begin{align}\n\\text{Let } u & = x\n\\text{ and } dv = x e^{-\\pi \\lambda^2 (x^2)} dx \\\\\n\\Rightarrow du & = dx  \n& \\because \\frac{du}{dx} = 1 \\\\\n\\Rightarrow v & = -\\frac{1}{2\\pi \\lambda^2} e^{-\\pi \\lambda^2 x^2}\n\\\\ \\\\\n\\Rightarrow\n\\sigma^2 & =\n\\lambda \\int_{-\\infty}^{+\\infty}{x \\times \\lambda e^{-\\pi \\lambda^2(x^2)} \\, dx}\n= \\lambda\n\\int_{-\\infty}^{+\\infty}{u \\, dv}\n\\\\ & = \\lambda \\left( uv -\\int_{-\\infty}^{+\\infty}{v \\, dv} \\right)\n\\\\\n& = \\lambda \\left( \\left[ x\\left(\n  \\frac{-1}{2 \\pi \\lambda^2} e^{-\\pi \\lambda^2 x^2}\n  \\right) \\right]_{-\\infty}^{+\\infty} -\n  \\int_{-\\infty}^{+\\infty}{\n    \\frac{-1}{2 \\pi \\lambda^2} e^{-\\pi \\lambda^2 x^2}\n    \\, dx}\n  \\right)\n\\end{align}\n}\n\\]\nNow as both \\(x \\rightarrow \\infty\\) and \\(x \\rightarrow -\\infty\\), \\(e^{-\\pi \\lambda^2 x^2}\\rightarrow 0\\), so the first term drops out entirely.\n\\[\n\\displaylines{\n\\begin{align}\n\\sigma^2\n& = \\lambda \\left( \\cancel{ \\left[ x\\left(\n  \\frac{-1}{2 \\pi \\lambda^2} e^{-\\pi \\lambda^2 x^2}\n  \\right) \\right]_{-\\infty}^{+\\infty} } -\n  \\int_{-\\infty}^{\\infty}{\n    \\frac{-1}{2 \\pi \\lambda^2} e^{-\\pi \\lambda^2 x^2}\n    \\, dx}\n  \\right)\n\\\\ & = \\lambda\n  \\int_{-\\infty}^{\\infty}{\n    \\frac{1}{2 \\pi \\lambda^2} e^{-\\pi \\lambda^2 x^2}\n    \\, dx}\n\\\\ & = \\frac{1}{2 \\pi \\lambda^2}\n  \\int_{-\\infty}^{\\infty}{\n    \\lambda e^{-\\pi \\lambda^2 x^2}\n    \\, dx}\n\\\\ & = \\frac{1}{2 \\pi \\lambda^2}\n  \\int_{-\\infty}^{\\infty}{P_X(x) \\, dx}\n\\\\ & = \\frac{1}{2 \\pi \\lambda^2}\n& \\because \\text{pdfs must sum to 1}\n\\\\\n\\\\\n\\therefore \\lambda & = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\n\\end{align}\n}\n\\]\nAnd now we can plug \\(\\lambda\\) back into \\(P_X(x)\\):\n\\[\n\\displaylines{\n\\begin{align}\nP_X(x) = \\lambda e^{-\\pi \\lambda^2x^2} = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{\\frac{-x^2}{2\\sigma^2}}\n\\end{align}\n}\n\\]\nWhich gives us the normal distribution (where \\(\\mu=0\\)).\nFin."
  },
  {
    "objectID": "posts/fundamentals_gaussian_dist.html#footnotes",
    "href": "posts/fundamentals_gaussian_dist.html#footnotes",
    "title": "Deriving the normal distribution",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFor any base \\(b\\), we can reformulate it in terms of the natural number \\(e\\) \\[\n  \\displaylines{\n  \\begin{align}\n  b^{x^2} & = e^{kx^2} \\\\\n  \\therefore \\ln{b^{x^2}} & = kx^2 \\\\\n  \\therefore x^2 \\ln{b} & = kx^2 \\\\\n  \\therefore \\ln{b} & = k \\\\\n  \\end{align}\n  }\n  \\]↩︎"
  },
  {
    "objectID": "posts/fundamentals_gaussian_integral.html",
    "href": "posts/fundamentals_gaussian_integral.html",
    "title": "The gaussian integral",
    "section": "",
    "text": "What are we exploring?\n\n\n\nWhy does the area under the gaussian integral \\(\\int_{-\\infty}^{+\\infty}{e^{-x^2}\\,dx}=\\sqrt{\\pi}\\)?"
  },
  {
    "objectID": "posts/fundamentals_gaussian_integral.html#setting-the-scene",
    "href": "posts/fundamentals_gaussian_integral.html#setting-the-scene",
    "title": "The gaussian integral",
    "section": "Setting the scene",
    "text": "Setting the scene\nWe want to estimate the area under the curve of the gaussian distribution, between negative and positive infinity.\nThis would usually be generally difficult (or impossible). But since we know the gaussian distribution is radially symmetric, we do have a few extra tricks up our sleeve this time.\nFirst, lets define a gaussian distribution as \\(I\\):\n\\[\n\\displaylines{\n\\begin{align}\nI & = \\int_{-\\infty}^{+\\infty}{e^{-x^2}\\,dx}\n\\end{align}\n}\n\\]\nAnd now let’s look at the square of this distribution: Concretely:\n\\[\n\\displaylines{\n\\begin{align}\n\\text{Let } I & = \\int_{-\\infty}^{\\infty}{e^{-x^2}\\,dx} \\\\\n\\Rightarrow I^2\n& = \\left(\\int_{-\\infty}^{\\infty}{e^{-x^2}\\,dx}\\right)\\times\\left(\\int_{-\\infty}^{\\infty}{e^{-x^2}\\,dx}\\right) \\\\\n& \\equiv \\left(\\int_{-\\infty}^{\\infty}{e^{-x^2}\\,dx}\\right)\\times\\left(\\int_{-\\infty}^{\\infty}{e^{-y^2}\\,dy}\\right) \\\\\n& = \\int_{-\\infty}^{\\infty}{\\left(\\int_{-\\infty}^{\\infty}{e^{-y^2}\\,dy}\\right) e^{-x^2}\\,dx} \\\\\n& = \\int_{-\\infty}^{\\infty}{\\int_{-\\infty}^{\\infty}{e^{-x^2-y^2}\\,dy\\,dx}} \\\\\n& = \\int_{-\\infty}^{\\infty}{\\int_{-\\infty}^{\\infty}{e^{-(x^2+y^2)}\\,dy\\,dx}} \\\\\n\\end{align}\n}\n\\]\nNow \\(I^2\\) looks more complicated to evaluate, but stay with me - in reality its easier! The visual below shows what this looks like, and what we want to find is the volume under this surface:\n\n\nCode\nimport numpy as np\nimport plotly.graph_objects as go\nfrom math import pi, sqrt, exp\n\nbounds = [-2,2,100]\n\n# Generate x and y values\nx = np.linspace(*bounds)\ny = np.linspace(*bounds)\nzeros = np.zeros(len(x))\nxx, yy = np.meshgrid(x, y)\n# calculate gausian values\nz = np.exp(-(xx**2))\nzz = np.exp(-(xx**2+yy**2))\n\n# Create the surface plot\nfig = go.Figure()\nfig.add_trace(\n  go.Surface(\n    x=xx, y=yy, z=zz, \n    opacity=0.3,\n    colorscale=[(0, 'gray'), (1, 'gray')],\n    showscale=False,\n    contours = dict(\n        x= {\"show\": True, \"size\": 0.5},\n        y= {\"show\": True, \"size\": 0.5},\n        # z= {\"show\": True, \"start\": 0, \"end\": 1, \"size\": 0.025},\n    )\n  )\n)\n\nfig.update_layout(\n    scene = {\n      \"aspectratio\": {\"x\": 1, \"y\": 1, \"z\": 0.8},\n      \"xaxis\": {\"showgrid\": True, \"showbackground\": True},\n      \"yaxis\": {\"showgrid\": True, \"showbackground\": True},\n      \"yaxis\": {\"showgrid\": True, \"showbackground\": True},\n    },\n    showlegend=False,\n)\n\nfig.show()\n\n\n                                                \n\n\nHow might we go about estimating the volume? Well remember that any plot \\(x^2+y^2=r^2\\) is a circle with radius \\(r\\).\nSo given the gaussian distribution is radially symmetric, we can reformulate \\(f(x,y) = -(x^2+y^2)\\) as \\(f(r) = -(r^2)\\), i.e. the probability density only depends on the distance from the centre, not the direction.\nSo what we are looking for are shapes that have the same radial symmetry as the gaussian distribution. So a tube (a hollow cylinder) would be a perfect candidate.\nThe visual below shows how we might approximate the volume under the surface using a series of tubes:\n\n\nCode\ndef create_tube(\n        radius_outside = 0.3,\n        radius_inside = 0,\n        height = 0.9,\n        intervals = 200,\n    ):\n\n    # points around circumference of the cylinder, using number of intervals specified (e.g. 30)\n    theta_discr = np.linspace(0, 2*np.pi, intervals) # generate values from 0 to 2*pi\n    x, y = np.cos(theta_discr), np.sin(theta_discr) # generate x and y values using sin and cos\n\n    # generate top and bottom rings for the tube\n    xi, xo = x * radius_inside, x * radius_outside\n    yi, yo = y * radius_inside, y * radius_outside\n    tube_x = [i for l in zip(xi, xi, xo, xo) for i in l]\n    tube_y = [i for l in zip(yi, yi, yo, yo) for i in l]\n    tube_z = np.tile([0,height,height,0],int(len(tube_x)/4))\n\n    return dict(x=tube_x, y=tube_y, z=tube_z)\n\nfig.add_trace(\n  go.Scatter3d(**create_tube(0.3,0.0,0.9), mode='lines', line=dict(color=\"rgba(57,106,170,0.3)\",width=2))\n  )\n\nfig.add_trace(\n  go.Scatter3d(**create_tube(0.6,0.3,0.7), mode='lines', line=dict(color=\"rgba(218,124,48,0.3)\",width=2))\n  )  \n\nfig.add_trace(\n  go.Scatter3d(**create_tube(0.8,0.7,0.5), mode='lines', line=dict(color=\"rgba(62,150,81,0.5)\",width=2))\n  )\n\nfig.add_trace(\n  go.Scatter3d(**create_tube(1.1,0.8,0.3), mode='lines', line=dict(color=\"rgba(204,37,41,0.3)\",width=3))\n  )  \nfig.add_trace(\n  go.Scatter3d(**create_tube(1.5,1.1,0.1), mode='lines', line=dict(color=\"rgba(40,102,200,0.5)\",width=3))\n  )    \n\nfig.update_layout(showlegend=True)  \n\nfig.show()\n\n\n                                                \n\n\nNow we can make the estimation more accurate by increasing the number of tubes: thus decreasing the height and thickness of each tube. In fact, we would want an infinite number of tubes, with radii increasing from 0 to infinity, and thus each one having an infinitesimally small width.\nA formula for working out the volume of each tube would thus approximate to the following:\n\\[\n\\displaylines{\n\\begin{align}\n\\text{volume} & = \\text{circumference} \\times \\text{height} \\times \\text{thickness} \\\\\n& = 2\\pi r \\times e^{-r^2} \\times \\delta r\n\\end{align}\n}\n\\]\n\n\nWe don’t need to worry that the circumference on the outside if slightly larger than the circumference on the inside. Since we are eventually estimating an infinite number of tubes, the thickness of each tube will be infinitesimally small, and thus the difference in circumference will be infinitesimally small too.\nSince we want to perform an infinite sum of these tube volumes, we are really just looking for the integral across all tube radii from zero to infinity:\n\\[\n\\displaylines{\n\\begin{align}\n\\int_{0}^{\\infty}{2\\pi r \\times e^{-r^2}\\, dr}\n\\end{align}\n}\n\\]\nWe can solve this by substitution:\n\\[\n\\displaylines{\n\\begin{align}\n\\text{Let } u & = r^2 \\\\\n\\Rightarrow du & = 2rdr & \\because \\frac{du}{dr} = 2r &  \\\\ \\\\\n\\Rightarrow & \\int_{0}^{\\infty}{2\\pi r \\times e^{-r^2}\\, dr} \\\\\n= & \\int_{0}^{\\infty}{\\pi e^{-u}\\, du} \\\\\n= & \\pi \\bigg[-e^{-u} \\bigg]_{0}^{\\infty} \\\\\n= & \\pi \\bigg[-(0-1)\\bigg] \\\\\n= & \\pi\n\\end{align}\n}\n\\]\nThus:\n\\[\n\\displaylines{\n\\begin{align}\nI^2 & = \\pi \\\\\n\\Rightarrow I & = \\sqrt{\\pi}\n\\end{align}\n}\n\\]\nFin."
  },
  {
    "objectID": "posts/fundamentals_gaussian_integral_copy.html",
    "href": "posts/fundamentals_gaussian_integral_copy.html",
    "title": "The gaussian integral",
    "section": "",
    "text": "What are we exploring?\n\n\n\nWhy does the area under the gaussian integral \\(\\int_{-\\infty}^{+\\infty}{e^{-x^2}\\,dx}=\\sqrt{\\pi}\\)?"
  },
  {
    "objectID": "posts/fundamentals_gaussian_integral_copy.html#radial-symmetry",
    "href": "posts/fundamentals_gaussian_integral_copy.html#radial-symmetry",
    "title": "The gaussian integral",
    "section": "Radial symmetry",
    "text": "Radial symmetry\nWe want to estimate the area under the curve of the gaussian distribution, between negative and positive infinity.\nFirst, lets define another gaussian distribution, called \\(I^2\\), and in terms of \\(r\\):\n\\[\n\\displaylines{\n\\begin{align}\nI^2 & = \\int_{-\\infty}^{+\\infty}{e^{-r^2}\\,dr}\n\\end{align}\n}\n\\]\nWhy \\(r\\)? Well because we are going to take advantage of the fact that the gaussian distribution is radially symmetric. This means that the probability density only depends on the distance from the centre, not the direction. So if we think of \\(r\\) as the radius of the circle, we can reformulate \\(f(r) = -r^2\\) as \\(f(x,y) = -(x^2+y^2)\\) (using pythagoras).\nWe can thus show that \\(I^2\\) is just the square of the integral we are after:\n\\[\n\\displaylines{\n\\begin{align}\nI^2 = & \\int_{-\\infty}^{+\\infty}{e^{-(r^2)}\\,dr} \\\\\n= & \\int_{-\\infty}^{\\infty}{\\int_{-\\infty}^{\\infty}{e^{-(x^2+y^2)}\\,dy\\,dx}} \\\\\n= & \\int_{-\\infty}^{\\infty}{\\int_{-\\infty}^{\\infty}{e^{-x^2} \\times e^{-y^2}\\,dy\\,dx}} \\\\\n= & \\int_{-\\infty}^{\\infty}{\\left(\\int_{-\\infty}^{\\infty}{e^{-y^2}\\,dy}\\right) e^{-x^2}\\,dx} \\\\\n= & \\left(\\int_{-\\infty}^{\\infty}{e^{-x^2}\\,dx}\\right)\\times\\left(\\int_{-\\infty}^{\\infty}{e^{-y^2}\\,dy}\\right) \\\\\n\\equiv & \\left(\\int_{-\\infty}^{\\infty}{e^{-x^2}\\,dx}\\right)\\times\\left(\\int_{-\\infty}^{\\infty}{e^{-x^2}\\,dx}\\right) \\\\\n= & I \\times I \\\\\n\\end{align}\n}\n\\]\nSo if we can solve \\(\\int_{-\\infty}^{\\infty}{\\int_{-\\infty}^{\\infty}{e^{-(x^2+y^2)}\\,dy\\,dx}}\\), then we just need to find its square root to get the answer to the original question."
  },
  {
    "objectID": "posts/fundamentals_gaussian_integral_copy.html#calculating-the-volume-under-the-2d-gaussian-distribution",
    "href": "posts/fundamentals_gaussian_integral_copy.html#calculating-the-volume-under-the-2d-gaussian-distribution",
    "title": "The gaussian integral",
    "section": "Calculating the volume under the 2d gaussian distribution",
    "text": "Calculating the volume under the 2d gaussian distribution\nSo let’s visualise this: \\(I^2\\) is a 2d gaussian distribution along the x and y axes:\n\n\nCode\nimport numpy as np\nimport plotly.graph_objects as go\nfrom math import pi, sqrt, exp\n\nbounds = [-2,2,100]\n\n# Generate x and y values\nx = np.linspace(*bounds)\ny = np.linspace(*bounds)\nzeros = np.zeros(len(x))\nxx, yy = np.meshgrid(x, y)\n# calculate gausian values\nz = np.exp(-(xx**2))\nzz = np.exp(-(xx**2+yy**2))\n\n# Create the surface plot\nfig = go.Figure()\nfig.add_trace(\n  go.Surface(\n    x=xx, y=yy, z=zz, \n    opacity=0.3,\n    colorscale=[(0, 'gray'), (1, 'gray')],\n    showscale=False,\n    contours = dict(\n        x= {\"show\": True, \"size\": 0.5},\n        y= {\"show\": True, \"size\": 0.5},\n        # z= {\"show\": True, \"start\": 0, \"end\": 1, \"size\": 0.025},\n    )\n  )\n)\n\nfig.update_layout(\n    scene = {\n      \"aspectratio\": {\"x\": 1, \"y\": 1, \"z\": 0.8},\n      \"xaxis\": {\"showgrid\": True, \"showbackground\": True},\n      \"yaxis\": {\"showgrid\": True, \"showbackground\": True},\n      \"yaxis\": {\"showgrid\": True, \"showbackground\": True},\n    },\n    scene_camera_eye=dict(x=1.25, y=1.25, z=0.8),\n    showlegend=False,\n)\n\nfig.show()\n\n\n                                                \n\n\n How might we go about estimating the volume? What we are looking for are shapes that have the same radial symmetry as the 2d gaussian distribution.\nA tube (a hollow cylinder) would be a perfect candidate for this. The visual below shows how we might approximate the volume under the surface using a series of tubes:\n\n\nCode\ndef create_tube(\n        radius_outside = 0.3,\n        radius_inside = 0,\n        height = 0.9,\n        intervals = 200,\n    ):\n\n    # points around circumference of the cylinder, using number of intervals specified (e.g. 30)\n    theta_discr = np.linspace(0, 2*np.pi, intervals) # generate values from 0 to 2*pi\n    x, y = np.cos(theta_discr), np.sin(theta_discr) # generate x and y values using sin and cos\n\n    # generate top and bottom rings for the tube\n    xi, xo = x * radius_inside, x * radius_outside\n    yi, yo = y * radius_inside, y * radius_outside\n    tube_x = [i for l in zip(xi, xi, xo, xo) for i in l]\n    tube_y = [i for l in zip(yi, yi, yo, yo) for i in l]\n    tube_z = np.tile([0,height,height,0],int(len(tube_x)/4))\n\n    return dict(x=tube_x, y=tube_y, z=tube_z)\n\nfig.add_trace(\n  go.Scatter3d(**create_tube(0.3,0.0,0.9), mode='lines', line=dict(color=\"rgba(57,106,170,0.3)\",width=2), name = \"Tube 1\"), \n  )\n\nfig.add_trace(\n  go.Scatter3d(**create_tube(0.6,0.3,0.7), mode='lines', line=dict(color=\"rgba(218,124,48,0.3)\",width=2), name = \"Tube 2\"),\n  )  \n\nfig.add_trace(\n  go.Scatter3d(**create_tube(0.8,0.7,0.5), mode='lines', line=dict(color=\"rgba(62,150,81,0.5)\",width=2), name = \"Tube 3\"),\n  )\n\nfig.add_trace(\n  go.Scatter3d(**create_tube(1.1,0.8,0.3), mode='lines', line=dict(color=\"rgba(204,37,41,0.3)\",width=3), name = \"Tube 4\"),\n  )  \nfig.add_trace(\n  go.Scatter3d(**create_tube(1.5,1.1,0.1), mode='lines', line=dict(color=\"rgba(40,102,200,0.5)\",width=3), name = \"Tube 5\"),\n  )    \n\nfig.add_trace(\n  go.Surface(\n    x=xx, y=yy, z=zz, \n    opacity=0.1,\n    colorscale=[(0, 'gray'), (1, 'gray')],\n    showscale=False,\n    contours = dict(\n        x= {\"show\": True, \"size\": 0.5},\n        y= {\"show\": True, \"size\": 0.5},\n        # z= {\"show\": True, \"start\": 0, \"end\": 1, \"size\": 0.025},\n    )\n  )\n)  \n\nfig.update_layout(\n    showlegend=True, \n    scene = dict(\n        aspectratio=dict(x=1, y=1,z=1),\n        aspectmode=\"manual\"\n        )\n)  \n\nfig.show()\n\n\n                                                \n\n\n Now we can make the estimation more accurate by increasing the number of tubes: thus decreasing the height and thickness of each tube. In fact, we would want an infinite number of tubes, with radii increasing from 0 to infinity, and thus each one having an infinitesimally small width.\nA formula for working out the volume of each tube would thus approximate to the following:\n\\[\n\\displaylines{\n\\begin{align}\n\\text{volume} & = \\text{circumference} \\times \\text{height} \\times \\text{thickness} \\\\\n& = 2\\pi r \\times e^{-r^2} \\times \\delta r\n\\end{align}\n}\n\\]\n\n\nWe don’t need to worry that the circumference on the outside being slightly larger than the circumference on the inside. Since we are eventually estimating an infinite number of tubes, the thickness of each tube will be infinitesimally small, and thus the difference in circumference will be infinitesimally small too.\nSince we want to perform an infinite sum of these tube volumes, we are really just looking for the integral across all tube radii from zero to infinity:\n\\[\n\\displaylines{\n\\begin{align}\n\\therefore I^2 = \\int_{-\\infty}^{\\infty}{\\int_{-\\infty}^{\\infty}{e^{-(x^2+y^2)}\\,dy\\,dx}}\n\\equiv \\int_{0}^{\\infty}{2\\pi r \\times e^{-r^2}\\, dr}\n\\end{align}\n}\n\\]"
  },
  {
    "objectID": "posts/fundamentals_gaussian_integral_copy.html#solving-the-integral",
    "href": "posts/fundamentals_gaussian_integral_copy.html#solving-the-integral",
    "title": "The gaussian integral",
    "section": "Solving the integral",
    "text": "Solving the integral\nNow we can solve this by substitution:\n\\[\n\\displaylines{\n\\begin{align}\n\\text{Let } u & = r^2 \\\\\n\\Rightarrow du & = 2rdr & \\because \\frac{du}{dr} = 2r &  \\\\ \\\\\n\\Rightarrow & \\int_{0}^{\\infty}{2\\pi r \\times e^{-r^2}\\, dr} \\\\\n= & \\int_{0}^{\\infty}{\\pi e^{-u}\\, du} \\\\\n= & \\pi \\bigg[-e^{-u} \\bigg]_{0}^{\\infty} \\\\\n= & \\pi \\bigg[-(0-1)\\bigg] \\\\\n= & \\pi\n\\end{align}\n}\n\\]\nThus:\n\\[\n\\displaylines{\n\\begin{align}\n& I^2 = \\pi \\\\\n\\Rightarrow \\text{ } & I = \\sqrt{\\pi}\n\\end{align}\n}\n\\]\nAnd so:\n\\[\n\\int_{-\\infty}^{+\\infty}{e^{-x^2}\\,dx}=\\sqrt{\\pi}\n\\]\nFin."
  },
  {
    "objectID": "posts/fundamentals_gaussian_integral.html#radial-symmetry",
    "href": "posts/fundamentals_gaussian_integral.html#radial-symmetry",
    "title": "The gaussian integral",
    "section": "Radial symmetry",
    "text": "Radial symmetry\nWe want to estimate the area under the curve of the gaussian distribution, between negative and positive infinity.\nFirst, lets define another gaussian distribution, called \\(I^2\\), and in terms of \\(r\\):\n\\[\n\\displaylines{\n\\begin{align}\nI^2 & = \\int_{-\\infty}^{+\\infty}{e^{-r^2}\\,dr}\n\\end{align}\n}\n\\]\nWhy \\(r\\)? Well because we are going to take advantage of the fact that the gaussian distribution is radially symmetric. This means that the probability density only depends on the distance from the centre, not the direction.\nSo really, \\(I^2\\) is a 2d gaussian distribution. And if we think of \\(r\\) as the radius of the circle, we can reformulate \\(f(r) = -r^2\\) as \\(f(x,y) = -(x^2+y^2)\\) (using pythagoras). The height of the pdf, \\(z\\), is then \\(e^{-(r^2)} = e^{-(x^2+y^2)}\\).\nWe can thus show that \\(I^2\\) is simply the square of the integral we are after:\n\\[\n\\displaylines{\n\\begin{align}\nI^2 = & \\int_{-\\infty}^{+\\infty}{e^{-(r^2)}\\,dr} \\\\\n= & \\int_{-\\infty}^{\\infty}{\\int_{-\\infty}^{\\infty}{e^{-(x^2+y^2)}\\,dy\\,dx}} \\\\\n= & \\int_{-\\infty}^{\\infty}{\\int_{-\\infty}^{\\infty}{e^{-x^2} \\times e^{-y^2}\\,dy\\,dx}} \\\\\n= & \\int_{-\\infty}^{\\infty}{\\left(\\int_{-\\infty}^{\\infty}{e^{-y^2}\\,dy}\\right) e^{-x^2}\\,dx} \\\\\n= & \\left(\\int_{-\\infty}^{\\infty}{e^{-x^2}\\,dx}\\right)\\times\\left(\\int_{-\\infty}^{\\infty}{e^{-y^2}\\,dy}\\right) \\\\\n\\equiv & \\left(\\int_{-\\infty}^{\\infty}{e^{-x^2}\\,dx}\\right)\\times\\left(\\int_{-\\infty}^{\\infty}{e^{-x^2}\\,dx}\\right) \\\\\n= & I \\times I \\\\\n\\end{align}\n}\n\\]\nSo if we can solve \\(\\int_{-\\infty}^{\\infty}{\\int_{-\\infty}^{\\infty}{e^{-(x^2+y^2)}\\,dy\\,dx}}\\), then we just need to find its square root to get the answer to the original question."
  },
  {
    "objectID": "posts/fundamentals_gaussian_integral.html#calculating-the-volume-under-the-2d-gaussian-distribution",
    "href": "posts/fundamentals_gaussian_integral.html#calculating-the-volume-under-the-2d-gaussian-distribution",
    "title": "The gaussian integral",
    "section": "Calculating the volume under the 2d gaussian distribution",
    "text": "Calculating the volume under the 2d gaussian distribution\nSo let’s visualise this: \\(I^2\\) is a 2d gaussian distribution along the x and y axes:\n\n\nCode\nimport numpy as np\nimport plotly.graph_objects as go\nfrom math import pi, sqrt, exp\n\nbounds = [-2,2,100]\n\n# Generate x and y values\nx = np.linspace(*bounds)\ny = np.linspace(*bounds)\nzeros = np.zeros(len(x))\nxx, yy = np.meshgrid(x, y)\n# calculate gausian values\nz = np.exp(-(xx**2))\nzz = np.exp(-(xx**2+yy**2))\n\n# Create the surface plot\nfig = go.Figure()\nfig.add_trace(\n  go.Surface(\n    x=xx, y=yy, z=zz, \n    opacity=0.3,\n    colorscale=[(0, 'gray'), (1, 'gray')],\n    showscale=False,\n    contours = dict(\n        x= {\"show\": True, \"size\": 0.5},\n        y= {\"show\": True, \"size\": 0.5},\n        # z= {\"show\": True, \"start\": 0, \"end\": 1, \"size\": 0.025},\n    )\n  )\n)\n\nfig.update_layout(\n    scene = {\n      \"aspectratio\": {\"x\": 1, \"y\": 1, \"z\": 0.8},\n      \"xaxis\": {\"showgrid\": True, \"showbackground\": True},\n      \"yaxis\": {\"showgrid\": True, \"showbackground\": True},\n      \"yaxis\": {\"showgrid\": True, \"showbackground\": True},\n    },\n    scene_camera_eye=dict(x=1.25, y=1.25, z=0.8),\n    showlegend=False,\n)\n\nfig.show()\n\n\n                                                \n\n\n How might we go about estimating the volume? What we are looking for are shapes that have the same radial symmetry as the 2d gaussian distribution.\nA tube (a hollow cylinder) would be a perfect candidate for this. The visual below shows how we might approximate the volume under the surface using a series of tubes:\n\n\nCode\ndef create_tube(\n        radius_outside = 0.3,\n        radius_inside = 0,\n        height = 0.9,\n        intervals = 200,\n    ):\n\n    # points around circumference of the cylinder, using number of intervals specified (e.g. 30)\n    theta_discr = np.linspace(0, 2*np.pi, intervals) # generate values from 0 to 2*pi\n    x, y = np.cos(theta_discr), np.sin(theta_discr) # generate x and y values using sin and cos\n\n    # generate top and bottom rings for the tube\n    xi, xo = x * radius_inside, x * radius_outside\n    yi, yo = y * radius_inside, y * radius_outside\n    tube_x = [i for l in zip(xi, xi, xo, xo) for i in l]\n    tube_y = [i for l in zip(yi, yi, yo, yo) for i in l]\n    tube_z = np.tile([0,height,height,0],int(len(tube_x)/4))\n\n    return dict(x=tube_x, y=tube_y, z=tube_z)\n\nfig.add_trace(\n  go.Scatter3d(**create_tube(0.3,0.0,0.9), mode='lines', line=dict(color=\"rgba(57,106,170,0.3)\",width=2), name = \"Tube 1\"), \n  )\n\nfig.add_trace(\n  go.Scatter3d(**create_tube(0.6,0.3,0.7), mode='lines', line=dict(color=\"rgba(218,124,48,0.3)\",width=2), name = \"Tube 2\"),\n  )  \n\nfig.add_trace(\n  go.Scatter3d(**create_tube(0.8,0.7,0.5), mode='lines', line=dict(color=\"rgba(62,150,81,0.5)\",width=2), name = \"Tube 3\"),\n  )\n\nfig.add_trace(\n  go.Scatter3d(**create_tube(1.1,0.8,0.3), mode='lines', line=dict(color=\"rgba(204,37,41,0.3)\",width=3), name = \"Tube 4\"),\n  )  \nfig.add_trace(\n  go.Scatter3d(**create_tube(1.5,1.1,0.1), mode='lines', line=dict(color=\"rgba(40,102,200,0.5)\",width=3), name = \"Tube 5\"),\n  )    \n\nfig.add_trace(\n  go.Surface(\n    x=xx, y=yy, z=zz, \n    opacity=0.1,\n    colorscale=[(0, 'gray'), (1, 'gray')],\n    showscale=False,\n    contours = dict(\n        x= {\"show\": True, \"size\": 0.5},\n        y= {\"show\": True, \"size\": 0.5},\n        # z= {\"show\": True, \"start\": 0, \"end\": 1, \"size\": 0.025},\n    )\n  )\n)  \n\nfig.update_layout(\n    showlegend=True, \n    scene = dict(\n        aspectratio=dict(x=1, y=1,z=1),\n        aspectmode=\"manual\"\n        )\n)  \n\nfig.show()\n\n\n                                                \n\n\n Now we can make the estimation more accurate by increasing the number of tubes: thus decreasing the height and thickness of each tube. In fact, we would want an infinite number of tubes, with radii increasing from 0 to infinity, and thus each one having an infinitesimally small width.\nA formula for working out the volume of each tube would thus approximate to the following:\n\\[\n\\displaylines{\n\\begin{align}\n\\text{volume} & = \\text{circumference} \\times \\text{height} \\times \\text{thickness} \\\\\n& = 2\\pi r \\times e^{-r^2} \\times \\delta r\n\\end{align}\n}\n\\]\n\n\nWe don’t need to worry that the circumference on the outside being slightly larger than the circumference on the inside. Since we are eventually estimating an infinite number of tubes, the thickness of each tube will be infinitesimally small, and thus the difference in circumference will be infinitesimally small too.\nSince we want to perform an infinite sum of these tube volumes, we are really just looking for the integral across all tube radii from zero to infinity:\n\\[\n\\displaylines{\n\\begin{align}\n\\therefore I^2 = \\int_{-\\infty}^{\\infty}{\\int_{-\\infty}^{\\infty}{e^{-(x^2+y^2)}\\,dy\\,dx}}\n\\equiv \\int_{0}^{\\infty}{2\\pi r \\times e^{-r^2}\\, dr}\n\\end{align}\n}\n\\]"
  },
  {
    "objectID": "posts/fundamentals_gaussian_integral.html#solving-the-integral",
    "href": "posts/fundamentals_gaussian_integral.html#solving-the-integral",
    "title": "The gaussian integral",
    "section": "Solving the integral",
    "text": "Solving the integral\nNow we can solve this by substitution:\n\\[\n\\displaylines{\n\\begin{align}\n\\text{Let } u & = r^2 \\\\\n\\Rightarrow du & = 2rdr & \\because \\frac{du}{dr} = 2r &  \\\\ \\\\\n\\Rightarrow & \\int_{0}^{\\infty}{2\\pi r \\times e^{-r^2}\\, dr} \\\\\n= & \\int_{0}^{\\infty}{\\pi e^{-u}\\, du} \\\\\n= & \\pi \\bigg[-e^{-u} \\bigg]_{0}^{\\infty} \\\\\n= & \\pi \\bigg[-(0-1)\\bigg] \\\\\n= & \\pi\n\\end{align}\n}\n\\]\nThus:\n\\[\n\\displaylines{\n\\begin{align}\n& I^2 = \\pi \\\\\n\\Rightarrow \\text{ } & I = \\sqrt{\\pi}\n\\end{align}\n}\n\\]\nAnd so:\n\\[\n\\int_{-\\infty}^{+\\infty}{e^{-x^2}\\,dx}=\\sqrt{\\pi}\n\\]\nFin."
  },
  {
    "objectID": "posts/glm_exponential_dispersion.html",
    "href": "posts/glm_exponential_dispersion.html",
    "title": "Generalized Linear Models from scratch",
    "section": "",
    "text": "What we are exploring\n\n\n\nDeriving (and implementing in python) a generic way to optimize GLMs, by using the exponential family form for poisson, binomial and gaussian distributions."
  },
  {
    "objectID": "posts/glm_exponential_dispersion.html#intro-random-and-systematic-components-are-connected-via-link-functions",
    "href": "posts/glm_exponential_dispersion.html#intro-random-and-systematic-components-are-connected-via-link-functions",
    "title": "Generalized Linear Models from scratch",
    "section": "",
    "text": "There many types of generalized linear regression models: such as linear regression, logistic regression, poisson regression etc. Every one of these models is made up of a “random component” and a “systematic component”. Each also has a “link function” that combines the random and systematic parts.\nLet’s take the example of a poisson regression: \\[\n\\displaylines{\n\\underbrace{y_i \\sim Pois(\\mu_i)}_{\\text{random}} \\\\\n\\underbrace{\\ln{(\\mu_i)}}_{\\text{link function}} = \\underbrace{X_i^{\\intercal}\\beta}_{\\text{systematic}}\n}\n\\]\n\n\nThe random component determines how we want to model the distribution of \\(y\\). For example, if \\(y_i\\) is a count outcome, then it could be well suited to a poisson distribution:\n\\[\ny_i \\sim Pois(\\mu_i)\n\\]\nThe mean rate of the count is \\(\\mu_i\\) - so we expect \\(y_i\\) to be around \\(\\mu_i\\) on average. However, for any individual observation \\(i\\), the actual observed \\(y_i\\) will vary above and below the mean rate \\(\\mu_i\\). In fact, by using poisson we assume the variance of \\(y_i\\) increases as the mean rate \\(\\mu_i\\) increases too. And this is why it is called the “random component”: since \\(y\\) varies randomly around the mean, following the poisson distribution, it is a random variable.\n!!!! GRAPH OF POISSON !!!!\nBut how do we find a good estimation for \\(\\mathbb{E}[y_i|X_i]=\\mu_i\\)? Concretely, how do we best map our independent variables \\(X_i\\) to \\(\\mu_i\\)? This is down to our systematic component and link function.\n\n\n\nIn most cases, the systematic component \\(\\eta(X)\\) is usually just a linear transformation of \\(X\\), most often the result of multiplying each value by some good fitted coefficients \\(\\beta\\).\n\\[\n\\eta(X_i) = X_i^{\\intercal}\\beta\n\\]\nThe link function is a way of choosing how to map the systematic component to the natural parameter of the random component. For example, in poisson regression, we use a log link function, which means the systematic component predicts the natural log of the mean rate of the count.\n\\[\n\\ln{(\\mu_i)} = \\eta(X) = X^{\\intercal}\\beta\n\\]\nOkay, so now we want to find the best values for \\(\\beta\\). These coefficients will transform our features \\(X\\) to make the best predictions for \\(\\ln{(\\mu_i)}\\), given we want to predict \\(y\\) as accurately as possible (but permit larger residuals when \\(\\mu_i\\) is larger, following the poisson distribution).\nTo achieve this: we can try some initial coefficients, calculate the cost function and its first derivative, update the coefficients, and continue to minimize the cost function through gradient descent.\nBut how cumbersome that would be to do for every type of distribution we want to model! Wouldn’t it be nice if we can derive a generic representation for the cost function and its first derivative, so that we can re-use the same code for every type of regression?"
  },
  {
    "objectID": "posts/glm_exponential_dispersion.html#exponential-dispersion-family-of-distributions",
    "href": "posts/glm_exponential_dispersion.html#exponential-dispersion-family-of-distributions",
    "title": "Generalized Linear Models from scratch",
    "section": "Exponential Dispersion Family of Distributions",
    "text": "Exponential Dispersion Family of Distributions\n\nNotation for Generalized linear models\nIt can be shown that many common distributions can be reformulated into the “Exponential Dispersion Family of Distributions”. This generic representation makes it easier to re-use the same code to run a regression, rather than hand calculate each pdf in different ways.\nFirst let’s define some generic notation for generalized linear models:\n\\[\n\\displaylines{\n\\underbrace{g{(\\xi_i)}}_{\\text{link function}} = \\underbrace{\\eta(X_i)}_{\\text{systematic}} \\\\\n\\underbrace{y_i \\sim f_y(\\xi_i,\\phi_i)}_{\\text{random}}\n}\n\\]\n\\(\\xi_i\\) is a shape parameter, governing the shape of the distribution (e.g. in poisson, the expected value is the mean \\(\\xi_i=\\lambda_i\\)). Or for bernoulli, \\(\\xi_i=p_i\\). In fact for many distributions, \\(\\xi_i\\) is the expected value of \\(y_i\\) i.e. \\(\\xi_i = \\mu_i\\).\n\\(\\phi\\) is a dispersion parameter, governing the spread of the data (e.g. gaussian has the standard deviation \\(\\phi = \\sigma\\)). It is not always necessary if the dispersion is determined by the shape parameter already (e.g. in poisson the variance is already equated to the expected rate \\(\\lambda_i\\)).\nHere are some examples of the common forms of poisson, bernoulli and gaussian probability distribution functions \\(y \\sim f_{\\theta_i}(\\mu_i)\\), along with some choices for link functions \\(g(\\mu)\\) to use in regression:\n\n\n\n\n\n\n\n\nType\nProbability Density Function:\nLink Function for Regression:\n\n\n\n\nCount\\(\\xi_i = \\mu_i \\equiv \\lambda_i\\)\n\\(y_i \\sim Pois(\\mu_i)\\)\\(y_i \\sim \\frac{\\mu_i^{y} \\times e^{-\\mu_i}}{y!}\\)\n\\(g(\\mu) = \\ln{[\\mu_i]}\\)(log-link)\n\n\nBinary\\(\\xi_i = \\mu_i \\equiv p_i\\)\n\\(y_i \\sim Bern(\\mu_i)\\)\\(y_i \\sim \\mu_i^y \\times (1-\\mu_i)^{(1-y)}\\)\n\\(g(\\mu) = \\ln{\\left[\\frac{\\mu_i}{1-\\mu_i}\\right]}\\)(logit-link)\n\n\nBinary\\(\\xi_i = \\mu_i \\equiv p_i\\)\n\\(y_i \\sim Bern(\\mu_i)\\)\\(y_i \\sim \\mu_i^y \\times (1-\\mu_i)^{(1-y)}\\)\n\\(g(\\mu) = \\Phi^{-1}[\\mu_i]\\)(probit-link)\n\n\nNormal\\(\\xi_i = \\mu_i\\)\n\\(y_i \\sim N(\\mu_i,\\sigma^2)\\)\\(y_i \\sim \\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\frac{1}{2}\\left(\\frac{y_i-\\mu_i}{\\sigma^2}\\right)^2}\\)\n\\(g(\\mu) = \\mu\\)(identity-link)\n\n\n\n\n\nFormulating normal, bernoulli and poisson distributions into the exponential family\nAll of the probability density functions above - poisson, gaussian, bernoulli - can be rewritten into a the exponential family form!\nSee the footnotes that dive into the derivations for the exponential forms for each of the poisson1, gaussian2 and bernoulli3 distributions.\n\\[\n\\displaylines{\n\\begin{align}\nf(y;\\xi,\\phi)\n& =\n\\exp{ \\left\\{ \\frac{T(y) r(\\xi) - b(\\xi)}{a(\\phi)} + c(y,\\phi) \\right\\} } & \\tag{1.1}\n\\\\ \\\\\nPois(y;\\xi,\\phi)\n& = \\frac{\\mu_i^{y_i}e^{-\\mu_i}}{y_i!}\n\\\\ & \\equiv \\exp{ \\left\\{\n\\frac{\n  \\underbrace{y_i}_{T(y_i)}\n  \\underbrace{\\ln{ \\left[ \\xi_i \\right]}}_{r(\\xi_i)}\n  - \\underbrace{\\mu_i}_{b(\\xi_i)}\n}{\n  \\underbrace{1}_{a(\\phi)}\n}\n- \\underbrace{\\ln{ \\left[ y_i! \\right] }}_{c(y,\\phi)}\n\\right\\}} \\tag{1.2}\n\\\\ \\\\\nBern(y;\\xi,\\phi)\n& = \\xi_i^{y_i}(1-\\xi_i)^{1-y_i}\n\\\\ & \\equiv \\exp{ \\left\\{ \\frac{\n  \\underbrace{y_i}_{T(y)}\n\\underbrace{ \\ln{\n  \\left[\\frac{\\xi_i}{1-\\xi_i} \\right]\n} }_{r(\\xi_i)}\n  + \\underbrace{ \\ln{[1-\\xi_i]} }_{b(\\xi_i)}\n  }{\n    \\underbrace{1}_{a(\\phi)}\n  }\n  + \\underbrace{0}_{c(y_i,\\phi)}\n\\right\\}} \\tag{1.3}\n\\\\ \\\\\nN(y;\\xi,\\phi)\n& = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp{ \\left\\{-\\frac{(y_i-\\xi_i)^2}{2\\sigma^2} \\right\\} }\n\\\\ & \\equiv \\exp{\\left\\{\n  \\frac{\n    \\underbrace{y_i}_{T(y_i)} \\underbrace{\\xi_i}_{r(\\xi_i)} -\n    \\underbrace{\\frac{\\mu_i^2}{2}}_{b(\\xi_i)}\n  }{\n    \\underbrace{\\sigma^2}_{a(\\phi)}\n  }\n-\n\\underbrace{\n  \\frac{1}{2}\n  \\left(\n    \\frac{y_i^2}{\\sigma^2} +\n    \\ln{ \\left[ 2 \\pi \\sigma^2 \\right]}\n  \\right)\n}_{c(y_i,\\phi)}\n\\right\\}} \\tag{1.4}\n\\end{align}\n}\n\\]\n\n\n\n\n\n\nThe “canonical” link function\nYou may have noticed that \\(r(\\xi_i)\\) looks very familiar… in fact, it is the link functions we commonly use for that regression! For example in logistic regression, we use the logit link function \\(g(\\xi_i) = \\ln{\\left[\\frac{\\xi_i}{1-\\xi_i}\\right]} = \\eta(X)\\).\n\n\n\n\n\n\nCanonical link function\n\n\n\nFor any exponential distribution, \\(r(\\xi_i)\\) is the default choice for link function: the so-called “canonical link function”.\n\n\nIn fact, if the canonical link function is used, so \\(g(\\xi_i)=r(\\xi_i)\\), and no transformation means \\(T(y_i) = y_i\\), then we can simplify the formula to its “canonical form”, where \\(\\theta_i = g(\\xi_i)\\):\n\\[\n\\displaylines{\n\\begin{align}\nf(y;\\xi,\\phi)\n& = \\exp{ \\left\\{ \\frac{T(y) r(\\xi) - b(\\xi)}{a(\\phi)} + c(y,\\phi) \\right\\} }\n\\\\ \\\\\n\\equiv f(y; \\theta_i, \\phi)\n& = \\exp{ \\left\\{ \\frac{y \\theta - b(\\theta)}{a(\\phi)} + c(y,\\phi) \\right\\} } & \\tag{2.1}\n\\\\\n\\\\\n& \\iff T(y) = y; \\text{ } g(\\xi) = r(\\xi) = \\theta\n\\end{align}\n}\n\\]\nWhere this falls down is when you don’t use the canonical link function, such as in probit regression. While the canonical link function for bernoulli is logit, the probit link is the inverse cumulative distribution function of the standard normal \\(g(\\xi_i) = \\Phi^{-1}[\\xi_i] \\neq r(\\xi_i)\\). So because it is not canonical, it’s a bit more complicated to deal with (which we explore in a later post).\n\n\nSome intutition into the terms:\nWe go into detail into the intution behind the other terms in the footnotes, but for now, here is a quick summary:\n\n\\(a(\\phi)\\) normalizes the pdf using the dispersion parameter \\(\\phi\\). E.g. for gaussian regression, it is the variance of the residuals, \\(a(\\phi) = \\sigma^2\\).\n\\(c(y,\\phi)\\) is an adjustment function that ensures the pdf sums to one. For example, the exponential form of the poisson distribution would sum to more than one if it wasn’t included.\n\\(b(\\theta)\\) is the integral of the inverse of the link function. This might seem unintuitive right now, but it is easier to see when we derive the generic cost function and minimize it."
  },
  {
    "objectID": "posts/glm_exponential_dispersion.html#solving-for-any-exponential-family-using-maximum-likelihood-estimation",
    "href": "posts/glm_exponential_dispersion.html#solving-for-any-exponential-family-using-maximum-likelihood-estimation",
    "title": "Generalized Linear Models from scratch",
    "section": "Solving for any exponential family using maximum likelihood estimation",
    "text": "Solving for any exponential family using maximum likelihood estimation\nSo far we have reformulated poisson, binomial and normal regressions into the exponential family form. So now we want to derive our “one-use formula” to estimate the best coefficients for any GLM via maximum likelihood estimation!\n\nCost function\nLet’s define our generic cost function in negative log-likelihood form assuming the canonical link function is used (so in terms of \\(\\theta_i\\)):\n\n\nMaximising the the likelihood is hard, because it involves calculating the total product across every observation. Instead, taking the log likelihood makes everything a sum, far easier to calculate. Also, taking the negative ensures we are looking to minimize the cost.\n\\[\n\\displaylines{\n\\begin{align}\nL(\\theta)\n& = \\prod_{i=1}^n{\n  f(y_i;\\theta_i,\\phi)\n}\n\\\\ & = \\prod_{i=1}^n{\n  \\exp{ \\left\\{\n    \\frac{y \\theta - b(\\theta)}{a(\\phi)}\n    + c(y,\\phi)\n  \\right\\} }\n}\n\\\\ \\therefore \\mathcal{L}(\\theta)\n& = -\\ln \\left\\{ \\prod_{i=1}^n{\n  \\exp{ \\left\\{\n    \\frac{y_i \\theta_i - b(\\theta_i)}{a(\\phi)}\n    + c(y_i,\\phi)\n  \\right\\} }\n} \\right\\}\n\\\\ & = -\\sum_{i=1}^n{ \\left\\{\n  \\frac{y_i \\theta_i - b(\\theta_i)}{a(\\phi)}\n  + c(y_i,\\phi)\n\\right\\} }\n\\\\ & = -\\left(\n\\frac{1}{a(\\phi)}\n\\sum_{i=1}^n{ \\bigg\\{\n    y_i \\theta_i - b(\\theta_i)\n\\bigg\\} } +\n\\sum_{i=1}^n{ \\bigg\\{\n  c(y_i,\\phi)\n\\bigg\\}} \\right)\n\\end{align}\n}\n\\]\n\n\nInformant (score function)\nNext we want to minimize this generic cost function with respect to \\(\\beta\\). Common methods include “Newton-Raphson”, “Fisher-Scoring”, “Iteratively-reweighted Least Squares” or “Gradient Descent”.\nTo execute any of these, we need to derive the the “score” (or “informant”): the first derivative of the negative log likelihood with respect to \\(\\beta\\):\n\\[\n\\displaylines{\n\\begin{align}\n\\arg \\min_\\beta \\left[ \\mathcal{L}(\\theta) \\right]\n& = \\arg \\min_\\beta \\left[ -\\left(\n\\frac{1}{a(\\phi)}\n\\sum_{i=1}^n{ \\bigg\\{\n    y_i \\theta_i - b(\\theta_i)\n\\bigg\\} } +\n\\sum_{i=1}^n{ \\bigg\\{\n  c(y_i,\\phi)\n\\bigg\\}}\n\\right)\\right]\n\\\\\n& = \\arg \\min_\\beta \\left[ -\\left(\n\\frac{1}{a(\\phi)}\n\\sum_{i=1}^n{ \\bigg\\{\n    y_i X_i^{\\intercal}\\beta - b(X_i^{\\intercal}\\beta)\n\\bigg\\} } +\n\\sum_{i=1}^n{ \\bigg\\{\n  c(y_i,\\phi)\n\\bigg\\}}\n\\right)\\right]\n\\\\ & \\because \\theta = \\eta(X) \\text{ in the canonical case}\n\\\\\n\\\\\n\\therefore \\frac{\\partial \\mathcal{L}(\\theta)}{\\partial\\beta_i}\n& = -\\frac{1}{a(\\phi)} \\sum_{i=1}^n{ \\bigg\\{\n      y_i X_i  - \\frac{\\partial b(\\theta)}{\\partial \\theta} X_i\n  \\bigg\\} }\n\\\\ & \\because c() \\text{ is constant with respect to } \\beta\n\\\\\n& =\\underbrace{\\frac{1}{a(\\phi)}}_{\\text{Constant}}  \\times \\sum_{i=1}^n{ \\bigg\\{ \\left( b'(\\theta) - y_i \\right) X_i \\bigg\\} }\n\\end{align}\n}\n\\]\n\n\nOnly \\(\\theta\\) changes with respect to the choice of \\(\\beta\\). So \\(a(\\phi)\\) and \\(c(y_i,\\phi)\\), which are not a function of \\(\\theta\\), drop out.\nSo hopefully this now gives a bit more intuition behind why \\(b(\\theta)\\) is the integral of the inverse canonical link. Since \\(b'(\\theta)\\) is the activation function, we can see that the score function is just the difference between the predicted value and the actual value, multiplied by the feature. This is the same as the gradient of the cost function used for gradient descent!\nThis also uncovers an interesting property of GLMs - that the average prediction \\(b'(\\theta)\\) must be equal to the average value of Y too:\n\\[\n\\displaylines{\n\\begin{align}\n\\frac{\\partial \\mathcal{L}(\\theta)}{\\partial\\beta_i}\n& = 0 \\text{ (cost is minimized at stationary point)}\n\\\\\n& = \\cancel{\\frac{1}{a(\\phi)}}  \\times \\sum_{i=1}^n{ \\bigg\\{ \\left( b'(\\theta) - y_i \\right) \\cancel{X_i} \\bigg\\} }\n\\\\\n\\therefore\n\\sum_{i=1}^n{ y_i } & = \\sum_{i=1}^n{ b'(\\theta) }\n\\\\ & \\div N\n\\\\ \\\\\n\\Rightarrow \\bar{y} & = \\mathbb{E}[y] = b'(\\theta)\n\\end{align}\n}\n\\]\n\n\nHessian\nFinally, we need to derive the Hessian matrix, which is the second derivative of the cost function with respect to \\(\\beta\\). This is required for second order optimization methods, like Newton’s method, which can converge faster than gradient descent:\n\\[\n\\displaylines{\n\\begin{align}\n\\frac{\\partial^2 \\mathcal{L}(\\theta)}{\\partial\\beta_i^2}\n& = \\frac{\\partial}{\\partial\\beta_i}\\left(\n  \\frac{1}{a(\\phi)} \\sum_{i=1}^n{ \\bigg\\{ \\left( b'(\\theta) - y_i \\right) X_i \\bigg\\} }\n\\right)\n\\\\\n& = -\\frac{1}{a(\\phi)} \\sum_{i=1}^n{ \\bigg\\{\n      \\frac{\\partial b'(\\theta)}{\\partial \\theta} X_i^2\n  \\bigg\\} }\n\\end{align}\n}\n\\]\nWell done on getting this far! We can now start to write out some python to create this ourselves."
  },
  {
    "objectID": "posts/glm_exponential_dispersion.html#writing-a-generic-base-class",
    "href": "posts/glm_exponential_dispersion.html#writing-a-generic-base-class",
    "title": "Generalized Linear Models from scratch",
    "section": "Writing a generic base class",
    "text": "Writing a generic base class\nLet’s start creating a generic base class then that we can use for any distribution that can be represented by the canonical exponential dispersion family:\n\nclass base_canonical_exponential_dispersion_family():\n\n    def __init__(self,y,X,b,phi,a,c,seed=0):\n      \"\"\"\n      y:    Dependent variable. A 1d vector of numeric values \n      X:    Independent variables. A 2d matrix of numeric values \n      b:    Function for the integral of the activation function. See db for more details.\n      db:   Function for the first derivative of b(θ) with respect to theta.\n            Note that this should be the inverse of the link function\n            E.g. for poisson reg, g(µ) = ln(µ) = η(X) = θ. So b(θ) = exp(θ).\n      phi:  Parameter: the \"dispersion parameter\", φ. \n            A measure of the dispersion of the distribution\n            E.g. for gaussian reg, φ is the standard deviation of residuals.\n      a:    Function of φ. Normalizes the pdf using the dispersion parameter. \n            E.g. for gaussian reg, it is the variance of the residuals a(φ) = φ^2.\n      c:    Function for y and φ. Adjusts the likelihood so that the pdf sums to 1.\n      \"\"\"\n\n      self.y = np.array(y).reshape(-1,1)\n      self.X = np.array(X).reshape(y.shape[0],-1)\n      self.b = b\n      self.phi = phi\n      self.a = a\n      self.c = c\n      \n      np.random.seed(seed)\n      \n      # Initialize beta with random values\n      self.n, self.k = self.X\n      self.beta = np.random.normal(0, 0.5, self.k).reshape((self.k,1))\n\n    def canonical_theta(X, beta):\n      \"\"\" In the canonical form, theta is simply θ = η(X)= X'β \"\"\"\n      return X.dot(beta)\n\n    def negative_log_likelihood(self, beta):\n        \"\"\" Negative log likelihood i.e. the current cost\n        \"\"\"\n        y, X = self.y, self.X\n        theta = self.canonical_theta(self.X, beta)\n        phi = self.phi\n        a, b, c = self.a, self.b, self.c\n        log_likelihood = ( y * _theta - b(_theta) ) / a(phi) + c(y,phi)\n        J = -1 * log_likelihood\n        return J\n\n    def informant(self, theta=None, phi=None):\n        \"\"\" First derivative of the cost function with respect to theta \"\"\"\n        y, X, a, b = self.y, self.X, self.a, self.b\n        if theta is None:\n            theta = self.canonical_theta()\n        if phi is None:\n            phi = self.phi\n        dJ = ( X.T/a(phi) ).dot( db( theta ) - y )\n        return dJ\n\n    def hessian(self, theta, phi):\n        \"\"\" Second derivative of the cost function with respect to theta \"\"\"\n        a, d2b, X = self.a, self.d2b, self.X\n        V = self.var_y(theta, phi)\n        d2J = X.T.dot( V / a(phi)**2 ).dot(X)\n        return d2J\n\n    def update_beta(self, fit_type='Netwon-Raphson'):\n        \"\"\" A single step towards optimizing beta \"\"\"\n        beta, X = self.beta, self.X\n        theta, phi =  self.theta(), self.phi()\n        d2J = self.hessian( theta, phi )\n        learning_rate = np.linalg.solve( d2J, np.eye(X.shape[1]) )\n        if fit_type in ['Netwon-Raphson']:\n            dJ = self.score_function( theta, phi )\n        else:\n            raise ValueError('Please select \"Newton-Raphson\". \"IRLS\" coming soon')\n        beta -= learning_rate.dot(dJ)\n        return beta\n\n    def fitting_mle(self, max_iter = 100, fit_type='Netwon-Raphson', epsilon = 1e-8):\n        \"\"\" Fitting using MLE \"\"\"\n        for i in range(max_iter):\n            old_beta = self.beta.copy()\n            new_beta = self.update_beta(fit_type=fit_type)\n            self.beta = new_beta\n            if (np.abs(new_beta - old_beta)/(0.1 + np.abs(new_beta)) &lt;= epsilon).all():\n                print(\"Fully converged by iteration \" + str(i))\n                break\n            if (i == max_iter):\n                print(\"Warning - coefficients did not fully converge within \" + str(max_iter) + \" iterations.\")\n        self.fitted = True\n        self.unadj_r_squared = -1\n        self.residuals = (self.y - self.db(self.theta())).reshape(-1,1)\n\nAnd now we have our base class, we can utilise it for specific classes of the poisson, bernoulli and gaussian distributions:\n\nclass gaussian_family(base_canonical_exponential_dispersion_family):\n\n  def __init__(self,y,X,seed = 0):\n\n      def b(theta):\n          \"\"\" Integral of the activation function \"\"\"\n          return 0.5*theta**2\n\n      def db(theta):\n          \"\"\" Activation function is identity (inverse of the indentity function):\"\"\"\n          return theta\n\n      def d2b(theta):\n          \"\"\" Differential of the activation function \"\"\"\n          return 1\n\n      def dispersion(y, theta):\n          \"\"\" Std.dev of residuals = sqrt(RSS / DoF) \"\"\"\n          n, k = self.n, self.k\n          std_dev = np.sqrt(np.sum( (y - theta)**2 ) / ( n - k ) )\n          return std_dev\n\n      def a(phi):\n          \"\"\" Variance of residuals \"\"\"\n          return phi**2\n\n      def c(y,phi):\n          \"\"\" Adjustment needed so pdf sums to one \"\"\"\n          return -0.5 * ( y**2/phi + np.log(2*pi*phi))\n\n      super().__init__(y,X,b,db,d2b,dispersion,a,c)\n\nclass bernoulli_family(base_canonical_exponential_dispersion_family):\n\n  def __init__(self,y,X,seed = 0):\n      \n      def b(theta):\n          \"\"\" Integral of the activation function \"\"\"\n          return np.log(1 + np.exp(theta))\n\n      def db(theta):\n          \"\"\" Activation function is logistic (inverse of the logit function):\"\"\"\n          return np.exp(theta)/(1 + np.exp(theta))\n\n      def d2b(theta):\n          \"\"\" Differential of the activation function \"\"\"\n          return np.exp(theta)/(1 + np.exp(theta))**2\n\n      def dispersion(y, theta):\n          \"\"\" Not needed - one parameter distribution \"\"\"\n          return 1\n\n      def a(dispersion):\n          \"\"\" Not needed - one parameter distribution \"\"\"\n          return 1\n\n      def c(y, dispersion):\n          \"\"\" No adjustment needed (pdf already sums to one) \"\"\"\n          return 0\n\n      super().__init__(y,X,b,db,d2b,dispersion,a,c) \n\nclass poisson_family(base_canonical_exponential_dispersion_family):\n\n  def __init__(self,y,X,seed = 0):\n      \n      def b(theta):\n          \"\"\" Integral of the activation function \"\"\"\n          return np.exp(theta)\n\n      def db(theta):\n          \"\"\" Activation function is exponential (inverse of the log-link function):\"\"\"\n          return np.exp(theta)\n\n      def d2b(theta):\n          \"\"\" Differential of the activation function \"\"\"\n          return np.exp(theta)\n\n      def dispersion(y, theta):\n          \"\"\" Not needed - one parameter distribution \"\"\"\n          return 1\n\n      def a(dispersion):\n          \"\"\" Not needed - one parameter distribution \"\"\"\n          return 1\n\n      def c(y, dispersion):\n          \"\"\" Adjustment needed so pdf sums to one \"\"\"\n          return np.vectorize(-np.log(np.math.factorial(y)))\n\n      super().__init__(y,X,b,db,d2b,dispersion,a,c)"
  },
  {
    "objectID": "posts/glm_exponential_dispersion.html#footnotes",
    "href": "posts/glm_exponential_dispersion.html#footnotes",
    "title": "Generalized Linear Models from scratch",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nRearranging poisson into exponential form, with mean rate of \\(\\lambda_i = \\mu_i = \\xi_i\\): \\[\n  \\displaylines{\n  \\begin{align}\n  f_y(\\xi_i,\\phi)\n  & =\n  \\frac{\\mu_i^{y_i}e^{-\\mu_i}}{y_i!}\n  \\\\ & = \\exp{ \\left\\{\n  \\ln{ \\left[ \\frac{\\mu_i^{y_i} \\times e^{-\\mu_i}}{y_i!} \\right]}\n  \\right\\}}\n  \\\\ & = \\exp{ \\left\\{\n  \\ln{ \\left[ \\mu_i^{y_i} \\right]}\n  + \\ln{ \\left[ e^{-\\mu_i} \\right] }\n  - \\ln{ \\left[ y_i! \\right] }\n  \\right\\}}\n  \\\\ & = \\exp{ \\left\\{\n  y_i \\ln{ \\left[ \\mu_i \\right]}\n  -\\mu_i\n  - \\ln{ \\left[ y_i! \\right] }\n  \\right\\}}\n  \\\\ & \\equiv \\exp{ \\left\\{\n  \\frac{\n\\underbrace{y_i}_{T(y_i)}\n\\underbrace{\\ln{ \\left[ \\mu_i \\right]}}_{r(\\xi_i)}\n- \\underbrace{\\mu_i}_{b(\\xi_i)}\n  }{\n\\underbrace{1}_{a(\\phi)}\n  }\n  - \\underbrace{\\ln{ \\left[ y_i! \\right] }}_{c(y,\\phi)}\n  \\right\\}}\n  \\\\ & = \\exp{ \\left\\{ \\frac{T(y_i) r(\\xi_i) - b(\\xi_i)}{a(\\phi)} + c(y_i,\\phi) \\right\\} } & \\tag{1.2}\n  \\end{align}\n  }\n  \\]↩︎\nRearranging gaussian into exponential form, with mean success \\(\\mu_i=\\xi_i\\): \\[\n  \\displaylines{\n  \\begin{align}\n  f(y;\\theta,\\phi)\n  & = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp{ \\left\\{-\\frac{(y_i-\\mu_i)^2}{2\\sigma^2} \\right\\} }\n  \\\\ & =\n  \\exp{\\left\\{\n  \\ln{ \\left[ \\frac{1}{\\sqrt{2 \\pi \\sigma^2}}\n  \\exp{ \\left\\{ -\\frac{(y_i-\\mu_i)^2}{2\\sigma^2} \\right\\} }\n  \\right]}\n  \\right\\}}\n  \\\\ & =\n  \\exp{\\left\\{\n  \\ln{ \\left[ \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\right]}\n  - \\frac{(y_i-\\mu_i)^2}{2\\sigma^2}\n  \\right\\}}\n  \\\\ & =\n  \\exp{\\left\\{\n  \\cancel{\\ln{ \\left[ 1 \\right]}}\n  + \\ln{ \\left[ 2 \\pi \\sigma^2 \\right]}^{-1/2}\n  - \\frac{y_i^2+\\mu_i^2-2y\\mu_i}{2\\sigma^2}\n  \\right\\}}\n  \\\\ & =\n  \\exp{\\left\\{\n  - \\frac{1}{2}\\ln{ \\left[ 2 \\pi \\sigma^2 \\right]}\n  - \\frac{y_i^2}{2\\sigma^2}\n  - \\frac{\\frac{\\mu_i^2}{2}}{\\sigma^2}\n  + \\frac{\\cancel{2}y_i\\mu_i}{\\cancel{2}\\sigma^2}\n  \\right\\}}\n  \\\\ & =\n  \\exp{\\left\\{\n\\frac{\n  y \\underbrace{\\mu_i}_{\\theta_i} -\n  \\underbrace{\\frac{\\mu_i^2}{2}}_{b(\\theta_i)}\n}{\n  \\underbrace{\\sigma^2}_{a(\\phi)}\n}\n  -\n  \\underbrace{\n\\frac{1}{2}\n\\left(\n  \\frac{y_i^2}{\\sigma^2} +\n  \\ln{ \\left[ 2 \\pi \\sigma^2 \\right]}\n\\right)\n  }_{c(y_i,\\phi)}\n  \\right\\}}\n  \\\\ & = \\exp{ \\left\\{ \\frac{y_i \\theta_i - b(\\theta_i)}{a(\\phi)} + c(y_i,\\phi) \\right\\} }\n  \\end{align}\n  }\n  \\]↩︎\nRearranging bernoulli into exponential form, with expected success probability \\(p_i=\\mu_i=\\xi_i\\): \\[\n  \\displaylines{\n  \\begin{align}\n  f(y;\\theta,\\phi)\n  & = \\mu_i^{y_i}(1-\\mu_i)^{1-y_i} \\\\\n  & = \\exp{ \\left\\{\\ln{\\left[\\mu_i^{y_i}(1-\\mu_i)^{(1-y_i)} \\\\\\right]} \\right\\}} \\\\\n  & = \\exp{ \\left\\{\n\\ln{\\left[\\mu_i^{y_i}\\right]} + \\ln{\\left[(1-\\mu_i)^{(1-y_i)}\\right]}\n\\right\\}} \\\\\n  & = \\exp{ \\left\\{ y_i \\ln{[\\mu_i]} + (1-y_i)\\ln{[1-\\mu_i]} \\right\\}} \\\\\n  & = \\exp{ \\left\\{ y_i \\ln{\n\\left[\\frac{\\mu_i}{1-\\mu_i} \\right]} + \\ln{[1-\\mu_i]} \\right\\}} \\\\\n  & = \\exp{ \\left\\{ \\frac{\n\\underbrace{y_i}_{T(y)}\n  \\underbrace{ \\ln{\n\\left[\\frac{\\mu_i}{1-\\mu_i} \\right]\n  } }_{r(\\xi_i)}\n+ \\underbrace{ \\ln{[1-\\mu_i]} }_{b(\\xi_i)}\n}{\n  \\underbrace{1}_{a(\\phi)}\n}\n+ \\underbrace{0}_{c(y_i,\\phi)}\n  \\right\\}}\n  \\\\\n  & = \\exp{ \\left\\{ \\frac{y_i\n  \\underbrace{ \\ln{\n\\left[\\frac{\\mu_i}{1-\\mu_i} \\right]\n  } }_{\\theta_i}\n+ \\underbrace{ \\ln{[1-\\mu_i]} }_{b(\\theta_i)}\n}{\n  \\underbrace{1}_{a(\\phi)}\n}\n+ \\underbrace{0}_{c(y_i,\\phi)}\n  \\right\\}}\n  \\\\ & = \\exp{ \\left\\{ \\frac{y_i \\theta_i - b(\\theta_i)}{a(\\phi)} + c(y_i,\\phi) \\right\\} }\n  \\end{align}\n  }\n  \\] And showing how \\(\\ln[1-p] = -\\ln[1+e^{\\theta}]\\) \\[\n  \\displaylines{\n  \\begin{align}\n  \\theta & = \\ln{\\left[\\frac{p}{1-p} \\right]}\n  \\\\ \\text{ (1) Put } p \\text{ in terms of } \\theta \\text{:}\n  \\\\ \\therefore e^{\\theta} & = \\frac{p}{1-p} & \\text{raise by }e\n  \\\\ \\therefore e^{\\theta}-pe^{\\theta} & = p & \\times (1-p)\n  \\\\ \\therefore e^{\\theta} & = p(1+e^\\theta) & + pe^\\theta\n  \\\\ \\therefore p & = \\frac{e^{\\theta}}{1+e^\\theta} & \\div (1+e^\\theta)\n  \\\\\\\\\n  \\\\ \\text{ (2) Substitute in } p = \\frac{e^{\\theta}}{1+e^\\theta}\n  \\\\\n  \\Rightarrow \\ln[1-p]\n  & = \\ln\\left[1-\\frac{e^{\\theta}}{1+e^\\theta} \\right]\n  \\\\ & \\equiv \\ln\\left[\\frac{1+e^\\theta}{1+e^\\theta}-\\frac{e^{\\theta}}{1+e^\\theta} \\right]\n  \\\\ & = \\ln\\left[\\frac{1}{1+e^\\theta} \\right]\n  \\\\ & \\equiv \\ln\\left[(1+e^\\theta \\right)^{-1}]\n  \\\\ & \\equiv -\\ln\\left[1+e^\\theta \\right]\n  \\end{align}\n  }\n  \\]↩︎"
  },
  {
    "objectID": "posts/glm_exponential_dispersion.html#coding-it-up-from-scratch",
    "href": "posts/glm_exponential_dispersion.html#coding-it-up-from-scratch",
    "title": "Generalized Linear Models from scratch",
    "section": "Coding it up from scratch",
    "text": "Coding it up from scratch\nWell done on getting this far! We can now start to write out some python to create this ourselves. We will start by creating a parent class for exponential dispersion families, and then create child classes for the poisson, bernoulli and gaussian distributions specifically.\n\nParent class for the canonical form:\nFirst, let’s create a generic parent class then that we can use for any canonical exponential dispersion family. Note that this contains the MLE proceedure to minimize the cost function for regression (we will explain this in a future post - just know for now that this uses maximum likelihood estimation to optimize the coefficients).\n\n\nCreating the exponential regression parent class\nclass exponential_regression_parent():\n\n    def __init__(self,seed=0,y=None,X=None):\n      \"\"\"\n      seed: Scalar sets seed for random coefficient initialization\n      y:    Dependent variable. A 1d vector of numeric values \n      X:    Independent variables. A 2d matrix of numeric values \n      \"\"\"\n      self.seed = seed\n      self.y = y\n      self.X = X\n\n    def _initialize(self,y=None,X=None):\n        if y is None:\n            if self.y is None:\n                raise ValueError('Please provide y')\n            else:\n                y = self.y\n        if X is None:\n            if self.X is None:\n                raise ValueError('Please provide X')\n            else:\n                X = np.array(self.X).reshape(y.shape[0],-1)\n        self.y = np.array(y).reshape(-1,1)\n        self.X = np.array(X).reshape(y.shape[0],-1)\n        self.n, self.k = X.shape\n        np.random.seed(self.seed)\n        self.beta = np.random.normal(0, 0.5, self.k).reshape((self.k,1))\n\n    def _theta(self, theta=None):\n        \"\"\" helper function \"\"\"\n        if theta is None:\n            theta = self.theta()\n        return theta\n\n    def _phi(self,phi=None):\n        \"\"\" helper function \"\"\"\n        if phi is None:\n            phi = self.phi()\n        return phi\n\n    def negative_log_likelihood(self):\n        \"\"\" Negative log likelihood i.e. the current cost\"\"\"\n        y = self.y\n        theta, phi = self._theta(), self._phi()\n        a, b, c = self.a, self.b, self.c\n        log_likelihood = ( y * theta - b(theta) ) / a(phi) + c(y,phi)\n        J = -1 * log_likelihood\n        return J\n\n    def informant(self, theta=None, phi=None):\n        \"\"\" First derivative of the cost function with respect to theta \"\"\"\n        y, X, a, b, db = self.y, self.X, self.a, self.b, self.db\n        theta, phi = self._theta(theta), self._phi(phi)\n        dJ = (1/a(phi)) * X.T.dot( db( theta ) - y )\n        return dJ\n    \n    def hessian(self, theta=None, phi=None):\n        \"\"\" Second derivative of the cost function with respect to theta \"\"\"\n        X, y, phi = self.X, self.y, self._phi(phi)\n        a, d2b = self.a, self.d2b\n        # VarY = a(phi) * np.diagflat(self.d2b(theta))\n        VarY = np.diag(((self.db(theta)-y)**2).squeeze(axis=1)) \n        d2J = X.T.dot(VarY/a(phi)**2).dot(X)\n        return d2J\n\n    def update_beta(self, fit_type='Newton-Raphson'):\n        \"\"\" A single step towards optimizing beta \"\"\"\n        X, beta = self.X, self.beta\n        theta, phi = self._theta(), self._phi()\n        learning_rate = np.linalg.inv(self.hessian(theta))\n        if fit_type in ['Newton-Raphson']:\n            dJ = self.informant( theta, phi )\n        else:\n            raise ValueError('Please select \"Newton-Raphson\". \"IRLS\" coming soon')\n        beta -= learning_rate.dot(dJ)\n        return beta\n\n    def fit(self, y=None, X=None, max_iter = 100, fit_type='Newton-Raphson', epsilon = 1e-8):\n        # Initialize beta with random values, store in self.beta\n        self._initialize(y,X)\n        # Fitting using MLE\n        for i in range(max_iter):\n            old_beta = self.beta.copy()\n            new_beta = self.update_beta(fit_type=fit_type)\n            self.beta = new_beta\n            if (np.abs(new_beta - old_beta)/(0.1 + np.abs(new_beta)) &lt;= epsilon).all():\n                print(\"Fully converged by iteration \" + str(i))\n                break\n            if (i == max_iter):\n                print(\"Warning - coefficients did not fully converge within \" + str(max_iter) + \" iterations.\")\n\n    def predict(self, X):\n        \"\"\" Predicted value of y uses the activation function, b'(θ) \"\"\"\n        y_hat = self.db( self.theta(X=X) )\n        return y_hat\n\n\nAnd now we have our base class, we can utilise it for specific classes of the poisson, bernoulli and gaussian distributions:\n\n\nGaussian (normal) regression\nOkay now we can write our gaussian class:\n\n\n\nCreating the gaussian base class\nclass gaussian(exponential_regression_parent):\n\n  def __init__(self,seed = 0,**kwargs):\n      super().__init__(seed, **kwargs)\n\n  def theta(self, X=None):\n        \"\"\" As canonical form, theta == link function. \n        So we simply equate θ = g(µ) = η(X)= X'β \"\"\"\n        beta = self.beta\n        if X is None:\n            X = self.X\n        return X.dot(beta)\n\n  def b(self,theta):\n      \"\"\" b(θ) = 0.5*θ^2: Integral of the activation function \"\"\"\n      return 0.5*theta**2\n  \n  def db(self,theta):\n      \"\"\" b'(θ) = θ: Activation function is the identity \"\"\"\n      return theta\n\n  def d2b(self,theta):\n      \"\"\" b''(θ) = 1: Differential of the activation function is just one \"\"\"\n      return np.ones(theta.shape)\n\n  def phi(self):\n      \"\"\" φ = σ: Dispersion measure is std.dev.\n      It is estimated from the residuals, i.e. s = sqrt(RSS / DoF)\n      \"\"\"\n      y, n, k = self.y, self.n, self.k\n      y_hat = self.predict(self.X)\n      std_dev = np.sqrt(np.sum( (y - y_hat)**2 ) / ( n - k ) )\n      return std_dev\n  \n  def a(self,phi):\n      \"\"\" a(φ) = σ^2: The variance of y \"\"\"\n      return phi**2\n\n  def c(self,y,phi):\n      \"\"\" Adjustment needed so pdf sums to one \"\"\"\n      return -0.5 * ( y**2/phi + np.log(2*pi*phi))\n\n\nNow we can test the gaussian regression on some dummy data. We also compare this with OLS derived coefficients (and check against statsmodels) to see if we are correct:\n\n\nTesting the gaussian regression\n# Fit the model and predict y\nmodel_gaus = gaussian()\nmodel_gaus.fit(y,X)\n\n# Compare with OLS\nXtX = X.transpose().dot(X)\nXty = X.transpose().dot(y).flatten()\nls_beta = (np.linalg.pinv(XtX).dot(Xty)).reshape(-1,1)\n\n# Compare with statsmodels\nglm_stats_model = sm.GLM(y, X, family=sm.families.Gaussian())\nresults = glm_stats_model.fit()\n\nfor i in range(len(ls_beta)):\n    if np.round(i,5) != np.round(results.params[i],5):\n        ValueError('Coefficients do not match with OLS')\n    if np.round(i,5) != np.round(results.params[i],5):\n        ValueError('Coefficients do not match with Statsmodels')\n\nols_stats_model = sm.OLS(y, X)\nresults = ols_stats_model.fit()\n\npd.DataFrame(\n  np.hstack([\n    model_gaus.beta,\n    results.params.reshape(-1,1),\n    ls_beta,\n    ]),\n  columns=['True coefficients','StatsModel coefficients','OLS coefficients']\n)\n\n\nFully converged by iteration 10\n\n\n\n\n\n\n\n\n\nTrue coefficients\nStatsModel coefficients\nOLS coefficients\n\n\n\n\n0\n0.704655\n0.704655\n0.704655\n\n\n1\n0.302300\n0.302300\n0.302300\n\n\n2\n0.507925\n0.507925\n0.507925\n\n\n\n\n\n\n\n\n\nLogistic\nNow let’s try the same for a logistic regression:\n\n\nCreating the bernoulli base class\nclass bernoulli(exponential_regression_parent):\n\n  def __init__(self,seed = 0, **kwargs):\n      super().__init__(seed, **kwargs)\n\n  def theta(self, X=None):\n        \"\"\" As canonical form, theta == link function. \n        So we simply equate θ = g(µ) = η(X)= X'β \"\"\"\n        beta = self.beta\n        if X is None:\n            X = self.X\n        return X.dot(beta)\n\n  def b(self,theta):\n      \"\"\" b(θ) = -ln[1+exp{θ}]: integral of the activation function \"\"\"\n      return -np.log(1 + np.exp(theta))\n      # return np.exp(-theta)/(1+np.exp(-theta))**2\n  \n  def db(self,theta):\n      \"\"\" b'(θ) = Λ(θ) = 1/(1+exp{-θ}): Activation function is logistic (inverse of the logit-link function)\"\"\"\n      return (1+np.exp(-theta))**-1\n      \n  def d2b(self,theta):\n      \"\"\" b''(θ) = Λ(θ)*(1-Λ(θ)): Differential of the activation function is the logistic distribution \"\"\"\n      _db = self.db(theta)\n      return _db*(1-_db)\n\n  def phi(self):\n      \"\"\" Not needed - one parameter distribution \"\"\"\n      return 1\n  \n  def a(self,phi):\n      \"\"\" Not needed - one parameter distribution \"\"\"\n      return 1\n  \n  def c(self,y,phi):\n      \"\"\" No adjustment needed (pdf already sums to one) \"\"\"\n      return 0\n\n\n\n\nTesting the bernoulli regression\ny2 = (y &gt; y.mean()).astype(int)\n\nglm2 = sm.GLM(y2, X, family=sm.families.Binomial())\nresults2 = glm2.fit(  method='newton')\n\nmodel_bern = bernoulli(X=X,y=y2)\nmodel_bern.fit()\n\npd.DataFrame(\n  np.hstack([\n    model_bern.beta,\n    results2.params.reshape(-1,1),\n    ]),\n  columns=['Bernoulli coefficients','StatsModel coefficients']\n)\n\n\nFully converged by iteration 10\n\n\n\n\n\n\n\n\n\nBernoulli coefficients\nStatsModel coefficients\n\n\n\n\n0\n0.371638\n0.371638\n\n\n1\n-0.709617\n-0.709617\n\n\n2\n0.345754\n0.345754\n\n\n\n\n\n\n\n\n\nPoisson\nNow let’s try the same for a poisson regression:\n\n\nCreating the poisson base class\nclass poisson(exponential_regression_parent):\n\n  def __init__(self,seed = 0):\n      super().__init__(seed)\n\n  def theta(self, X=None):\n        \"\"\" As canonical form, theta == link function. \n        So we simply equate θ = g(µ) = η(X)= X'β \"\"\"\n        beta = self.beta\n        if X is None:\n            X = self.X\n        return X.dot(beta)\n\n  def b(self,theta):\n      \"\"\" b(θ) = exp{θ}: integral of the activation function \"\"\"\n      return np.exp(theta)\n  \n  def db(self,theta):\n      \"\"\" b'(θ) = exp{θ}: Activation function is exponential (inverse of the log-link function)\"\"\"\n      return np.exp(theta)\n\n  def d2b(self,theta):\n      \"\"\" b''(θ) = exp{θ}: Differential of the activation function \"\"\"\n      return np.exp(theta)\n  \n  def phi(self):\n      \"\"\" Not needed - one parameter distribution \"\"\"\n      return 1\n  \n  def a(self,phi):\n      \"\"\" Not needed - one parameter distribution \"\"\"\n      return 1\n  \n  def c(self,y,phi):\n      \"\"\" Adjustment needed so pdf sums to one \"\"\"\n      return np.vectorize(-np.log(np.math.factorial(y)))\n\n\n\n\nTesting the poisson regression\nmodel_pois = poisson()\nmodel_pois.fit(y3,X)\n\nglm3 = sm.GLM(y3, X, family=sm.families.Poisson())\nresults3 = glm3.fit()\n\npd.DataFrame(\n  np.hstack([\n    model_pois.beta,\n    results3.params.reshape(-1,1),\n    ]),\n  columns=['Poisson coefficients','StatsModel coefficients']\n)\n\n\nFully converged by iteration 16\n\n\n\n\n\n\n\n\n\nPoisson coefficients\nStatsModel coefficients\n\n\n\n\n0\n0.530279\n0.530279\n\n\n1\n0.340200\n0.340200\n\n\n2\n0.628620\n0.628620"
  },
  {
    "objectID": "posts/glm_exponential_dispersion.html#introduction",
    "href": "posts/glm_exponential_dispersion.html#introduction",
    "title": "Generalized Linear Models from scratch",
    "section": "Introduction",
    "text": "Introduction\nThere many types of generalized linear regression models: such as linear regression, logistic regression, poisson regression etc. Every one of these models is made up of a “random component” and a “systematic component”. Each also has a “link function” that combines the random and systematic parts.\nTo make it easier to contrast and compare, first let’s take a normal (gaussian) linear regression:\n\\[\n\\displaylines{\n\\mu_i = X_i^{\\intercal}\\beta \\\\\ny_i \\sim N(\\mu_i,\\sigma^2)\n}\n\\]\nIn other words, for every observation \\(i\\):\n\nThe expected value \\(\\mathbb{E}[y_i|X_i]=\\mu_i\\) can be perfectly explained: by taking the dot product of the features and the true coefficients, so \\(\\mu_i = X_i^{\\intercal}\\beta\\).\nHowever, in reality the actual observation \\(y_i\\) varies around this expected value: its variation follows a normal distribution with uniform variance \\(\\mathbb{V}[y_i]=\\sigma^2\\), i.e. \\(y_i \\sim N(\\mu_i,\\sigma^2)\\)\n\nThis idiosyncratic error, \\(y_i - \\mu_i\\), is irreducible (can never be predicted - it is random)\n\nWhen regressing, we try to find the best coefficients \\(\\hat{\\beta}\\) to predict \\(\\hat{\\mu}_i=X_i^{\\intercal}\\hat{\\beta}\\) by minimizing the residuals equally across all observations (so that, hopefully, only the idiosyncratic error remains).\n\n\n\nClick here to show the code\nimport numpy as np, pandas as pd\nfrom scipy.stats import norm, poisson, gamma\nimport statsmodels.api as sm\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n\nnp.random.seed(42)\n\n# Generate some X\nn = 300\nX = np.random.rand(n,3).astype('float64') + 2\n\n# Generate some y using true coefficients β\ntrue_beta = np.random.normal(0.5,0.1,3).reshape(-1,1)\neta = X.dot(true_beta)\n\n# Sort data by eta\neta = eta.ravel()\nidxs = np.argsort(eta)\nX, eta = X[idxs], eta[idxs]\n\n# Estimate coefficients to the data\nsigma_sq=0.5\ny = np.random.normal(eta,sigma_sq).reshape(-1,1)\n\nfig = make_subplots(\n  rows=2,cols=1,\n  subplot_titles=(\n    \"Relationship is identity: E[y|X,β] = μ = X'β\",\n    \"Variance is constant: y ~ N(μ,σ^2)\",\n    ),\n  specs=[\n    [{\"type\": \"scatter\", \"l\": 0.2, \"r\": 0.2, \"b\":0}],\n    [{\"type\": \"scatter3d\", \"b\": 0.1, \"t\": 0}]\n  ],\n  row_heights = [2,5],\n  horizontal_spacing = 0,\n)\n\nfig.add_trace(\n  go.Scatter(\n    x=eta, \n    y=y.flatten(), \n    mode='markers', name='Observations',\n    marker=dict(size=2, color=\"blue\"),\n  ),\n  row=1, col=1\n)\n\nfig.update_xaxes(title=dict(text=\"μ = X'β\", font_size=16),row=1, col=1)\nfig.update_yaxes(title=dict(text=\"y\", font_size=16),row=1, col=1)\n\nfig.add_trace(\n  go.Scatter(\n    x=eta, \n    y=eta,\n    line_shape='spline',\n    mode='lines',\n  ),\n  row=1, col=1\n)\n\n# 3d plot of data\nfig.add_trace(\n  go.Scatter3d(\n    x=eta, \n    y=y.flatten(),\n    z=norm.pdf(y.flatten(), eta, sigma_sq),\n    marker=dict(size=1, color=\"blue\"),\n    mode='markers', name='Observations'\n  ),\n  row=2, col=1\n)\n\n# 3d line of data\nfig.add_trace(\n  go.Scatter3d(\n    x=eta, \n    y=eta,\n    z=norm.pdf(x=eta,loc=eta,scale=sigma_sq),\n    line=dict(color=\"red\"),\n    mode='lines',\n  ),\n  row=2, col=1\n)\n\ngaus_x = np.arange(3,4.5,0.05)\ngaus_y = np.arange(2,6,0.05)\nxv, yv = np.meshgrid(gaus_x, gaus_y)\ngaus_pdf = norm.pdf(yv, xv, sigma_sq)\n\nfig.add_trace(\n  go.Surface(\n    x=xv, y=yv, z=gaus_pdf, \n    opacity=0.2,\n    colorscale=[(0, 'gray'), (1, 'gray')],\n    showscale=False,\n    contours = dict(x= {\"show\": True})\n  ),\n  row=2, col=1\n)\n\nfig.update_layout(\n    scene = dict(\n      aspectratio=dict(x=1.5,y=1.8,z=1),\n      xaxis_title=\"μ = X'β\",\n      yaxis_title=\"y\",\n      zaxis_title=\"N(μ,σ^2)\",\n    ),\n\n    scene_camera = dict(\n        eye = dict(x=1.8, y=-1.5, z=0.3),\n        center = dict(x=0,y=0,z=-0.35),\n    ),\n    showlegend=False,\n  )\n\nfig.show()\n\n\n                                                \n\n\n\n\nThe top plot shows the linear (identity) relationship linking the expected value of \\(y_i\\) to the prediction \\(\\mu_i = X_i^{\\intercal}\\beta\\). We see \\(y\\) is equally dispersed for all values of \\(\\mu\\) (equal to \\(\\sigma^2\\)). \nThe bottom plot is identical to the first, but adds a third axis with the probability density of the gaussian distribution. The peak of the probability density is at its expected value \\(\\mu_i\\), the density is symmetric around \\(\\mu_i\\) and the variance is constant \\(\\sigma^2\\).\nWith this in mind, let’s now go through poisson regression, to compare and contrast: \\[\n\\displaylines{\ny_i \\sim Pois(\\lambda_i) \\\\\n\\ln{(\\lambda_i)} = X_i^{\\intercal}\\beta \\\\\n}\n\\]\nThere’s lots of similarities, but lots of differences too:\n\nThe expected value \\(\\mathbb{E}[y_i|X_i]=\\lambda_i\\) can be perfectly explained: also by taking the dot product of the features and the true coefficients, but then raising it to the exponential as well: \\(\\lambda_i=e^{X_i^{\\intercal}\\beta}\\)\n\nSince \\(\\ln{(\\lambda_i)} = X_i^{\\intercal}\\beta\\), then \\(\\lambda_i = e^{X_i^{\\intercal}\\beta}\\)\n\nIn reality, the actual observation \\(y_i\\) varies around the expected value: but this time its variation follows a poisson distribution. The poisson variance is equal to its expected value \\(\\mathbb{V}[y_i] = \\lambda_i\\): thus we expect higher variance at higher counts.\nConsequently, this impacts how we aim to minimize residuals and estimate our ideal coefficients \\(\\beta\\). A poisson regression permits larger residuals at higher counts vs normal regression.\n\n\n\nClick here to show the code\n# Poisson:\ny3 = np.random.poisson(np.exp(eta))\n\nfig = make_subplots(\n  rows=2,cols=1,\n  subplot_titles=(\n    \"Relationship is exponential: E[y|X,β] = λ = exp(X'β)\",\n    \"Variance increases with λ: y ~ Pois(λ)\",\n    ),\n  specs=[\n    [{\"type\": \"scatter\", \"l\": 0.2, \"r\": 0.2, \"b\":0}],\n    [{\"type\": \"scatter3d\", \"b\": 0.1, \"t\": 0}]\n  ],\n  row_heights = [2,5],\n  horizontal_spacing = 0,\n)\n\nfig.add_trace(\n  go.Scatter(\n    x=eta, \n    y=y3.flatten(), \n    mode='markers', name='Observations',\n    marker=dict(size=2, color=\"blue\"),\n  ),\n  row=1, col=1\n)\n\nfig.update_xaxes(title=dict(text=\"ln(λ) = X'β\", font_size=16),row=1, col=1)\nfig.update_yaxes(title=dict(text=\"y\", font_size=16),row=1, col=1)\n\nfig.add_trace(\n  go.Scatter(\n    x=eta, \n    y= np.exp(eta),\n    line_shape='spline',\n    mode='lines', name='Poisson fit',\n  ),\n  row=1, col=1\n)\n\n# 3d plot of data\nfig.add_trace(\n  go.Scatter3d(\n    x=eta, \n    y=y3.flatten(),\n    z=poisson.pmf(y3.flatten(), np.exp(eta)),\n    marker=dict(size=1, color=\"blue\"),\n    mode='markers', name='Observations'\n  ),\n  row=2, col=1\n)\n\n# 3d line of data\nfig.add_trace(\n  go.Scatter3d(\n    x=eta, \n    y=np.exp(eta),\n    z=gamma.pdf(x=np.exp(eta),a=np.exp(eta)+1),\n    line=dict(color=\"red\"),\n    mode='lines', name='Poisson fit',\n  ),\n  row=2, col=1\n)\n\npois_mu = np.arange(3,4.5,0.05)\npois_z = np.arange(0,100,1)\nxv, yv = np.meshgrid(pois_mu, pois_z)\npois_pmf = poisson.pmf(yv, np.exp(xv))\n\nfig.add_trace(\n  go.Surface(\n    x=xv, y=yv, z=pois_pmf, \n    opacity=0.2,\n    colorscale=[(0, 'gray'), (1, 'gray')],\n    showscale=False,\n    contours = dict(x= {\"show\": True})\n  ),\n  row=2, col=1\n)\n\nfig.update_layout(\n    scene = dict(\n      aspectratio=dict(x=1.5,y=1.8,z=1),\n      xaxis_title=\"ln(λ) ~ X'β\",\n      yaxis_title=\"y\",\n      zaxis_title=\"Pois(λ)\",\n    ),\n    \n    scene_camera = dict(\n        eye = dict(x=1.8, y=-1.5, z=0.3),\n        center = dict(x=0,y=0,z=-0.35),\n    ),\n    showlegend=False,\n  )\n\nfig.show()\n\n\n                                                \n\n\n\n\nThe top plot shows the exponential relationship linking the systematic component and the observed count \\(y\\), where \\(y\\) is more dispersed at higher values of \\(\\lambda\\) (higher variance).\nThe bottom plot is identical to the first, but adds a third axis with the probability mass of the poisson distribution. Again, the higher variance makes the peak of the poisson probability mass function lower, and its probability density is more spread.\nIt can now help to start to denote this into its random and systematic components, and their link function: \\[\n\\displaylines{\n\\underbrace{y_i \\sim Pois(\\lambda_i)}_{\\text{random}} \\\\\n\\underbrace{\\ln{(\\lambda_i)}}_{\\text{link function}} = \\underbrace{X_i^{\\intercal}\\beta}_{\\text{systematic}}\n}\n\\]\nLet’s dive into this in a bit more detail:\n\nRandom component\nThe random component determines how we want to model the distribution of \\(y\\). For example, if \\(y_i\\) is a count outcome, then it could be well suited to a poisson distribution:\n\\[\ny_i \\sim Pois(\\lambda_i)\n\\]\nThe expected rate of the count is \\(\\lambda_i\\). This means we expect \\(y_i\\) to be around \\(\\lambda_i\\), i.e. \\(\\mathbb{E}[y_i|X_i]=\\lambda_i\\). However, for any individual observation \\(i\\), the actual observed \\(y_i\\) will vary above and below this expected rate. By using poisson, we assume the variance is equal to the mean rate: \\(\\mathbb{V}[y_i|X_i] = \\lambda_i\\). Thus we permit the dispersion of observations to increase if the expected rate is higher.\nThis is why it is called the “random component”: since \\(y_i\\) varies randomly around \\(\\lambda_i\\), following the poisson distribution, it is a random variable.\nBut how do we find a good estimation for \\(\\mathbb{E}[y_i|X_i]=\\lambda_i\\)? Concretely, how do we best map our independent variables \\(X_i\\) to \\(\\lambda_i\\)? This is down to our systematic component and link function.\n\n\nSystematic component and link function\nIn most cases, the systematic component \\(\\eta(X)\\) is usually just a linear transformation of \\(X\\), most often the result of multiplying each value by some good fitted coefficients \\(\\beta\\). It is deterministic (non-random), so it is systematic.\n\\[\n\\eta(X_i) = X_i^{\\intercal}\\beta\n\\]\nThe link function is a way of choosing how to map the systematic component to the natural parameter of the random component. For example, in poisson regression, we use a log link function, which means the systematic component predicts the natural log of the mean rate of the count.\n\\[\n\\ln{(\\lambda_i)} = \\eta(X) = X^{\\intercal}\\beta\n\\]\nOkay, so now we want to find the best values for \\(\\beta\\). These coefficients will transform our features \\(X_i\\) to make the best predictions for \\(\\ln{(\\lambda_i)}\\), given we want to predict \\(y_i\\) as accurately as possible (but permit larger residuals when \\(\\lambda_i\\) is larger, following the poisson distribution).\nTo achieve this: we can try some initial coefficients, calculate the cost function and its first derivative, update the coefficients, and continue to minimize the cost function through gradient descent.\nBut how cumbersome and error prone would it be to write bespoke code for every type of distribution we want to model! Wouldn’t it be nice if we can derive a generic representation for the cost function and its first derivative, so that we can re-use the same code for every type of regression?"
  },
  {
    "objectID": "posts/glm_exponential_dispersion.html#applying-the-exponential-families-to-regression",
    "href": "posts/glm_exponential_dispersion.html#applying-the-exponential-families-to-regression",
    "title": "Generalized Linear Models from scratch",
    "section": "Applying the exponential families to regression",
    "text": "Applying the exponential families to regression\nSo far we have reformulated poisson, binomial and normal regressions into the exponential family form. So now we want to derive a “generic-use formula” to estimate the best coefficients via maximum likelihood estimation for any GLM.\n\nCost function\nFirst let’s define our generic cost function in negative log-likelihood form, assuming the canonical link function is used (so in terms of \\(\\theta_i\\)):\n\n\nMaximising the likelihood is hard, because it involves calculating the total product across every observation. Instead, taking the log likelihood makes everything a sum, far easier to calculate. Also, taking the negative ensures we are looking to minimize the cost.\n\\[\n\\displaylines{\n\\begin{align}\nL(\\theta)\n& = \\prod_{i=1}^n{\n  f(y_i;\\theta_i,\\phi)\n}\n\\\\ & = \\prod_{i=1}^n{\n  \\exp{ \\left\\{\n    \\frac{y \\theta - b(\\theta)}{a(\\phi)}\n    + c(y,\\phi)\n  \\right\\} }\n}\n\\\\ \\therefore \\mathcal{L}(\\theta)\n& = -\\ln \\left\\{ \\prod_{i=1}^n{\n  \\exp{ \\left\\{\n    \\frac{y_i \\theta_i - b(\\theta_i)}{a(\\phi)}\n    + c(y_i,\\phi)\n  \\right\\} }\n} \\right\\}\n\\\\ & = -\\sum_{i=1}^n{ \\left\\{\n  \\frac{y_i \\theta_i - b(\\theta_i)}{a(\\phi)}\n  + c(y_i,\\phi)\n\\right\\} }\n\\\\ & = -\n\\frac{1}{a(\\phi)}\n\\sum_{i=1}^n{ \\bigg\\{\n    y_i \\theta_i - b(\\theta_i)\n\\bigg\\} } -\n\\sum_{i=1}^n{ \\bigg\\{\n  c(y_i,\\phi)\n\\bigg\\}}\n\\end{align}\n}\n\\]\n\n\nOptimization\nCommon methods for optimization include “Newton-Raphson”, “Fisher-Scoring”, “Iteratively-reweighted Least Squares” or “Gradient Descent”. For all of these we need to derive the “score” (or “informant”): the first derivative of the negative log likelihood with respect to \\(\\theta\\). Second order optimization methods, like Newton’s method, also need the second differential: using the inverse hessian as the learning rate instead helps the regression converge faster than gradient descent.\n\\[\n\\displaylines{\n\\begin{align}\n\\beta_{\\text{new}} & := \\beta_{\\text{old}} - \\underbrace{\\frac{\\partial^2 \\mathcal{L}(\\theta)}{\\partial\\theta\\theta^{\\intercal}}^{-1}}_{\\text{Learning rate}} \\times \\underbrace{\\frac{\\partial \\mathcal{L}(\\theta)}{\\partial\\theta}}_{\\text{Score}}\n\\\\ \\\\\n& \\equiv \\beta_{\\text{old}} - H^{-1} \\nabla J\n\\end{align}\n}\n\\]\n\n\nInformant (score function)\nSo first we want to minimize this generic cost function with respect to \\(\\theta\\). Common methods for optimization include “Newton-Raphson”, “Fisher-Scoring”, “Iteratively-reweighted Least Squares” or “Gradient Descent”.\n\n\nIt is trival to connect \\(\\theta\\) to the coefficients \\(\\beta\\), since \\[\n\\displaylines{\n\\frac{\\partial\\theta}{\\partial\\beta_j}=\\frac{\\partial} {\\partial\\beta_j}[\\eta(X)]=X_j\n\\\\ \\iff \\eta(X)=X^{\\intercal}\\beta\n}\n\\]\nTo run any of these, we need to derive the “score” (or “informant”): the first derivative of the negative log likelihood with respect to \\(\\beta\\):\n\\[\n\\displaylines{\n\\begin{align}\n\\arg \\min_\\theta \\left[ \\mathcal{L}(\\theta) \\right]\n& = \\arg \\min_\\theta \\left[\n-\\frac{1}{a(\\phi)}\n\\sum_{i=1}^n{ \\bigg\\{\n    y_i \\theta_i - b(\\theta_i)\n\\bigg\\} }\n\\underbrace{\\cancel{ -\\sum_{i=1}^n{ \\bigg\\{\n  c(y_i,\\phi)\n\\bigg\\}}\n}}_{\n  \\because c() \\text{ is constant with respect to } \\theta\n}\n\\right]\n\\\\ \\\\\n\\therefore \\frac{\\partial \\mathcal{L}(\\theta)}{\\partial\\theta_i}\n& = - \\frac{1}{a(\\phi)} \\sum_{i=1}^n{ \\bigg\\{\n      y_i  - \\frac{\\partial b(\\theta)}{\\partial \\theta}\n  \\bigg\\} }\n\\\\ &\n\\\\\n& \\equiv \\underbrace{\\frac{1}{a(\\phi)}}_{\\text{Constant}} \\sum_{i=1}^n{ \\bigg\\{ \\left( b'(\\theta) - y_i \\right) \\bigg\\} }\n\\end{align}\n}\n\\]\n\n\nThe cost is minimized by differentiating with respect to \\(\\theta\\). Since the derivative of \\(c(.)\\) is zero, it drops out.\nSo hopefully this now gives a bit more intuition behind why \\(b(\\theta)\\) is the integral of the inverse canonical link. Since \\(b'(\\theta)\\) is the activation function, we can see that the score function is just the difference between the predicted value and the actual value, multiplied by the feature. This is the same as the gradient of the cost function used for gradient descent!\nThis also uncovers an interesting property of GLMs - that the average prediction \\(b'(\\theta)\\) must be equal to the average value of \\(\\bar{y}\\) too:\n\\[\n\\displaylines{\n\\begin{align}\n\\frac{\\partial \\mathcal{L}(\\theta)}{\\partial\\theta}\n& = 0 \\text{ (cost is minimized at stationary point)}\n\\\\\n& = \\cancel{\\frac{1}{a(\\phi)}}  \\times \\sum_{i=1}^n{ \\bigg\\{ \\left( b'(\\theta) - y_i \\right) \\bigg\\} }\n\\\\\n\\therefore\n\\sum_{i=1}^n{ y_i } & = \\sum_{i=1}^n{ b'(\\theta) }\n\\\\ \\\\\n\\therefore\n\\frac{\\sum_{i=1}^n{ y_i }}{n} & = \\mathbb{E}[y] = \\frac{\\sum_{i=1}^n{ b'(\\theta)}}{n} = b'(\\theta)\n\\\\ \\\\\n\\Rightarrow\n\\mathbb{E}[y] & = b'(\\theta)\n\\end{align}\n}\n\\]\n\n\nThe cost is minimized at the stationary point. So by equating to zero, \\(a(.)\\) drops out as it is a constant.\n\n\nHessian\nFinally, we can derive the Hessian matrix, which is the second derivative of the cost function with respect to \\(\\beta\\).\n\\[\n\\displaylines{\n\\begin{align}\n\\frac{\\partial^2 \\mathcal{L}(\\theta)}{\\partial\\theta\\theta^{\\intercal}}\n& = \\frac{\\partial}{\\partial\\theta}\\left(\n  \\frac{1}{a(\\phi)} \\sum_{i=1}^n{ \\bigg\\{ \\left( b'(\\theta) - y_i \\right) X_i \\bigg\\} }\n\\right)\n\\\\\n& = -\\frac{1}{a(\\phi)} \\sum_{i=1}^n{ \\bigg\\{\n      \\frac{\\partial b'(\\theta)}{\\partial \\theta} X_i^2\n  \\bigg\\} }\n\\end{align}\n}\n\\]\nNow we have all the parts we need to optimize the coefficients for any GLM using maximum likelihood estimation."
  },
  {
    "objectID": "posts/glm_exponential_dispersion.html#notation-for-generalized-linear-models",
    "href": "posts/glm_exponential_dispersion.html#notation-for-generalized-linear-models",
    "title": "Generalized Linear Models from scratch",
    "section": "Notation for Generalized linear models",
    "text": "Notation for Generalized linear models\nIt can be shown that many common distributions can be reformulated into the “Exponential Dispersion Family of Distributions”. This generic representation makes it easier to re-use the same code to run a regression, rather than hand calculate each pdf in different ways.\nFirst let’s define some generic notation for generalized linear models:\n\\[\n\\displaylines{\n\\underbrace{g{(\\xi_i)}}_{\\text{link function}} = \\underbrace{\\eta(X_i)}_{\\text{systematic}} \\\\\n\\underbrace{y_i \\sim f_y(\\xi_i,\\phi_i)}_{\\text{random}}\n}\n\\]\n\\(\\xi_i\\) is a shape parameter, governing the shape of the distribution (e.g. in poisson, the expected value is the mean \\(\\xi_i=\\lambda_i\\)). Or for bernoulli, \\(\\xi_i=p_i\\). In fact for many distributions, \\(\\xi_i\\) is the expected value of \\(y_i\\) i.e. \\(\\xi_i = \\mu_i\\).\n\\(\\phi\\) is a dispersion parameter, governing the spread of the data (e.g. gaussian has the standard deviation \\(\\phi = \\sigma\\)). It is not always necessary if the dispersion is determined by the shape parameter already (e.g. in poisson the variance is already equated to the expected rate \\(\\lambda_i\\)).\nHere are some examples of the common forms of poisson, bernoulli and gaussian probability distribution functions \\(y \\sim f_{\\theta_i}(\\mu_i)\\), along with some choices for link functions \\(g(\\mu)\\) to use in regression:\n\n\n\n\n\n\n\n\nType\nProbability Density Function:\nLink Function for Regression:\n\n\n\n\nCount\\(\\xi_i = \\mu_i \\equiv \\lambda_i\\)\n\\(y_i \\sim Pois(\\mu_i)\\)\\(y_i \\sim \\frac{\\mu_i^{y} \\times e^{-\\mu_i}}{y!}\\)\n\\(g(\\mu) = \\ln{[\\mu_i]}\\)(log-link)\n\n\nBinary\\(\\xi_i = \\mu_i \\equiv p_i\\)\n\\(y_i \\sim Bern(\\mu_i)\\)\\(y_i \\sim \\mu_i^y \\times (1-\\mu_i)^{(1-y)}\\)\n\\(g(\\mu) = \\ln{\\left[\\frac{\\mu_i}{1-\\mu_i}\\right]}\\)(logit-link)\n\n\nBinary\\(\\xi_i = \\mu_i \\equiv p_i\\)\n\\(y_i \\sim Bern(\\mu_i)\\)\\(y_i \\sim \\mu_i^y \\times (1-\\mu_i)^{(1-y)}\\)\n\\(g(\\mu) = \\Phi^{-1}[\\mu_i]\\)(probit-link)\n\n\nNormal\\(\\xi_i = \\mu_i\\)\n\\(y_i \\sim N(\\mu_i,\\sigma^2)\\)\\(y_i \\sim \\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\frac{1}{2}\\left(\\frac{y_i-\\mu_i}{\\sigma^2}\\right)^2}\\)\n\\(g(\\mu) = \\mu\\)(identity-link)"
  }
]