[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Sciencing Data",
    "section": "",
    "text": "Deriving Logistic Regression Coefficients\n\n\n\n\n\n\nMaximum Likelihood\n\n\nGeneralized Linear Models\n\n\n\n\n\n\n\n\n\nFeb 21, 2025\n\n\nChris Kelly\n\n\n\n\n\n\n\n\n\n\n\n\nShrinkage priors for Lasso and Ridge\n\n\n\n\n\n\nBayes\n\n\nMaximum Likelihood\n\n\nRegularization\n\n\nLaplace\n\n\n\n\n\n\n\n\n\nFeb 24, 2024\n\n\nChris Kelly\n\n\n\n\n\n\n\n\n\n\n\n\nOLS vs MLE with gaussian noise\n\n\n\n\n\n\nLinear Models\n\n\nMaximum Likelihood\n\n\nGauss-Markov\n\n\n\n\n\n\n\n\n\nFeb 23, 2024\n\n\nChris Kelly\n\n\n\n\n\n\n\n\n\n\n\n\nSandwiches: robust covariance error estimation\n\n\n\n\n\n\nLinear Models\n\n\nGauss-Markov\n\n\nStandard errors\n\n\n\n\n\n\n\n\n\nFeb 22, 2024\n\n\nChris Kelly\n\n\n\n\n\n\n\n\n\n\n\n\nBLUE coefficients: bias and efficiency\n\n\n\n\n\n\nLinear Models\n\n\nOLS\n\n\nGauss-Markov\n\n\n\n\n\n\n\n\n\nFeb 21, 2024\n\n\nChris Kelly\n\n\n\n\n\n\n\n\n\n\n\n\nDeriving OLS coefficients (multivariate)\n\n\n\n\n\n\nLinear models\n\n\nOLS\n\n\n\n\n\n\n\n\n\nFeb 20, 2024\n\n\nChris Kelly\n\n\n\n\n\n\n\n\n\n\n\n\nMean Squared Error vs Mean Absolute Error\n\n\n\n\n\n\nCost functions\n\n\n\n\n\n\n\n\n\nFeb 8, 2024\n\n\nChris Kelly\n\n\n\n\n\n\n\n\n\n\n\n\nDifferentiation formulae\n\n\n\n\n\n\nFundamentals\n\n\n\n\n\n\n\n\n\nFeb 8, 2024\n\n\nChris Kelly\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/blue_ols.html",
    "href": "posts/blue_ols.html",
    "title": "BLUE coefficients: bias and efficiency",
    "section": "",
    "text": "What are we exploring?\n\n\n\nProving that the OLS coefficient is the best linear unbiased estimator."
  },
  {
    "objectID": "posts/blue_ols.html#are-the-ols-coefficients-blue",
    "href": "posts/blue_ols.html#are-the-ols-coefficients-blue",
    "title": "BLUE coefficients: bias and efficiency",
    "section": "Are the OLS coefficients “BLUE”?",
    "text": "Are the OLS coefficients “BLUE”?\nWe find a unique solution to the set of coefficients that minimize the sum of squared residuals analytically (see its derivation here):\n\\[\n\\hat{\\beta}^{OLS}=(X^{\\intercal}X)^{-1}X^{\\intercal}y\n\\]\nHowever, how do we know if these coefficients are the best ones we can estimate?\nFor the estimated coefficients to be the Best Linear Unbiased Estimator (i.e. “BLUE”):\n\nThe best estimator has to be unbiased: \\(E[\\hat{\\beta}^*] = \\beta\\)\nAnd among all possible linear, unbiased estimators, it must have the smallest variance: \\(V[\\hat{\\beta}^{*}] &lt; V[\\hat{\\beta}^{Z}]\\)\n\nWe want to ensure our OLS estimate is the best, i.e. that \\(\\hat{\\beta}^{OLS} = \\hat{\\beta}^{*}\\). To achieve this, first we need to confirm it is unbiased. Then given this is true, we can check that the coefficient is most efficient vs all other unbiased estimators.\n\n\n\n\n\n\nGauss Markov Assumptions\n\n\n\nAlong the way, we will outline the Gauss-Markov assumptions utilised that ensure the OLS coefficient is BLUE."
  },
  {
    "objectID": "posts/blue_ols.html#setting-the-scene",
    "href": "posts/blue_ols.html#setting-the-scene",
    "title": "BLUE coefficients: bias and efficiency",
    "section": "Setting the scene",
    "text": "Setting the scene\n\nThe true coefficient and model\n\\(\\beta\\) is the true (unobserved) relationship between all the relevant explanatory features, \\(X\\), and their associated observed outcomes, \\(y\\). In other words, if we observed the entire population of data, it is the relationship we would find.\nConcretely, we assume the outcome is a linear function of all its relevant features. This “true model” perfectly predicts the outcome, except for random noise \\(\\epsilon\\) that influences the observed outcome: \\(y = X\\beta + \\epsilon\\)\n\n\nThe estimated coefficient\n\\(\\hat{\\beta}\\) is our estimated coefficient for the true relationship \\(\\beta\\). In reality, we estimate \\(\\hat{\\beta}\\) from the small, finite sample of size \\(n\\) that is collected, not the whole population. Given any random sample could be collected, we can term the coefficient resulting from the optimum estimation proceedure to be \\(\\hat{\\beta}^*\\). We want to understand if \\(\\hat{\\beta}^{OLS} = \\hat{\\beta}^*\\).\n\n\nThe expected estimated coefficient\n\\(E[\\hat{\\beta}]\\) is the “expected” estimated coefficient. Imagine we repeat the action of estimating the coefficient \\(\\hat{\\beta}\\) many times, each time collecting a new sample (where each observation is sampled i.i.d), and recording the value for the estimated coefficient. \\(E[\\hat{\\beta}]\\) would then be the average of all of those estimated coefficients. If the OLS coefficient is unbiased, then the expected coefficient estimate should be equal to the true one, \\(E[\\hat{\\beta}^{OLS}]=\\beta\\).\n\n\nThe variance of the estimated coefficient\n\\(V[\\hat{\\beta}]\\) is the variance of the estimated coefficient. It determines how much we might expect our estimate \\(\\hat{\\beta}\\) to differ from the true \\(\\beta\\) for any sample drawn. Given the OLS coefficient has been shown to be unbiased, if it is BLUE we expect its variance to be lower than another other unbiased choice \\(\\hat{\\beta}^{Z}\\). Concretely, we want to find \\(V[\\hat{\\beta}^{OLS}] &lt; V[\\hat{\\beta}^{Z}]\\)."
  },
  {
    "objectID": "posts/blue_ols.html#bias",
    "href": "posts/blue_ols.html#bias",
    "title": "BLUE coefficients: bias and efficiency",
    "section": "Bias",
    "text": "Bias\nOften our small finite samples of size \\(n\\) are not a perfect reflection of the population they are drawn from. This “sampling error” means we might estimate a different relationship between \\(X\\) and \\(y\\) than the true relationship of the population, i.e. \\(\\hat{\\beta} \\neq \\beta\\).\nHowever, we should expect our estimated coefficient to be equal to the true value on average. This means we do not want to have a bias towards the estimate being systematically too small or too large, for example. In other words, if we repeated the whole proceedure thousands of times (each time taking new samples, and estimating a coefficient from the new sample) then the average of all the estimated coefficients values should be equal to the true value, i.e. \\(E[\\hat{\\beta}] = \\beta\\).\nRecall that we believe there is a true model that follows the form:\n\\[\ny = X\\beta + \\epsilon\n\\]\n\n\n\n\n\n\nGM1: Linearity \n\n\n\nThe formula above relies on the first Gauss-Markov assumption - that the dependent variable \\(y\\) is assumed to be a linear function of the variables \\(X\\). Note that implies that the proper functional form has been selected (i.e. the relationship is linear) and there are no omitted variables - a huge assumption!\n\n\nIf we substitute this into our estimated coefficient:\n\\[\n\\displaylines{\n\\begin{align}\n\\hat{\\beta}^{OLS} & = (X^{\\intercal}X)^{-1}X^{\\intercal}y\n\\\\ & = (X^{\\intercal}X)^{-1}X^{\\intercal}(X\\beta+\\epsilon)\n\\\\ & = (X^{\\intercal}X)^{-1}X^{\\intercal}X\\beta+(X^{\\intercal}X)^{-1}X^{\\intercal}\\epsilon\n\\\\ & = \\beta+(X^{\\intercal}X)^{-1}X^{\\intercal}\\epsilon\n\\end{align}\n}\n\\]\nwe show that the estimated coefficient \\(\\hat{\\beta}^{OLS}\\) will differ from the true value depending on the random error \\(\\epsilon\\) associated with the particular finite sample collected.\nNow let’s now take the expectation, to determine when the coefficient is unbiased. In other words, what is the “average” coefficient if we took the sample many times: \\[\n\\displaylines{\n\\begin{align}\nE[\\hat{\\beta}^{OLS}]\n& = \\beta +(X^{\\intercal}X)^{-1}X^{\\intercal}E[\\epsilon]\n\\\\ & = \\beta & \\iff E[\\epsilon] & = 0\n\\end{align}\n\\\\\n}\n\\]\nWe find that the coefficient is unbiased as long as the expected error is also zero.\n\n\n\n\n\n\nGM2: Strict Exogeneity \n\n\n\nThe second Gauss-Markov assumption is strict exogeneity, where the expected error is zero for all feature values: \\(E[\\epsilon|X] = 0\\). By definition, the weaker exogeneity statement of \\(E[\\epsilon] = 0\\) is implied by having the expected error conditional being equal to zero."
  },
  {
    "objectID": "posts/blue_ols.html#efficiency",
    "href": "posts/blue_ols.html#efficiency",
    "title": "BLUE coefficients: bias and efficiency",
    "section": "Efficiency",
    "text": "Efficiency\nTo be the “best”, the OLS estimator also needs to be efficient. This means that it has the lowest variance of all unbiased estimators. This section looks to prove this.\n\nCoefficient variance for OLS\nFirst, let’s derive the variance from the coefficients estimated using OLS, termed \\(V[\\hat{\\beta}^{OLS}]\\). As before, we substitute the true model \\(y = X\\beta + \\epsilon\\) into the coefficient estimated through OLS:\n\\[\n\\displaylines{\n\\begin{align}\n\\hat{\\beta}^{OLS}-\\beta\n& = \\beta +  ((X^{\\intercal}X)^{-1}X^{\\intercal}\\epsilon) - \\beta \\\\\n& = (X^{\\intercal}X)^{-1}X^{\\intercal}\\epsilon\n\\\\ \\\\\n\\therefore\n(\\hat{\\beta}^{OLS}-\\beta)(\\hat{\\beta}^{OLS}-\\beta)^{\\intercal}\n& = ((X^{\\intercal}X)^{-1}X^{\\intercal}\\epsilon)((X^{\\intercal}X)^{-1}X^{\\intercal}\\epsilon)^{\\intercal}\n\\\\ & =\n(X^{\\intercal}X)^{-1}X^{\\intercal}\\epsilon\\epsilon^{\\intercal}X(X^{\\intercal}X)^{-1} \\\\ \\\\\n\\therefore\nV(\\hat{\\beta}^{OLS}) & = E[(\\hat{\\beta}^{OLS}-\\beta)(\\hat{\\beta}^{OLS}-\\beta)^{\\intercal}]\n\\\\ & =\n(X^{\\intercal}X)^{-1}X^{\\intercal}E[\\epsilon\\epsilon^{\\intercal}]X(X^{\\intercal}X)^{-1}\n\\end{align}\n}\n\\]\nThis is sometimes called the sandwich estimator - post soon to follow on this!\n\n\nCoefficient variance assuming “spherical errors”\nWe can simplify this further by appling some assumptions to the estimated error variance \\(E[\\epsilon\\epsilon^{\\intercal}]\\):\n\\[\n\\displaylines{\n\\begin{align}\nE[\\epsilon \\epsilon^{\\intercal}] & =  \n\\begin{bmatrix}\nE[\\epsilon_1^2] & \\cdots & E[\\epsilon_1\\epsilon_n] \\\\\n\\vdots & \\ddots & \\vdots \\\\\nE[\\epsilon_n\\epsilon_1] & \\cdots & E[\\epsilon_n^2]\n\\end{bmatrix} \\\\ \\\\\n& = \\begin{bmatrix}\n\\frac{1}{n}\\sum_{i=1}^{n}{\\epsilon_i^2} & \\cdots & 0 \\\\\n\\vdots & \\ddots & \\vdots \\\\\n0 & \\cdots & \\frac{1}{n}\\sum_{i=1}^{n}{\\epsilon_i^2}\n\\end{bmatrix} \\\\ \\\\\n& = \\begin{bmatrix}\n\\hat{\\sigma}^2 & \\cdots & 0 \\\\\n\\vdots & \\ddots & \\vdots \\\\\n0 & \\cdots & \\hat{\\sigma}^2\n\\end{bmatrix} \\\\ \\\\\n& = \\hat{\\sigma}^2I\n\\end{align}\n}\n\\]\nHow can we jump to this result? Well we are making two assumptions:\n\nNo serial correlation: \\(\\rho_{\\epsilon_{i},\\epsilon_{i \\neq j}} = 0\\). No correlation between sample errors means that \\(E[\\epsilon_i \\epsilon_{j \\neq i}] = 0\\), and hence the off-diagonals of the error covariance matrix are zero.\nHomoskedasticity: the assumption of uniform error variance for all samples means that \\(V[\\epsilon_i^2] = V[\\epsilon_{j \\neq i}^2] = \\hat{\\sigma}^2\\). And our best approximation for \\(\\hat{\\sigma}^2\\) is simply taking the average squared error: \\(\\hat{\\sigma}^2 = \\frac{1}{n}\\sum_{i=1}^{n}{\\epsilon_i^2}\\)\n\n\n\n\n\n\n\nGM3: Spherical errors \n\n\n\nThe third Gauss-Markov assumption is spherical errors, \\(E[\\epsilon\\epsilon^{\\intercal}|X] = 0\\). This means that the outer product of the expected errors is a scalar matrix, which implies no serial correlation and homoskedasticity.\n\n\n\n\nIt is especially important to make the right assumptions about \\(E[\\epsilon\\epsilon^{\\intercal}|X]\\) as it impacts where our estimate of the standard errors is correct! We will dive into what happens to SE if we violate these assumptions in another post.\nSince we now see that \\(\\hat{\\sigma}^2\\) is a scalar matrix, we can thus simplfy the variance formula further:\n\\[\n\\displaylines{\n\\begin{align}\nV(\\hat{\\beta}^{OLS}) & =\n(X^{\\intercal}X)^{-1}X^{\\intercal}E[\\epsilon\\epsilon^{\\intercal}]X(X^{\\intercal}X)^{-1}\n\\\\ & =\n(X^{\\intercal}X)^{-1}X^{\\intercal} \\hat{\\sigma}^2I X(X^{\\intercal}X)^{-1}\n\\\\ & =\n\\hat{\\sigma}^2\n\\cancel{(X^{\\intercal}X)^{-1}}\n\\cancel{X^{\\intercal} X}\n(X^{\\intercal}X)^{-1}\n\\\\ & = \\hat{\\sigma}^2(X^{\\intercal}X)^{-1}\n\\end{align}\n}\n\\]\n\n\nFormulating an alternative unbiased coefficient\nNext step - lets formulate another estimator, \\(\\hat{\\beta}^{z}\\), which differs from \\(\\hat{\\beta}^{OLS}\\) by a non-zero matrix \\(A\\). See how they both differ below:\n\\[\n\\displaylines{\n\\begin{align}\n\\hat{\\beta}^{OLS} & =(X^{\\intercal}X)^{-1}X^{\\intercal}y \\\\\n\\hat{\\beta}^{Z} & =\\left((X^{\\intercal}X)^{-1}X^{\\intercal}+A\\right)y\n\\end{align}\n}\n\\]\nNow we need to ensure this new estimator is not biased. So by taking the expectation in the same was as for OLS…\n\\[\n\\displaylines{\n\\begin{align}\nE[\\hat{\\beta}^{Z}] & = E\\left[ \\left((X^{\\intercal}X)^{-1}X^{\\intercal}+A\\right)y \\right]\n\\\\ & = \\left((X^{\\intercal}X)^{-1}X^{\\intercal}+A\\right)(X\\beta+ \\cancel{E\\left[\\epsilon \\right]}) & \\because E[\\epsilon] = 0\n\\\\ & = (X^{\\intercal}X)^{-1}X^{\\intercal}X\\beta+AX\\beta\n\\\\ & = \\beta+AX\\beta\n\\end{align}\n}\n\\]\nSo the estimator is only unbiased iff \\(AX=0\\). This is important to note when comparing the variance between unbiased coefficients - see below!\n\n\nVariance of the alternative unbiased coefficient\nJust like before, we calculate the variance:\n\\[\n\\displaylines{\n\\begin{align}\nV[\\hat{\\beta}^{Z}]\n& = V\\left[ \\left((X^{\\intercal}X)^{-1}X^{\\intercal}+A\\right)y \\right]\n\\\\ & = \\left((X^{\\intercal}X)^{-1}X^{\\intercal}+A\\right) V[y] \\left((X^{\\intercal}X)^{-1}X^{\\intercal}+A\\right)^{\\intercal}\n\\\\ & = \\hat{\\sigma}^2 \\left((X^{\\intercal}X)^{-1}X^{\\intercal}+A\\right) \\left((X^{\\intercal}X)^{-1}X^{\\intercal}+A\\right)^{\\intercal}\n& \\because E[\\epsilon \\epsilon^{\\intercal}|X] = 0\n\\\\ & = \\hat{\\sigma}^2\\left((X^{\\intercal}X)^{-1}X^{\\intercal}+A\\right) \\left(X(X^{\\intercal}X)^{-1}+A^{\\intercal}\\right)\n\\\\ & = \\hat{\\sigma}^2 \\left(\n(X^{\\intercal}X)^{-1}X^{\\intercal} X(X^{\\intercal}X)^{-1} + AX(X^{\\intercal}X)^{-1} + (X^{\\intercal}X)^{-1}X^{\\intercal}A^{\\intercal} + AA^{\\intercal}\n\\right)\n\\\\ & = \\hat{\\sigma}^2\n(X^{\\intercal}X)^{-1} + \\hat{\\sigma}^2AA^{\\intercal}\n& \\because AX = 0\n\\\\ & = V[\\beta^{OLS}] + \\hat{\\sigma}^2AA^{\\intercal}\n\\end{align}\n}\n\\]\nNow since AA is surely a positive semi-definite matrix, then we know that \\(V[\\hat{\\beta}^{Z}] &gt; V[\\hat{\\beta}^{OLS}]\\).\nWe have shown that \\(\\hat{\\beta}^{OLS}\\) has the smallest variance among all unbiased estimators!"
  },
  {
    "objectID": "posts/blue_ols.html#summarising-the-gauss-markov-assumptions",
    "href": "posts/blue_ols.html#summarising-the-gauss-markov-assumptions",
    "title": "BLUE coefficients: bias and efficiency",
    "section": "Summarising the Gauss-markov assumptions",
    "text": "Summarising the Gauss-markov assumptions\nAlong the way, we showed where assumptions were needed to ensure the OLS coefficient estimation is BLUE.\nWe actually missed one out, but it is actually quite trivial to see from the OLS coefficient formula:\n\\[\n\\hat{\\beta}^{OLS}=(X^{\\intercal}X)^{-1}X^{\\intercal}y\n\\]\n\n\n\n\n\n\nGM4: Full rank \n\n\n\nThe matrix \\(X\\) must be of full rank \\(k\\), so that it is possible to invert the matrix \\(X^{\\intercal}X\\). This is equivalent to having no perfect multi-collinearity.\n\n\nWe have now collected our full set of Gauss-Markov assumptions required for the OLS coefficient to be BLUE:\n\nLinearity\nStrict Exogeneity\nSpherical Errors\nFull rank"
  },
  {
    "objectID": "posts/mse_mean_mae_median.html",
    "href": "posts/mse_mean_mae_median.html",
    "title": "Mean Squared Error vs Mean Absolute Error",
    "section": "",
    "text": "What we are solving\n\n\n\nMinimizing MAE tends predictions towards the sample median, whereas for MSE predictions tend towards the sample mean.\n\n\nIn the absence of informative features, an ML algorithm minimizing the sum of squared errors will tend towards predicting the mean of the sample. However, minimizing the sum of absolute errors will tend towards predicting the median of the sample.\nThis post dives into why this is the case.\n\nMinimizing residual sum-of-squares\nLet’s define the residual for sample \\(i\\) as \\(\\epsilon_i\\). We now want to find the prediction \\(\\hat{y}\\) that minimizes the sum of all squared residuals (i.e. where the gradient is zero):\n\\[\n\\displaylines{\n\\begin{align}\n\\min_\\hat{y}{\\left[\\sum_{i=1}^N{\\epsilon_i^2}\\right]}\n\\Rightarrow &\n\\frac{\\partial}{\\partial \\hat{y}} \\sum_{i=1}^N{\\epsilon_i^2} \\\\ = &\n\\frac{\\partial \\left( \\sum_{i=1}^N{\\epsilon_i^2} \\right) }{\\partial\\epsilon}\n\\left( \\frac{\\partial\\epsilon}{\\partial \\hat{y} } \\right) \\\\ = &\n\\sum_{i=1}^N 2\\epsilon_i \\left( \\frac{\\partial\\epsilon_i}{\\partial \\hat{y} }\\right) = 0\n\\end{align}\n}\n\\]\nWe can now substitue in \\(\\epsilon_i = y - \\hat{y}\\):\n\\[\n\\displaylines{\n\\begin{align}\n\\sum_{i=1}^N 2\\epsilon_i \\left( \\frac{\\partial\\epsilon_i}{\\partial \\hat{y} }\\right) &\n= \\sum_{i=1}^N2( y_i- \\hat{y})\\left(\\frac{\\partial( y_i-\\hat{y})}{\\partial \\hat{y}}\\right) \\\\  &\n= \\sum_{i=1}^N2( y_i- \\hat{y} )(-1) \\\\ &\n= \\sum_{i=1}^N2( y_i) - 2ny = 0 \\\\ &\n\\therefore n \\hat{y} = \\sum_{i=1}^N( y_i) \\\\ &\n\\therefore \\hat{y} = \\frac{\\sum_{i=1}^N{y_i}}{n} = \\bar{y}\n\\end{align}\n}\n\\]\nThus we can see that the prediction that minimizes the sum of squared residuals, is simply the mean.\n\n\nMinimize sum of absolute residuals\nWe now do the same think again, but this time look to minimize the sum of all absolute residuals instead.\n\\[\n\\displaylines{\n\\begin{align}\n\\min_\\hat{y}{\\left[\\sum_{i=1}^N{\\mid \\epsilon_i \\mid}\\right]}\n\\Rightarrow &\n\\frac{\\partial}{\\partial \\hat{y}} \\sum_{i=1}^N{\\left(\\epsilon_i^2\\right)^{1/2}} \\\\ = &\n\\frac{\\partial \\sum_{i=1}^N{ \\left(\\epsilon_i^2\\right)^{1/2} } }{\\partial\\epsilon_i^2}\n\\times \\frac{\\partial\\epsilon_i^2}{\\partial \\epsilon_i }\n\\times \\frac{\\partial\\epsilon_i}{\\partial \\hat{y} }  \\\\ = &\n\\frac{1}{2} \\sum_{i=1}^N{ \\left(\\epsilon_i^2\\right)^{-1/2} }\n\\times 2 \\epsilon_i\n\\times \\frac{\\partial\\epsilon_i}{\\partial \\hat{y} }  \\\\ = &\n\\sum_{i=1}^N{ \\left(\\epsilon_i^2\\right)^{-1/2} } \\times \\epsilon_i\n\\times \\frac{\\partial\\epsilon_i}{\\partial \\hat{y} }  \\\\ = &\n\\sum_{i=1}^N \\frac{\\epsilon_i}{\\mid \\epsilon_i \\mid} \\left( \\frac{\\partial\\epsilon_i}{\\partial \\hat{y} }\\right) = 0\n\\end{align}\n}\n\\]\nAnd similarly to before, we can now substitute in \\(\\epsilon_i = y - \\hat{y}\\):\n\\[\n\\displaylines{\n\\begin{align}\n\\sum_{i=1}^N \\frac{\\epsilon_i}{\\mid \\epsilon_i \\mid} \\left( \\frac{\\partial\\epsilon_i}{\\partial \\hat{y} }\\right) &\n= \\sum_{i=1}^N \\frac{ y_i- \\hat{y} }{\\mid y_i- \\hat{y} \\mid}\\left(\\frac{\\partial( y_i-\\hat{y})}{\\partial \\hat{y}}\\right) \\\\  &\n= \\sum_{i=1}^N \\frac{ y_i- \\hat{y} }{\\mid y_i- \\hat{y} \\mid}(-1) = 0\n\\end{align}\n}\n\\]\nNow \\(f(x) = \\frac{ x }{\\mid x \\mid}\\) is an cool transformation, keeping its sign but getting rid of the magnitude of the size of \\(x\\), i.e.:\n\n\\(f(x &lt; 0) = -1\\)\n\\(f(x &gt; 0) = 1\\)\n\nSo to ensure that \\(\\sum f(\\epsilon_i)=0\\), we need to pick a value for \\(\\hat{y}\\) that means half of the errors are \\(&lt;0\\) and half of the errors are \\(&gt;0\\).\nSo that means half the errors must be negative, and half are positive. So \\(\\hat{y}\\) has to be the median value!\nFin."
  },
  {
    "objectID": "posts/ols_coef_derivation.html",
    "href": "posts/ols_coef_derivation.html",
    "title": "Deriving OLS coefficients (matrix algebra)",
    "section": "",
    "text": "What we are exploring\n\n\n\nDeriving a unique, analytical solution to the set of coefficients that minimize the sum of squared residuals."
  },
  {
    "objectID": "posts/ols_coef_derivation.html#summary",
    "href": "posts/ols_coef_derivation.html#summary",
    "title": "Deriving OLS coefficients (matrix algebra)",
    "section": "Summary",
    "text": "Summary\nThe cost function for OLS is the sum of squared residuals, \\(\\hat{\\epsilon}^{\\intercal}\\hat{\\epsilon}\\). In order to fit a good linear model, we want to find optimum values for the estimated vector of coefficients, \\(\\hat{\\beta}^*\\), that minimizes this cost function.\nFirst we do partial differentiation of the cost function with respect to the coefficients. Finding the coefficient values where the partial differential is equal to zero reveals the stationary points of the cost function. For OLS in particular, we can find a unique solution for the choice of coefficients that can be found analytically. The hessian matrix then further proves that this is a global minima."
  },
  {
    "objectID": "posts/ols_coef_derivation.html#deriving-the-optimum-coefficients",
    "href": "posts/ols_coef_derivation.html#deriving-the-optimum-coefficients",
    "title": "Deriving OLS coefficients (matrix algebra)",
    "section": "Deriving the optimum coefficients",
    "text": "Deriving the optimum coefficients\n\n0. Defining the notation\nFor a sample \\(i\\), we observe an outcome \\(y_i\\). \\(y\\) is a vector of all \\(n\\) observed outcomes.\n\\[\n\\underset{n \\times 1} {y} =\n\\begin{bmatrix}\n    y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_{n-1} \\\\ y_n\n\\end{bmatrix}\n\\]\nWe also observe \\(k\\) features for every sample \\(i\\). \\(X\\) is a matrix of these observed features. Note the first column is usually all ones, to include an intercept to optimize (or “bias” term).\n\\[\n\\underset{n \\times k} {X} =\n\\begin{bmatrix}\n  1 & x_{11} & \\cdots & x_{1,k-1} & x_{1,k} \\\\\n  1 & x_{21} & \\cdots & x_{2,k-1} & x_{2,k} \\\\\n  1 & x_{31} & \\cdots & x_{3,k-1} & x_{3,k} \\\\\n  \\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\\n  1 & x_{n-2,1} & \\cdots & x_{n-2,k-1} & x_{n-2,k} \\\\\n  1 & x_{n-1,1} & \\cdots & x_{n-1,k-1} & x_{n-1,k} \\\\\n  1 & x_{n,1} & \\cdots & x_{n,k-1} & x_{n,k}\n\\end{bmatrix}\n\\]\nThe contribution of each feature to the prediction is estimated by the coefficients \\(\\hat{\\beta}\\).\n\\[\n\\underset{k \\times 1} {\\hat{\\beta}} =\n\\begin{bmatrix}\n  \\beta_0 \\\\\n  \\beta_1 \\\\\n  \\vdots \\\\\n  \\beta_{k-1} \\\\\n  \\beta_{k}\n\\end{bmatrix}\n\\]\nWe make predictions, \\(\\hat{y}\\), by calculating the dot product of the features \\(X\\) and the coefficients \\(\\hat{\\beta}\\).\n\\[\n\\hat{y} = X \\hat{\\beta}\n\\]\nwhich is shorthand for this:\n\\[\n\\displaylines{\n\\begin{align}\n\\begin{bmatrix}\n  \\hat{y}_1 \\\\ \\hat{y}_2 \\\\ \\hat{y}_3 \\\\ \\vdots \\\\ \\hat{y}_{n-2} \\\\ \\hat{y}_{n-1} \\\\ \\hat{y}_n\n\\end{bmatrix} & =\n\\begin{bmatrix}\n  1 & x_{11} & \\cdots & x_{1,k-1} & x_{1,k} \\\\\n  1 & x_{21} & \\cdots & x_{2,k-1} & x_{2,k} \\\\\n  1 & x_{31} & \\cdots & x_{3,k-1} & x_{3,k} \\\\\n  \\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\\n  1 & x_{n-2,1} & \\cdots & x_{n-2,k-1} & x_{n-2,k} \\\\\n  1 & x_{n-1,1} & \\cdots & x_{n-1,k-1} & x_{n-1,k} \\\\\n  1 & x_{n,1} & \\cdots & x_{n,k-1} & x_{n,k}\n\\end{bmatrix}\n\\begin{bmatrix}\n  \\hat{\\beta}_0 \\\\\n  \\hat{\\beta}_1 \\\\\n  \\vdots \\\\\n  \\hat{\\beta}_{k-1} \\\\\n  \\hat{\\beta}_{k}\n\\end{bmatrix}\n\\\\ \\\\ & =\n\\begin{bmatrix}\n  \\hat{\\beta}_0 + \\hat{\\beta}_{1}x_{1,1} + \\cdots + \\hat{\\beta}_{k-1}x_{1,k-1} + \\hat{\\beta}_{k}x_{1,k} \\\\\n  \\hat{\\beta}_0 + \\hat{\\beta}_{1}x_{2,1} + \\cdots + \\hat{\\beta}_{k-1}x_{2,k-1} + \\hat{\\beta}_{k}x_{2,k} \\\\\n  \\hat{\\beta}_0 + \\hat{\\beta}_{1}x_{3,1} + \\cdots + \\hat{\\beta}_{k-1}x_{3,k-1} + \\hat{\\beta}_{k}x_{3,k} \\\\\n  \\vdots \\\\\n  \\hat{\\beta}_0 + \\hat{\\beta}_{1}x_{n-2,1} + \\cdots + \\hat{\\beta}_{k-1}x_{2,k-1} + \\hat{\\beta}_{k}x_{n-2,k} \\\\\n  \\hat{\\beta}_0 + \\hat{\\beta}_{1}x_{n-1,1} + \\cdots + \\hat{\\beta}_{k-1}x_{2,k-1} + \\hat{\\beta}_{k}x_{n-1,k} \\\\\n  \\hat{\\beta}_0 + \\hat{\\beta}_{1}x_{n,1} + \\cdots + \\hat{\\beta}_{k-1}x_{2,k-1} + \\hat{\\beta}_{k}x_{n,k} \\\\\n\\end{bmatrix}\n\\end{align}\n}\n\\]\nThe residual is the difference between the true outcome and the model prediction.\n\\[\n\\hat{\\epsilon} = y_i -\\hat{y}_i\n\\]\nwhich is shorthand for this:\n\\[\n\\begin{bmatrix}\n  \\hat{\\epsilon_1} \\\\\n  \\hat{\\epsilon_2} \\\\\n  \\hat{\\epsilon_3} \\\\\n  \\vdots \\\\\n  \\hat{\\epsilon_{n-2}} \\\\\n  \\hat{\\epsilon_{n-1}} \\\\\n  \\hat{\\epsilon_{n}}\n\\end{bmatrix} =\n\\begin{bmatrix}\n    y_1 \\\\ y_2 \\\\ y_3 \\\\ \\vdots \\\\ y_{n-2} \\\\ y_{n-1} \\\\ y_n\n\\end{bmatrix} -\n\\begin{bmatrix}\n  \\hat{y}_1 \\\\ \\hat{y}_2 \\\\ \\hat{y}_3 \\\\ \\vdots \\\\ \\hat{y}_{n-2} \\\\ \\hat{y}_{n-1} \\\\ \\hat{y}_n\n\\end{bmatrix}\n\\]\nOur aim is to find the optimum vector of coefficients, \\(\\hat{\\beta}^*\\), that minimizes the sum of squared residuals:\n\\[\n\\min_{\\beta} \\left( \\epsilon^{\\intercal}\\epsilon \\right)\n\\]\n\n\n1. Expand the sum of squared residuals\nThe first step involves expanding the sum of squared residuals, and substituting in \\(X \\hat{\\beta}\\) for \\(\\hat{y}\\). \\[\n\\displaylines{\n\\begin{align}\n\\sum_{i=1}^n{\\hat{\\epsilon}_i^2} & = \\hat{\\epsilon}^{\\intercal}\\hat{\\epsilon}\n\\\\& =(y-X\\hat{\\beta})^{\\intercal}(y-X\\hat{\\beta})\n\\\\& = y^{\\intercal}y - y^{\\intercal}X\\hat{\\beta}-\n\\hat{\\beta}^{\\intercal} X^{\\intercal}y+\n\\hat{\\beta}^{\\intercal}X^{\\intercal}X\\hat{\\beta}\n\\\\& = y^{\\intercal}y - 2y^{\\intercal}X\\hat{\\beta}\n+\\hat{\\beta}^{\\intercal}X^{\\intercal}X\\hat{\\beta}\n\\end{align}\n}\n\\]\n\n\nNote we can simply add the two middle terms, since are both scalars:\n\\[\n\\displaylines{\ny^{\\intercal}X\\hat{\\beta} =\n\\hat{\\beta}^{\\intercal} X^{\\intercal}y \\\\\n\\because \\underset{1 \\times n}{y^{\\intercal}} \\times\n\\underset{n \\times k}{X} \\times\n\\hat{\\underset{k \\times 1}{\\beta}}\n=\n\\hat{\\underset{1 \\times k}{\\beta}^{\\intercal}} \\times\n\\underset{k \\times n}{X^{\\intercal}} \\times\n\\underset{n \\times 1}{y}\n}\n\\]\n\n\n2. Partially differentiate RSS with respect to beta\nThe second step involves partially differentiating the cost function with respect to its parameters, to understand how it changes as the coefficients vary.\n\\[\n\\displaylines{\n\\begin{align}\n\\frac{\\partial}{\\partial\\hat{\\beta}}\\hat{\\epsilon}^{\\intercal}\\hat{\\epsilon} & \\equiv\n\\begin{bmatrix}\n    \\frac{\\partial}{\\partial\\hat{\\beta}_1}\\hat{\\epsilon}^{\\intercal}\\hat{\\epsilon} \\\\\n    \\frac{\\partial}{\\partial\\hat{\\beta}_2}\\hat{\\epsilon}^{\\intercal}\\hat{\\epsilon} \\\\\n    \\vdots \\\\\n    \\frac{\\partial}{\\partial\\hat{\\beta}_k}\\hat{\\epsilon}^{\\intercal}\\hat{\\epsilon}\n\\end{bmatrix}\n\\\\ & = \\frac{d}{d\\hat{\\beta}}(\ny^{\\intercal}y - 2y^{\\intercal}X\\hat{\\beta}\n+\\hat{\\beta}^{\\intercal}X^{\\intercal}X\\hat{\\beta}) \\\\ & = 0 - 2X^{\\intercal}y +((X^{\\intercal}X)\\hat{\\beta} + (X^{\\intercal}X)^{\\intercal}\\hat{\\beta})\n\\\\ & = -2X^{\\intercal}y +  2((X^{\\intercal}X)\\hat{\\beta})\n\\end{align}\n}\n\\]\n\n\nTwo matrix differentiation rules used here for reference:\n\\[\n\\displaylines{\n\\frac{\\partial}{dx}(Ax) = A^{\\intercal}x \\\\\n\\frac{\\partial}{dx}(xAx) = Ax + A^{\\intercal}x\n}\n\\]\nAnd note \\(X^{\\intercal}X = (X^{\\intercal}X)^{\\intercal}\\) by definition, so we can add the two last terms.\n\n\n3. Find the coefficient values at the stationary point\nNow we find the choices of \\(\\beta\\) where the partial differential is equal to zero. These stationary points for the cost function are either at its maximum or minimum.\nFor OLS - we actually only find one unique solution!\n\\[\n\\displaylines{\n\\begin{align}\n\\cancel{2}X^{\\intercal}y +  \\cancel{2}((X^{\\intercal}X)\\hat{\\beta}) & = 0 \\\\\n\\therefore (X^{\\intercal}X)\\hat{\\beta} & = X^{\\intercal}y \\\\\n\\therefore \\hat{\\beta} & = (X^{\\intercal}X)^{-1}X^{\\intercal}y\n\\end{align}\n}\n\\]\n\n\nNote the need to invert \\(X^{\\intercal}X\\). This is only possible for a full rank matrix.\nThe first term is the (inverse) variance matrix of \\(X\\). This term normalizes the coefficient with respect to the magnitude of \\(X\\).\nThe second term is the covariance matrix between \\(X\\) and \\(y\\). This incorporates the linear relationship between the two in the coefficient.\nHence, the coefficient can be interpreted as the estimated change in \\(y\\) given a one unit change in \\(X\\).\n\n\n4. Check the stationary point is a global minimum (hessian matrix)\nFinally, we derive the hessian matrix, by double-differentiating the cost function with respect to the coefficients:\n\\[\n\\displaylines{\n\\frac{\\partial^2}{\\partial\\hat{\\beta}\\partial\\hat{\\beta}^{\\intercal}}\n\\left(\n  \\hat{\\epsilon}^{\\intercal}\\hat{\\epsilon}\n\\right)\n\\\\ = \\frac{\\partial}{\\partial \\hat{\\beta}^{\\intercal}} \\left( -2X^{\\intercal}y +  2((X^{\\intercal}X)\\hat{\\beta}) \\right)\n\\\\ = 2(X^{\\intercal}X)^{\\intercal}\n}\n\\]\nSince \\(X^{\\intercal}X\\) is clearly positive definite, the cost function is convex. Thus, we know our unique solution for \\(\\beta\\) where the partial differential is at zero is indeed a global minimum for the cost function."
  },
  {
    "objectID": "posts/ols_coef_derivation.html#final-reflections",
    "href": "posts/ols_coef_derivation.html#final-reflections",
    "title": "Deriving OLS coefficients (matrix algebra)",
    "section": "Final reflections",
    "text": "Final reflections\nUnlike logistic regression, or the multiple hidden-layer structure of neural networks, we can “jump” straight to the optimum coefficients for OLS. Why can we do this? Well chiefly its because OLS is a bit of a special case:\n\nThe minima is a global minima: The hessian matrix is positive definite, and hence the cost function is strictly convex. This means we know that when a choice of coefficients is found that ensure the partially differentiated cost function is equal zero, this minima is also a global one, not a local one.\nThere is only one solution for the optimum coefficient: We assume that the matrix is full rank (every feature provides additional predictive power) and that the number of predictors is smaller than the number of obervations. This also means that partially differentiating is okay to do!\nA closed-form solution can be found The predictions are generated from \\(X\\) using a simple, purely algebraic function, i.e. the sum-product of \\(X\\) by \\(\\beta\\). This means we can find an analytical solution to the optimal choice \\(\\beta^*\\). Note this often isn’t possible since non-linear activation functions (i.e. link functions) are often transcendental.\n\nWe will dive into this in another post.\nFin."
  },
  {
    "objectID": "posts/differentiation.html",
    "href": "posts/differentiation.html",
    "title": "Differentiation formulae",
    "section": "",
    "text": "What we are solving\n\n\n\nDeriving why \\(\\frac{d}{dx} \\left(x^{n} \\right) = nx^{n-1}\\)"
  },
  {
    "objectID": "posts/differentiation.html#formulating-an-approach-to-the-problem",
    "href": "posts/differentiation.html#formulating-an-approach-to-the-problem",
    "title": "Differentiation formulae",
    "section": "Formulating an approach to the problem",
    "text": "Formulating an approach to the problem\nThe gradient between two points, \\(x\\) and \\(a\\), is equal to the change in the y axis divided by the change in the x axis.\nFor example, if \\(y=f(x)=x^n\\), then we can approximate the gradient as follows:\n\\[\n\\frac{\\Delta y}{\\Delta x} =\n\\frac{f(x)-f(a)}{x-a} =\n\\frac{x^n-a^n}{x-a}\n\\]\nThis isn’t the same as saying a line which passes through both of these points has the same gradient though. There is a gap between this linear approximation and the exact curve.\nHowever, the smaller the movement across the x-axis, the closer the approximation is to the actual curve. For example, in Figure 1 below, the line drawn between \\(f(1)\\) and \\(f(2)\\) is closer to the gradient at \\(f(1)\\) of the true curve, compared to the line drawn between \\(f(1)\\) and \\(f(3)\\).\n\n\nCode\nimport plotly.graph_objects as go\nimport numpy as np\n\ndef interpolate(n,x1,x2,num=50):\n  x = np.linspace(x1,x2,num)\n  m = (x2**n - x1**n)/(x2 - x1)\n  c = x1**n - m*x1\n  y = m*x+c\n  return({'x':x,'y':y})\n\nn=3\nactual = {'x': np.arange(0,4,0.1)}\nactual['y'] = actual['x']**n\nlb, ub = 1, [3.5,3,2]\nlin0 = interpolate(n,lb,ub[0])\nlin1 = interpolate(n,lb,ub[1])\nlin2 = interpolate(n,lb,ub[2])\n\nfig = go.Figure(data = go.Scatter(mode='lines'))\nfig.add_trace( go.Scatter( x=actual['x'], y=actual['y'], line=dict(dash='solid'), name=f\"f(x)=x^{n}\" ))\nfig.add_trace( go.Scatter( x=lin0['x'], y=lin0['y'], name=f\"f({lb}) -&gt; f({ub[0]})\", line=dict(dash='dot') ))\nfig.add_trace( go.Scatter( x=lin1['x'], y=lin1['y'], name=f\"f({lb}) -&gt; f({ub[1]})\", line=dict(dash='dot') ))\nfig.add_trace( go.Scatter( x=lin2['x'], y=lin2['y'], name=f\"f({lb}) -&gt; f({ub[2]})\", line=dict(dash='dot') ))\nfig.show()\n\n\n\n\n                                                \n\n\nFigure 1: The smaller the change in x, the closer the linear approximation is to the true gradient of f(x)\n\n\n\n\nConsequently, we want to determine the gradient at the limit i.e. where \\(a \\rightarrow x\\), to get the true gradient."
  },
  {
    "objectID": "posts/differentiation.html#factoring-out-x-a",
    "href": "posts/differentiation.html#factoring-out-x-a",
    "title": "Differentiation formulae",
    "section": "Factoring out \\((x-a)\\)",
    "text": "Factoring out \\((x-a)\\)\nFirst though, let’s factor out the \\(x-a\\) term for simplicity. Let’s derive a generic formula for this:\n\nIf \\(n=2\\), then \\(x^2-a^2 = (x-a)(x+a)\\)\nIf \\(n=3\\), then \\(x^3-a^3 = (x-a)(x^2+xa+a^2)\\)\nIf \\(n=4\\), then \\(x^4-a^4 = (x-a)(x^3+x^2a+xa^2+a^3)\\)\nIf \\(n=5\\), then \\(x^5-a^5 = (x-a)(x^4+x^3a+x^2a^2+xa^3+a^4)\\)\nAnd so on. In fact for any \\(n\\), we can derive \\(x^n-a^n = (x-a)\\sum_{i=1}^n \\left( x^{n-i}a^{i-1} \\right)\\)\n\nAnd we can now sub that into our formula, and the \\(x-a\\) cancels out:\n\\[\n\\frac{x^n-a^n}{x-a} =\n\\frac{x-a}{x-a} \\sum_{i=1}^n \\left( x^{n-i}a^{i-1} \\right) =\n\\sum_{i=1}^n \\left( x^{n-i}a^{i-1} \\right)\n\\]"
  },
  {
    "objectID": "posts/differentiation.html#getting-our-result",
    "href": "posts/differentiation.html#getting-our-result",
    "title": "Differentiation formulae",
    "section": "Getting our result",
    "text": "Getting our result\nAnd now let’s calculate the result in the limit, where \\(x\\) approaches \\(a\\):\n\\[\n\\lim_{x \\rightarrow a} \\sum_{i=1}^n \\left( x^{n-i}a^{i-1} \\right) \\sim\n\\sum_{i=1}^n \\left( a^{n-i}a^{i-1} \\right) =\n\\sum_{i=1}^n \\left( a^{n-1} \\right) =\nna^{n-1}\n\\]\nHence \\(\\frac{d}{dx} \\left(x^{n} \\right) = nx^{n-1}\\).\nFin."
  },
  {
    "objectID": "posts/sandwich_estimators.html",
    "href": "posts/sandwich_estimators.html",
    "title": "Sandwiches: robust covariance error estimation",
    "section": "",
    "text": "What are we exploring?\n\n\n\nEstimating the correct coefficient variance under different error assumptions"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "I’ve taught myself Data Science over the last few years, and I’ve taken lots of notes to solidify my understanding along the way. I thought it was worthwhile to share them!\nAll notes and code are my own, or references have been added.\nAll images are generated using Microsoft Copilot in Bing."
  },
  {
    "objectID": "posts/sandwich_estimators.html#introducing-sandwiches",
    "href": "posts/sandwich_estimators.html#introducing-sandwiches",
    "title": "Sandwiches: robust covariance error estimation",
    "section": "Introducing sandwiches",
    "text": "Introducing sandwiches\nThe variance for the OLS coefficient estimator is equal to the following:\n\\[\n\\displaylines{\nV(\\hat{\\beta}) =\n(X^{\\intercal}X)^{-1}X^{\\intercal}E[\\epsilon\\epsilon^{\\intercal}]X(X^{\\intercal}X)^{-1}\n}\n\\]\nThis can be though to as a sandwich:\n\nThe “bread” either side: \\((X^{\\intercal}X)^{-1}X^{\\intercal}\\) on the left and its transpose \\(X(X^{\\intercal}X)^{-1}\\) on the right\nThe “meat” in the middle: what we assume for \\(E[\\epsilon\\epsilon^{\\intercal}]\\)\n\nNote that this is the same as the error variance, since \\(V[\\epsilon]=E[\\epsilon\\epsilon^{\\intercal}]-E[\\epsilon]E[\\epsilon^{\\intercal}]\\) and \\(E[\\epsilon] = 0\\)\n\n\nOur coefficient will only be efficient if these assumptions about the expected error are correct! We will explore what happens when the errors are assumed to be homoskedastic, heteroskedastic or clustered."
  },
  {
    "objectID": "posts/sandwich_estimators.html#cucumber-filling-constant-error-variance",
    "href": "posts/sandwich_estimators.html#cucumber-filling-constant-error-variance",
    "title": "Sandwiches: robust covariance error estimation",
    "section": "Cucumber filling: Constant Error Variance 🥒",
    "text": "Cucumber filling: Constant Error Variance 🥒\nUsual OLS is efficient if the true model has “spherical errors”. What does this mean in practice?\n\nErrors are homoskedastic: \\(V(\\epsilon_i)=\\sigma^2\\) for all observations\nErrors are serially uncorrelated: \\(cov(\\epsilon_i,\\epsilon_{j\\neq i})=0\\)\n\nWhat does this look like for \\(E[\\epsilon\\epsilon^{\\intercal}]\\)?\n\nThe diagonal of the matrix is a constant value (scalar), \\(\\sigma^2\\)\nThe off-diagonals are all zero\n\n\\[\n\\hat{\\sigma}^2\\underset{n\\times n}{I} =\n\\begin{bmatrix}\n\\hat{\\sigma}^2 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n0 & \\hat{\\sigma}^2 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n0 & 0 & \\hat{\\sigma}^2 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n0 & 0 & 0 & \\hat{\\sigma}^2 & 0 & 0 & 0 & 0 & 0\\\\\n0 & 0 & 0 & 0 & \\hat{\\sigma}^2 & 0 & 0 & 0 & 0\\\\\n0 & 0 & 0 & 0 & 0 & \\hat{\\sigma}^2 & 0 & 0 & 0\\\\\n0 & 0 & 0 & 0 & 0 & 0 & \\hat{\\sigma}^2 & 0 & 0\\\\\n0 & 0 & 0 & 0 & 0 & 0 & 0 & \\hat{\\sigma}^2 & 0\\\\\n0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & \\hat{\\sigma}^2\n\\end{bmatrix}\n\\]\nA good estimation of the constant error variance \\(\\sigma^2\\) is to apply the standard formula to the residuals (i.e. method of moments):\n\\[\n\\hat{\\sigma^2}=\\frac{1}{n-k}\\sum{\\hat{\\epsilon_i}^2} \\equiv \\frac{\\epsilon^{\\intercal}\\epsilon }{n-k}\n\\]\n\n\n\n\n\n\nThus our sandwich is:\n\n\n\n\\[\n\\underset{k \\times k}{(X^{\\intercal}X)}^{-1}\n\\underset{k \\times n}\n{X^{\\intercal}}\n\\left(\\frac{1}{n-k} \\times \\underset{1 \\times 1}{(\\epsilon^{\\intercal}\\epsilon)}\n\\times\\underset{n \\times n}{I} \\right)\n\\underset{n \\times k}{X}\n\\underset{k \\times k}{(X^{\\intercal}X)}^{-1}\n\\]\n\n\nIn this scenario, the only things that impact the standard error of the coefficient \\(\\beta_k\\) is:\n\nThe variance of all the residuals, \\(\\sigma^2\\)\nThe variance of the feature \\(V(X_k)\\)\n\n\n\nNote that under spherical errors, the sandwich can be simplified: \\[\n\\displaylines{\n\\begin{align}\nV[\\hat{\\beta}]\n& = (X^{\\intercal}X)^{-1}X^{\\intercal}E[\\epsilon\\epsilon^{\\intercal}]X(X^{\\intercal}X)^{-1}\n\\\\ & = (X^{\\intercal}X)^{-1}X^{\\intercal}\\sigma^2IX(X^{\\intercal}X)^{-1}\n\\\\ & = \\sigma^2(X^{\\intercal}X)^{-1}\\cancel{X^{\\intercal}X}\\cancel{(X^{\\intercal}X)^{-1}}\n\\\\ & = \\sigma^2(X^{\\intercal}X)^{-1}\n\\end{align}\n}\n\\]\n\\[\n1\n\\]"
  },
  {
    "objectID": "posts/sandwich_estimators.html#cucumber-filling-constant-error-variance-1",
    "href": "posts/sandwich_estimators.html#cucumber-filling-constant-error-variance-1",
    "title": "Sandwiches: robust covariance error estimation",
    "section": "🥒 Cucumber filling: Constant Error Variance",
    "text": "🥒 Cucumber filling: Constant Error Variance\nAs shown before, usual OLS is efficient if the true model has “spherical errors. What does this mean in practice?\n\nErrors are homoskedastic: \\(V(\\epsilon_i)=\\sigma^2\\) for all observations\nErrors are serially uncorrelated: \\(cov(\\epsilon_i,\\epsilon_{j\\neq i})=0\\)\n\nWhat does this look like for \\(E[\\epsilon\\epsilon^{\\intercal}]\\)?\n\nThe diagonal of the matrix is a constant value (scalar), \\(\\sigma^2\\)\nThe off-diagonals are all zero\n\n\\[\n\\sigma^2\\underset{n\\times n}{I} = \\begin{bmatrix}\n\\sigma^2 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n0 & \\sigma^2 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n0 & 0 & \\sigma^2 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n0 & 0 & 0 & \\sigma^2 & 0 & 0 & 0 & 0 & 0\\\\\n0 & 0 & 0 & 0 & \\sigma^2 & 0 & 0 & 0 & 0\\\\\n0 & 0 & 0 & 0 & 0 & \\sigma^2 & 0 & 0 & 0\\\\\n0 & 0 & 0 & 0 & 0 & 0 & \\sigma^2 & 0 & 0\\\\\n0 & 0 & 0 & 0 & 0 & 0 & 0 & \\sigma^2 & 0\\\\\n0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & \\sigma^2\n\\end{bmatrix}\n\\]\n\nA good estimation of the constant error variance \\(\\sigma^2\\) is the standard formula (i.e. method of moments):\n\n\\[\n\\hat{\\sigma^2}=\\frac{1}{n-k}\\sum{\\hat{\\epsilon_i}^2} \\equiv \\frac{\\epsilon^{\\intercal}\\epsilon }{n-k}\n\\]\n\n💡 Thus our sandwich is: \\(\\underset{k \\times k}{(X^{\\intercal}X)}^{-1}\n\\underset{k \\times n}\n{X^{\\intercal}}\n\\left(\\frac{1}{n-k} \\times \\underset{1 \\times 1}{(\\epsilon^{\\intercal}\\epsilon)}\n\\times\\underset{n \\times n}{I} \\right)\n\\underset{n \\times k}{X}\n\\underset{k \\times k}{(X^{\\intercal}X)}^{-1}\\)\n\nIn this scenario, the only things that impact the standard error of coefficient \\(k\\) is:\n\nThe variance of all the errors, \\(\\sigma^2\\)\nThe variance of the feature \\(V(X_k)\\)\nFinally, note that under homoskedasticity, the sandwich can be simplified:\n\\[\n  V[\\hat{\\beta}] = (X^{\\intercal}X)^{-1}X^{\\intercal}E[\\epsilon\\epsilon^{\\intercal}]X(X^{\\intercal}X)^{-1} \\\\\n  = (X^{\\intercal}X)^{-1}X^{\\intercal}\\sigma^2IX(X^{\\intercal}X)^{-1} \\\\\n  = \\sigma^2(X^{\\intercal}X)^{-1}\\cancel{X^{\\intercal}X}\\cancel{(X^{\\intercal}X)^{-1}}\n  \\]"
  },
  {
    "objectID": "posts/sandwich_estimators.html#cucumber-constant-error-variance",
    "href": "posts/sandwich_estimators.html#cucumber-constant-error-variance",
    "title": "Sandwiches: robust covariance error estimation",
    "section": "Cucumber: Constant Error Variance 🥒",
    "text": "Cucumber: Constant Error Variance 🥒\nUsual OLS is efficient if the true model has “spherical errors”. What does this mean in practice?\n\nErrors are homoskedastic: \\(V(\\epsilon_i)=\\sigma^2\\) for all observations\nErrors are serially uncorrelated: \\(cov(\\epsilon_i,\\epsilon_{j\\neq i})=0\\)\n\nWhat does this look like for \\(E[\\epsilon\\epsilon^{\\intercal}]\\)?\n\nThe diagonal of the matrix is a constant value (scalar), \\(\\sigma^2\\)\nThe off-diagonals are all zero\n\n\\[\n\\hat{\\sigma}^2\\underset{n\\times n}{I} =\n\\begin{bmatrix}\n\\hat{\\sigma}^2 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n0 & \\hat{\\sigma}^2 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n0 & 0 & \\hat{\\sigma}^2 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n0 & 0 & 0 & \\hat{\\sigma}^2 & 0 & 0 & 0 & 0 & 0\\\\\n0 & 0 & 0 & 0 & \\hat{\\sigma}^2 & 0 & 0 & 0 & 0\\\\\n0 & 0 & 0 & 0 & 0 & \\hat{\\sigma}^2 & 0 & 0 & 0\\\\\n0 & 0 & 0 & 0 & 0 & 0 & \\hat{\\sigma}^2 & 0 & 0\\\\\n0 & 0 & 0 & 0 & 0 & 0 & 0 & \\hat{\\sigma}^2 & 0\\\\\n0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & \\hat{\\sigma}^2\n\\end{bmatrix}\n\\]\nA good estimation of the constant error variance \\(\\sigma^2\\) is to apply the standard formula to the residuals (i.e. method of moments):\n\\[\n\\hat{\\sigma^2}=\\frac{1}{n-k}\\sum{\\hat{\\epsilon_i}^2} \\equiv \\frac{\\epsilon^{\\intercal}\\epsilon }{n-k}\n\\]\n\n\n\n\n\n\nThus our “cucumber sandwich” is:\n\n\n\n\\[\n\\underset{k \\times k}{(X^{\\intercal}X)}^{-1}\n\\underset{k \\times n}\n{X^{\\intercal}}\n\\left(\\frac{1}{n-k} \\times \\underset{1 \\times 1}{(\\epsilon^{\\intercal}\\epsilon)}\n\\times\\underset{n \\times n}{I} \\right)\n\\underset{n \\times k}{X}\n\\underset{k \\times k}{(X^{\\intercal}X)}^{-1}\n\\]\n\n\nIn this scenario, the only things that impact the standard error of the coefficient \\(\\beta_k\\) is:\n\nThe variance of all the residuals, \\(\\sigma^2\\)\nThe variance of the feature \\(V(X_k)\\)\n\n\n\nNote that under spherical errors, the sandwich can be simplified: \\[\n\\displaylines{\n\\begin{align}\nV[\\hat{\\beta}]\n& = (X^{\\intercal}X)^{-1}X^{\\intercal}E[\\epsilon\\epsilon^{\\intercal}]X(X^{\\intercal}X)^{-1}\n\\\\ & = (X^{\\intercal}X)^{-1}X^{\\intercal}\\sigma^2IX(X^{\\intercal}X)^{-1}\n\\\\ & = \\sigma^2(X^{\\intercal}X)^{-1}\\cancel{X^{\\intercal}X}\\cancel{(X^{\\intercal}X)^{-1}}\n\\\\ & = \\sigma^2(X^{\\intercal}X)^{-1}\n\\end{align}\n}\n\\]"
  },
  {
    "objectID": "posts/sandwich_estimators.html#ham-heteroskedastic-errors",
    "href": "posts/sandwich_estimators.html#ham-heteroskedastic-errors",
    "title": "Sandwiches: robust covariance error estimation",
    "section": "Ham: Heteroskedastic errors 🍖",
    "text": "Ham: Heteroskedastic errors 🍖\nHeteroskedastic correction is needed if:\n\nErrors vary for every individual: \\(V(\\epsilon_i)=\\sigma_i^2\\) for all observations\nBut they are still independent aka serially uncorrelated: \\(cov(\\epsilon_i,\\epsilon_{j\\neq i})=0\\)\n\nWhat does this look like for \\(E[\\epsilon\\epsilon^{\\intercal}]\\)?\n\nThe diagonal of the matrix is the estimate of variance which is unique for each observation, \\(\\sigma_i^2\\)\nThe off-diagonals are all zero\n\\[\n  \\underset{n \\times n}{\\sigma^2} = \\begin{bmatrix}\n  \\sigma_1^2 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n  0 & \\sigma_2^2 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n  0 & 0 & \\sigma_3^2 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n  0 & 0 & 0 & \\sigma_4^2 & 0 & 0 & 0 & 0 & 0\\\\\n  0 & 0 & 0 & 0 & \\sigma_5^2 & 0 & 0 & 0 & 0\\\\\n  0 & 0 & 0 & 0 & 0 & \\sigma_6^2 & 0 & 0 & 0\\\\\n  0 & 0 & 0 & 0 & 0 & 0 & \\sigma_7^2 & 0 & 0\\\\\n  0 & 0 & 0 & 0 & 0 & 0 & 0 & \\sigma_8^2 & 0\\\\\n  0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & \\sigma_{..}^2\\\\\n  \\end{bmatrix}\n  \\]\nA good estimation of the vector of heteroskedastic error variances \\(\\sigma^2\\) is again to apply the standard formula to the residuals for each individual:\n\n\\[\n\\hat{\\sigma_i^2}=\\frac{1}{n-k}\\sum{\\hat{\\epsilon_i}^2}\n\\]\n\n\n\n\n\n\nThus our “ham sandwich” is:\n\n\n\n\\[\n\\underset{k \\times k}{(X^{\\intercal}X)}^{-1}\n\\underset{k \\times n}{X^{\\intercal}}\n\\left(\\frac{1}{n-k} \\times \\underset{1 \\times n}{(\\epsilon\\odot \\epsilon)} ^{\\intercal}\n\\times\\underset{n \\times n}{I} \\right)\n\\underset{n \\times k}{X}\n\\underset{k \\times k}{(X^{\\intercal}X)}^{-1}\n\\]\n\n\nSimilar to homoskedastic errors, the things that impacts the standard error of coefficient \\(k\\) is:\n\nThe variance of all the individual errors, \\(\\sigma_i^2\\)\nThe variance of the feature \\(V(X_k)\\)"
  },
  {
    "objectID": "posts/sandwich_estimators.html#salmon-spherical-errors",
    "href": "posts/sandwich_estimators.html#salmon-spherical-errors",
    "title": "Sandwiches: robust covariance error estimation",
    "section": "Salmon: Spherical Errors 🐟",
    "text": "Salmon: Spherical Errors 🐟\nUsual OLS is efficient if the true model has “spherical errors”. What does this mean in practice?\n\nErrors are homoskedastic: \\(V(\\epsilon_i)=\\sigma^2\\) for all observations\nErrors are serially uncorrelated: \\(cov(\\epsilon_i,\\epsilon_{j\\neq i})=0\\)\n\nWhat does this look like for \\(E[\\epsilon\\epsilon^{\\intercal}]\\)?\n\nThe diagonal of the matrix is a constant value (scalar), \\(\\sigma^2\\)\nThe off-diagonals are all zero\n\n\\[\n\\hat{\\sigma}^2\\underset{n\\times n}{I} =\n\\begin{bmatrix}\n\\hat{\\sigma}^2 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n0 & \\hat{\\sigma}^2 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n0 & 0 & \\hat{\\sigma}^2 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n0 & 0 & 0 & \\hat{\\sigma}^2 & 0 & 0 & 0 & 0 & 0\\\\\n0 & 0 & 0 & 0 & \\hat{\\sigma}^2 & 0 & 0 & 0 & 0\\\\\n0 & 0 & 0 & 0 & 0 & \\hat{\\sigma}^2 & 0 & 0 & 0\\\\\n0 & 0 & 0 & 0 & 0 & 0 & \\hat{\\sigma}^2 & 0 & 0\\\\\n0 & 0 & 0 & 0 & 0 & 0 & 0 & \\hat{\\sigma}^2 & 0\\\\\n0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & \\hat{\\sigma}^2\n\\end{bmatrix}\n\\]\nA good estimation of the constant error variance \\(\\sigma^2\\) is to apply the standard formula to the residuals (i.e. method of moments):\n\\[\n\\hat{\\sigma^2}=\\frac{1}{n-k}\\sum{\\hat{\\epsilon_i}^2} \\equiv \\frac{\\epsilon^{\\intercal}\\epsilon }{n-k}\n\\]\n\n\n\n\n\n\nThus our “salmon sandwich” is:\n\n\n\n\\[\n\\underset{k \\times k}{(X^{\\intercal}X)}^{-1}\n\\underset{k \\times n}\n{X^{\\intercal}}\n\\left(\\frac{1}{n-k} \\times \\underset{1 \\times 1}{(\\epsilon^{\\intercal}\\epsilon)}\n\\times\\underset{n \\times n}{I} \\right)\n\\underset{n \\times k}{X}\n\\underset{k \\times k}{(X^{\\intercal}X)}^{-1}\n\\]\n\n\nIn this scenario, the only things that impact the standard error of the coefficient \\(\\beta_k\\) is:\n\nThe variance of all the residuals, \\(\\sigma^2\\)\nThe variance of the feature \\(V(X_k)\\)\n\n\n\nNote that under spherical errors, the sandwich can be simplified: \\[\n\\displaylines{\n\\begin{align}\nV[\\hat{\\beta}]\n& = (X^{\\intercal}X)^{-1}X^{\\intercal}E[\\epsilon\\epsilon^{\\intercal}]X(X^{\\intercal}X)^{-1}\n\\\\ & = (X^{\\intercal}X)^{-1}X^{\\intercal}\\sigma^2IX(X^{\\intercal}X)^{-1}\n\\\\ & = \\sigma^2(X^{\\intercal}X)^{-1}\\cancel{X^{\\intercal}X}\\cancel{(X^{\\intercal}X)^{-1}}\n\\\\ & = \\sigma^2(X^{\\intercal}X)^{-1}\n\\end{align}\n}\n\\]"
  },
  {
    "objectID": "posts/sandwich_estimators.html#cheese-clustered-errors",
    "href": "posts/sandwich_estimators.html#cheese-clustered-errors",
    "title": "Sandwiches: robust covariance error estimation",
    "section": "Cheese: Clustered Errors 🧀",
    "text": "Cheese: Clustered Errors 🧀\nCluster-robust errors are correct if:\n\nErrors vary for every individual: \\(V(\\epsilon_i)=\\sigma_i^2\\) for all observations i.e. still heteroskedastic\nAND errors within the same cluster \\(C_l\\) are serially correlated: \\(cov(\\epsilon_i,\\epsilon_{j}) \\neq 0 \\text{ if } \\epsilon_i,\\epsilon_j \\in C_l\\)\n\nNote - errors between clusters are assumed not to be serially correlated though i.e. \\(cov(\\epsilon_i,\\epsilon_{j}) =0 \\text{ if } \\epsilon_i \\in C_l,\\epsilon_j \\in C_{m \\neq l}\\)\n\n\nWhat does this look like for \\(\\epsilon\\epsilon^{\\intercal}\\)?\n\nThe diagonal of the matrix is the estimate of variance which is unique for each observation, \\(\\sigma_i^2\\)\nThe off-diagonals are also populated with the covariance - but only when they are both in the same cluster\n\nTake an example where observations 1, 2 and 3 are in cluster A, observations 4 and 5 are in cluster B, observations 6, 7 and 8 are in cluster C etc.\n\\[\n\\underset{n \\times n}{\\epsilon\\epsilon^{\\intercal}} \\sim \\begin{bmatrix}\n\\epsilon_1^2 & \\epsilon_1\\epsilon_2 & \\epsilon_1\\epsilon_3 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n\\epsilon_2\\epsilon_1 & \\epsilon_2^2 & \\epsilon_2\\epsilon_3 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n\\epsilon_3\\epsilon_1 & \\epsilon_3\\epsilon_2 & \\epsilon_3^2 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n0 & 0 & 0 & \\epsilon_4^2 & \\epsilon_4\\epsilon_5 & 0 & 0 & 0 & 0\\\\\n0 & 0 & 0 & \\epsilon_5\\epsilon_4 & \\epsilon_5^2 & 0 & 0 & 0 & 0\\\\\n0 & 0 & 0 & 0 & 0 & \\epsilon_6^2 & \\epsilon_6\\epsilon_7 & \\epsilon_6\\epsilon_8 & 0\\\\\n0 & 0 & 0 & 0 & 0 & \\epsilon_7\\epsilon_6 & \\epsilon_7^2 & \\epsilon_7\\epsilon_8 & 0\\\\\n0 & 0 & 0 & 0 & 0 & \\epsilon_8\\epsilon_6 & \\epsilon_8\\epsilon_7 & \\epsilon_8^2 & 0\\\\\n0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & \\epsilon_{..}^2\\\\\n\\end{bmatrix}\n\\]\nAdditionally, we have to do a finite-sample correction as well on the degrees of freedom, based on the number of clusters \\(n_c\\)\n\n💡 Thus our sandwich is: \\(\\underset{k \\times k}{(X^{\\intercal}X)}^{-1}\n\\underset{k \\times n}{X^{\\intercal}}\n\\left(\\frac{n-1}{n-k}\\frac{n_c}{n_c-1} \\times\n\\underset{n \\times n}{(\\epsilon \\epsilon^{\\intercal})}\n\\right)\n\\underset{n \\times k}{X}\n\\underset{k \\times k}{(X^{\\intercal}X)}^{-1}\\)\n\n\nIn this scenario, there are a few additional things that impact the standard error of coefficient \\(k\\):\n\nIf errors are correlated within clusters, this will increase the error.\nIf features are correlated within clusters, this will also increase the error (due to the off-diagonals in the error variance matrix)\nAnd if both the errors and feature correlations are the same sign, this will also increase the standard error.\n\nNote the sandwich formula above is equivalent to calculating mini sandwiches per cluster before summing. This is preferable to creating an enormous matrix from the error outer product and zeroing out covariances that aren’t the same cluster\ndef cluster_XeeX(cluster_index):\n    j = X[:,'cluster_id']== cluster_id # j is the indexes of the cluster\n    ec = e[j] # ec is a vector of errors for that cluster\n    sigma_sqc = ec.dot(ec.transpose()) # outerproduct of clustered errors\n    Xc = X[j,:] # Xc is a matrix of features for that cluster\n    XeeXc = Xc.transpose().dot(sigma_sqc).dot(Xc) # X'ee'X for that cluster \n        return(XeeXc) # shape is kxk\n\nXeeXc = np.sum([cluster_XeeX(i) for i in np.unique(cl)], axis = 0)\nn_cl = len(np.unique(cl)) # number of clusters\n\n       # finite-sample correction factor.    # sum XeeX across all clusters\nXeeX = ((n-1) / (n-k)) * (n_cl / (n_cl-1)) * np.sum(XeeXc, axis = 0)\nsandwich = var_X_inv.dot(XeeX).var_X_inv\n\n\n💡 Thus our sandwich can also be written: \\(\\underset{k \\times k}{(X^{\\intercal}X)^{-1}}\n\\left(\n\\frac{n-1}{n-k}\\frac{n_c}{n_c-1} \\times\n\\sum_{c=1}^{n_c}{\\underset{k \\times k}{(X_c^{\\intercal} \\epsilon_c \\epsilon_c^{\\intercal}X_c)}}\n\\right)\n\\underset{k \\times k}\n{(X^{\\intercal}X)^{-1}}\\)\n\nOne word of caution about averaging of clusters (requires a large number of clusters)"
  },
  {
    "objectID": "posts/sandwich_estimators.html#salmon-bagel-spherical-errors",
    "href": "posts/sandwich_estimators.html#salmon-bagel-spherical-errors",
    "title": "Sandwiches: robust covariance error estimation",
    "section": "Salmon bagel: Spherical Errors 🐟",
    "text": "Salmon bagel: Spherical Errors 🐟\nUsual OLS is efficient if the true model has “spherical errors”. What does this mean in practice?\n\nErrors are homoskedastic: \\(V(\\epsilon_i)=\\sigma^2\\) for all observations\nErrors are serially uncorrelated: \\(cov(\\epsilon_i,\\epsilon_{j\\neq i})=0\\)\n\nWhat does this look like for \\(E[\\epsilon\\epsilon^{\\intercal}]\\)?\n\nThe diagonal of the matrix is a constant value (scalar), \\(\\sigma^2\\)\nThe off-diagonals are all zero\n\n\\[\n\\hat{\\sigma}^2\\underset{n\\times n}{I} =\n\\begin{bmatrix}\n\\hat{\\sigma}^2 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n0 & \\hat{\\sigma}^2 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n0 & 0 & \\hat{\\sigma}^2 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n0 & 0 & 0 & \\hat{\\sigma}^2 & 0 & 0 & 0 & 0 & 0\\\\\n0 & 0 & 0 & 0 & \\hat{\\sigma}^2 & 0 & 0 & 0 & 0\\\\\n0 & 0 & 0 & 0 & 0 & \\hat{\\sigma}^2 & 0 & 0 & 0\\\\\n0 & 0 & 0 & 0 & 0 & 0 & \\hat{\\sigma}^2 & 0 & 0\\\\\n0 & 0 & 0 & 0 & 0 & 0 & 0 & \\hat{\\sigma}^2 & 0\\\\\n0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & \\hat{\\sigma}^2\n\\end{bmatrix}\n\\]\nA good estimation of the constant error variance \\(\\sigma^2\\) is to apply the standard formula to the residuals (i.e. method of moments):\n\\[\n\\hat{\\sigma^2}=\\frac{1}{n-k}\\sum{\\hat{\\epsilon_i}^2} \\equiv \\frac{\\epsilon^{\\intercal}\\epsilon }{n-k}\n\\]\n\n\n\n\n\n\nThus our “salmon sandwich” is:\n\n\n\n\\[\n\\underset{k \\times k}{(X^{\\intercal}X)}^{-1}\n\\underset{k \\times n}\n{X^{\\intercal}}\n\\left(\\frac{1}{n-k} \\times \\underset{1 \\times 1}{(\\epsilon^{\\intercal}\\epsilon)}\n\\times\\underset{n \\times n}{I} \\right)\n\\underset{n \\times k}{X}\n\\underset{k \\times k}{(X^{\\intercal}X)}^{-1}\n\\]\n\n\n\n\nNote that under spherical errors, the sandwich can be simplified: \\[\n\\displaylines{\n\\begin{align}\nV[\\hat{\\beta}]\n& = (X^{\\intercal}X)^{-1}X^{\\intercal}E[\\epsilon\\epsilon^{\\intercal}]X(X^{\\intercal}X)^{-1}\n\\\\ & = (X^{\\intercal}X)^{-1}X^{\\intercal}\\sigma^2IX(X^{\\intercal}X)^{-1}\n\\\\ & = \\sigma^2(X^{\\intercal}X)^{-1}\\cancel{X^{\\intercal}X}\\cancel{(X^{\\intercal}X)^{-1}}\n\\\\ & = \\sigma^2(X^{\\intercal}X)^{-1}\n\\end{align}\n}\n\\]\nIn this scenario, the only things that impact the standard error of the coefficient \\(\\beta_k\\) is:\n\nThe variance of all the residuals, \\(\\sigma^2\\)\nThe variance of the feature \\(V(X_k)\\)"
  },
  {
    "objectID": "posts/sandwich_estimators.html#ham-sarnie-heteroskedastic-errors",
    "href": "posts/sandwich_estimators.html#ham-sarnie-heteroskedastic-errors",
    "title": "Sandwiches: robust covariance error estimation",
    "section": "Ham sarnie: Heteroskedastic errors 🍖",
    "text": "Ham sarnie: Heteroskedastic errors 🍖\nHeteroskedastic correction is needed if:\n\nErrors vary for every individual: \\(V(\\epsilon_i)=\\sigma_i^2\\) for all observations\nBut they are still independent aka serially uncorrelated: \\(cov(\\epsilon_i,\\epsilon_{j\\neq i})=0\\)\n\nWhat does this look like for \\(E[\\epsilon\\epsilon^{\\intercal}]\\)?\n\nThe diagonal of the matrix is the estimate of variance which is unique for each observation, \\(\\sigma_i^2\\)\nThe off-diagonals are all zero\n\\[\n  \\underset{n \\times n}{\\sigma^2} = \\begin{bmatrix}\n  \\sigma_1^2 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n  0 & \\sigma_2^2 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n  0 & 0 & \\sigma_3^2 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n  0 & 0 & 0 & \\sigma_4^2 & 0 & 0 & 0 & 0 & 0\\\\\n  0 & 0 & 0 & 0 & \\sigma_5^2 & 0 & 0 & 0 & 0\\\\\n  0 & 0 & 0 & 0 & 0 & \\sigma_6^2 & 0 & 0 & 0\\\\\n  0 & 0 & 0 & 0 & 0 & 0 & \\sigma_7^2 & 0 & 0\\\\\n  0 & 0 & 0 & 0 & 0 & 0 & 0 & \\sigma_8^2 & 0\\\\\n  0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & \\sigma_{..}^2\\\\\n  \\end{bmatrix}\n  \\]\nA good estimation of the vector of heteroskedastic error variances \\(\\sigma^2\\) is again to apply the standard formula to the residuals for each individual:\n\n\\[\n\\hat{\\sigma_i^2}=\\frac{1}{n-k}\\sum{\\hat{\\epsilon_i}^2}\n\\]\n\n\n\n\n\n\nThus our “ham sandwich” is:\n\n\n\n\\[\n\\underset{k \\times k}{(X^{\\intercal}X)}^{-1}\n\\underset{k \\times n}{X^{\\intercal}}\n\\left(\\frac{1}{n-k} \\times \\underset{1 \\times n}{(\\epsilon\\odot \\epsilon)} ^{\\intercal}\n\\times\\underset{n \\times n}{I} \\right)\n\\underset{n \\times k}{X}\n\\underset{k \\times k}{(X^{\\intercal}X)}^{-1}\n\\]\n\n\nSimilar to homoskedastic errors, the things that impacts the standard error of coefficient \\(k\\) is:\n\nThe variance of all the individual errors, \\(\\sigma_i^2\\)\nThe variance of the feature \\(V(X_k)\\)"
  },
  {
    "objectID": "posts/sandwich_estimators.html#cheese-baguette-clustered-errors",
    "href": "posts/sandwich_estimators.html#cheese-baguette-clustered-errors",
    "title": "Sandwiches: robust covariance error estimation",
    "section": "Cheese baguette: Clustered Errors 🧀",
    "text": "Cheese baguette: Clustered Errors 🧀\nCluster-robust errors are correct if:\n\nErrors vary for every individual: \\(V(\\epsilon_i)=\\sigma_i^2\\) for all observations i.e. still heteroskedastic\nAND errors within the same cluster \\(C_l\\) are serially correlated: \\(cov(\\epsilon_i,\\epsilon_{j}) \\neq 0 \\text{ if } \\epsilon_i,\\epsilon_j \\in C_l\\)\n\nNote - errors between clusters are assumed not to be serially correlated though i.e. \\(cov(\\epsilon_i,\\epsilon_{j}) =0 \\text{ if } \\epsilon_i \\in C_l,\\epsilon_j \\in C_{m \\neq l}\\)\n\n\nWhat does this look like for \\(\\epsilon\\epsilon^{\\intercal}\\)?\n\nThe diagonal of the matrix is the estimate of variance which is unique for each observation, \\(\\sigma_i^2\\)\nThe off-diagonals are also populated with the covariance - but only when they are both in the same cluster\n\nTake an example where observations 1, 2 and 3 are in cluster A, observations 4 and 5 are in cluster B, observations 6, 7 and 8 are in cluster C etc.\n\\[\n\\underset{n \\times n}{\\epsilon\\epsilon^{\\intercal}} \\sim \\begin{bmatrix}\n\\epsilon_1^2 & \\epsilon_1\\epsilon_2 & \\epsilon_1\\epsilon_3 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n\\epsilon_2\\epsilon_1 & \\epsilon_2^2 & \\epsilon_2\\epsilon_3 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n\\epsilon_3\\epsilon_1 & \\epsilon_3\\epsilon_2 & \\epsilon_3^2 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n0 & 0 & 0 & \\epsilon_4^2 & \\epsilon_4\\epsilon_5 & 0 & 0 & 0 & 0\\\\\n0 & 0 & 0 & \\epsilon_5\\epsilon_4 & \\epsilon_5^2 & 0 & 0 & 0 & 0\\\\\n0 & 0 & 0 & 0 & 0 & \\epsilon_6^2 & \\epsilon_6\\epsilon_7 & \\epsilon_6\\epsilon_8 & 0\\\\\n0 & 0 & 0 & 0 & 0 & \\epsilon_7\\epsilon_6 & \\epsilon_7^2 & \\epsilon_7\\epsilon_8 & 0\\\\\n0 & 0 & 0 & 0 & 0 & \\epsilon_8\\epsilon_6 & \\epsilon_8\\epsilon_7 & \\epsilon_8^2 & 0\\\\\n0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & \\epsilon_{..}^2\\\\\n\\end{bmatrix}\n\\]\nAdditionally, we have to do a finite-sample correction as well on the degrees of freedom, based on the number of clusters \\(n_c\\)\n\n💡 Thus our sandwich is: \\(\\underset{k \\times k}{(X^{\\intercal}X)}^{-1}\n\\underset{k \\times n}{X^{\\intercal}}\n\\left(\\frac{n-1}{n-k}\\frac{n_c}{n_c-1} \\times\n\\underset{n \\times n}{(\\epsilon \\epsilon^{\\intercal})}\n\\right)\n\\underset{n \\times k}{X}\n\\underset{k \\times k}{(X^{\\intercal}X)}^{-1}\\)\n\n\nIn this scenario, there are a few additional things that impact the standard error of coefficient \\(k\\):\n\nIf errors are correlated within clusters, this will increase the error.\nIf features are correlated within clusters, this will also increase the error (due to the off-diagonals in the error variance matrix)\nAnd if both the errors and feature correlations are the same sign, this will also increase the standard error.\n\nNote the sandwich formula above is equivalent to calculating mini sandwiches per cluster before summing. This is preferable to creating an enormous matrix from the error outer product and zeroing out covariances that aren’t the same cluster\ndef cluster_XeeX(cluster_index):\n    j = X[:,'cluster_id']== cluster_id # j is the indexes of the cluster\n    ec = e[j] # ec is a vector of errors for that cluster\n    sigma_sqc = ec.dot(ec.transpose()) # outerproduct of clustered errors\n    Xc = X[j,:] # Xc is a matrix of features for that cluster\n    XeeXc = Xc.transpose().dot(sigma_sqc).dot(Xc) # X'ee'X for that cluster \n        return(XeeXc) # shape is kxk\n\nXeeXc = np.sum([cluster_XeeX(i) for i in np.unique(cl)], axis = 0)\nn_cl = len(np.unique(cl)) # number of clusters\n\n       # finite-sample correction factor.    # sum XeeX across all clusters\nXeeX = ((n-1) / (n-k)) * (n_cl / (n_cl-1)) * np.sum(XeeXc, axis = 0)\nsandwich = var_X_inv.dot(XeeX).var_X_inv\n\n\n💡 Thus our sandwich can also be written: \\(\\underset{k \\times k}{(X^{\\intercal}X)^{-1}}\n\\left(\n\\frac{n-1}{n-k}\\frac{n_c}{n_c-1} \\times\n\\sum_{c=1}^{n_c}{\\underset{k \\times k}{(X_c^{\\intercal} \\epsilon_c \\epsilon_c^{\\intercal}X_c)}}\n\\right)\n\\underset{k \\times k}\n{(X^{\\intercal}X)^{-1}}\\)\n\nOne word of caution about averaging of clusters (requires a large number of clusters)"
  },
  {
    "objectID": "posts/sandwich_estimators.html#cheese-roll-clustered-errors",
    "href": "posts/sandwich_estimators.html#cheese-roll-clustered-errors",
    "title": "Sandwiches: robust covariance error estimation",
    "section": "Cheese roll: Clustered Errors 🧀",
    "text": "Cheese roll: Clustered Errors 🧀\nCluster-robust errors are needed if:\n\nErrors vary for every individual: \\(V(\\epsilon_i)=\\sigma_i^2\\) for all observations i.e. still heteroskedastic\nAND errors within the same cluster \\(C_l\\) are serially correlated: \\(cov(\\epsilon_i,\\epsilon_{j}) \\neq 0 \\text{ if } \\epsilon_i,\\epsilon_j \\in C_l\\)\n\nNote - errors between clusters are assumed not to be serially correlated though i.e. \\(cov(\\epsilon_i,\\epsilon_{j}) =0 \\text{ if } \\epsilon_i \\in C_l,\\epsilon_j \\in C_{m \\neq l}\\)\n\n\nWhat does this look like for \\(E[\\epsilon\\epsilon^{\\intercal}]\\)?\n\nThe diagonal of the matrix is the estimate of variance which is unique for each observation, \\(\\sigma_i^2\\)\nThe off-diagonals are also populated with the covariance - but only when they are both in the same cluster\n\nHere is an example where observations 1, 2 and 3 are in cluster A, observations 4 and 5 are in cluster B, observations 6, 7 and 8 are in cluster C etc.\n\\[\n\\underset{n \\times n}{\\epsilon\\epsilon^{\\intercal}} \\sim \\begin{bmatrix}\n\\epsilon_1^2 & \\epsilon_1\\epsilon_2 & \\epsilon_1\\epsilon_3 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n\\epsilon_2\\epsilon_1 & \\epsilon_2^2 & \\epsilon_2\\epsilon_3 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n\\epsilon_3\\epsilon_1 & \\epsilon_3\\epsilon_2 & \\epsilon_3^2 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n0 & 0 & 0 & \\epsilon_4^2 & \\epsilon_4\\epsilon_5 & 0 & 0 & 0 & 0\\\\\n0 & 0 & 0 & \\epsilon_5\\epsilon_4 & \\epsilon_5^2 & 0 & 0 & 0 & 0\\\\\n0 & 0 & 0 & 0 & 0 & \\epsilon_6^2 & \\epsilon_6\\epsilon_7 & \\epsilon_6\\epsilon_8 & 0\\\\\n0 & 0 & 0 & 0 & 0 & \\epsilon_7\\epsilon_6 & \\epsilon_7^2 & \\epsilon_7\\epsilon_8 & 0\\\\\n0 & 0 & 0 & 0 & 0 & \\epsilon_8\\epsilon_6 & \\epsilon_8\\epsilon_7 & \\epsilon_8^2 & 0\\\\\n0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & \\epsilon_{..}^2\\\\\n\\end{bmatrix}\n\\]\nAdditionally, we have to do a finite-sample correction as well on the degrees of freedom, based on the number of clusters \\(n_c\\)\n\n\n\n\n\n\nThus our cheese sandwich is:\n\n\n\n\\(\\underset{k \\times k}{(X^{\\intercal}X)}^{-1}\n\\underset{k \\times n}{X^{\\intercal}}\n\\left(\\frac{n-1}{n-k}\\frac{n_c}{n_c-1} \\times\n\\underset{n \\times n}{(\\epsilon \\epsilon^{\\intercal})}\n\\right)\n\\underset{n \\times k}{X}\n\\underset{k \\times k}{(X^{\\intercal}X)}^{-1}\\)\n\n\nIn this scenario, there are a few additional things that impact the standard error of coefficient \\(k\\):\n\nIf errors are correlated within clusters, this will increase the error.\nIf features are correlated within clusters, this will also increase the error (due to the off-diagonals in the error variance matrix)\nAnd if both the errors and feature correlations are the same sign, this will also increase the standard error.\nAs well as the variance of the individual errors, \\(\\sigma_i^2\\), and the variance of each feature \\(V(X_k)\\), as before"
  },
  {
    "objectID": "posts/ols_coefs_multivariate.html",
    "href": "posts/ols_coefs_multivariate.html",
    "title": "Deriving OLS coefficients (multivariate)",
    "section": "",
    "text": "What we are exploring\n\n\n\nDeriving a unique, analytical solution to the set of coefficients that minimize the sum of squared residuals."
  },
  {
    "objectID": "posts/ols_coefs_multivariate.html#summary",
    "href": "posts/ols_coefs_multivariate.html#summary",
    "title": "Deriving OLS coefficients (multivariate)",
    "section": "Summary",
    "text": "Summary\nThe cost function for OLS is the sum of squared residuals, \\(\\hat{\\epsilon}^{\\intercal}\\hat{\\epsilon}\\). In order to fit a good linear model, we want to find optimum values for the estimated vector of coefficients, \\(\\hat{\\beta}^*\\), that minimizes this cost function.\nFirst we do partial differentiation of the cost function with respect to the coefficients. Finding the coefficient values where the partial differential is equal to zero reveals the stationary points of the cost function. For OLS in particular, we can find a unique solution for the choice of coefficients that can be found analytically. The hessian matrix then further proves that this is a global minima."
  },
  {
    "objectID": "posts/ols_coefs_multivariate.html#deriving-the-optimum-coefficients",
    "href": "posts/ols_coefs_multivariate.html#deriving-the-optimum-coefficients",
    "title": "Deriving OLS coefficients (multivariate)",
    "section": "Deriving the optimum coefficients",
    "text": "Deriving the optimum coefficients\n\n0. Defining the notation\nFor a sample \\(i\\), we observe an outcome \\(y_i\\). \\(y\\) is a vector of all \\(n\\) observed outcomes.\n\\[\n\\underset{n \\times 1} {y} =\n\\begin{bmatrix}\n    y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_{n-1} \\\\ y_n\n\\end{bmatrix}\n\\]\nWe also observe \\(k\\) features for every sample \\(i\\). \\(X\\) is a matrix of these observed features. Note the first column is usually all ones, to include an intercept to optimize (or “bias” term).\n\\[\n\\underset{n \\times k} {X} =\n\\begin{bmatrix}\n  1 & x_{11} & \\cdots & x_{1,k-1} & x_{1,k} \\\\\n  1 & x_{21} & \\cdots & x_{2,k-1} & x_{2,k} \\\\\n  1 & x_{31} & \\cdots & x_{3,k-1} & x_{3,k} \\\\\n  \\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\\n  1 & x_{n-2,1} & \\cdots & x_{n-2,k-1} & x_{n-2,k} \\\\\n  1 & x_{n-1,1} & \\cdots & x_{n-1,k-1} & x_{n-1,k} \\\\\n  1 & x_{n,1} & \\cdots & x_{n,k-1} & x_{n,k}\n\\end{bmatrix}\n\\]\nThe contribution of each feature to the prediction is estimated by the coefficients \\(\\hat{\\beta}\\).\n\\[\n\\underset{k \\times 1} {\\hat{\\beta}} =\n\\begin{bmatrix}\n  \\beta_0 \\\\\n  \\beta_1 \\\\\n  \\vdots \\\\\n  \\beta_{k-1} \\\\\n  \\beta_{k}\n\\end{bmatrix}\n\\]\nWe make predictions, \\(\\hat{y}\\), by calculating the dot product of the features \\(X\\) and the coefficients \\(\\hat{\\beta}\\).\n\\[\n\\hat{y} = X \\hat{\\beta}\n\\]\nwhich is shorthand for this:\n\\[\n\\displaylines{\n\\begin{align}\n\\begin{bmatrix}\n  \\hat{y}_1 \\\\ \\hat{y}_2 \\\\ \\hat{y}_3 \\\\ \\vdots \\\\ \\hat{y}_{n-2} \\\\ \\hat{y}_{n-1} \\\\ \\hat{y}_n\n\\end{bmatrix} & =\n\\begin{bmatrix}\n  1 & x_{11} & \\cdots & x_{1,k-1} & x_{1,k} \\\\\n  1 & x_{21} & \\cdots & x_{2,k-1} & x_{2,k} \\\\\n  1 & x_{31} & \\cdots & x_{3,k-1} & x_{3,k} \\\\\n  \\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\\n  1 & x_{n-2,1} & \\cdots & x_{n-2,k-1} & x_{n-2,k} \\\\\n  1 & x_{n-1,1} & \\cdots & x_{n-1,k-1} & x_{n-1,k} \\\\\n  1 & x_{n,1} & \\cdots & x_{n,k-1} & x_{n,k}\n\\end{bmatrix}\n\\begin{bmatrix}\n  \\hat{\\beta}_0 \\\\\n  \\hat{\\beta}_1 \\\\\n  \\vdots \\\\\n  \\hat{\\beta}_{k-1} \\\\\n  \\hat{\\beta}_{k}\n\\end{bmatrix}\n\\\\ \\\\ & =\n\\begin{bmatrix}\n  \\hat{\\beta}_0 + \\hat{\\beta}_{1}x_{1,1} + \\cdots + \\hat{\\beta}_{k-1}x_{1,k-1} + \\hat{\\beta}_{k}x_{1,k} \\\\\n  \\hat{\\beta}_0 + \\hat{\\beta}_{1}x_{2,1} + \\cdots + \\hat{\\beta}_{k-1}x_{2,k-1} + \\hat{\\beta}_{k}x_{2,k} \\\\\n  \\hat{\\beta}_0 + \\hat{\\beta}_{1}x_{3,1} + \\cdots + \\hat{\\beta}_{k-1}x_{3,k-1} + \\hat{\\beta}_{k}x_{3,k} \\\\\n  \\vdots \\\\\n  \\hat{\\beta}_0 + \\hat{\\beta}_{1}x_{n-2,1} + \\cdots + \\hat{\\beta}_{k-1}x_{2,k-1} + \\hat{\\beta}_{k}x_{n-2,k} \\\\\n  \\hat{\\beta}_0 + \\hat{\\beta}_{1}x_{n-1,1} + \\cdots + \\hat{\\beta}_{k-1}x_{2,k-1} + \\hat{\\beta}_{k}x_{n-1,k} \\\\\n  \\hat{\\beta}_0 + \\hat{\\beta}_{1}x_{n,1} + \\cdots + \\hat{\\beta}_{k-1}x_{2,k-1} + \\hat{\\beta}_{k}x_{n,k} \\\\\n\\end{bmatrix}\n\\end{align}\n}\n\\]\nThe residual is the difference between the true outcome and the model prediction.\n\\[\n\\hat{\\epsilon} = y_i -\\hat{y}_i\n\\]\nwhich is shorthand for this:\n\\[\n\\begin{bmatrix}\n  \\hat{\\epsilon_1} \\\\\n  \\hat{\\epsilon_2} \\\\\n  \\hat{\\epsilon_3} \\\\\n  \\vdots \\\\\n  \\hat{\\epsilon_{n-2}} \\\\\n  \\hat{\\epsilon_{n-1}} \\\\\n  \\hat{\\epsilon_{n}}\n\\end{bmatrix} =\n\\begin{bmatrix}\n    y_1 \\\\ y_2 \\\\ y_3 \\\\ \\vdots \\\\ y_{n-2} \\\\ y_{n-1} \\\\ y_n\n\\end{bmatrix} -\n\\begin{bmatrix}\n  \\hat{y}_1 \\\\ \\hat{y}_2 \\\\ \\hat{y}_3 \\\\ \\vdots \\\\ \\hat{y}_{n-2} \\\\ \\hat{y}_{n-1} \\\\ \\hat{y}_n\n\\end{bmatrix}\n\\]\nOur aim is to find the optimum vector of coefficients, \\(\\hat{\\beta}^*\\), that minimizes the sum of squared residuals:\n\\[\n\\min_{\\beta} \\left( \\epsilon^{\\intercal}\\epsilon \\right)\n\\]\n\n\n1. Expand the sum of squared residuals\nThe first step involves expanding the sum of squared residuals, and substituting in \\(X \\hat{\\beta}\\) for \\(\\hat{y}\\). \\[\n\\displaylines{\n\\begin{align}\n\\sum_{i=1}^n{\\hat{\\epsilon}_i^2} & = \\hat{\\epsilon}^{\\intercal}\\hat{\\epsilon}\n\\\\& =(y-X\\hat{\\beta})^{\\intercal}(y-X\\hat{\\beta})\n\\\\& = y^{\\intercal}y - y^{\\intercal}X\\hat{\\beta}-\n\\hat{\\beta}^{\\intercal} X^{\\intercal}y+\n\\hat{\\beta}^{\\intercal}X^{\\intercal}X\\hat{\\beta}\n\\\\& = y^{\\intercal}y - 2y^{\\intercal}X\\hat{\\beta}\n+\\hat{\\beta}^{\\intercal}X^{\\intercal}X\\hat{\\beta}\n\\end{align}\n}\n\\]\n\n\nNote we can simply add the two middle terms, since are both scalars:\n\\[\n\\displaylines{\ny^{\\intercal}X\\hat{\\beta} =\n\\hat{\\beta}^{\\intercal} X^{\\intercal}y \\\\\n\\because \\underset{1 \\times n}{y^{\\intercal}} \\times\n\\underset{n \\times k}{X} \\times\n\\hat{\\underset{k \\times 1}{\\beta}}\n=\n\\hat{\\underset{1 \\times k}{\\beta}^{\\intercal}} \\times\n\\underset{k \\times n}{X^{\\intercal}} \\times\n\\underset{n \\times 1}{y}\n}\n\\]\n\n\n2. Partially differentiate RSS with respect to beta\nThe second step involves partially differentiating the cost function with respect to its parameters, to understand how it changes as the coefficients vary.\n\\[\n\\displaylines{\n\\begin{align}\n\\frac{\\partial}{\\partial\\hat{\\beta}}\\hat{\\epsilon}^{\\intercal}\\hat{\\epsilon} & \\equiv\n\\begin{bmatrix}\n    \\frac{\\partial}{\\partial\\hat{\\beta}_1}\\hat{\\epsilon}^{\\intercal}\\hat{\\epsilon} \\\\\n    \\frac{\\partial}{\\partial\\hat{\\beta}_2}\\hat{\\epsilon}^{\\intercal}\\hat{\\epsilon} \\\\\n    \\vdots \\\\\n    \\frac{\\partial}{\\partial\\hat{\\beta}_k}\\hat{\\epsilon}^{\\intercal}\\hat{\\epsilon}\n\\end{bmatrix}\n\\\\ & = \\frac{d}{d\\hat{\\beta}}(\ny^{\\intercal}y - 2y^{\\intercal}X\\hat{\\beta}\n+\\hat{\\beta}^{\\intercal}X^{\\intercal}X\\hat{\\beta}) \\\\ & = 0 - 2X^{\\intercal}y +((X^{\\intercal}X)\\hat{\\beta} + (X^{\\intercal}X)^{\\intercal}\\hat{\\beta})\n\\\\ & = -2X^{\\intercal}y +  2((X^{\\intercal}X)\\hat{\\beta})\n\\end{align}\n}\n\\]\n\n\nTwo matrix differentiation rules used here for reference:\n\\[\n\\displaylines{\n\\frac{\\partial}{dx}(Ax) = A^{\\intercal}x \\\\\n\\frac{\\partial}{dx}(xAx) = Ax + A^{\\intercal}x\n}\n\\]\nAnd note \\(X^{\\intercal}X = (X^{\\intercal}X)^{\\intercal}\\) by definition, so we can add the two last terms.\n\n\n3. Find the coefficient values at the stationary point\nNow we find the choices of \\(\\beta\\) where the partial differential is equal to zero. These stationary points for the cost function are either at its maximum or minimum.\nFor OLS - we actually only find one unique solution!\n\\[\n\\displaylines{\n\\begin{align}\n\\cancel{2}X^{\\intercal}y +  \\cancel{2}((X^{\\intercal}X)\\hat{\\beta}) & = 0 \\\\\n\\therefore (X^{\\intercal}X)\\hat{\\beta} & = X^{\\intercal}y \\\\\n\\therefore \\hat{\\beta} & = (X^{\\intercal}X)^{-1}X^{\\intercal}y\n\\end{align}\n}\n\\]\n\n\nNote the need to invert \\(X^{\\intercal}X\\). This is only possible for a full rank matrix.\nThe first term is the (inverse) variance matrix of \\(X\\). This term normalizes the coefficient with respect to the magnitude of \\(X\\).\nThe second term is the covariance matrix between \\(X\\) and \\(y\\). This incorporates the linear relationship between the two in the coefficient.\nHence, the coefficient can be interpreted as the estimated change in \\(y\\) given a one unit change in \\(X\\).\n\n\n4. Check the stationary point is a global minimum (hessian matrix)\nFinally, we derive the hessian matrix, by double-differentiating the cost function with respect to the coefficients:\n\\[\n\\displaylines{\n\\frac{\\partial^2}{\\partial\\hat{\\beta}\\partial\\hat{\\beta}^{\\intercal}}\n\\left(\n  \\hat{\\epsilon}^{\\intercal}\\hat{\\epsilon}\n\\right)\n\\\\ = \\frac{\\partial}{\\partial \\hat{\\beta}^{\\intercal}} \\left( -2X^{\\intercal}y +  2((X^{\\intercal}X)\\hat{\\beta}) \\right)\n\\\\ = 2(X^{\\intercal}X)^{\\intercal}\n}\n\\]\nSince \\(X^{\\intercal}X\\) is clearly positive definite, the cost function is convex. Thus, we know our unique solution for \\(\\beta\\) where the partial differential is at zero is indeed a global minimum for the cost function."
  },
  {
    "objectID": "posts/ols_coefs_multivariate.html#final-reflections",
    "href": "posts/ols_coefs_multivariate.html#final-reflections",
    "title": "Deriving OLS coefficients (multivariate)",
    "section": "Final reflections",
    "text": "Final reflections\nUnlike logistic regression, or the multiple hidden-layer structure of neural networks, we can “jump” straight to the optimum coefficients for OLS. Why can we do this? Well chiefly its because OLS is a bit of a special case:\n\nThe minima is a global minima: The hessian matrix is positive definite, and hence the cost function is strictly convex. This means we know that when a choice of coefficients is found that ensure the partially differentiated cost function is equal zero, this minima is also a global one, not a local one.\nThere is only one solution for the optimum coefficient: We assume that the matrix is full rank (every feature provides additional predictive power) and that the number of predictors is smaller than the number of obervations. This also means that partially differentiating is okay to do!\nA closed-form solution can be found The predictions are generated from \\(X\\) using a simple, purely algebraic function, i.e. the sum-product of \\(X\\) by \\(\\beta\\). This means we can find an analytical solution to the optimal choice \\(\\beta^*\\). Note this often isn’t possible since non-linear activation functions (i.e. link functions) are often transcendental.\n\nWe will dive into this in another post.\nFin."
  },
  {
    "objectID": "posts/ols_sandwich_estimators.html",
    "href": "posts/ols_sandwich_estimators.html",
    "title": "Sandwiches: robust covariance error estimation",
    "section": "",
    "text": "What are we exploring?\n\n\n\nEstimating the correct coefficient variance when relaxing homoskedastic error assumptions"
  },
  {
    "objectID": "posts/ols_sandwich_estimators.html#introducing-sandwiches",
    "href": "posts/ols_sandwich_estimators.html#introducing-sandwiches",
    "title": "Sandwiches: robust covariance error estimation",
    "section": "Introducing sandwiches",
    "text": "Introducing sandwiches\nThe variance for the OLS coefficient estimator is equal to the following:\n\\[\n\\displaylines{\nV(\\hat{\\beta}) =\n(X^{\\intercal}X)^{-1}X^{\\intercal}E[\\epsilon\\epsilon^{\\intercal}]X(X^{\\intercal}X)^{-1}\n}\n\\]\nThis can be though to as a sandwich:\n\nThe “bread” either side: \\((X^{\\intercal}X)^{-1}X^{\\intercal}\\) on the left and its transpose \\(X(X^{\\intercal}X)^{-1}\\) on the right\nThe “meat” in the middle: what we assume for \\(E[\\epsilon\\epsilon^{\\intercal}]\\)\n\nNote that this is the same as the error variance, since \\(V[\\epsilon]=E[\\epsilon\\epsilon^{\\intercal}]-E[\\epsilon]E[\\epsilon^{\\intercal}]\\) and \\(E[\\epsilon] = 0\\)\n\n\nOur coefficient will only be efficient if these assumptions about the expected error are correct! We will explore what happens when the errors are assumed to be homoskedastic, heteroskedastic or clustered."
  },
  {
    "objectID": "posts/ols_sandwich_estimators.html#salmon-bagel-spherical-errors",
    "href": "posts/ols_sandwich_estimators.html#salmon-bagel-spherical-errors",
    "title": "Sandwiches: robust covariance error estimation",
    "section": "Salmon bagel: Spherical Errors 🐟",
    "text": "Salmon bagel: Spherical Errors 🐟\nUsual OLS is efficient if the true model has “spherical errors”. What does this mean in practice?\n\nErrors are homoskedastic: \\(V(\\epsilon_i)=\\sigma^2\\) for all observations\nErrors are serially uncorrelated: \\(cov(\\epsilon_i,\\epsilon_{j\\neq i})=0\\)\n\nWhat does this look like for \\(E[\\epsilon\\epsilon^{\\intercal}]\\)?\n\nThe diagonal of the matrix is a constant value (scalar), \\(\\sigma^2\\)\nThe off-diagonals are all zero\n\n\\[\n\\hat{\\sigma}^2\\underset{n\\times n}{I} =\n\\begin{bmatrix}\n\\hat{\\sigma}^2 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n0 & \\hat{\\sigma}^2 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n0 & 0 & \\hat{\\sigma}^2 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n0 & 0 & 0 & \\hat{\\sigma}^2 & 0 & 0 & 0 & 0 & 0\\\\\n0 & 0 & 0 & 0 & \\hat{\\sigma}^2 & 0 & 0 & 0 & 0\\\\\n0 & 0 & 0 & 0 & 0 & \\hat{\\sigma}^2 & 0 & 0 & 0\\\\\n0 & 0 & 0 & 0 & 0 & 0 & \\hat{\\sigma}^2 & 0 & 0\\\\\n0 & 0 & 0 & 0 & 0 & 0 & 0 & \\hat{\\sigma}^2 & 0\\\\\n0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & \\hat{\\sigma}^2\n\\end{bmatrix}\n\\]\nA good estimation of the constant error variance \\(\\sigma^2\\) is to apply the standard formula to the residuals (i.e. method of moments):\n\\[\n\\hat{\\sigma^2}=\\frac{1}{n-k}\\sum{\\hat{\\epsilon_i}^2} \\equiv \\frac{\\epsilon^{\\intercal}\\epsilon }{n-k}\n\\]\n\n\n\n\n\n\nThus our “salmon sandwich” is:\n\n\n\n\\[\n\\underset{k \\times k}{(X^{\\intercal}X)}^{-1}\n\\underset{k \\times n}\n{X^{\\intercal}}\n\\left(\\frac{1}{n-k} \\times \\underset{1 \\times 1}{(\\epsilon^{\\intercal}\\epsilon)}\n\\times\\underset{n \\times n}{I} \\right)\n\\underset{n \\times k}{X}\n\\underset{k \\times k}{(X^{\\intercal}X)}^{-1}\n\\]\n\n\n\n\nNote that under spherical errors, the sandwich can be simplified: \\[\n\\displaylines{\n\\begin{align}\nV[\\hat{\\beta}]\n& = (X^{\\intercal}X)^{-1}X^{\\intercal}E[\\epsilon\\epsilon^{\\intercal}]X(X^{\\intercal}X)^{-1}\n\\\\ & = (X^{\\intercal}X)^{-1}X^{\\intercal}\\sigma^2IX(X^{\\intercal}X)^{-1}\n\\\\ & = \\sigma^2(X^{\\intercal}X)^{-1}\\cancel{X^{\\intercal}X}\\cancel{(X^{\\intercal}X)^{-1}}\n\\\\ & = \\sigma^2(X^{\\intercal}X)^{-1}\n\\end{align}\n}\n\\]\nIn this scenario, the only things that impact the standard error of the coefficient \\(\\beta_k\\) is:\n\nThe variance of all the residuals, \\(\\sigma^2\\)\nThe variance of the feature \\(V(X_k)\\)"
  },
  {
    "objectID": "posts/ols_sandwich_estimators.html#ham-sarnie-heteroskedastic-errors",
    "href": "posts/ols_sandwich_estimators.html#ham-sarnie-heteroskedastic-errors",
    "title": "Sandwiches: robust covariance error estimation",
    "section": "Ham sarnie: Heteroskedastic errors 🍖",
    "text": "Ham sarnie: Heteroskedastic errors 🍖\nHeteroskedastic correction is needed if:\n\nErrors vary for every individual: \\(V(\\epsilon_i)=\\sigma_i^2\\) for all observations\nBut they are still independent aka serially uncorrelated: \\(cov(\\epsilon_i,\\epsilon_{j\\neq i})=0\\)\n\nWhat does this look like for \\(E[\\epsilon\\epsilon^{\\intercal}]\\)?\n\nThe diagonal of the matrix is the estimate of variance which is unique for each observation, \\(\\sigma_i^2\\)\nThe off-diagonals are all zero\n\\[\n  \\underset{n \\times n}{\\sigma^2} = \\begin{bmatrix}\n  \\sigma_1^2 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n  0 & \\sigma_2^2 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n  0 & 0 & \\sigma_3^2 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n  0 & 0 & 0 & \\sigma_4^2 & 0 & 0 & 0 & 0 & 0\\\\\n  0 & 0 & 0 & 0 & \\sigma_5^2 & 0 & 0 & 0 & 0\\\\\n  0 & 0 & 0 & 0 & 0 & \\sigma_6^2 & 0 & 0 & 0\\\\\n  0 & 0 & 0 & 0 & 0 & 0 & \\sigma_7^2 & 0 & 0\\\\\n  0 & 0 & 0 & 0 & 0 & 0 & 0 & \\sigma_8^2 & 0\\\\\n  0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & \\sigma_{..}^2\\\\\n  \\end{bmatrix}\n  \\]\nA good estimation of the vector of heteroskedastic error variances \\(\\sigma^2\\) is again to apply the standard formula to the residuals for each individual:\n\n\\[\n\\hat{\\sigma_i^2}=\\frac{1}{n-k}\\sum{\\hat{\\epsilon_i}^2}\n\\]\n\n\n\n\n\n\nThus our “ham sandwich” is:\n\n\n\n\\[\n\\underset{k \\times k}{(X^{\\intercal}X)}^{-1}\n\\underset{k \\times n}{X^{\\intercal}}\n\\left(\\frac{1}{n-k} \\times \\underset{1 \\times n}{(\\epsilon\\odot \\epsilon)} ^{\\intercal}\n\\times\\underset{n \\times n}{I} \\right)\n\\underset{n \\times k}{X}\n\\underset{k \\times k}{(X^{\\intercal}X)}^{-1}\n\\]\n\n\nSimilar to homoskedastic errors, the things that impacts the standard error of coefficient \\(k\\) is:\n\nThe variance of all the individual errors, \\(\\sigma_i^2\\)\nThe variance of the feature \\(V(X_k)\\)"
  },
  {
    "objectID": "posts/ols_sandwich_estimators.html#cheese-roll-clustered-errors",
    "href": "posts/ols_sandwich_estimators.html#cheese-roll-clustered-errors",
    "title": "Sandwiches: robust covariance error estimation",
    "section": "Cheese roll: Clustered Errors 🧀",
    "text": "Cheese roll: Clustered Errors 🧀\nCluster-robust errors are needed if:\n\nErrors vary for every individual: \\(V(\\epsilon_i)=\\sigma_i^2\\) for all observations i.e. still heteroskedastic\nAND errors within the same cluster \\(C_l\\) are serially correlated: \\(cov(\\epsilon_i,\\epsilon_{j}) \\neq 0 \\text{ if } \\epsilon_i,\\epsilon_j \\in C_l\\)\n\nNote - errors between clusters are assumed not to be serially correlated though i.e. \\(cov(\\epsilon_i,\\epsilon_{j}) =0 \\text{ if } \\epsilon_i \\in C_l,\\epsilon_j \\in C_{m \\neq l}\\)\n\n\nWhat does this look like for \\(E[\\epsilon\\epsilon^{\\intercal}]\\)?\n\nThe diagonal of the matrix is the estimate of variance which is unique for each observation, \\(\\sigma_i^2\\)\nThe off-diagonals are also populated with the covariance - but only when they are both in the same cluster\n\nHere is an example where observations 1, 2 and 3 are in cluster A, observations 4 and 5 are in cluster B, observations 6, 7 and 8 are in cluster C etc.\n\\[\n\\underset{n \\times n}{\\epsilon\\epsilon^{\\intercal}} \\sim \\begin{bmatrix}\n\\epsilon_1^2 & \\epsilon_1\\epsilon_2 & \\epsilon_1\\epsilon_3 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n\\epsilon_2\\epsilon_1 & \\epsilon_2^2 & \\epsilon_2\\epsilon_3 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n\\epsilon_3\\epsilon_1 & \\epsilon_3\\epsilon_2 & \\epsilon_3^2 & 0 & 0 & 0 & 0 & 0 & 0\\\\\n0 & 0 & 0 & \\epsilon_4^2 & \\epsilon_4\\epsilon_5 & 0 & 0 & 0 & 0\\\\\n0 & 0 & 0 & \\epsilon_5\\epsilon_4 & \\epsilon_5^2 & 0 & 0 & 0 & 0\\\\\n0 & 0 & 0 & 0 & 0 & \\epsilon_6^2 & \\epsilon_6\\epsilon_7 & \\epsilon_6\\epsilon_8 & 0\\\\\n0 & 0 & 0 & 0 & 0 & \\epsilon_7\\epsilon_6 & \\epsilon_7^2 & \\epsilon_7\\epsilon_8 & 0\\\\\n0 & 0 & 0 & 0 & 0 & \\epsilon_8\\epsilon_6 & \\epsilon_8\\epsilon_7 & \\epsilon_8^2 & 0\\\\\n0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & \\epsilon_{..}^2\\\\\n\\end{bmatrix}\n\\]\nAdditionally, we have to do a finite-sample correction as well on the degrees of freedom, based on the number of clusters \\(n_c\\)\n\n\n\n\n\n\nThus our cheese sandwich is:\n\n\n\n\\(\\underset{k \\times k}{(X^{\\intercal}X)}^{-1}\n\\underset{k \\times n}{X^{\\intercal}}\n\\left(\\frac{n-1}{n-k}\\frac{n_c}{n_c-1} \\times\n\\underset{n \\times n}{(\\epsilon \\epsilon^{\\intercal})}\n\\right)\n\\underset{n \\times k}{X}\n\\underset{k \\times k}{(X^{\\intercal}X)}^{-1}\\)\n\n\nIn this scenario, there are a few additional things that impact the standard error of coefficient \\(k\\):\n\nIf errors are correlated within clusters, this will increase the error.\nIf features are correlated within clusters, this will also increase the error (due to the off-diagonals in the error variance matrix)\nAnd if both the errors and feature correlations are the same sign, this will also increase the standard error.\nAs well as the variance of the individual errors, \\(\\sigma_i^2\\), and the variance of each feature \\(V(X_k)\\), as before"
  },
  {
    "objectID": "posts/mle_ols_normal.html",
    "href": "posts/mle_ols_normal.html",
    "title": "OLS vs MLE with gaussian noise",
    "section": "",
    "text": "What are we exploring?\n\n\n\nWhy MLE finds the same coefficients as OLS when assuming model errors are generated from a mean-zero gaussian probabilistic process\n\n\n\nNormality of errors\nLet’s assume the errors follow a normal distribution with a mean of zero: \\[\n\\epsilon  = y - X\\beta \\sim \\mathcal{N}(0,\\sigma^2)\n\\]\n\nYou might already notice how similar this is to the Gauss-Markov requirements to ensure OLS coefficients are BLUE!\n\nThe expected error is zero, and consistent for all values of X, so we have “strict exogeneity”: \\(E[\\epsilon|X] = 0\\)\nThe error variance is uniform, again consistent for all values of X, so we have “spherical errors”: \\(E[\\epsilon\\epsilon^{\\intercal}|X] = 0\\)\n\n\n\nApplying the normal pdf\nFor any datapoint \\(i\\), we can formulate the likelihood of observing the outcome \\(y_i\\) as being generated from the normal probability density function applied to the squared error:\n\\[\n\\displaylines{\n\\begin{align}\np(y_i|\\beta,X_i,\\sigma^2)\n& = \\frac{1}{\\sigma\\sqrt{2\\pi}}\\exp{\\left\\{-\\frac{1}{2\\sigma^2}\\epsilon_i^2\\right\\}} \\\\\n& = \\frac{1}{\\sigma\\sqrt{2\\pi}}\\exp{\\left\\{-\\frac{1}{2\\sigma^2}(y_i-X_i\\beta)^2\\right\\}}\n\\end{align}\n}\n\\]\nMaximum likelihood estimation aims to find the set of coefficients that maximises the likelihood of observing the evidence we have. We thus aim to find the coefficients \\(\\beta\\) that maximise the likelihood of observing \\(y\\) across all \\(n\\) samples:\n\\[\n\\displaylines{\n\\begin{align}\np(y|\\beta,X,\\sigma^2)\n& = \\prod_{i=1}^{n}{\\frac{1}{\\sigma\\sqrt{2\\pi}}}\\exp{\\left\\{-\\frac{1}{2\\sigma^2}\\epsilon_i^2\\right\\}}\n\\end{align}\n}\n\\]\n\n\nTaking the negative log-likelihood\nIn practice, dealing with a cost function made up of a sum product is tricky - it is easier to take the log and deal with addition instead. Further, rather than maximise, it is common to “minimize” cost functions, so the negative log-likelihood is usually used.\n\n\nRecall that \\(\\log{\\left(ab\\right)} = \\log{\\left(a\\right)} + \\log{\\left(b\\right)}\\)\n\\[\n\\displaylines{\n\\begin{align}\n\\max_\\beta{p(y|\\beta,X,\\sigma^2)}\n= &\n\\max_\\beta{\\left[ \\prod{ \\frac{1}{\\sigma\\sqrt{2\\pi}}\\exp{\\left\\{-\\frac{1}{2\\sigma^2}\\epsilon_i^2\\right\\}} } \\right]}\n\\\\ \\\\ \\Rightarrow &\n\\min_\\beta{\\left[ -\\sum{\\log{\\left(\\frac{1}{\\sigma\\sqrt{2\\pi}}\\exp{\\left\\{-\\frac{1}{2\\sigma^2}\\epsilon_i^2\\right\\}}\\right)}} \\right]}\n\\end{align}\n}\n\\]\n\n\nSimplifying the cost function\nAnd now we can look to simplify this: \\[\n\\displaylines{\n\\begin{align}\n& \\min_\\beta{\\left[ -\\sum{\\log{\\left(\\frac{1}{\\sigma\\sqrt{2\\pi}}\\exp{\\left\\{-\\frac{1}{2\\sigma^2}\\epsilon_i^2\\right\\}}\\right)}} \\right]}\n\\\\ = & \\min_\\beta{\\left[ -\\sum{\\log{\\left(\\frac{1}{\\sigma\\sqrt{2\\pi}}\\right)}} -\\sum{\\log{\\left(\\exp{\\left\\{-\\frac{1}{2\\sigma^2}\\epsilon_i^2\\right\\}}\\right)}} \\right]}\n\\\\ = & \\min_\\beta{\\left[ -\\sum{\\log{((2\\pi\\sigma^2)^{-\\frac{1}{2}})}} - \\sum{\\left(-\\frac{1}{2\\sigma^2} \\epsilon_i^2\\right)} \\right]}\n\\\\ = & \\min_\\beta{\\left[ \\frac{1}{2}\\sum{\\log{(2\\pi\\sigma^2)}} + \\frac{1}{2\\sigma^2}\\sum{\\epsilon_i^2} \\right]}\n\\\\ = & \\min_\\beta{\\left[ \\frac{1}{2} \\left(n\\log{(2\\pi\\sigma^2)} + \\frac{1}{\\sigma^{2}} \\sum{\\epsilon_i^2}  \\right)\\right]}\n\\end{align}\n}\n\\]\n\n\nCoefficient point-estimate is the same as OLS\nWe minimise the cost function by finding the optimum coefficient values \\(\\beta^*\\) so that the partial differential is equal to zero.\nThe constant \\(\\log{(2\\pi\\sigma^2)}\\) doesn’t vary with respect to \\(\\beta\\), so it drops out. The fraction \\(\\frac{1}{2}\\) also drops out when finding where differential is set to zero.\nHence we are left finding that we are solving the same problem as usual least-squares!\n\\[\n\\displaylines{\n\\begin{align}\n\\therefore \\beta^* & =\\arg\\min_\\beta{\\left[ \\frac{1}{2} \\left(n\\log{(2\\pi\\sigma^2)} + \\frac{1}{\\sigma^{2}} \\sum{\\epsilon_i^2}  \\right)\\right]}\n\\\\ & =\\arg\\min_\\beta{\\left[ \\epsilon^T\\epsilon \\right]}\n\\end{align}\n}\n\\]\n\n\nError-variance estimate is the same as OLS\nOLS estimates the variance of the models errors using the residuals from the sample:\n\\[\n\\sigma^2 = \\frac{1}{n}\\hat{\\epsilon}^{\\intercal}\\hat{\\epsilon}\n\\]\nDo we see the same with MLE? Well so far we have only found the optimum \\(\\hat{\\beta}^*\\) to ensure the expected conditional error is zero, we haven’t touched our other parameter \\(\\sigma^2\\).\nNow lets instead find the estimate of \\(\\sigma\\) that minimizes the negative log-likelihood:\n\\[\n\\displaylines{\n\\begin{align}\n& \\min_{\\sigma^2}{\\left[ \\frac{1}{2} \\left(n\\log{(2\\pi\\sigma^2)} + \\frac{1}{\\sigma^{2}} \\sum{\\epsilon_i^2}  \\right)\\right]} \\\\\n\\Rightarrow &\n\\frac{\\partial}{\\partial\\sigma^2}\n\\left[ \\frac{1}{2} \\left(n\\log{(2\\pi\\sigma^2)} + \\frac{1}{\\sigma^{2}} \\sum{\\epsilon_i^2}  \\right)\\right]\n\\\\ & =\n\\frac{1}{2} \\left(n\\frac{2\\pi}{2\\pi\\sigma^2} + \\frac{-2}{\\sigma^4} \\sum{\\epsilon_i^2}  \\right)\n\\\\ & =\n\\frac{n}{2\\sigma^2} - \\frac{1}{2\\sigma^4} \\sum{\\epsilon_i^2} = 0 \\\\\n& \\therefore \\sigma^2 = \\frac{1}{n}\\sum{\\epsilon_i^2} = 0 \\\\\n\\end{align}\n}\n\\]\nand hence we can see that the estimation of OLS is the same too.\n\n\nFinal reflections\nOne advantage of using MLE is we can generate a probabilistic estimate for \\(y_i\\), rather than just a point-estimate (assuming we have fitted \\(\\hat{\\sigma}^2\\) as above).\n\nPoint estimate: \\(\\hat{y} = X\\hat{\\beta}\\)\nPosterior estimate: \\(P(\\hat{y_i}| X_i,\\hat{\\beta},\\sigma^2) = \\mathcal{N}(\\hat{y_i}| X_i\\hat{\\beta},\\sigma^2)\\),\n\nYou might already have started to see how probabilistic predictions and coefficients fit by MLE nicely fit into the bayesian paradigm. This opens up nice extensions,such as using priors as a form of regularization. This is for another post though!\nFin."
  },
  {
    "objectID": "posts/ols_blue.html",
    "href": "posts/ols_blue.html",
    "title": "BLUE coefficients: bias and efficiency",
    "section": "",
    "text": "What are we exploring?\n\n\n\nProving that the OLS coefficient is the best linear unbiased estimator."
  },
  {
    "objectID": "posts/ols_blue.html#are-the-ols-coefficients-blue",
    "href": "posts/ols_blue.html#are-the-ols-coefficients-blue",
    "title": "BLUE coefficients: bias and efficiency",
    "section": "Are the OLS coefficients “BLUE”?",
    "text": "Are the OLS coefficients “BLUE”?\nWe find a unique solution to the set of coefficients that minimize the sum of squared residuals analytically (see its derivation here):\n\\[\n\\hat{\\beta}^{OLS}=(X^{\\intercal}X)^{-1}X^{\\intercal}y\n\\]\nHowever, how do we know if these coefficients are the best ones we can estimate?\nFor the estimated coefficients to be the Best Linear Unbiased Estimator (i.e. “BLUE”):\n\nThe best estimator has to be unbiased: \\(E[\\hat{\\beta}^*] = \\beta\\)\nAnd among all possible linear, unbiased estimators, it must have the smallest variance: \\(V[\\hat{\\beta}^{*}] &lt; V[\\hat{\\beta}^{Z}]\\)\n\nWe want to ensure our OLS estimate is the best, i.e. that \\(\\hat{\\beta}^{OLS} = \\hat{\\beta}^{*}\\). To achieve this, first we need to confirm it is unbiased. Then given this is true, we can check that the coefficient is most efficient vs all other unbiased estimators.\n\n\n\n\n\n\nGauss Markov Assumptions\n\n\n\nAlong the way, we will outline the Gauss-Markov assumptions utilised that ensure the OLS coefficient is BLUE."
  },
  {
    "objectID": "posts/ols_blue.html#setting-the-scene",
    "href": "posts/ols_blue.html#setting-the-scene",
    "title": "BLUE coefficients: bias and efficiency",
    "section": "Setting the scene",
    "text": "Setting the scene\n\nThe true coefficient and model\n\\(\\beta\\) is the true (unobserved) relationship between all the relevant explanatory features, \\(X\\), and their associated observed outcomes, \\(y\\). In other words, if we observed the entire population of data, it is the relationship we would find.\nConcretely, we assume the outcome is a linear function of all its relevant features. This “true model” perfectly predicts the outcome, except for random noise \\(\\epsilon\\) that influences the observed outcome: \\(y = X\\beta + \\epsilon\\)\n\n\nThe estimated coefficient\n\\(\\hat{\\beta}\\) is our estimated coefficient for the true relationship \\(\\beta\\). In reality, we estimate \\(\\hat{\\beta}\\) from the small, finite sample of size \\(n\\) that is collected, not the whole population. Given any random sample could be collected, we can term the coefficient resulting from the optimum estimation proceedure to be \\(\\hat{\\beta}^*\\). We want to understand if \\(\\hat{\\beta}^{OLS} = \\hat{\\beta}^*\\).\n\n\nThe expected estimated coefficient\n\\(E[\\hat{\\beta}]\\) is the “expected” estimated coefficient. Imagine we repeat the action of estimating the coefficient \\(\\hat{\\beta}\\) many times, each time collecting a new sample (where each observation is sampled i.i.d), and recording the value for the estimated coefficient. \\(E[\\hat{\\beta}]\\) would then be the average of all of those estimated coefficients. If the OLS coefficient is unbiased, then the expected coefficient estimate should be equal to the true one, \\(E[\\hat{\\beta}^{OLS}]=\\beta\\).\n\n\nThe variance of the estimated coefficient\n\\(V[\\hat{\\beta}]\\) is the variance of the estimated coefficient. It determines how much we might expect our estimate \\(\\hat{\\beta}\\) to differ from the true \\(\\beta\\) for any sample drawn. Given the OLS coefficient has been shown to be unbiased, if it is BLUE we expect its variance to be lower than another other unbiased choice \\(\\hat{\\beta}^{Z}\\). Concretely, we want to find \\(V[\\hat{\\beta}^{OLS}] &lt; V[\\hat{\\beta}^{Z}]\\)."
  },
  {
    "objectID": "posts/ols_blue.html#bias",
    "href": "posts/ols_blue.html#bias",
    "title": "BLUE coefficients: bias and efficiency",
    "section": "Bias",
    "text": "Bias\nOften our small finite samples of size \\(n\\) are not a perfect reflection of the population they are drawn from. This “sampling error” means we might estimate a different relationship between \\(X\\) and \\(y\\) than the true relationship of the population, i.e. \\(\\hat{\\beta} \\neq \\beta\\).\nHowever, we should expect our estimated coefficient to be equal to the true value on average. This means we do not want to have a bias towards the estimate being systematically too small or too large, for example. In other words, if we repeated the whole proceedure thousands of times (each time taking new samples, and estimating a coefficient from the new sample) then the average of all the estimated coefficients values should be equal to the true value, i.e. \\(E[\\hat{\\beta}] = \\beta\\).\nRecall that we believe there is a true model that follows the form:\n\\[\ny = X\\beta + \\epsilon\n\\]\n\n\n\n\n\n\nGM1: Linearity \n\n\n\nThe formula above relies on the first Gauss-Markov assumption - that the dependent variable \\(y\\) is assumed to be a linear function of the variables \\(X\\). Note that implies that the proper functional form has been selected (i.e. the relationship is linear) and there are no omitted variables - a huge assumption!\n\n\nIf we substitute this into our estimated coefficient:\n\\[\n\\displaylines{\n\\begin{align}\n\\hat{\\beta}^{OLS} & = (X^{\\intercal}X)^{-1}X^{\\intercal}y\n\\\\ & = (X^{\\intercal}X)^{-1}X^{\\intercal}(X\\beta+\\epsilon)\n\\\\ & = (X^{\\intercal}X)^{-1}X^{\\intercal}X\\beta+(X^{\\intercal}X)^{-1}X^{\\intercal}\\epsilon\n\\\\ & = \\beta+(X^{\\intercal}X)^{-1}X^{\\intercal}\\epsilon\n\\end{align}\n}\n\\]\nwe show that the estimated coefficient \\(\\hat{\\beta}^{OLS}\\) will differ from the true value depending on the random error \\(\\epsilon\\) associated with the particular finite sample collected.\nNow let’s now take the expectation, to determine when the coefficient is unbiased. In other words, what is the “average” coefficient if we took the sample many times: \\[\n\\displaylines{\n\\begin{align}\nE[\\hat{\\beta}^{OLS}]\n& = \\beta +(X^{\\intercal}X)^{-1}X^{\\intercal}E[\\epsilon]\n\\\\ & = \\beta & \\iff E[\\epsilon] & = 0\n\\end{align}\n\\\\\n}\n\\]\nWe find that the coefficient is unbiased as long as the expected error is also zero.\n\n\n\n\n\n\nGM2: Strict Exogeneity \n\n\n\nThe second Gauss-Markov assumption is strict exogeneity, where the expected error is zero for all feature values: \\(E[\\epsilon|X] = 0\\). By definition, the weaker exogeneity statement of \\(E[\\epsilon] = 0\\) is implied by having the expected error conditional being equal to zero."
  },
  {
    "objectID": "posts/ols_blue.html#efficiency",
    "href": "posts/ols_blue.html#efficiency",
    "title": "BLUE coefficients: bias and efficiency",
    "section": "Efficiency",
    "text": "Efficiency\nTo be the “best”, the OLS estimator also needs to be efficient. This means that it has the lowest variance of all unbiased estimators. This section looks to prove this.\n\nCoefficient variance for OLS\nFirst, let’s derive the variance from the coefficients estimated using OLS, termed \\(V[\\hat{\\beta}^{OLS}]\\). As before, we substitute the true model \\(y = X\\beta + \\epsilon\\) into the coefficient estimated through OLS:\n\\[\n\\displaylines{\n\\begin{align}\n\\hat{\\beta}^{OLS}-\\beta\n& = \\beta +  ((X^{\\intercal}X)^{-1}X^{\\intercal}\\epsilon) - \\beta \\\\\n& = (X^{\\intercal}X)^{-1}X^{\\intercal}\\epsilon\n\\\\ \\\\\n\\therefore\n(\\hat{\\beta}^{OLS}-\\beta)(\\hat{\\beta}^{OLS}-\\beta)^{\\intercal}\n& = ((X^{\\intercal}X)^{-1}X^{\\intercal}\\epsilon)((X^{\\intercal}X)^{-1}X^{\\intercal}\\epsilon)^{\\intercal}\n\\\\ & =\n(X^{\\intercal}X)^{-1}X^{\\intercal}\\epsilon\\epsilon^{\\intercal}X(X^{\\intercal}X)^{-1} \\\\ \\\\\n\\therefore\nV(\\hat{\\beta}^{OLS}) & = E[(\\hat{\\beta}^{OLS}-\\beta)(\\hat{\\beta}^{OLS}-\\beta)^{\\intercal}]\n\\\\ & =\n(X^{\\intercal}X)^{-1}X^{\\intercal}E[\\epsilon\\epsilon^{\\intercal}]X(X^{\\intercal}X)^{-1}\n\\end{align}\n}\n\\]\nThis is sometimes called the sandwich estimator - post soon to follow on this!\n\n\nCoefficient variance assuming “spherical errors”\nWe can simplify this further by appling some assumptions to the estimated error variance \\(E[\\epsilon\\epsilon^{\\intercal}]\\):\n\\[\n\\displaylines{\n\\begin{align}\nE[\\epsilon \\epsilon^{\\intercal}] & =  \n\\begin{bmatrix}\nE[\\epsilon_1^2] & \\cdots & E[\\epsilon_1\\epsilon_n] \\\\\n\\vdots & \\ddots & \\vdots \\\\\nE[\\epsilon_n\\epsilon_1] & \\cdots & E[\\epsilon_n^2]\n\\end{bmatrix} \\\\ \\\\\n& = \\begin{bmatrix}\n\\frac{1}{n}\\sum_{i=1}^{n}{\\epsilon_i^2} & \\cdots & 0 \\\\\n\\vdots & \\ddots & \\vdots \\\\\n0 & \\cdots & \\frac{1}{n}\\sum_{i=1}^{n}{\\epsilon_i^2}\n\\end{bmatrix} \\\\ \\\\\n& = \\begin{bmatrix}\n\\hat{\\sigma}^2 & \\cdots & 0 \\\\\n\\vdots & \\ddots & \\vdots \\\\\n0 & \\cdots & \\hat{\\sigma}^2\n\\end{bmatrix} \\\\ \\\\\n& = \\hat{\\sigma}^2I\n\\end{align}\n}\n\\]\nHow can we jump to this result? Well we are making two assumptions:\n\nNo serial correlation: \\(\\rho_{\\epsilon_{i},\\epsilon_{i \\neq j}} = 0\\). No correlation between sample errors means that \\(E[\\epsilon_i \\epsilon_{j \\neq i}] = 0\\), and hence the off-diagonals of the error covariance matrix are zero.\nHomoskedasticity: the assumption of uniform error variance for all samples means that \\(V[\\epsilon_i^2] = V[\\epsilon_{j \\neq i}^2] = \\hat{\\sigma}^2\\). And our best approximation for \\(\\hat{\\sigma}^2\\) is simply taking the average squared error: \\(\\hat{\\sigma}^2 = \\frac{1}{n}\\sum_{i=1}^{n}{\\epsilon_i^2}\\)\n\n\n\n\n\n\n\nGM3: Spherical errors \n\n\n\nThe third Gauss-Markov assumption is spherical errors, \\(E[\\epsilon\\epsilon^{\\intercal}|X] = 0\\). This means that the outer product of the expected errors is a scalar matrix, which implies no serial correlation and homoskedasticity.\n\n\n\n\nIt is especially important to make the right assumptions about \\(E[\\epsilon\\epsilon^{\\intercal}|X]\\) as it impacts where our estimate of the standard errors is correct! We will dive into what happens to SE if we violate these assumptions in another post.\nSince we now see that \\(\\hat{\\sigma}^2\\) is a scalar matrix, we can thus simplfy the variance formula further:\n\\[\n\\displaylines{\n\\begin{align}\nV(\\hat{\\beta}^{OLS}) & =\n(X^{\\intercal}X)^{-1}X^{\\intercal}E[\\epsilon\\epsilon^{\\intercal}]X(X^{\\intercal}X)^{-1}\n\\\\ & =\n(X^{\\intercal}X)^{-1}X^{\\intercal} \\hat{\\sigma}^2I X(X^{\\intercal}X)^{-1}\n\\\\ & =\n\\hat{\\sigma}^2\n\\cancel{(X^{\\intercal}X)^{-1}}\n\\cancel{X^{\\intercal} X}\n(X^{\\intercal}X)^{-1}\n\\\\ & = \\hat{\\sigma}^2(X^{\\intercal}X)^{-1}\n\\end{align}\n}\n\\]\n\n\nFormulating an alternative unbiased coefficient\nNext step - lets formulate another estimator, \\(\\hat{\\beta}^{z}\\), which differs from \\(\\hat{\\beta}^{OLS}\\) by a non-zero matrix \\(A\\). See how they both differ below:\n\\[\n\\displaylines{\n\\begin{align}\n\\hat{\\beta}^{OLS} & =(X^{\\intercal}X)^{-1}X^{\\intercal}y \\\\\n\\hat{\\beta}^{Z} & =\\left((X^{\\intercal}X)^{-1}X^{\\intercal}+A\\right)y\n\\end{align}\n}\n\\]\nNow we need to ensure this new estimator is not biased. So by taking the expectation in the same was as for OLS…\n\\[\n\\displaylines{\n\\begin{align}\nE[\\hat{\\beta}^{Z}] & = E\\left[ \\left((X^{\\intercal}X)^{-1}X^{\\intercal}+A\\right)y \\right]\n\\\\ & = \\left((X^{\\intercal}X)^{-1}X^{\\intercal}+A\\right)(X\\beta+ \\cancel{E\\left[\\epsilon \\right]}) & \\because E[\\epsilon] = 0\n\\\\ & = (X^{\\intercal}X)^{-1}X^{\\intercal}X\\beta+AX\\beta\n\\\\ & = \\beta+AX\\beta\n\\end{align}\n}\n\\]\nSo the estimator is only unbiased iff \\(AX=0\\). This is important to note when comparing the variance between unbiased coefficients - see below!\n\n\nVariance of the alternative unbiased coefficient\nJust like before, we calculate the variance:\n\\[\n\\displaylines{\n\\begin{align}\nV[\\hat{\\beta}^{Z}]\n& = V\\left[ \\left((X^{\\intercal}X)^{-1}X^{\\intercal}+A\\right)y \\right]\n\\\\ & = \\left((X^{\\intercal}X)^{-1}X^{\\intercal}+A\\right) V[y] \\left((X^{\\intercal}X)^{-1}X^{\\intercal}+A\\right)^{\\intercal}\n\\\\ & = \\hat{\\sigma}^2 \\left((X^{\\intercal}X)^{-1}X^{\\intercal}+A\\right) \\left((X^{\\intercal}X)^{-1}X^{\\intercal}+A\\right)^{\\intercal}\n& \\because E[\\epsilon \\epsilon^{\\intercal}|X] = 0\n\\\\ & = \\hat{\\sigma}^2\\left((X^{\\intercal}X)^{-1}X^{\\intercal}+A\\right) \\left(X(X^{\\intercal}X)^{-1}+A^{\\intercal}\\right)\n\\\\ & = \\hat{\\sigma}^2 \\left(\n(X^{\\intercal}X)^{-1}X^{\\intercal} X(X^{\\intercal}X)^{-1} + AX(X^{\\intercal}X)^{-1} + (X^{\\intercal}X)^{-1}X^{\\intercal}A^{\\intercal} + AA^{\\intercal}\n\\right)\n\\\\ & = \\hat{\\sigma}^2\n(X^{\\intercal}X)^{-1} + \\hat{\\sigma}^2AA^{\\intercal}\n& \\because AX = 0\n\\\\ & = V[\\beta^{OLS}] + \\hat{\\sigma}^2AA^{\\intercal}\n\\end{align}\n}\n\\]\nNow since AA is surely a positive semi-definite matrix, then we know that \\(V[\\hat{\\beta}^{Z}] &gt; V[\\hat{\\beta}^{OLS}]\\).\nWe have shown that \\(\\hat{\\beta}^{OLS}\\) has the smallest variance among all unbiased estimators!"
  },
  {
    "objectID": "posts/ols_blue.html#summarising-the-gauss-markov-assumptions",
    "href": "posts/ols_blue.html#summarising-the-gauss-markov-assumptions",
    "title": "BLUE coefficients: bias and efficiency",
    "section": "Summarising the Gauss-markov assumptions",
    "text": "Summarising the Gauss-markov assumptions\nAlong the way, we showed where assumptions were needed to ensure the OLS coefficient estimation is BLUE.\nWe actually missed one out, but it is actually quite trivial to see from the OLS coefficient formula:\n\\[\n\\hat{\\beta}^{OLS}=(X^{\\intercal}X)^{-1}X^{\\intercal}y\n\\]\n\n\n\n\n\n\nGM4: Full rank \n\n\n\nThe matrix \\(X\\) must be of full rank \\(k\\), so that it is possible to invert the matrix \\(X^{\\intercal}X\\). This is equivalent to having no perfect multi-collinearity.\n\n\nWe have now collected our full set of Gauss-Markov assumptions required for the OLS coefficient to be BLUE:\n\nLinearity\nStrict Exogeneity\nSpherical Errors\nFull rank"
  },
  {
    "objectID": "posts/techniques_differentiation.html",
    "href": "posts/techniques_differentiation.html",
    "title": "Differentiation formulae",
    "section": "",
    "text": "What we are solving\n\n\n\nDeriving why \\(\\frac{d}{dx} \\left(x^{n} \\right) = nx^{n-1}\\)"
  },
  {
    "objectID": "posts/techniques_differentiation.html#formulating-an-approach-to-the-problem",
    "href": "posts/techniques_differentiation.html#formulating-an-approach-to-the-problem",
    "title": "Differentiation formulae",
    "section": "Formulating an approach to the problem",
    "text": "Formulating an approach to the problem\nThe gradient between two points, \\(x\\) and \\(a\\), is equal to the change in the y axis divided by the change in the x axis.\nFor example, if \\(y=f(x)=x^n\\), then we can approximate the gradient as follows:\n\\[\n\\frac{\\Delta y}{\\Delta x} =\n\\frac{f(x)-f(a)}{x-a} =\n\\frac{x^n-a^n}{x-a}\n\\]\nThis isn’t the same as saying a line which passes through both of these points has the same gradient though. There is a gap between this linear approximation and the exact curve.\nHowever, the smaller the movement across the x-axis, the closer the approximation is to the actual curve. For example, in Figure 1 below, the line drawn between \\(f(1)\\) and \\(f(2)\\) is closer to the gradient at \\(f(1)\\) of the true curve, compared to the line drawn between \\(f(1)\\) and \\(f(3)\\).\n\n\nCode\nimport plotly.graph_objects as go\nimport numpy as np\n\ndef interpolate(n,x1,x2,num=50):\n  x = np.linspace(x1,x2,num)\n  m = (x2**n - x1**n)/(x2 - x1)\n  c = x1**n - m*x1\n  y = m*x+c\n  return({'x':x,'y':y})\n\nn=3\nactual = {'x': np.arange(0,4,0.1)}\nactual['y'] = actual['x']**n\nlb, ub = 1, [3.5,3,2]\nlin0 = interpolate(n,lb,ub[0])\nlin1 = interpolate(n,lb,ub[1])\nlin2 = interpolate(n,lb,ub[2])\n\nfig = go.Figure(data = go.Scatter(mode='lines'))\nfig.add_trace( go.Scatter( x=actual['x'], y=actual['y'], line=dict(dash='solid'), name=f\"f(x)=x^{n}\" ))\nfig.add_trace( go.Scatter( x=lin0['x'], y=lin0['y'], name=f\"f({lb}) -&gt; f({ub[0]})\", line=dict(dash='dot') ))\nfig.add_trace( go.Scatter( x=lin1['x'], y=lin1['y'], name=f\"f({lb}) -&gt; f({ub[1]})\", line=dict(dash='dot') ))\nfig.add_trace( go.Scatter( x=lin2['x'], y=lin2['y'], name=f\"f({lb}) -&gt; f({ub[2]})\", line=dict(dash='dot') ))\nfig.show()\n\n\n\n\n                                                \n\n\nFigure 1: The smaller the change in x, the closer the linear approximation is to the true gradient of f(x)\n\n\n\n\nConsequently, we want to determine the gradient at the limit i.e. where \\(a \\rightarrow x\\), to get the true gradient."
  },
  {
    "objectID": "posts/techniques_differentiation.html#factoring-out-x-a",
    "href": "posts/techniques_differentiation.html#factoring-out-x-a",
    "title": "Differentiation formulae",
    "section": "Factoring out \\((x-a)\\)",
    "text": "Factoring out \\((x-a)\\)\nFirst though, let’s factor out the \\(x-a\\) term for simplicity. Let’s derive a generic formula for this:\n\nIf \\(n=2\\), then \\(x^2-a^2 = (x-a)(x+a)\\)\nIf \\(n=3\\), then \\(x^3-a^3 = (x-a)(x^2+xa+a^2)\\)\nIf \\(n=4\\), then \\(x^4-a^4 = (x-a)(x^3+x^2a+xa^2+a^3)\\)\nIf \\(n=5\\), then \\(x^5-a^5 = (x-a)(x^4+x^3a+x^2a^2+xa^3+a^4)\\)\nAnd so on. In fact for any \\(n\\), we can derive \\(x^n-a^n = (x-a)\\sum_{i=1}^n \\left( x^{n-i}a^{i-1} \\right)\\)\n\nAnd we can now sub that into our formula, and the \\(x-a\\) cancels out:\n\\[\n\\frac{x^n-a^n}{x-a} =\n\\frac{x-a}{x-a} \\sum_{i=1}^n \\left( x^{n-i}a^{i-1} \\right) =\n\\sum_{i=1}^n \\left( x^{n-i}a^{i-1} \\right)\n\\]"
  },
  {
    "objectID": "posts/techniques_differentiation.html#getting-our-result",
    "href": "posts/techniques_differentiation.html#getting-our-result",
    "title": "Differentiation formulae",
    "section": "Getting our result",
    "text": "Getting our result\nAnd now let’s calculate the result in the limit, where \\(x\\) approaches \\(a\\):\n\\[\n\\lim_{x \\rightarrow a} \\sum_{i=1}^n \\left( x^{n-i}a^{i-1} \\right) \\sim\n\\sum_{i=1}^n \\left( a^{n-i}a^{i-1} \\right) =\n\\sum_{i=1}^n \\left( a^{n-1} \\right) =\nna^{n-1}\n\\]\nHence \\(\\frac{d}{dx} \\left(x^{n} \\right) = nx^{n-1}\\).\nFin."
  },
  {
    "objectID": "posts/bayes_lasso.html",
    "href": "posts/bayes_lasso.html",
    "title": "Shrinkage priors for Lasso and Ridge",
    "section": "",
    "text": "What are we exploring?\n\n\n\nShowing Laplace priors in bayesian regression are equivalent to Lasso regularization\n\n\n\n\nBayesian Linear regression:\nWhen performing ordinary linear regression using maximum likelihood, we model the noise around \\(y\\) as being generated from a gaussian distributed process - conditional on the data \\(X\\) and estimated model parameters \\(\\beta\\) and \\(\\sigma\\):\n\\[\n\\displaylines{\n\\begin{align}\np(y|\\beta,X,\\sigma)\n& = \\prod_{i=1}^{n}{\\frac{1}{\\sigma\\sqrt{2\\pi}}}\\exp{\\left\\{-\\frac{1}{2\\sigma^2}\\epsilon_i^2\\right\\}} \\\\\n& \\sim \\prod_{i=1}^{n}{\\frac{1}{\\sigma\\sqrt{2\\pi}}}\\exp{\\left\\{-\\frac{1}{2\\sigma^2}\n\\left( y - X \\beta \\right)^2\n\\right\\}}\n\\end{align}\n}\n\\]\nWe can formulate our estimate of the coefficient in a bayesian way if we model \\(\\beta\\) as a random variable (rather than a fixed quantity as per frequenist thinking):\n\\[\n\\displaylines{\n\\begin{align}\np(\\beta|X,y,\\sigma)\n& =\n\\frac{\n  p(y|\\beta,X,\\sigma)\n  p(\\beta|X,\\sigma)\n  }\n  {p(y|X,\\sigma)}\n\\\\ \\\\ & =\n\\frac{\n  p(y|\\beta,X,\\sigma)\n  p(\\beta|X,\\sigma)\n  }\n{\\int{}{} p(y|X,\\beta,\\sigma)p(\\beta|X,\\sigma) \\mathrm{d}\\beta}\n\\end{align}\n}\n\\]\nWhere:\n\n\\(p(Y|\\beta,X,\\sigma)\\) is the likelihood function\n\\(p(Y|X,\\sigma)\\) is the evidence (the data we feed into the model)\n\\(p(\\beta|X,\\sigma)\\) is the prior for the coefficient\n\nNow in frequentist regression, we assume no prior at all: \\(\\beta\\) is fixed, so \\(p(\\beta|X,\\sigma)=p(\\beta)=1\\), regardless of the evidence observed. Thus we are just left with finding the coefficients that maximise \\(p(Y|\\beta,X,\\sigma)\\).\nBy taking the negative log likelihood, we find this is identical to finding the coefficient values that minimise the sum of squared residuals (see the derivation here):\n\\[\n\\displaylines{\n\\begin{align}\n\\beta^* & =\\arg\\min_\\beta{\\left[\n  \\sum_{i=1}^N{\\epsilon_i^2}\n\\right]}\n\\end{align}\n}\n\\]\nHowever, we could use other types of priors, with mass around zero, to apply regularization on our coefficients.\n\n\nUsing Laplace priors to shrink coefficients:\nRegularization aims to eliminate some of our predictors to create a more parsimonious model in a systematic way, and/or reduce their magnitude to prevent overfitting.\nPicking a prior for our coefficient that is concentrated at zero can help achieve this - for example, we could use a Laplace distribution, with a location parameter \\(\\mu\\) of zero as visualised below:\n\n\nCode\nlaplace_dist &lt;- function(x, mu = 0, gamma = 1) {\n  laplace_pdf &lt;- function(x,mu,gamma) {\n    return(\n      exp(-abs(x-mu)/gamma)/(2*gamma)\n    )\n  }\n  y = sapply(x, FUN = function(i) laplace_pdf(i,mu,gamma))\n  return(y)\n}\n\nx = seq(-10,10,0.1)\nplot(\n  x=x,\n  y=laplace_dist(x, gamma=1),\n  type='l',\n  main='Laplace(gamma=0.5)'\n  )\n\n\n\n\n\n\n\n\n\nNot only does the Laplace pdf increase when closer to zero, but it increases at an accelerating rate. Thus, we can imagine that the closer the likelihood estimate of the coefficient is to zero, the greater the influence of the prior.\n\nLaplace priors ~ Lasso Regression\nRecall that the probability density function of Laplace is \\[\nf(x|\\mu,\\gamma) =\n\\frac{1}{2\\gamma}\n\\exp{ \\left\\{\n  \\frac{x - \\mu}{\\gamma} \\right\n\\} }\n\\]\nThen the prior can be written as the maximum likelihood across the estimated coefficients each of the \\(K\\) features in the model, for a given penalty importance \\(\\lambda\\): \\[\n\\displaylines{\n\\begin{align}\np(\\beta|X,\\sigma) \\sim p(\\beta)\n& = \\prod_{k=1}^{K}{\n  \\frac{1}{2 \\gamma}\n  \\exp{\\left\\{ \\frac{ -\\mid \\beta - \\mu \\mid}{\\gamma} \\right\\}}\n}\n\\\\ \\\\ & = \\prod_{k=1}^{K}{\n  \\frac{1}{2 \\gamma}\n  \\exp{\\left\\{ \\frac{ -\\mid \\beta \\mid}{\\gamma} \\right\\}}\n}\n\\end{align}\n}\n\\]\n\n\nThis is how the prior is defined in Tibshirani (1996). Since the prior is assumed before any data is observed, intuitively \\(\\beta\\) should not need conditioning on \\(X\\).\nHowever, Park and Casella (2008) found that not conditioning on \\(\\sigma^2\\) can result in non-unimodal posterior, so in practice a non-informative scale-invariant marginal prior \\(\\pi(\\sigma^2) = 1/\\sigma\\) on \\(\\sigma^2\\) is used.\nNow if set this as the prior, we can derive the cost function that we aim to minimize when \\(X\\) is observed.\n\\[\n\\displaylines{\n\\begin{align}\n\\max_{\\beta} {\\left\\{ p(\\beta|y,X,\\sigma) \\right\\}}\n& = \\max_\\beta{\n  \\left[ p(y|\\beta,X,\\sigma) \\times p(\\beta)\\right]\n} \\\\\n& \\sim \\min_\\beta{\n  \\left\\{ -\\log{\n    \\left[ p(y|\\beta,X,\\sigma) \\times p(\\beta)\\right]\n    }\\right\\}\n} \\\\\n& = \\min_\\beta{\n  \\left\\{\n    -\\log{\\left[ p(y|\\beta,X,\\sigma) \\right]\n    -\\log{\\left[p(\\beta)\\right]}\n    }\\right\\}\n} \\\\\n& = \\min_\\beta \\left\\{\n\\frac{1}{2} \\left(\n  \\cancel{ n \\log{[2 \\pi \\sigma^2]} }\n  + \\frac{1}{\\sigma^2} \\sum_{i=1}^N{\\epsilon_i^2}\n\\right)\n- \\log{\\left[\n  \\prod_{k=1}^{K}{\n    \\frac{1}{2\\gamma}\n    \\exp{\\left\\{ \\frac{ -\\mid \\beta \\mid}{\\gamma} \\right \\}}\n  }\n\\right]}\n\\right\\} \\\\\n& = \\min_\\beta \\left\\{\n  \\frac{1}{2\\sigma^2} \\sum_{i=1}^N{\\epsilon_i^2}\n- \\sum_{k=1}^{K}{ \\log{ \\left[\n    \\frac{1}{2\\gamma}\n    \\exp{ \\left\\{ \\frac{ -\\mid \\beta \\mid}{\\gamma} \\right\\} }\n    \\right]\n    }\n  }\n\\right\\} \\\\\n& = \\min_\\beta \\left\\{\n  \\frac{1}{2\\sigma^2} \\sum_{i=1}^N{\\epsilon_i^2}\n- \\cancel{ K \\log{ \\left[ \\frac{1}{2\\gamma} \\right] } }\n- \\sum_{k=1}^{K}{\n    \\log{ \\left[ \\exp{\\left\\{ \\frac{ -\\mid \\beta \\mid}{\\gamma} \\right\\}} \\right] }\n  }  \n\\right\\} \\\\\n& = \\min_\\beta \\left\\{\n  \\frac{1}{2\\sigma^2} \\sum_{i=1}^N{\\epsilon_i^2}\n+ \\frac{1}{\\gamma} \\sum_{k=1}^{K}{ \\mid \\beta \\mid}  \n\\right\\} \\\\ \\\\\n\\therefore \\beta^*\n& =\\arg\\min_\\beta{\\left\\{\n  \\frac{1}{2\\sigma^2} \\sum_{i=1}^N{\\epsilon_i^2}\n+ \\frac{1}{\\gamma}\\sum_{k=1}^{K}{\\mid\\beta\\mid}\n  \\right\\}\n}\n\\end{align}\n}\n\\]\n\n\nNote that terms that do not vary with the choice of \\(\\beta\\) drop out\n\n\n\n\n\n\nKey point\n\n\n\nHence setting a Laplace prior on the coefficients is - almost - equivalent to running L1 regularization, where \\(1/\\gamma\\) is the parameter influencing the penalty size.\n\n\nAlthough there is sparsity in the mode of the prior, when combining this with the likelihood distribution, the sparsity will not continue into the posterior distribution. In practice, if the posterior of \\(\\beta\\) is sufficiently small, we would want to drop it - so a threshold value for the size at which coefficients are zero-oed out is set as a hyperparameter.\n\n\n\nUsing Gaussian priors to shrink coefficients:\nIn a similar way to before for Lasso, we set our coefficient priors to each have a Gaussian distribution, with a location parameter \\(\\mu=0\\):\n\n\nCode\nnormal_pdf &lt;- function(x, mu = 0, sigma = 2) {\n  z = (x-mu)/sigma\n  y = (2*pi*sigma^2)^(-1/2) * exp(-0.5*z^2)\n  return(y)\n}\n\nx = seq(-10,10,0.1)\nplot(\n  x=x,\n  y=normal_pdf(x),\n  type='l',\n  main='Normal(sigma=1)'\n  )\n\n\n\n\n\n\n\n\n\n\nGaussian priors ~ Ridge Regression\nThe gaussian prior can be written as the maximum likelihood across the estimated coefficients each of the \\(K\\) features in the model, for a given penalty importance \\(\\lambda\\):\n\\[\n\\displaylines{\n\\begin{align}\np(\\beta|X,\\sigma) \\sim p(\\beta)\n& = \\prod_{k=1}^{K}{\\frac{1}{\\sigma\\sqrt{2\\pi}}}\\exp{\\left\\{-\\frac{1}{2\\sigma^2}\\beta_k^2\\right\\}}\n\\end{align}\n}\n\\]\nNow if set this as the prior, we can derive the cost function that we aim to minimize when \\(X\\) is observed.\n\\[\n\\displaylines{\n\\begin{align}\n\\max_{\\beta} {\\left\\{ p(\\beta|y,X,\\sigma) \\right\\}}\n& \\sim \\min_\\beta{\n  \\left\\{\n    -\\log{\\left[ p(y|\\beta,X,\\sigma) \\right]\n    -\\log{\\left[p(\\beta)\\right]}\n    }\\right\\}\n} \\\\\n& = \\min_\\beta \\left\\{\n\\frac{1}{2} \\left(\n  \\cancel{ n \\log{[2 \\pi \\sigma^2]} }\n  + \\frac{1}{\\sigma^2} \\sum_{i=1}^N{\\epsilon_i^2}\n\\right)\n- \\log{\\left[\n  \\prod_{k=1}^{K}{\n    \\frac{1}{\\sigma\\sqrt{2\\pi}}\n    \\exp{\\left\\{-\\frac{1}{2\\sigma^2}\\beta_k^2\\right\\}}\n    }\n\\right]}\n\\right\\} \\\\\n& = \\min_\\beta \\left\\{\n  \\frac{1}{2\\sigma^2} \\sum_{i=1}^N{\\epsilon_i^2}\n- \\sum_{k=1}^{K}{ \\log{ \\left[\n    \\frac{1}{\\sigma\\sqrt{2\\pi}}\n    \\exp{\\left\\{-\\frac{1}{2\\sigma^2}\\beta_k^2\\right\\}}\n    \\right]\n    }\n  }\n\\right\\} \\\\\n& = \\min_\\beta \\left\\{\n  \\frac{1}{2\\sigma^2} \\sum_{i=1}^N{\\epsilon_i^2}\n- \\cancel{ K \\log{ \\left[ \\frac{1}{\\sigma\\sqrt{2\\pi}} \\right] } }\n- \\sum_{k=1}^{K}{\n    \\log{ \\left[ \\exp{\\left\\{ -\\frac{1}{2\\sigma^2}\\beta_k^2 \\right\\}} \\right] }\n  }  \n\\right\\} \\\\\n& = \\min_\\beta \\left\\{\n  \\frac{1}{2\\sigma^2} \\sum_{i=1}^N{\\epsilon_i^2}\n+ \\frac{1}{2\\sigma^2} \\sum_{k=1}^{K}{ \\beta^2 }  \n\\right\\} \\\\ \\\\\n\\therefore \\beta^*\n& =\\arg\\min_\\beta{\\left\\{\n  \\frac{1}{2\\sigma^2} \\sum_{i=1}^N{\\epsilon_i^2}\n+ \\frac{1}{2\\sigma^2} \\sum_{k=1}^{K}{ \\beta^2 }\n  \\right\\}\n}\n\\end{align}\n}\n\\]\n\n\n\n\n\n\nKey point\n\n\n\nHence setting a Gaussian prior on the coefficients is - almost - equivalent to running L1 regularization, where the variance of the prior - \\(\\sigma^2\\) is the parameter directly influencing the penalty size.\n\n\n\n\n\n\n\n\nReferences\n\nPark, Trevor, and George Casella. 2008. “The Bayesian Lasso.” Journal of the American Statistical Association 103 (482): 681–86. https://doi.org/10.1198/016214508000000337.\n\n\nTibshirani, Robert. 1996. “Regression Shrinkage and Selection via the Lasso.” Journal of the Royal Statistical Society: Series B (Methodological) 58 (1): 267–88. https://doi.org/10.1111/j.1467-9868.2011.00771.x."
  },
  {
    "objectID": "posts/bayes_regularization.html",
    "href": "posts/bayes_regularization.html",
    "title": "Shrinkage priors for Lasso and Ridge",
    "section": "",
    "text": "What are we exploring?\n\n\n\nShowing Laplace and Gaussian priors in Bayesian regression are equivalent to Lasso and Ridge regularization\n\n\n\n\nBayesian Linear regression:\nWhen performing ordinary linear regression using maximum likelihood, we model the noise around \\(y\\) as being generated from a gaussian distributed process - conditional on the data \\(X\\) and estimated model parameters \\(\\beta\\) and \\(\\sigma\\):\n\\[\n\\displaylines{\n\\begin{align}\np(y|\\beta,X,\\sigma)\n& = \\prod_{i=1}^{n}{\\frac{1}{\\sigma\\sqrt{2\\pi}}}\\exp{\\left\\{-\\frac{1}{2\\sigma^2}\\epsilon_i^2\\right\\}} \\\\\n& \\sim \\prod_{i=1}^{n}{\\frac{1}{\\sigma\\sqrt{2\\pi}}}\\exp{\\left\\{-\\frac{1}{2\\sigma^2}\n\\left( y - X \\beta \\right)^2\n\\right\\}}\n\\end{align}\n}\n\\]\nWe can formulate our estimate of the coefficient in a bayesian way if we model \\(\\beta\\) as a random variable (rather than a fixed quantity as per frequenist thinking):\n\\[\n\\displaylines{\n\\begin{align}\np(\\beta|X,y,\\sigma)\n& =\n\\frac{\n  p(y|\\beta,X,\\sigma)\n  p(\\beta|X,\\sigma)\n  }\n  {p(y|X,\\sigma)}\n\\\\ \\\\ & =\n\\frac{\n  p(y|\\beta,X,\\sigma)\n  p(\\beta|X,\\sigma)\n  }\n{\\int{}{} p(y|X,\\beta,\\sigma)p(\\beta|X,\\sigma) \\mathrm{d}\\beta}\n\\end{align}\n}\n\\]\nWhere:\n\n\\(p(Y|\\beta,X,\\sigma)\\) is the likelihood function\n\\(p(Y|X,\\sigma)\\) is the evidence (the data we feed into the model)\n\\(p(\\beta|X,\\sigma)\\) is the prior for the coefficient\n\nNow in frequentist regression, we assume no prior at all: \\(\\beta\\) is fixed, so \\(p(\\beta|X,\\sigma)=p(\\beta)=1\\), regardless of the evidence observed. Thus we are just left with finding the coefficients that maximise \\(p(Y|\\beta,X,\\sigma)\\).\nBy taking the negative log likelihood, we find this is identical to finding the coefficient values that minimise the sum of squared residuals (see the derivation here):\n\\[\n\\displaylines{\n\\begin{align}\n\\beta^* & =\\arg\\min_\\beta{\\left[\n  \\sum_{i=1}^N{\\epsilon_i^2}\n\\right]}\n\\end{align}\n}\n\\]\nHowever, we could use other types of priors, with mass around zero, to apply regularization on our coefficients.\n\n\nUsing Laplace priors to shrink coefficients:\nRegularization aims to eliminate some of our predictors to create a more parsimonious model in a systematic way, and/or reduce their magnitude to prevent overfitting.\nPicking a prior for our coefficient that is concentrated at zero can help achieve this - for example, we could use a Laplace distribution, with a location parameter \\(\\mu\\) of zero as visualised below:\n\n\nCode\nlaplace_dist &lt;- function(x, mu = 0, gamma = 1) {\n  laplace_pdf &lt;- function(x,mu,gamma) {\n    return(\n      exp(-abs(x-mu)/gamma)/(2*gamma)\n    )\n  }\n  y = sapply(x, FUN = function(i) laplace_pdf(i,mu,gamma))\n  return(y)\n}\n\nx = seq(-10,10,0.1)\nplot(\n  x=x,\n  y=laplace_dist(x, gamma=1),\n  type='l',\n  main='Laplace(gamma=0.5)'\n  )\n\n\n\n\n\n\n\n\n\nNot only does the Laplace pdf increase when closer to zero, but it increases at an accelerating rate. Thus, we can imagine that the closer the likelihood estimate of the coefficient is to zero, the greater the influence of the prior.\n\nLaplace priors ~ Lasso Regression\nRecall that the probability density function of Laplace is \\[\nf(x|\\mu,\\gamma) =\n\\frac{1}{2\\gamma}\n\\exp{ \\left\\{\n  \\frac{x - \\mu}{\\gamma} \\right\n\\} }\n\\]\nThen the prior can be written as the maximum likelihood across the estimated coefficients each of the \\(K\\) features in the model, for a given penalty importance \\(\\lambda\\): \\[\n\\displaylines{\n\\begin{align}\np(\\beta|X,\\sigma) \\sim p(\\beta)\n& = \\prod_{k=1}^{K}{\n  \\frac{1}{2 \\gamma}\n  \\exp{\\left\\{ \\frac{ -\\mid \\beta - \\mu \\mid}{\\gamma} \\right\\}}\n}\n\\\\ \\\\ & = \\prod_{k=1}^{K}{\n  \\frac{1}{2 \\gamma}\n  \\exp{\\left\\{ \\frac{ -\\mid \\beta \\mid}{\\gamma} \\right\\}}\n}\n\\end{align}\n}\n\\]\n\n\nThis is how the prior is defined in Tibshirani (1996). Since the prior is assumed before any data is observed, intuitively \\(\\beta\\) should not need conditioning on \\(X\\).\nHowever, Park and Casella (2008) found that not conditioning on \\(\\sigma^2\\) can result in non-unimodal posterior, so in practice a non-informative scale-invariant marginal prior \\(\\pi(\\sigma^2) = 1/\\sigma\\) on \\(\\sigma^2\\) is used.\nNow if set this as the prior, we can derive the cost function that we aim to minimize when \\(X\\) is observed.\n\\[\n\\displaylines{\n\\begin{align}\n\\max_{\\beta} {\\left\\{ p(\\beta|y,X,\\sigma) \\right\\}}\n& = \\max_\\beta{\n  \\left[ p(y|\\beta,X,\\sigma) \\times p(\\beta)\\right]\n} \\\\\n& \\sim \\min_\\beta{\n  \\left\\{ -\\log{\n    \\left[ p(y|\\beta,X,\\sigma) \\times p(\\beta)\\right]\n    }\\right\\}\n} \\\\\n& = \\min_\\beta{\n  \\left\\{\n    -\\log{\\left[ p(y|\\beta,X,\\sigma) \\right]\n    -\\log{\\left[p(\\beta)\\right]}\n    }\\right\\}\n} \\\\\n& = \\min_\\beta \\left\\{\n\\frac{1}{2} \\left(\n  \\cancel{ n \\log{[2 \\pi \\sigma^2]} }\n  + \\frac{1}{\\sigma^2} \\sum_{i=1}^N{\\epsilon_i^2}\n\\right)\n- \\log{\\left[\n  \\prod_{k=1}^{K}{\n    \\frac{1}{2\\gamma}\n    \\exp{\\left\\{ \\frac{ -\\mid \\beta \\mid}{\\gamma} \\right \\}}\n  }\n\\right]}\n\\right\\} \\\\\n& = \\min_\\beta \\left\\{\n  \\frac{1}{2\\sigma^2} \\sum_{i=1}^N{\\epsilon_i^2}\n- \\sum_{k=1}^{K}{ \\log{ \\left[\n    \\frac{1}{2\\gamma}\n    \\exp{ \\left\\{ \\frac{ -\\mid \\beta \\mid}{\\gamma} \\right\\} }\n    \\right]\n    }\n  }\n\\right\\} \\\\\n& = \\min_\\beta \\left\\{\n  \\frac{1}{2\\sigma^2} \\sum_{i=1}^N{\\epsilon_i^2}\n- \\cancel{ K \\log{ \\left[ \\frac{1}{2\\gamma} \\right] } }\n- \\sum_{k=1}^{K}{\n    \\log{ \\left[ \\exp{\\left\\{ \\frac{ -\\mid \\beta \\mid}{\\gamma} \\right\\}} \\right] }\n  }  \n\\right\\} \\\\\n& = \\min_\\beta \\left\\{\n  \\frac{1}{2\\sigma^2} \\sum_{i=1}^N{\\epsilon_i^2}\n+ \\frac{1}{\\gamma} \\sum_{k=1}^{K}{ \\mid \\beta \\mid}  \n\\right\\} \\\\ \\\\\n\\therefore \\beta^*\n& =\\arg\\min_\\beta{\\left\\{\n  \\frac{1}{2\\sigma^2} \\sum_{i=1}^N{\\epsilon_i^2}\n+ \\frac{1}{\\gamma}\\sum_{k=1}^{K}{\\mid\\beta\\mid}\n  \\right\\}\n}\n\\end{align}\n}\n\\]\n\n\nNote that terms that do not vary with the choice of \\(\\beta\\) drop out\n\n\n\n\n\n\nKey point\n\n\n\nHence setting a Laplace prior on the coefficients is - almost - equivalent to running L1 regularization, where \\(1/\\gamma\\) is the parameter influencing the penalty size.\n\n\nAlthough there is sparsity in the mode of the prior, when combining this with the likelihood distribution, the sparsity will not continue into the posterior distribution. In practice, if the posterior of \\(\\beta\\) is sufficiently small, we would want to drop it - so a threshold value for the size at which coefficients are zero-oed out is set as a hyperparameter.\n\n\n\nUsing Gaussian priors to shrink coefficients:\nIn a similar way to before for Lasso, we set our coefficient priors to each have a Gaussian distribution, with a location parameter \\(\\mu=0\\):\n\n\nCode\nnormal_pdf &lt;- function(x, mu = 0, sigma = 2) {\n  z = (x-mu)/sigma\n  y = (2*pi*sigma^2)^(-1/2) * exp(-0.5*z^2)\n  return(y)\n}\n\nx = seq(-10,10,0.1)\nplot(\n  x=x,\n  y=normal_pdf(x),\n  type='l',\n  main='Normal(sigma=1)'\n  )\n\n\n\n\n\n\n\n\n\n\nGaussian priors ~ Ridge Regression\nThe gaussian prior can be written as the maximum likelihood across the estimated coefficients each of the \\(K\\) features in the model, for a given penalty importance \\(\\lambda\\):\n\\[\n\\displaylines{\n\\begin{align}\np(\\beta|X,\\sigma) \\sim p(\\beta)\n& = \\prod_{k=1}^{K}{\\frac{1}{\\sigma\\sqrt{2\\pi}}}\\exp{\\left\\{-\\frac{1}{2\\sigma^2}\\beta_k^2\\right\\}}\n\\end{align}\n}\n\\]\nNow if set this as the prior, we can derive the cost function that we aim to minimize when \\(X\\) is observed.\n\\[\n\\displaylines{\n\\begin{align}\n\\max_{\\beta} {\\left\\{ p(\\beta|y,X,\\sigma) \\right\\}}\n& \\sim \\min_\\beta{\n  \\left\\{\n    -\\log{\\left[ p(y|\\beta,X,\\sigma) \\right]\n    -\\log{\\left[p(\\beta)\\right]}\n    }\\right\\}\n} \\\\\n& = \\min_\\beta \\left\\{\n\\frac{1}{2} \\left(\n  \\cancel{ n \\log{[2 \\pi \\sigma^2]} }\n  + \\frac{1}{\\sigma^2} \\sum_{i=1}^N{\\epsilon_i^2}\n\\right)\n- \\log{\\left[\n  \\prod_{k=1}^{K}{\n    \\frac{1}{\\sigma\\sqrt{2\\pi}}\n    \\exp{\\left\\{-\\frac{1}{2\\sigma^2}\\beta_k^2\\right\\}}\n    }\n\\right]}\n\\right\\} \\\\\n& = \\min_\\beta \\left\\{\n  \\frac{1}{2\\sigma^2} \\sum_{i=1}^N{\\epsilon_i^2}\n- \\sum_{k=1}^{K}{ \\log{ \\left[\n    \\frac{1}{\\sigma\\sqrt{2\\pi}}\n    \\exp{\\left\\{-\\frac{1}{2\\sigma^2}\\beta_k^2\\right\\}}\n    \\right]\n    }\n  }\n\\right\\} \\\\\n& = \\min_\\beta \\left\\{\n  \\frac{1}{2\\sigma^2} \\sum_{i=1}^N{\\epsilon_i^2}\n- \\cancel{ K \\log{ \\left[ \\frac{1}{\\sigma\\sqrt{2\\pi}} \\right] } }\n- \\sum_{k=1}^{K}{\n    \\log{ \\left[ \\exp{\\left\\{ -\\frac{1}{2\\sigma^2}\\beta_k^2 \\right\\}} \\right] }\n  }  \n\\right\\} \\\\\n& = \\min_\\beta \\left\\{\n  \\frac{1}{2\\sigma^2} \\sum_{i=1}^N{\\epsilon_i^2}\n+ \\frac{1}{2\\sigma^2} \\sum_{k=1}^{K}{ \\beta^2 }  \n\\right\\} \\\\ \\\\\n\\therefore \\beta^*\n& =\\arg\\min_\\beta{\\left\\{\n  \\frac{1}{2\\sigma^2} \\sum_{i=1}^N{\\epsilon_i^2}\n+ \\frac{1}{2\\sigma^2} \\sum_{k=1}^{K}{ \\beta^2 }\n  \\right\\}\n}\n\\end{align}\n}\n\\]\n\n\n\n\n\n\nKey point\n\n\n\nHence setting a Gaussian prior on the coefficients is - almost - equivalent to running L1 regularization, where the variance of the prior - \\(\\sigma^2\\) is the parameter directly influencing the penalty size.\n\n\n\n\n\n\n\n\nReferences\n\nPark, Trevor, and George Casella. 2008. “The Bayesian Lasso.” Journal of the American Statistical Association 103 (482): 681–86. https://doi.org/10.1198/016214508000000337.\n\n\nTibshirani, Robert. 1996. “Regression Shrinkage and Selection via the Lasso.” Journal of the Royal Statistical Society: Series B (Methodological) 58 (1): 267–88. https://doi.org/10.1111/j.1467-9868.2011.00771.x."
  },
  {
    "objectID": "posts/glm_logistic.html",
    "href": "posts/glm_logistic.html",
    "title": "Deriving Logistic Regression Coefficients",
    "section": "",
    "text": "What are we exploring?\n\n\n\nDeriving coefficients to predict the likelihood of a binary outcome using maximum likelihood estimation and the logit link funciton.\n\n\n\nSetting some intuition\nImagine a basketball player has a 90% chance of making a freethrow.\nFor freethrow attempt \\(i\\), we observe the outcome, \\(y_i\\), as being 1 if there is a success (i.e. the freethrow is made), or 0 if it is a failure (a miss).\nWe can denote \\(p\\) as a fixed probability for success. It follows that the probability of failure as \\(1-p\\) (i.e. a 10% chance of missing).\nThis can be formalised as the following (the probability mass function of the bernoulli):\n\\[\n\\displaylines{\n\\begin{align}\np(y_i) =\n\\begin{cases}\n  p & \\text{if}\\ y_i=1 \\\\\n  1-p & \\text{if}\\ y_i=0\n\\end{cases}\n\\end{align}\n}\n\\]\nWhich is equivalent to the following:\n\\[\np(y_i) = p^{y_i}(1-p)^{1-y_i}\n\\]\n\n\nSince \\(x^0 = 1\\):\n\nif \\(y_i=1\\), \\(p^{1}(1-p)^{0} = p\\)\nif \\(y_i=0\\), \\(p^{0}(1-p)^{1} = 1-p\\)\n\nNow imagine that, rather than \\(p\\) being fixed at one value, there are some external variables that influence the shooter (for example, whether the game is at home or away). Let’s denote these relevant variables \\(X\\), and the relationship they have on the probability of success as \\(\\beta\\).\nWe can denote then denote the probability of success as the following:\n\\[\np(y_i|X_i\\beta) = p(y_i=1|X_i\\beta)^{y_i}(1-p(y_i=1|X_i\\beta))^{1-y_i}\n\\]\nNow imagine we observe the shooter take 100 freethrows (sample size \\(N\\)), against many different teams, point differentials etc. We want to learn from this past experience to estimate the probability that they make the next one.\n\n\n\nCost function for bernoulli regression\n\nApplying the bernoulli pdf\nAcross \\(N\\) observations collected, we want to find the values of \\(\\beta\\) that maximise the likelihood of observing all outcomes (the vector of results \\(y\\)).\nLet’s split the outcomes between successes and failures. We thus derive the cost function:\n\\[\np(y|X\\beta) =\n\\underbrace{\n  \\prod_{i=1}^{N}{ p(y_i=1|X_i\\beta)^{y_i} }\n}_{y_i=1} \\times\n\\underbrace{\n  \\prod_{i=1}^{N}{ (1-p(y_i=1|X_i\\beta))^{1-y_i} }    \n}_{y_i=0}\n\\]\nMaximum likelihood estimation aims to find the set of coefficients that maximises the likelihood of observing the evidence we have. We thus aim to find the coefficients \\(\\beta\\) that maximise the likelihood of observing \\(y\\) across all \\(n\\) samples.\n\n\nTaking the negative log-likelihood\nIn practice, dealing with a cost function made up of a sum product is tricky - it is easier to take the log and deal with addition instead. Further, rather than maximise, it is common to “minimize” cost functions, so the negative log-likelihood is usually used.\n\n\nRecall that \\(\\log{\\left(ab\\right)} = \\log{\\left(a\\right)} + \\log{\\left(b\\right)}\\)\n\\[\n\\displaylines{\n\\begin{align}\n& \\max_\\beta{p(y|\\beta,X)} \\\\\n= &\n\\max_\\beta{\\left\\{\n  \\prod_{i=1}^{N}{ p(y_i=1|X_i\\beta)^{y_i} }\n  \\times\n  \\prod_{i=1}^{N}{ (1-p(y_i=1|X_i\\beta))^{1-y_i} }\n  \\right\\}}\n\\\\ \\\\ \\Rightarrow &\n\\min_\\beta{\\left\\{ -\\log{ \\left[\n  \\prod_{i=1}^{N}{ p(y_i=1|X_i\\beta)^{y_i} }\n  \\times\n  \\prod_{i=1}^{N}{ (1-p(y_i=1|X_i\\beta))^{1-y_i} }\n  \\right] } \\right\\}}\n\\\\ \\\\ = &\n\\min_\\beta{\\left\\{\n  \\sum_{i=1}^{N}{ -\\log{ \\left[ p(y_i=1|X_i\\beta)^{y_i} \\right] } } +\n  \\sum_{i=1}^{N}{ -\\log{ \\left[ (1-p(y_i=1|X_i\\beta))^{1-y_i}\\right] } }\n  \\right\\}}  \n\\\\ \\\\ = &\n\\min_\\beta{\\left\\{\n  \\sum_{i=1}^{N}{ -y_i\\log{ \\left[ p(y_i=1|X_i\\beta) \\right] } } +\n  \\sum_{i=1}^{N}{ -(1-y_i)\\log{ \\left[ 1-p(y_i=1|X_i\\beta)\\right] } }\n  \\right\\}}  \n\\end{align}\n}\n\\]\n\n\nDeriving the gradient with respect to the coefficients\nWe minimise the cost function by finding the optimum coefficient values \\(\\beta^*\\) so that the partial differential is equal to zero.\n\\[\n\\displaylines{\n\\begin{align}\n& \\frac{\\partial}{\\partial \\beta_j}p(y|\\beta,X) \\\\ \\\\\n= &\n\\frac{\\partial}{\\partial \\beta_j} \\left(\n\\sum_{i=1}^{N}{ -y_i\\log{ \\left[ p(y_i=1|X_i\\beta) \\right] } } +\n  \\sum_{i=1}^{N}{ -(1-y_i)\\log{ \\left[ 1-p(y_i=1|X_i\\beta)\\right] } }\n\\right) \\\\ \\\\\n= &\n\\sum_{i=1}^{N}{ y_i\n  \\frac{\\frac{\\partial}{\\partial \\beta_j}p(y_i=1|X_i\\beta)}{p(y_i=1|X_i\\beta)}\n  } +\n  \\sum_{i=1}^{N}{ (1-y_i)\n    \\frac{\\frac{\\partial}{\\partial \\beta_j}\\left(1-p(y_i=1|X_i\\beta)\\right)}{1-p(y_i=1|X_i\\beta)}\n    }\n\\end{align}\n}\n\\]\nThis is as far as we can get, without now making some more assumptions. Let’s imagine that we can model the\n\n\n\nLogistic activation\nWe might assume that the the log-odds - the logarithm of the probability of success divided by the probability of failure - is linearly related to its predictors, i.e.\n\\[\n\\displaylines{\n\\begin{align}\n\\ln{\\left(\\frac{p}{1-p}\\right)} = X\\beta + \\epsilon\n&& \\epsilon \\sim N(0,\\sigma^2)\n\\end{align}\n}\n\\]\nThis is called a “link function” - the link between the outcome, \\(y\\), and the linear predictors \\(X\\beta\\). This specific link function is called the “logit link function”.\nTo make predictions then for the probability of success, we need the inverse of the link function - sometimes called the “activation function” in the context of neural network.\nWe can derive the inverse of the logit link by rearranging it in terms of \\(p\\):\n\\[\n\\displaylines{\n\\begin{align}\nE\\left[\n  \\ln{\\left(\\frac{p}{1-p}\\right)}\n  \\right] = X\\beta\n& \\Rightarrow \\frac{p}{1-p}\\ = \\exp{\\{X\\beta\\}}\n\\\\ \\\\\n& \\Rightarrow p = \\exp{\\{X\\beta\\}}(1-p)\n\\\\ \\\\\n& \\Rightarrow p - (1+\\exp{\\{X\\beta\\}}) = \\exp{\\{X\\beta\\}}\n\\\\ \\\\\n& \\Rightarrow p = \\frac{\\exp{\\{X\\beta\\}}}{1 + \\exp{\\{X\\beta\\}}}\n=  \\left( 1 + \\exp{\\{-X\\beta\\}} \\right)^{-1}\n\\end{align}\n}\n\\]\nWe can see that this activation function “squashes” all outputs \\(X\\beta \\in [-\\infty,\\infty]\\) between 0 and 1:\n\n\nCode\nlogistic_dist &lt;- function(x) {\n  return( (1+exp(-x))^-1 )\n}\nx = seq(-10,10,0.1)\nplot(\n  x=x,\n  y=logistic_dist(x),\n  type='l',col=\"blue\",,lty=1\n  )\nlines(\n  x=seq(-3,3,0.1),\n  y=(1/4)*seq(-3,3,0.1)+0.5,\n  type='l',col=\"red\",lty=2\n)\nlegend(\n  -10,1,\n  legend=c(\n    \"logistic activation\",\n    \"linear activation\"\n    ),\n  col=c(\"blue\",\"red\"),\n  lty=c(1,2)\n  )\n\n\n\n\n\n\n\n\n\n\n\nFor probabilities of between 0.3 to 0.7, we see that the logistic activation function maps very closely to that of a simply linear one. It is only at the more extreme probabilities that they diverge.\nWe can now use this activation function to derive some coefficients.\n\n\nOptimal coefficients for the coefficient\nGiven that:\n\\[\n\\hat{p_i} = \\hat{p}(y_i=1|X_i \\hat{\\beta}) =\n\\frac{1}{1+\\exp{\\left\\{-X_i\\hat{\\beta}\\right\\}}}\n\\]\nThen the partial differential of the probability with respect to feature \\(j\\) is:\n\\[\n\\displaylines{\n\\begin{align}\n& \\frac{\\partial }{\\partial \\beta_j}\n\\hat{p}(y_i=1|X_i \\hat{\\beta_j})\n\\\\ = &\n\\frac{\\partial }{\\partial \\beta_j} (1+\\exp{\\{-X_i\\hat{\\beta}\\}})^{-1}\n\\\\ = &\n\\frac{\\partial }{\\partial \\beta_j} -1(1+\\exp{\\{-X_i\\hat{\\beta}\\}})^{-2}\n\\times x_{ij}\\exp{\\{-X_i\\hat{\\beta}\\}}\n\\\\ = &\nx_{ij} \\left( \\frac{\\exp{\\{-X_i\\hat{\\beta}\\}}}{(1+\\exp{\\{-X_i\\hat{\\beta}\\}})^{2}} \\right)\n\\\\ = &\nx_{ij} \\left( \\frac{1}{(1+\\exp{\\{-X_i\\hat{\\beta}\\}})} \\times \\frac{\\exp{\\{-X_i\\hat{\\beta}\\}}}{(1+\\exp{\\{-X_i\\hat{\\beta}\\}})} \\right)\n\\\\ = &\nx_{ij} \\left( \\frac{1}{(1+\\exp{\\{-X_i\\hat{\\beta}\\}})} \\times \\left( 1 - \\frac{1}{(1+\\exp{\\{-X_i\\hat{\\beta}\\}})} \\right) \\right)\n\\\\ \\\\ = &\nx_{ij} ( \\hat{p}(y_i=1|X_i \\hat{\\beta_j}) \\times (1-\\hat{p}(y_i=1|X_i \\hat{\\beta_j})) )\n\\\\ \\\\ = &\nx_{ij} ( \\hat{p_i} \\times (1-\\hat{p_i}) )\n\\end{align}\n}\n\\]\nAnd thus we can substitute this into our cost function:\n\\[\n\\displaylines{\n\\begin{align}\n& \\frac{\\partial}{\\partial \\beta_j}p(y|\\beta,X) \\\\ \\\\\n= &\n\\sum_{i=1}^{N}{ y_i\n  \\frac{\\frac{\\partial}{\\partial \\beta_j}p(y_i=1|X_i\\beta)}{p(y_i=1|X_i\\beta)}\n  } +\n\\sum_{i=1}^{N}{ (1-y_i)\n  \\frac{\\frac{\\partial}{\\partial \\beta_j}\\left(1-p(y_i=1|X_i\\beta)\\right)}{1-p(y_i=1|X_i\\beta)}\n  } \\\\ \\\\\n= &\n\\sum_{i=1}^{N}{ y_i\n  \\frac{x_{ij} ( \\hat{p_i} \\times (1-\\hat{p_i}) )}{\\hat{p_i}}\n  } +\n\\sum_{i=1}^{N}{ (1-y_i)\n  \\frac{x_{ij} ( \\hat{p_i} \\times (1-\\hat{p_i}) )}{(1-\\hat{p_i})}\n  } \\\\ \\\\\n= &\n\\sum_{i=1}^{N}{ y_i\n  \\frac{x_{ij} ( \\cancel{\\hat{p_i}} \\times (1-\\hat{p_i}) )}{\\cancel{\\hat{p_i}}}\n  } +\n\\sum_{i=1}^{N}{ (1-y_i)\n  \\frac{x_{ij} ( \\hat{p_i} \\times \\cancel{(1-\\hat{p_i})} )}{\\cancel{(1-\\hat{p_i})}}\n  } \\\\ \\\\  \n= &\n\\sum_{i=1}^{N}{ y_ix_{ij} (1-\\hat{p_i})} +\n\\sum_{i=1}^{N}{ (1-y_i)x_{ij} \\hat{p_i}} \\\\ \\\\\n= &\n\\sum_{i=1}^{N}{ x_{ij} \\left[\n    y_i(1-\\hat{p_i}) (1-y_i)(\\hat{p_i})\n   \\right]\n  }\n\\end{align}\n}\n\\]\nWhich is the coefficient from logistic regression.\nFin."
  }
]