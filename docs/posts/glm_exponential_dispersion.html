<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.549">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Chris Kelly">
<meta name="dcterms.date" content="2024-05-15">

<title>Chris Kelly Blog - Generalized Linear Models from scratch</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">Chris Kelly Blog</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/chris-kelly"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/ccrkelly/"> <i class="bi bi-linkedin" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
          <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Generalized Linear Models from scratch</h1>
                      </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Chris Kelly </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">May 15, 2024</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#intro-random-and-systematic-components-are-connected-via-link-functions" id="toc-intro-random-and-systematic-components-are-connected-via-link-functions" class="nav-link active" data-scroll-target="#intro-random-and-systematic-components-are-connected-via-link-functions">Intro: random and systematic components are connected via link functions</a></li>
  <li><a href="#exponential-dispersion-family-of-distributions" id="toc-exponential-dispersion-family-of-distributions" class="nav-link" data-scroll-target="#exponential-dispersion-family-of-distributions">Exponential Dispersion Family of Distributions</a>
  <ul class="collapse">
  <li><a href="#intuition-behind-the-exponential-form" id="toc-intuition-behind-the-exponential-form" class="nav-link" data-scroll-target="#intuition-behind-the-exponential-form">Intuition behind the exponential form</a></li>
  <li><a href="#poisson" id="toc-poisson" class="nav-link" data-scroll-target="#poisson">Poisson</a></li>
  <li><a href="#bernoulli-logit-vs-probit" id="toc-bernoulli-logit-vs-probit" class="nav-link" data-scroll-target="#bernoulli-logit-vs-probit">Bernoulli (Logit vs probit)</a></li>
  <li><a href="#gaussian" id="toc-gaussian" class="nav-link" data-scroll-target="#gaussian">Gaussian</a></li>
  </ul></li>
  <li><a href="#solving-for-any-exponential-family-using-maximum-likelihood-estimation" id="toc-solving-for-any-exponential-family-using-maximum-likelihood-estimation" class="nav-link" data-scroll-target="#solving-for-any-exponential-family-using-maximum-likelihood-estimation">Solving for any exponential family using maximum likelihood estimation</a></li>
  <li><a href="#coding-it-up-from-scratch" id="toc-coding-it-up-from-scratch" class="nav-link" data-scroll-target="#coding-it-up-from-scratch">Coding it up from scratch</a>
  <ul class="collapse">
  <li><a href="#base-class-for-canonical-exponential-dispersion-family" id="toc-base-class-for-canonical-exponential-dispersion-family" class="nav-link" data-scroll-target="#base-class-for-canonical-exponential-dispersion-family">Base class for canonical exponential dispersion family</a></li>
  <li><a href="#base-class-for-canonical-exponential-dispersion-family-1" id="toc-base-class-for-canonical-exponential-dispersion-family-1" class="nav-link" data-scroll-target="#base-class-for-canonical-exponential-dispersion-family-1">Base class for canonical exponential dispersion family</a></li>
  </ul></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block page-columns page-full" id="quarto-document-content">





<section id="intro-random-and-systematic-components-are-connected-via-link-functions" class="level2">
<h2 class="anchored" data-anchor-id="intro-random-and-systematic-components-are-connected-via-link-functions">Intro: random and systematic components are connected via link functions</h2>
<p>There many types of generalized linear regression models: such as linear regression, logistic regression, poisson regression etc. Every one of these models is made up of a “random component” and a “systematic component”. Each also has a “link function” that combines the random and systematic parts.</p>
<p>Let’s take the example of a poisson regression: <span class="math display">\[
\displaylines{
\underbrace{y_i \sim Pois(\mu_i)}_{\text{random}} \\
\underbrace{\ln{(\mu_i)}}_{\text{link function}} = \underbrace{X_i^{\intercal}\beta}_{\text{systematic}}
}
\]</span></p>
<section id="random-component" class="level4">
<h4 class="anchored" data-anchor-id="random-component">Random component</h4>
<p>The random component determines how we want to model the distribution of <span class="math inline">\(y\)</span>. For example, if <span class="math inline">\(y_i\)</span> is a count outcome, then it could be well suited to a poisson distribution:</p>
<p><span class="math display">\[
y_i \sim Pois(\mu_i)
\]</span></p>
<p>The mean rate of the count is <span class="math inline">\(\mu_i\)</span> - so we expect <span class="math inline">\(y_i\)</span> to be around <span class="math inline">\(\mu_i\)</span> on average. However, for any individual observation <span class="math inline">\(i\)</span>, the actual observed <span class="math inline">\(y_i\)</span> will vary above and below the mean rate <span class="math inline">\(\mu_i\)</span>. In fact, by using poisson we assume the variance of <span class="math inline">\(y_i\)</span> increases as the mean rate <span class="math inline">\(\mu_i\)</span> increases too. And this is why it is called the “random component”: since <span class="math inline">\(y\)</span> varies randomly around the mean, following the poisson distribution, it is a random variable.</p>
<p>!!!! GRAPH OF POISSON !!!!</p>
<p>But how do we find a good estimation for <span class="math inline">\(\mathbb{E}[y_i|X_i]=\mu_i\)</span>? Concretely, how do we best map our independent variables <span class="math inline">\(X_i\)</span> to <span class="math inline">\(\mu_i\)</span>? This is down to our systematic component and link function.</p>
</section>
<section id="systematic-component-and-link-function" class="level4">
<h4 class="anchored" data-anchor-id="systematic-component-and-link-function">Systematic component and link function</h4>
<p>In most cases, the systematic component <span class="math inline">\(\eta(X)\)</span> is usually just a linear transformation of <span class="math inline">\(X\)</span>, most often the result of multiplying each value by some good fitted coefficients <span class="math inline">\(\beta\)</span>.</p>
<p><span class="math display">\[
\eta(X_i) = X_i^{\intercal}\beta
\]</span></p>
<p>The link function is a way of choosing how to map the systematic component to the natural parameter of the random component. For example, in poisson regression, we use a log link function, which means the systematic component predicts the natural log of the mean rate of the count.</p>
<p><span class="math display">\[
\ln{(\mu_i)} = \eta(X) = X^{\intercal}\beta
\]</span></p>
<p>Okay, so now we want to find the best values for <span class="math inline">\(\beta\)</span>. These coefficients will transform our features <span class="math inline">\(X\)</span> to make the best predictions for <span class="math inline">\(\ln{(\mu_i)}\)</span>, given we want to predict <span class="math inline">\(y\)</span> as accurately as possible (but permit larger residuals when <span class="math inline">\(\mu_i\)</span> is larger, following the poisson distribution).</p>
<p>To achieve this: we can try some initial coefficients, calculate the cost function and its first derivative, update the coefficients, and continue to minimize the cost function through gradient descent.</p>
<p>But how cumbersome that would be to do for every type of distribution we want to model! Wouldn’t it be nice if we can derive a generic representation for the cost function and its first derivative, so that we can re-use the same code for every type of regression?</p>
</section>
</section>
<section id="exponential-dispersion-family-of-distributions" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="exponential-dispersion-family-of-distributions">Exponential Dispersion Family of Distributions</h2>
<p>It can be shown that many common distributions can be reformulated into the “Exponential Dispersion Family of Distributions”. This generic representation makes it easier to re-use the same code to run a regression, rather than hand calculate each pdf in different ways.</p>
<p>First let’s define some generic notation for generalized linear models:</p>
<p><span class="math display">\[
\displaylines{
\underbrace{g{(\xi_i)}}_{\text{link function}} = \underbrace{\eta(X_i)}_{\text{systematic}} \\
\underbrace{y_i \sim f_y(\xi_i,\phi_i)}_{\text{random}}
}
\]</span></p>
<p><span class="math inline">\(\xi_i\)</span> is a shape parameter, governing the shape of the distribution (e.g.&nbsp;in poisson, the expected value is the mean <span class="math inline">\(\xi_i=\mu_i\)</span>).</p>
<p><span class="math inline">\(\phi\)</span> is a dispersion parameter, although it is not always necessary (e.g.&nbsp;gaussian has the standard deviation <span class="math inline">\(\phi = \sigma\)</span>, but in poisson the variance is equated to the expected rate so not required).</p>
<p>Here are some examples of the common forms of poisson, bernoulli and gaussian probability distribution functions <span class="math inline">\(y \sim f_{\theta_i}(\mu_i)\)</span>, along with some choices for link functions <span class="math inline">\(g(\mu)\)</span> to use in regression:</p>
<table class="table">
<colgroup>
<col style="width: 33%">
<col style="width: 33%">
<col style="width: 33%">
</colgroup>
<thead>
<tr class="header">
<th>Type</th>
<th>Probability Density Function:</th>
<th>Link Function for Regression:</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Count</td>
<td><span class="math inline">\(y_i \sim Pois(\mu_i)\)</span><br><span class="math inline">\(y_i \sim \frac{\mu_i^{y} \times e^{-\mu_i}}{y!}\)</span></td>
<td><span class="math inline">\(g(\mu) = \ln{[\mu_i]}\)</span><br>(log-link)</td>
</tr>
<tr class="even">
<td>Binary</td>
<td><span class="math inline">\(y_i \sim Bern(\mu_i)\)</span><br><span class="math inline">\(y_i \sim \mu_i^y \times (1-\mu_i)^{(1-y)}\)</span></td>
<td><span class="math inline">\(g(\mu) = \ln{\left[\frac{\mu_i}{1-\mu_i}\right]}\)</span><br>(logit-link)</td>
</tr>
<tr class="odd">
<td>Binary</td>
<td><span class="math inline">\(y_i \sim Bern(\mu_i)\)</span><br><span class="math inline">\(y_i \sim \mu_i^y \times (1-\mu_i)^{(1-y)}\)</span></td>
<td><span class="math inline">\(g(\mu) = \Phi^{-1}[\mu_i]\)</span><br>(probit-link)</td>
</tr>
<tr class="even">
<td>Normal</td>
<td><span class="math inline">\(y_i \sim N(\mu_i,\sigma^2)\)</span><br><span class="math inline">\(y_i \sim \frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{1}{2}\left(\frac{y_i-\mu_i}{\sigma^2}\right)^2}\)</span></td>
<td><span class="math inline">\(g(\mu) = \mu\)</span><br>(identity-link)</td>
</tr>
</tbody>
</table>
<p>Now all of these probability density functions can be reformulated as being of the exponential family form!</p>
<p><span class="math display">\[
\displaylines{
\begin{align}
f(y;\xi,\phi)
&amp; =
\exp{ \left\{ \frac{T(y) r(\xi) - b(\xi)}{a(\phi)} + c(y,\phi) \right\} } &amp; \tag{1.1}
\end{align}
}
\]</span></p>
<p><span class="math display">\[
\displaylines{
\begin{align}
f(y;\xi,\phi)
&amp; =
\exp{ \left\{ \frac{T(y) r(\xi) - b(\xi)}{a(\phi)} + c(y,\phi) \right\} } &amp; \tag{1.1}
\\
\equiv f(y; \theta_i, \phi)
&amp; = \exp{ \left\{ \frac{y \theta - b(\theta)}{a(\phi)} + c(y,\phi) \right\} } &amp; \tag{1.2}
\\
\\
&amp; \iff T(y) = y; g(\xi) = r(\xi) = \theta
\end{align}
}
\]</span></p>
<p>Now when reformulating the probability density functions into the exponential family form, a good choice for the link function <span class="math inline">\(g(.)\)</span> often “reveals itself”: i.e.&nbsp;the “canonical” choice of <span class="math inline">\(g(\xi_i) = r(\xi_i)\)</span>.</p>
<p>If the chosen link function is this canonical one, then we can further simplify the non-canonical form <span class="math inline">\((1.1)\)</span> to its more common canonical form $(1.2), iff:</p>
<ol type="1">
<li><span class="math inline">\(T(y) = y\)</span> i.e.&nbsp;it happens to be the identity function.</li>
<li><span class="math inline">\(g(\xi) = r(\xi)\)</span> i.e.&nbsp;<span class="math inline">\(r(\xi)\)</span> is identical to the chosen link function <span class="math inline">\(g(\xi)\)</span>. If we are using a linear predictor too, so <span class="math inline">\(r(\xi_i)=\theta_i = \eta(X_i) = X_i^{\intercal}\beta\)</span>, then we actually find <span class="math inline">\(b(\xi) = b(\theta_i)\)</span> too, hence simplifying to <span class="math inline">\((1.2)\)</span>.</li>
</ol>
<p>Now it is actually true that both these conditions are met for poisson, logistic and gaussian regression! However isn’t true for probit, because the choice of link function is not “canonical”, i.e.&nbsp;<span class="math inline">\(r(\mu_i) \neq \Phi^{-1}[\mu_i]\)</span>. We will go through this later though.</p>
<section id="intuition-behind-the-exponential-form" class="level3">
<h3 class="anchored" data-anchor-id="intuition-behind-the-exponential-form">Intuition behind the exponential form</h3>
<p>There’s a lot of terms in the formula above. We will try to give some intuition behind <span class="math inline">\((1.2)\)</span>, but don’t worry if this still seems hard to get your head around: we are going to show how to reformulate each of the poisson, binomial and gaussian distributions into an exponential dispersion family form after this, to provide further intuition:</p>
<table class="table">
<colgroup>
<col style="width: 25%">
<col style="width: 75%">
</colgroup>
<thead>
<tr class="header">
<th>Parameter / Function</th>
<th style="text-align: left;">intuition</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><span class="math inline">\(\theta\)</span> is the “natural parameter”</td>
<td style="text-align: left;">This is a parameter influencing the shape of the distribution. It is clearly related to location, but sometimes the dispersion too. For example, in bernoulli, the mean (expected) <span class="math inline">\(p_i = \mu_i\)</span>, so <span class="math inline">\(\theta_i = g(\mu_i)=\ln{\left(\frac{\mu_i}{1-\mu_i}\right)}=\ln{\left(\frac{p_i}{1-p_i}\right)}\)</span>. However, since <span class="math inline">\(\mathbb{V}[y]=p_i(1-p_i)\)</span>, it also governs the dispersion of the bernoulli too.</td>
</tr>
<tr class="even">
<td><span class="math inline">\(\phi\)</span> is the “dispersion parameter”,</td>
<td style="text-align: left;">A parameter for the expected dispersion of <span class="math inline">\(y_i\)</span> around <span class="math inline">\(\mu_i\)</span>. For example, gaussian regression has <span class="math inline">\(\phi=\sigma\)</span>, the standard deviation of all the residuals (assuming homoskedasticity). However, it is not needed for one parameter distributions, like the poisson, where we already assume the variance <span class="math inline">\(\mathbb{V}[y_i] = \lambda_i\)</span> (i.e.&nbsp;already determind by <span class="math inline">\(\theta_i\)</span>).</td>
</tr>
<tr class="odd">
<td><span class="math inline">\(a(.)\)</span> is a normalizing function</td>
<td style="text-align: left;">A function that normalizes the pdf using the dispersion parameter <span class="math inline">\(\phi\)</span>. E.g. for gaussian regression, it is the variance of the residuals, <span class="math inline">\(a(\phi) = \sigma^2\)</span>. Again this isn’t needed for one parameter distributions.</td>
</tr>
<tr class="even">
<td><span class="math inline">\(c(.)\)</span> an adjustment function</td>
<td style="text-align: left;">A function that adjusts the normalized pdf so that it sums to one. For example, the exponential form of the poisson distribution would sum to more than one if it wasn’t included.</td>
</tr>
<tr class="odd">
<td><span class="math inline">\(b(.)\)</span> is the integral of the inverse-link function</td>
<td style="text-align: left;">It helps to look at <span class="math inline">\(b'(\theta)\)</span> first. Whereas the link function maps <span class="math inline">\(\mu \rightarrow \theta\)</span>, i.e.&nbsp;<span class="math inline">\(g(\mu) = \theta\)</span>, the inverse of the link function maps the other way <span class="math inline">\(\theta \rightarrow \mu\)</span> i.e.&nbsp;<span class="math inline">\(b'(\theta)=g^{-1}(\theta)=\mu\)</span>. You might also recognise this as the “activation function” in the outer layer of neural networks. Then, the integral <span class="math inline">\(b(\theta) = \int{b'(\theta)\,d\theta}\)</span> by definition. <br> For example, poisson regression has log-link function: <span class="math inline">\(g(\mu) = \ln{(\mu)} = \theta\)</span>. So the inverse-link function is the exponential: <span class="math inline">\(b'(\theta) = g'(\mu)= \exp{(\theta)}\)</span>. It just so happens the integral is the same in this case: <span class="math inline">\(b(\theta) = \int{\exp{\{\theta\}}} = \exp{\{\theta\}}\)</span></td>
</tr>
</tbody>
</table>
</section>
<section id="poisson" class="level3">
<h3 class="anchored" data-anchor-id="poisson">Poisson</h3>
<p>Now let’s have a look at poisson, with mean rate of <span class="math inline">\(\lambda_i = \mu_i\)</span>:</p>
<p><span class="math display">\[
\displaylines{
\begin{align}
f(y;\theta,\phi)
&amp; =
\frac{\mu_i^{y_i}e^{-\mu_i}}{y_i!}
\\ &amp; = \exp{ \left\{
\ln{ \left[ \frac{\mu_i^{y_i} \times e^{-\mu_i}}{y_i!} \right]}
\right\}}
\\ &amp; = \exp{ \left\{
\ln{ \left[ \mu_i^{y_i} \right]}
+ \ln{ \left[ e^{-\mu_i} \right] }
- \ln{ \left[ y_i! \right] }
\right\}}
\\ &amp; = \exp{ \left\{
y_i \ln{ \left[ \mu_i \right]}
-\mu_i
- \ln{ \left[ y_i! \right] }
\right\}}
\\ &amp; \equiv \exp{ \left\{
\frac{
  \underbrace{y_i}_{T(y)}
  \underbrace{\ln{ \left[ \mu_i \right]}}_{r(\xi_i)}
  - \underbrace{\mu_i}_{b(\xi_i)}
}{
  \underbrace{1}_{a(\phi)}
}
- \underbrace{\ln{ \left[ y_i! \right] }}_{c(y,\phi)}
\right\}}
\\ &amp; \equiv \exp{ \left\{
\frac{y_i
  \underbrace{\ln{ \left[ \mu_i \right]}}_{\theta_i}
  - \underbrace{\mu_i}_{b(\theta_i)}
}{
  \underbrace{1}_{a(\phi)}
}
- \underbrace{\ln{ \left[ y_i! \right] }}_{c(y,\phi)}
\right\}}
\\ &amp; = \exp{ \left\{ \frac{y_i \theta_i - b(\theta_i)}{a(\phi)} + c(y_i,\phi) \right\} }
\end{align}
}
\]</span></p>
<p>So we find that:</p>
<ul>
<li>Since we are using the log-link, our choice of link function is canonical: <span class="math inline">\(r(\xi_i) = \ln{[\mu_i]} = g(\mu_i)\)</span>. So we can simplify to <span class="math inline">\(\theta_i\)</span>!</li>
<li><span class="math inline">\(b(\theta_i) = \mu_i = \exp\{\theta_i\}\)</span>.
<ul>
<li>Note that the differential of this with respect to <span class="math inline">\(\theta_i\)</span> is still the exponential function, <span class="math inline">\(\frac{\partial b(\theta_i)}{\partial \theta_i}=\exp\{{\theta_i}\}\)</span>. So the differential is the inverse of the log-link function!</li>
</ul></li>
<li>We can ignore <span class="math inline">\(a(\phi)\)</span>. This term usually helps normalize the pdf by its dispersion, but the poisson distribution only has one parameter: the variance is identical to the mean, <span class="math inline">\(\mathbb{V}[y_i] = \mu_i\)</span>. So no normalisation is needed.</li>
<li>Since the exponential form of the poisson distribution actually sums to more than one, we need to adjust it using <span class="math inline">\(c(y_i,\phi)=\ln{ \left[ y_i! \right] }\)</span>.</li>
</ul>
</section>
<section id="bernoulli-logit-vs-probit" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="bernoulli-logit-vs-probit">Bernoulli (Logit vs probit)</h3>
<p>Let’s take the bernoulli pdf with expected success probability <span class="math inline">\(p_i=\mu_i\)</span>, and reformulate it to the exponential dispersion family form:</p>
<p><span class="math display">\[
\displaylines{
\begin{align}
f(y;\theta,\phi)
&amp; = \mu_i^{y_i}(1-\mu_i)^{1-y_i} \\
&amp; = \exp{ \left\{\ln{\left[\mu_i^{y_i}(1-\mu_i)^{(1-y_i)} \\\right]} \right\}} \\
&amp; = \exp{ \left\{
  \ln{\left[\mu_i^{y_i}\right]} + \ln{\left[(1-\mu_i)^{(1-y_i)}\right]}
  \right\}} \\
&amp; = \exp{ \left\{ y_i \ln{[\mu_i]} + (1-y_i)\ln{[1-\mu_i]} \right\}} \\
&amp; = \exp{ \left\{ y_i \ln{
  \left[\frac{\mu_i}{1-\mu_i} \right]} + \ln{[1-\mu_i]} \right\}} \\
&amp; = \exp{ \left\{ \frac{
  \underbrace{y_i}_{T(y)}
\underbrace{ \ln{
  \left[\frac{\mu_i}{1-\mu_i} \right]
} }_{r(\xi_i)}
  + \underbrace{ \ln{[1-\mu_i]} }_{b(\xi_i)}
  }{
    \underbrace{1}_{a(\phi)}
  }
  + \underbrace{0}_{c(y_i,\phi)}
\right\}}
\\
&amp; = \exp{ \left\{ \frac{y_i
\underbrace{ \ln{
  \left[\frac{\mu_i}{1-\mu_i} \right]
} }_{\theta_i}
  + \underbrace{ \ln{[1-\mu_i]} }_{b(\theta_i)}
  }{
    \underbrace{1}_{a(\phi)}
  }
  + \underbrace{0}_{c(y_i,\phi)}
\right\}}
\\ &amp; = \exp{ \left\{ \frac{y_i \theta_i - b(\theta_i)}{a(\phi)} + c(y_i,\phi) \right\} }
\end{align}
}
\]</span></p>
<p>So we find that:</p>
<ul>
<li>Now when using the logit-link, our choice of link function is canonical: <span class="math inline">\(r(\xi_i) = \ln{\left[\frac{\mu_i}{1-\mu_i} \right]} = g(\mu_i)\)</span>. So we can simplify to <span class="math inline">\(\theta_i\)</span> when using logistic regression!
<ul>
<li>However, this isn’t the case for probit, since <span class="math inline">\(r(\xi_i) = \ln{\left[\frac{\mu_i}{1-\mu_i} \right]} \neq g(\mu_i) = \Phi^{-1}[\mu_i]\)</span>. So to conduct probit regression, we need to use the non-canonical form.</li>
</ul></li>
<li><span class="math inline">\(b(\theta_i) = \ln{[1-\mu_i]} = \ln{[1+\exp\{\theta_i\}]}\)</span><a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>.
<ul>
<li>Note that the differential of this with respect to <span class="math inline">\(\theta\)</span> is just the logistic function, <span class="math inline">\(\frac{\partial b(\theta_i)}{\partial\theta_i}=\frac{\exp\{\theta_i\}}{1+\exp\{\theta_i\}}\)</span>. This differential is the inverse of the logit link function.</li>
</ul></li>
<li>We can ignore <span class="math inline">\(a(\phi)\)</span>. This term usually helps normalize the pdf by its dispersion, but the bernoulli distribution only has one parameter: the variance is <span class="math inline">\(\mathbb{V}[y_i] = \mu_i \times (1-\mu_i)\)</span></li>
<li>We can also ignore <span class="math inline">\(c(y_i,\phi)\)</span>. Since the exponential form of the bernoulli distribution already sums to one, we do not need to adjust it.</li>
</ul>

<div class="no-row-height column-margin column-container"><div class="">
<p>See footnotes for deriving <span class="math inline">\(\ln{[1-p]} = \ln{[1+\exp\{X_i^{\intercal}\beta\}]}\)</span>.</p>
</div></div></section>
<section id="gaussian" class="level3">
<h3 class="anchored" data-anchor-id="gaussian">Gaussian</h3>
<p><span class="math display">\[
\displaylines{
\begin{align}
f(y;\theta,\phi)
&amp; = \frac{1}{\sqrt{2 \pi \sigma^2}} \exp{ \left\{-\frac{(y_i-\mu_i)^2}{2\sigma^2} \right\} }
\\ &amp; =
\exp{\left\{
\ln{ \left[ \frac{1}{\sqrt{2 \pi \sigma^2}}
\exp{ \left\{ -\frac{(y_i-\mu_i)^2}{2\sigma^2} \right\} }
\right]}
\right\}}
\\ &amp; =
\exp{\left\{
\ln{ \left[ \frac{1}{\sqrt{2 \pi \sigma^2}} \right]}
- \frac{(y_i-\mu_i)^2}{2\sigma^2}
\right\}}
\\ &amp; =
\exp{\left\{
\cancel{\ln{ \left[ 1 \right]}}
+ \ln{ \left[ 2 \pi \sigma^2 \right]}^{-1/2}
- \frac{y_i^2+\mu_i^2-2y\mu_i}{2\sigma^2}
\right\}}
\\ &amp; =
\exp{\left\{
- \frac{1}{2}\ln{ \left[ 2 \pi \sigma^2 \right]}
- \frac{y_i^2}{2\sigma^2}
- \frac{\frac{\mu_i^2}{2}}{\sigma^2}
+ \frac{\cancel{2}y_i\mu_i}{\cancel{2}\sigma^2}
\right\}}
\\ &amp; =
\exp{\left\{
  \frac{
    y \underbrace{\mu_i}_{\theta_i} -
    \underbrace{\frac{\mu_i^2}{2}}_{b(\theta_i)}
  }{
    \underbrace{\sigma^2}_{a(\phi)}
  }
-
\underbrace{
  \frac{1}{2}
  \left(
    \frac{y_i^2}{\sigma^2} +
    \ln{ \left[ 2 \pi \sigma^2 \right]}
  \right)
}_{c(y_i,\phi)}
\right\}}
\\ &amp; = \exp{ \left\{ \frac{y_i \theta_i - b(\theta_i)}{a(\phi)} + c(y_i,\phi) \right\} }
\end{align}
}
\]</span></p>
<p>So we find that:</p>
<ul>
<li>Since we are using the identity-link, our choice of link function is canonical: <span class="math inline">\(r(\xi_i) = \xi_i = g(\xi_i)\)</span>. So we can simplify to <span class="math inline">\(\theta_i\)</span>!</li>
<li><span class="math inline">\(b(\theta_i) = \frac{\mu_i^2}{2}\)</span>. You might recognise this squared term as part of the usual cost function for least squares regression!
<ul>
<li>Note that the differential of this with respect to <span class="math inline">\(\theta_i\)</span> is the identity, <span class="math inline">\(\frac{\partial b(\theta_i)}{\partial \theta_i}=\theta_i\)</span>. It is the inverse of the link function (again the identity).</li>
</ul></li>
<li>Finally, we have a use for <span class="math inline">\(a(\phi)\)</span>. The gaussian distribution has two parameters, a second relating to its variance, so we need to normalize our cost function by this measure of the dispersion of our data.</li>
<li>And again, we have a use <span class="math inline">\(c(y_i,\phi)\)</span>. Without this correction, the exponential form of the gaussian distribution sums to more than one.</li>
</ul>
</section>
</section>
<section id="solving-for-any-exponential-family-using-maximum-likelihood-estimation" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="solving-for-any-exponential-family-using-maximum-likelihood-estimation">Solving for any exponential family using maximum likelihood estimation</h2>
<p>So now we have shown we can reformulate poisson, binomial and normal regressions into the exponential family form. Now we want to find our one-use formula to estimate the best coefficients via maximum likelihood estimation!</p>
<p>So let’s define our generic cost function in negative log-likelihood form:</p>

<div class="no-row-height column-margin column-container"><div class="">
<p>Maximising the the likelihood is hard, because it involves calculating the total product across every observation. Instead, taking the log likelihood makes everything a sum, far easier to calculate. Also, taking the negative ensures we are looking to minimize the cost.</p>
</div></div><p><span class="math display">\[
\displaylines{
\begin{align}
L(\theta)
&amp; = \prod_{i=1}^n{
  f(y_i;\theta_i,\phi)
}
\\ &amp; = \prod_{i=1}^n{
  \exp{ \left\{
    \frac{y \theta - b(\theta)}{a(\phi)}
    + c(y,\phi)
  \right\} }
}
\\ \therefore \mathcal{L}(\theta)
&amp; = -\ln \left\{ \prod_{i=1}^n{
  \exp{ \left\{
    \frac{y_i \theta_i - b(\theta_i)}{a(\phi)}
    + c(y_i,\phi)
  \right\} }
} \right\}
\\ &amp; = -\sum_{i=1}^n{ \left\{
  \frac{y_i \theta_i - b(\theta_i)}{a(\phi)}
  + c(y_i,\phi)
\right\} }
\\ &amp; = -\left(
\frac{1}{a(\phi)}
\sum_{i=1}^n{ \bigg\{
    y_i \theta_i - b(\theta_i)
\bigg\} } +
\sum_{i=1}^n{ \bigg\{
  c(y_i,\phi)
\bigg\}} \right)
\end{align}
}
\]</span></p>
<p>Next we want to minimize this generic cost function with respect to <span class="math inline">\(\beta\)</span>. Common methods include “Newton-Raphson”, “Fisher-Scoring”, “Iteratively-reweighted Least Squares” or “Gradient Descent”.</p>
<p>To execute any of these, we need to derive the the “score” (or “informant”): the first derivative of the negative log likelihood with respect to <span class="math inline">\(\beta\)</span>:</p>
<p><span class="math display">\[
\displaylines{
\begin{align}
\arg \min_\beta \left[ \mathcal{L}(\theta) \right]
&amp; = \arg \min_\beta \left[ -\left(
\frac{1}{a(\phi)}
\sum_{i=1}^n{ \bigg\{
    y_i \theta_i - b(\theta_i)
\bigg\} } +
\sum_{i=1}^n{ \bigg\{
  c(y_i,\phi)
\bigg\}}
\right)\right]
\\
&amp; = \arg \min_\beta \left[ -\left(
\frac{1}{a(\phi)}
\sum_{i=1}^n{ \bigg\{
    y_i X_i^{\intercal}\beta - b(X_i^{\intercal}\beta)
\bigg\} } +
\sum_{i=1}^n{ \bigg\{
  c(y_i,\phi)
\bigg\}}
\right)\right]
\\ &amp; \because \theta = \eta(X) \text{ in the canonical case}
\\
\\
\therefore \frac{\partial \mathcal{L}(\theta)}{\partial\beta_i}
&amp; = -\frac{1}{a(\phi)} \sum_{i=1}^n{ \bigg\{
      y_i X_i  - \frac{\partial b(\theta)}{\partial \theta} X_i
  \bigg\} }
\\ &amp; \because c() \text{ is constant with respect to } \beta
\\
&amp; =\underbrace{\frac{1}{a(\phi)}}_{\text{Constant}}  \times \sum_{i=1}^n{ \bigg\{ \left( b'(\theta) - y_i \right) X_i \bigg\} }
\end{align}
}
\]</span></p>

<div class="no-row-height column-margin column-container"><div class="">
<p>Only <span class="math inline">\(\theta\)</span> changes with respect to the choice of <span class="math inline">\(\beta\)</span>. So <span class="math inline">\(a(\phi)\)</span> and <span class="math inline">\(c(y_i,\phi)\)</span>, which are not a function of <span class="math inline">\(\theta\)</span>, drop out.</p>
</div></div><p>So this might start looking familar. Given we know that <span class="math inline">\(b'(\theta)\)</span> is the activation function, we can see that the score function is just the difference between the predicted value and the actual value, multiplied by the feature. This is the same as the gradient of the cost function for a neural network!</p>
<p>This also uncovers an interesting property of GLMs - that the average prediction <span class="math inline">\(b'(\theta)\)</span> must be equal to the average value of Y too:</p>
<p><span class="math display">\[
\displaylines{
\begin{align}
\frac{\partial \mathcal{L}(\theta)}{\partial\beta_i}
&amp; = 0 \text{ (cost is minimized at stationary point)}
\\
&amp; = \cancel{\frac{1}{a(\phi)}}  \times \sum_{i=1}^n{ \bigg\{ \left( b'(\theta) - y_i \right) \cancel{X_i} \bigg\} }
\\
\therefore
\sum_{i=1}^n{ y_i } &amp; = \sum_{i=1}^n{ b'(\theta) }
\\ &amp; \div N
\\ \\
\Rightarrow \bar{y} &amp; = \mathbb{E}[y] = b'(\theta)
\end{align}
}
\]</span></p>
<p>Finally, we need to derive the Hessian matrix, which is the second derivative of the cost function with respect to <span class="math inline">\(\beta\)</span>. This is useful for second order optimization methods, like Newton’s method, which can converge faster than gradient descent:</p>
<p><span class="math display">\[
\displaylines{
\begin{align}
\frac{\partial^2 \mathcal{L}(\theta)}{\partial\beta_i^2}
&amp; = \frac{\partial}{\partial\beta_i}\left(
  \frac{1}{a(\phi)} \sum_{i=1}^n{ \bigg\{ \left( b'(\theta) - y_i \right) X_i \bigg\} }
\right)
\\
&amp; = -\frac{1}{a(\phi)} \sum_{i=1}^n{ \bigg\{
      \frac{\partial b'(\theta)}{\partial \theta} X_i^2
  \bigg\} }
\end{align}
}
\]</span></p>
<p>Let’s now start to write out some python to create this ourselves.</p>
</section>
<section id="coding-it-up-from-scratch" class="level2">
<h2 class="anchored" data-anchor-id="coding-it-up-from-scratch">Coding it up from scratch</h2>
<section id="base-class-for-canonical-exponential-dispersion-family" class="level3">
<h3 class="anchored" data-anchor-id="base-class-for-canonical-exponential-dispersion-family">Base class for canonical exponential dispersion family</h3>
<p>Let’s start creating a generic base class then that we can use for any distribution that can be represented by the canonical exponential dispersion family:</p>
<div id="8d56a9cb" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> base_canonical_exponential_dispersion_family():</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>,y,X,b,phi,a,c,seed<span class="op">=</span><span class="dv">0</span>):</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>      <span class="co">"""</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="co">      y:    Dependent variable. A 1d vector of numeric values </span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="co">      X:    Independent variables. A 2d matrix of numeric values </span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="co">      b:    Function for the integral of the activation function. See db for more details.</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="co">      db:   Function for the first derivative of b(θ) with respect to theta.</span></span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="co">            Note that this should be the inverse of the link function</span></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="co">            E.g. for poisson reg, g(µ) = ln(µ) = η(X) = θ. So b(θ) = exp(θ).</span></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="co">      phi:  Parameter: the "dispersion parameter", φ. </span></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="co">            A measure of the dispersion of the distribution</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="co">            E.g. for gaussian reg, φ is the standard deviation of residuals.</span></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="co">      a:    Function of φ. Normalizes the pdf using the dispersion parameter. </span></span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="co">            E.g. for gaussian reg, it is the variance of the residuals a(φ) = φ^2.</span></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="co">      c:    Function for y and φ. Adjusts the likelihood so that the pdf sums to 1.</span></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a><span class="co">      """</span></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a>      <span class="va">self</span>.y <span class="op">=</span> np.array(y).reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)</span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a>      <span class="va">self</span>.X <span class="op">=</span> np.array(X).reshape(y.shape[<span class="dv">0</span>],<span class="op">-</span><span class="dv">1</span>)</span>
<span id="cb1-21"><a href="#cb1-21" aria-hidden="true" tabindex="-1"></a>      <span class="va">self</span>.b <span class="op">=</span> b</span>
<span id="cb1-22"><a href="#cb1-22" aria-hidden="true" tabindex="-1"></a>      <span class="va">self</span>.phi <span class="op">=</span> phi</span>
<span id="cb1-23"><a href="#cb1-23" aria-hidden="true" tabindex="-1"></a>      <span class="va">self</span>.a <span class="op">=</span> a</span>
<span id="cb1-24"><a href="#cb1-24" aria-hidden="true" tabindex="-1"></a>      <span class="va">self</span>.c <span class="op">=</span> c</span>
<span id="cb1-25"><a href="#cb1-25" aria-hidden="true" tabindex="-1"></a>      </span>
<span id="cb1-26"><a href="#cb1-26" aria-hidden="true" tabindex="-1"></a>      np.random.seed(seed)</span>
<span id="cb1-27"><a href="#cb1-27" aria-hidden="true" tabindex="-1"></a>      </span>
<span id="cb1-28"><a href="#cb1-28" aria-hidden="true" tabindex="-1"></a>      <span class="co"># Initialize beta with random values</span></span>
<span id="cb1-29"><a href="#cb1-29" aria-hidden="true" tabindex="-1"></a>      <span class="va">self</span>.n, <span class="va">self</span>.k <span class="op">=</span> <span class="va">self</span>.X</span>
<span id="cb1-30"><a href="#cb1-30" aria-hidden="true" tabindex="-1"></a>      <span class="va">self</span>.beta <span class="op">=</span> np.random.normal(<span class="dv">0</span>, <span class="fl">0.5</span>, <span class="va">self</span>.k).reshape((<span class="va">self</span>.k,<span class="dv">1</span>))</span>
<span id="cb1-31"><a href="#cb1-31" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-32"><a href="#cb1-32" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> canonical_theta(X, beta):</span>
<span id="cb1-33"><a href="#cb1-33" aria-hidden="true" tabindex="-1"></a>      <span class="co">""" In the canonical form, theta is simply θ = η(X)= X'β """</span></span>
<span id="cb1-34"><a href="#cb1-34" aria-hidden="true" tabindex="-1"></a>      <span class="cf">return</span> X.dot(beta)</span>
<span id="cb1-35"><a href="#cb1-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-36"><a href="#cb1-36" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> negative_log_likelihood(<span class="va">self</span>, beta):</span>
<span id="cb1-37"><a href="#cb1-37" aria-hidden="true" tabindex="-1"></a>        <span class="co">""" Negative log likelihood i.e. the current cost</span></span>
<span id="cb1-38"><a href="#cb1-38" aria-hidden="true" tabindex="-1"></a><span class="co">        """</span></span>
<span id="cb1-39"><a href="#cb1-39" aria-hidden="true" tabindex="-1"></a>        y, X <span class="op">=</span> <span class="va">self</span>.y, <span class="va">self</span>.X</span>
<span id="cb1-40"><a href="#cb1-40" aria-hidden="true" tabindex="-1"></a>        theta <span class="op">=</span> <span class="va">self</span>.canonical_theta(<span class="va">self</span>.X, beta)</span>
<span id="cb1-41"><a href="#cb1-41" aria-hidden="true" tabindex="-1"></a>        phi <span class="op">=</span> <span class="va">self</span>.phi</span>
<span id="cb1-42"><a href="#cb1-42" aria-hidden="true" tabindex="-1"></a>        a, b, c <span class="op">=</span> <span class="va">self</span>.a, <span class="va">self</span>.b, <span class="va">self</span>.c</span>
<span id="cb1-43"><a href="#cb1-43" aria-hidden="true" tabindex="-1"></a>        log_likelihood <span class="op">=</span> ( y <span class="op">*</span> _theta <span class="op">-</span> b(_theta) ) <span class="op">/</span> a(phi) <span class="op">+</span> c(y,phi)</span>
<span id="cb1-44"><a href="#cb1-44" aria-hidden="true" tabindex="-1"></a>        J <span class="op">=</span> <span class="op">-</span><span class="dv">1</span> <span class="op">*</span> log_likelihood</span>
<span id="cb1-45"><a href="#cb1-45" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> J</span>
<span id="cb1-46"><a href="#cb1-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-47"><a href="#cb1-47" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> informant(<span class="va">self</span>, theta<span class="op">=</span><span class="va">None</span>, phi<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb1-48"><a href="#cb1-48" aria-hidden="true" tabindex="-1"></a>        <span class="co">""" First derivative of the cost function with respect to theta """</span></span>
<span id="cb1-49"><a href="#cb1-49" aria-hidden="true" tabindex="-1"></a>        y, X, a, b <span class="op">=</span> <span class="va">self</span>.y, <span class="va">self</span>.X, <span class="va">self</span>.a, <span class="va">self</span>.b</span>
<span id="cb1-50"><a href="#cb1-50" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> theta <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb1-51"><a href="#cb1-51" aria-hidden="true" tabindex="-1"></a>            theta <span class="op">=</span> <span class="va">self</span>.canonical_theta()</span>
<span id="cb1-52"><a href="#cb1-52" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> phi <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb1-53"><a href="#cb1-53" aria-hidden="true" tabindex="-1"></a>            phi <span class="op">=</span> <span class="va">self</span>.phi</span>
<span id="cb1-54"><a href="#cb1-54" aria-hidden="true" tabindex="-1"></a>        dJ <span class="op">=</span> ( X.T<span class="op">/</span>a(phi) ).dot( db( theta ) <span class="op">-</span> y )</span>
<span id="cb1-55"><a href="#cb1-55" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> dJ</span>
<span id="cb1-56"><a href="#cb1-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-57"><a href="#cb1-57" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> hessian(<span class="va">self</span>, theta, phi):</span>
<span id="cb1-58"><a href="#cb1-58" aria-hidden="true" tabindex="-1"></a>        <span class="co">""" Second derivative of the cost function with respect to theta """</span></span>
<span id="cb1-59"><a href="#cb1-59" aria-hidden="true" tabindex="-1"></a>        a, d2b, X <span class="op">=</span> <span class="va">self</span>.a, <span class="va">self</span>.d2b, <span class="va">self</span>.X</span>
<span id="cb1-60"><a href="#cb1-60" aria-hidden="true" tabindex="-1"></a>        V <span class="op">=</span> <span class="va">self</span>.var_y(theta, phi)</span>
<span id="cb1-61"><a href="#cb1-61" aria-hidden="true" tabindex="-1"></a>        d2J <span class="op">=</span> X.T.dot( V <span class="op">/</span> a(phi)<span class="op">**</span><span class="dv">2</span> ).dot(X)</span>
<span id="cb1-62"><a href="#cb1-62" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> d2J</span>
<span id="cb1-63"><a href="#cb1-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-64"><a href="#cb1-64" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> update_beta(<span class="va">self</span>, fit_type<span class="op">=</span><span class="st">'Netwon-Raphson'</span>):</span>
<span id="cb1-65"><a href="#cb1-65" aria-hidden="true" tabindex="-1"></a>        <span class="co">""" A single step towards optimizing beta """</span></span>
<span id="cb1-66"><a href="#cb1-66" aria-hidden="true" tabindex="-1"></a>        beta, X <span class="op">=</span> <span class="va">self</span>.beta, <span class="va">self</span>.X</span>
<span id="cb1-67"><a href="#cb1-67" aria-hidden="true" tabindex="-1"></a>        theta, phi <span class="op">=</span>  <span class="va">self</span>.theta(), <span class="va">self</span>.phi()</span>
<span id="cb1-68"><a href="#cb1-68" aria-hidden="true" tabindex="-1"></a>        d2J <span class="op">=</span> <span class="va">self</span>.hessian( theta, phi )</span>
<span id="cb1-69"><a href="#cb1-69" aria-hidden="true" tabindex="-1"></a>        learning_rate <span class="op">=</span> np.linalg.solve( d2J, np.eye(X.shape[<span class="dv">1</span>]) )</span>
<span id="cb1-70"><a href="#cb1-70" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> fit_type <span class="kw">in</span> [<span class="st">'Netwon-Raphson'</span>]:</span>
<span id="cb1-71"><a href="#cb1-71" aria-hidden="true" tabindex="-1"></a>            dJ <span class="op">=</span> <span class="va">self</span>.score_function( theta, phi )</span>
<span id="cb1-72"><a href="#cb1-72" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb1-73"><a href="#cb1-73" aria-hidden="true" tabindex="-1"></a>            <span class="cf">raise</span> <span class="pp">ValueError</span>(<span class="st">'Please select "Newton-Raphson". "IRLS" coming soon'</span>)</span>
<span id="cb1-74"><a href="#cb1-74" aria-hidden="true" tabindex="-1"></a>        beta <span class="op">-=</span> learning_rate.dot(dJ)</span>
<span id="cb1-75"><a href="#cb1-75" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> beta</span>
<span id="cb1-76"><a href="#cb1-76" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-77"><a href="#cb1-77" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> fitting_mle(<span class="va">self</span>, max_iter <span class="op">=</span> <span class="dv">100</span>, fit_type<span class="op">=</span><span class="st">'Netwon-Raphson'</span>, epsilon <span class="op">=</span> <span class="fl">1e-8</span>):</span>
<span id="cb1-78"><a href="#cb1-78" aria-hidden="true" tabindex="-1"></a>        <span class="co">""" Fitting using MLE """</span></span>
<span id="cb1-79"><a href="#cb1-79" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(max_iter):</span>
<span id="cb1-80"><a href="#cb1-80" aria-hidden="true" tabindex="-1"></a>            old_beta <span class="op">=</span> <span class="va">self</span>.beta.copy()</span>
<span id="cb1-81"><a href="#cb1-81" aria-hidden="true" tabindex="-1"></a>            new_beta <span class="op">=</span> <span class="va">self</span>.update_beta(fit_type<span class="op">=</span>fit_type)</span>
<span id="cb1-82"><a href="#cb1-82" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.beta <span class="op">=</span> new_beta</span>
<span id="cb1-83"><a href="#cb1-83" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> (np.<span class="bu">abs</span>(new_beta <span class="op">-</span> old_beta)<span class="op">/</span>(<span class="fl">0.1</span> <span class="op">+</span> np.<span class="bu">abs</span>(new_beta)) <span class="op">&lt;=</span> epsilon).<span class="bu">all</span>():</span>
<span id="cb1-84"><a href="#cb1-84" aria-hidden="true" tabindex="-1"></a>                <span class="bu">print</span>(<span class="st">"Fully converged by iteration "</span> <span class="op">+</span> <span class="bu">str</span>(i))</span>
<span id="cb1-85"><a href="#cb1-85" aria-hidden="true" tabindex="-1"></a>                <span class="cf">break</span></span>
<span id="cb1-86"><a href="#cb1-86" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> (i <span class="op">==</span> max_iter):</span>
<span id="cb1-87"><a href="#cb1-87" aria-hidden="true" tabindex="-1"></a>                <span class="bu">print</span>(<span class="st">"Warning - coefficients did not fully converge within "</span> <span class="op">+</span> <span class="bu">str</span>(max_iter) <span class="op">+</span> <span class="st">" iterations."</span>)</span>
<span id="cb1-88"><a href="#cb1-88" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.fitted <span class="op">=</span> <span class="va">True</span></span>
<span id="cb1-89"><a href="#cb1-89" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.unadj_r_squared <span class="op">=</span> <span class="op">-</span><span class="dv">1</span></span>
<span id="cb1-90"><a href="#cb1-90" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.residuals <span class="op">=</span> (<span class="va">self</span>.y <span class="op">-</span> <span class="va">self</span>.db(<span class="va">self</span>.theta())).reshape(<span class="op">-</span><span class="dv">1</span>,<span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="base-class-for-canonical-exponential-dispersion-family-1" class="level3">
<h3 class="anchored" data-anchor-id="base-class-for-canonical-exponential-dispersion-family-1">Base class for canonical exponential dispersion family</h3>
<p>And now we have our base class, we can utilise it for specific classes of the poisson, bernoulli and gaussian distributions:</p>
<div id="af9626e2" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> gaussian_family(base_canonical_exponential_dispersion_family):</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>  <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>,y,X,seed <span class="op">=</span> <span class="dv">0</span>):</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>      <span class="kw">def</span> b(theta):</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>          <span class="co">""" Integral of the activation function """</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>          <span class="cf">return</span> <span class="fl">0.5</span><span class="op">*</span>theta<span class="op">**</span><span class="dv">2</span></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>      <span class="kw">def</span> db(theta):</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>          <span class="co">""" Activation function is identity (inverse of the indentity function):"""</span></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>          <span class="cf">return</span> theta</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>      <span class="kw">def</span> d2b(theta):</span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>          <span class="co">""" Differential of the activation function """</span></span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>          <span class="cf">return</span> <span class="dv">1</span></span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a>      <span class="kw">def</span> dispersion(y, theta):</span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a>          <span class="co">""" Std.dev of residuals = sqrt(RSS / DoF) """</span></span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a>          n, k <span class="op">=</span> <span class="va">self</span>.n, <span class="va">self</span>.k</span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a>          std_dev <span class="op">=</span> np.sqrt(np.<span class="bu">sum</span>( (y <span class="op">-</span> theta)<span class="op">**</span><span class="dv">2</span> ) <span class="op">/</span> ( n <span class="op">-</span> k ) )</span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a>          <span class="cf">return</span> std_dev</span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a>      <span class="kw">def</span> a(phi):</span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a>          <span class="co">""" Variance of residuals """</span></span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a>          <span class="cf">return</span> phi<span class="op">**</span><span class="dv">2</span></span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a>      <span class="kw">def</span> c(y,phi):</span>
<span id="cb2-28"><a href="#cb2-28" aria-hidden="true" tabindex="-1"></a>          <span class="co">""" Adjustment needed so pdf sums to one """</span></span>
<span id="cb2-29"><a href="#cb2-29" aria-hidden="true" tabindex="-1"></a>          <span class="cf">return</span> <span class="op">-</span><span class="fl">0.5</span> <span class="op">*</span> ( y<span class="op">**</span><span class="dv">2</span><span class="op">/</span>phi <span class="op">+</span> np.log(<span class="dv">2</span><span class="op">*</span>pi<span class="op">*</span>phi))</span>
<span id="cb2-30"><a href="#cb2-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-31"><a href="#cb2-31" aria-hidden="true" tabindex="-1"></a>      <span class="bu">super</span>().<span class="fu">__init__</span>(y,X,b,db,d2b,dispersion,a,c)</span>
<span id="cb2-32"><a href="#cb2-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-33"><a href="#cb2-33" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> bernoulli_family(base_canonical_exponential_dispersion_family):</span>
<span id="cb2-34"><a href="#cb2-34" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-35"><a href="#cb2-35" aria-hidden="true" tabindex="-1"></a>  <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>,y,X,seed <span class="op">=</span> <span class="dv">0</span>):</span>
<span id="cb2-36"><a href="#cb2-36" aria-hidden="true" tabindex="-1"></a>      </span>
<span id="cb2-37"><a href="#cb2-37" aria-hidden="true" tabindex="-1"></a>      <span class="kw">def</span> b(theta):</span>
<span id="cb2-38"><a href="#cb2-38" aria-hidden="true" tabindex="-1"></a>          <span class="co">""" Integral of the activation function """</span></span>
<span id="cb2-39"><a href="#cb2-39" aria-hidden="true" tabindex="-1"></a>          <span class="cf">return</span> np.log(<span class="dv">1</span> <span class="op">+</span> np.exp(theta))</span>
<span id="cb2-40"><a href="#cb2-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-41"><a href="#cb2-41" aria-hidden="true" tabindex="-1"></a>      <span class="kw">def</span> db(theta):</span>
<span id="cb2-42"><a href="#cb2-42" aria-hidden="true" tabindex="-1"></a>          <span class="co">""" Activation function is logistic (inverse of the logit function):"""</span></span>
<span id="cb2-43"><a href="#cb2-43" aria-hidden="true" tabindex="-1"></a>          <span class="cf">return</span> np.exp(theta)<span class="op">/</span>(<span class="dv">1</span> <span class="op">+</span> np.exp(theta))</span>
<span id="cb2-44"><a href="#cb2-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-45"><a href="#cb2-45" aria-hidden="true" tabindex="-1"></a>      <span class="kw">def</span> d2b(theta):</span>
<span id="cb2-46"><a href="#cb2-46" aria-hidden="true" tabindex="-1"></a>          <span class="co">""" Differential of the activation function """</span></span>
<span id="cb2-47"><a href="#cb2-47" aria-hidden="true" tabindex="-1"></a>          <span class="cf">return</span> np.exp(theta)<span class="op">/</span>(<span class="dv">1</span> <span class="op">+</span> np.exp(theta))<span class="op">**</span><span class="dv">2</span></span>
<span id="cb2-48"><a href="#cb2-48" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-49"><a href="#cb2-49" aria-hidden="true" tabindex="-1"></a>      <span class="kw">def</span> dispersion(y, theta):</span>
<span id="cb2-50"><a href="#cb2-50" aria-hidden="true" tabindex="-1"></a>          <span class="co">""" Not needed - one parameter distribution """</span></span>
<span id="cb2-51"><a href="#cb2-51" aria-hidden="true" tabindex="-1"></a>          <span class="cf">return</span> <span class="dv">1</span></span>
<span id="cb2-52"><a href="#cb2-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-53"><a href="#cb2-53" aria-hidden="true" tabindex="-1"></a>      <span class="kw">def</span> a(dispersion):</span>
<span id="cb2-54"><a href="#cb2-54" aria-hidden="true" tabindex="-1"></a>          <span class="co">""" Not needed - one parameter distribution """</span></span>
<span id="cb2-55"><a href="#cb2-55" aria-hidden="true" tabindex="-1"></a>          <span class="cf">return</span> <span class="dv">1</span></span>
<span id="cb2-56"><a href="#cb2-56" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-57"><a href="#cb2-57" aria-hidden="true" tabindex="-1"></a>      <span class="kw">def</span> c(y, dispersion):</span>
<span id="cb2-58"><a href="#cb2-58" aria-hidden="true" tabindex="-1"></a>          <span class="co">""" No adjustment needed (pdf already sums to one) """</span></span>
<span id="cb2-59"><a href="#cb2-59" aria-hidden="true" tabindex="-1"></a>          <span class="cf">return</span> <span class="dv">0</span></span>
<span id="cb2-60"><a href="#cb2-60" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-61"><a href="#cb2-61" aria-hidden="true" tabindex="-1"></a>      <span class="bu">super</span>().<span class="fu">__init__</span>(y,X,b,db,d2b,dispersion,a,c) </span>
<span id="cb2-62"><a href="#cb2-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-63"><a href="#cb2-63" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> poisson_family(base_canonical_exponential_dispersion_family):</span>
<span id="cb2-64"><a href="#cb2-64" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-65"><a href="#cb2-65" aria-hidden="true" tabindex="-1"></a>  <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>,y,X,seed <span class="op">=</span> <span class="dv">0</span>):</span>
<span id="cb2-66"><a href="#cb2-66" aria-hidden="true" tabindex="-1"></a>      </span>
<span id="cb2-67"><a href="#cb2-67" aria-hidden="true" tabindex="-1"></a>      <span class="kw">def</span> b(theta):</span>
<span id="cb2-68"><a href="#cb2-68" aria-hidden="true" tabindex="-1"></a>          <span class="co">""" Integral of the activation function """</span></span>
<span id="cb2-69"><a href="#cb2-69" aria-hidden="true" tabindex="-1"></a>          <span class="cf">return</span> np.exp(theta)</span>
<span id="cb2-70"><a href="#cb2-70" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-71"><a href="#cb2-71" aria-hidden="true" tabindex="-1"></a>      <span class="kw">def</span> db(theta):</span>
<span id="cb2-72"><a href="#cb2-72" aria-hidden="true" tabindex="-1"></a>          <span class="co">""" Activation function is exponential (inverse of the log-link function):"""</span></span>
<span id="cb2-73"><a href="#cb2-73" aria-hidden="true" tabindex="-1"></a>          <span class="cf">return</span> np.exp(theta)</span>
<span id="cb2-74"><a href="#cb2-74" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-75"><a href="#cb2-75" aria-hidden="true" tabindex="-1"></a>      <span class="kw">def</span> d2b(theta):</span>
<span id="cb2-76"><a href="#cb2-76" aria-hidden="true" tabindex="-1"></a>          <span class="co">""" Differential of the activation function """</span></span>
<span id="cb2-77"><a href="#cb2-77" aria-hidden="true" tabindex="-1"></a>          <span class="cf">return</span> np.exp(theta)</span>
<span id="cb2-78"><a href="#cb2-78" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-79"><a href="#cb2-79" aria-hidden="true" tabindex="-1"></a>      <span class="kw">def</span> dispersion(y, theta):</span>
<span id="cb2-80"><a href="#cb2-80" aria-hidden="true" tabindex="-1"></a>          <span class="co">""" Not needed - one parameter distribution """</span></span>
<span id="cb2-81"><a href="#cb2-81" aria-hidden="true" tabindex="-1"></a>          <span class="cf">return</span> <span class="dv">1</span></span>
<span id="cb2-82"><a href="#cb2-82" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-83"><a href="#cb2-83" aria-hidden="true" tabindex="-1"></a>      <span class="kw">def</span> a(dispersion):</span>
<span id="cb2-84"><a href="#cb2-84" aria-hidden="true" tabindex="-1"></a>          <span class="co">""" Not needed - one parameter distribution """</span></span>
<span id="cb2-85"><a href="#cb2-85" aria-hidden="true" tabindex="-1"></a>          <span class="cf">return</span> <span class="dv">1</span></span>
<span id="cb2-86"><a href="#cb2-86" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-87"><a href="#cb2-87" aria-hidden="true" tabindex="-1"></a>      <span class="kw">def</span> c(y, dispersion):</span>
<span id="cb2-88"><a href="#cb2-88" aria-hidden="true" tabindex="-1"></a>          <span class="co">""" Adjustment needed so pdf sums to one """</span></span>
<span id="cb2-89"><a href="#cb2-89" aria-hidden="true" tabindex="-1"></a>          <span class="cf">return</span> np.vectorize(<span class="op">-</span>np.log(np.math.factorial(y)))</span>
<span id="cb2-90"><a href="#cb2-90" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-91"><a href="#cb2-91" aria-hidden="true" tabindex="-1"></a>      <span class="bu">super</span>().<span class="fu">__init__</span>(y,X,b,db,d2b,dispersion,a,c)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>


</section>
</section>


<div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>Showing how <span class="math inline">\(\ln[1-p] = -\ln[1+e^{\theta}]\)</span> <span class="math display">\[
\displaylines{
\begin{align}
\theta &amp; = \ln{\left[\frac{p}{1-p} \right]}
\\ \text{ (1) Put } p \text{ in terms of } \theta \text{:}
\\ \therefore e^{\theta} &amp; = \frac{p}{1-p} &amp; \text{raise by }e
\\ \therefore e^{\theta}-pe^{\theta} &amp; = p &amp; \times (1-p)
\\ \therefore e^{\theta} &amp; = p(1+e^\theta) &amp; + pe^\theta
\\ \therefore p &amp; = \frac{e^{\theta}}{1+e^\theta} &amp; \div (1+e^\theta)
\\\\
\\ \text{ (2) Substitute in } p = \frac{e^{\theta}}{1+e^\theta}
\\
\Rightarrow \ln[1-p]
&amp; = \ln\left[1-\frac{e^{\theta}}{1+e^\theta} \right]
\\ &amp; \equiv \ln\left[\frac{1+e^\theta}{1+e^\theta}-\frac{e^{\theta}}{1+e^\theta} \right]
\\ &amp; = \ln\left[\frac{1}{1+e^\theta} \right]
\\ &amp; \equiv \ln\left[(1+e^\theta \right)^{-1}]
\\ &amp; \equiv -\ln\left[1+e^\theta \right]
\end{align}
}
\]</span><a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>