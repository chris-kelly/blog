{
  "hash": "b77c48ad5ec7eee42c022487f609198a",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Deriving OLS coefficients (multivariate)\"\nauthor: \"Chris Kelly\"\ndate: '02-20-24'\ncategories: [Linear Models, OLS]\nformat:\n  html:\n    code-fold: true\n    toc: true\n    # include-in-header: \n    #   text: <script src=\"https://cdn.jsdelivr.net/npm/mathjs@13.0.0/lib/browser/math.min.js\"></script>\nimage: '../images/ols_coefs_multivariate.jpeg'\n---\n\n<div id=\"tester\"\"></div>\n<button onclick=\"javascript:randomize();\">Randomize!</button>\n<input type=\"range\" min=\"1\" max=\"100\" value=\"50\" class=\"slider\" id=\"myRange1\">\n<input type=\"range\" min=\"1\" max=\"100\" value=\"50\" class=\"slider\" id=\"myRange2\">\n\n<script>\n  require([\"plotly\"], function(Plotly) {\n    window.PLOTLYENV = window.PLOTLYENV || {};\n    Plotly.newPlot(\n      \"tester\",\n      [{\n          x: [1, 2, 3],\n          y: [0, 0.5, 1],\n          line: {simplify: false},\n        }]\n      );                \n    } );\n</script>\n\n<script>\n  function randomize() {\n    require([\"plotly\"], function(Plotly) {\n    window.PLOTLYENV = window.PLOTLYENV || {};\n    Plotly.animate('tester', {\n      data: [{y: [Math.random(), Math.random(), Math.random()]}],\n      traces: [0],\n      layout: {}\n        }, { \n        transition: { duration: 500, easing: 'cubic-in-out' },\n        frame: { duration: 500 }\n        }\n      )\n    });\n  }\n</script>\n\n<script src=\"https://cdn.jsdelivr.net/npm/mathjs@13.0.0/lib/browser/math.min.js\"></script>\n\n<!-- <script>\n  math.ones(2);\n</script> -->\n\n::: {.callout-tip}\n## What we are exploring\nDeriving a unique, analytical solution to the set of coefficients that minimize the sum of squared residuals.\n:::\n\n## Summary\n\nThe cost function for OLS is the sum of squared residuals, $\\hat{\\epsilon}^{\\intercal}\\hat{\\epsilon}$. In order to fit a good linear model, we want to find optimum values for the estimated vector of coefficients, $\\hat{\\beta}^*$, that minimizes this cost function.\n\nFirst we do partial differentiation of the cost function with respect to the coefficients. Finding the coefficient values where the partial differential is equal to zero reveals the stationary points of the cost function. For OLS in particular, we can find a unique solution for the choice of coefficients that can be found analytically. The hessian matrix then further proves that this is a global minima.\n\n## Deriving the optimum coefficients\n\n### 0. Defining the notation\n\nFor a sample $i$, we observe an outcome $y_i$. $y$ is a vector of all $n$ observed outcomes.\n\n$$\n\\underset{n \\times 1} {y} = \n\\begin{bmatrix} \n    y_1 \\\\ y_2 \\\\ \\vdots \\\\ y_{n-1} \\\\ y_n\n\\end{bmatrix} \n$$\n\nWe also observe $k$ features for every sample $i$. $X$ is a matrix of these observed features. Note the first column is usually all ones, to include an intercept to optimize (or \"bias\" term).\n\n$$\n\\underset{n \\times k} {X} = \n\\begin{bmatrix}\n  1 & x_{11} & \\cdots & x_{1,k-1} & x_{1,k} \\\\ \n  1 & x_{21} & \\cdots & x_{2,k-1} & x_{2,k} \\\\ \n  1 & x_{31} & \\cdots & x_{3,k-1} & x_{3,k} \\\\ \n  \\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\\n  1 & x_{n-2,1} & \\cdots & x_{n-2,k-1} & x_{n-2,k} \\\\ \n  1 & x_{n-1,1} & \\cdots & x_{n-1,k-1} & x_{n-1,k} \\\\ \n  1 & x_{n,1} & \\cdots & x_{n,k-1} & x_{n,k}\n\\end{bmatrix}\n$$\n\nThe contribution of each feature to the prediction is estimated by the coefficients $\\hat{\\beta}$.\n\n$$\n\\underset{k \\times 1} {\\hat{\\beta}} =\n\\begin{bmatrix} \n  \\beta_0 \\\\\n  \\beta_1 \\\\\n  \\vdots \\\\\n  \\beta_{k-1} \\\\\n  \\beta_{k}\n\\end{bmatrix}\n$$\n\nWe make predictions, $\\hat{y}$, by calculating the dot product of the features $X$ and the coefficients $\\hat{\\beta}$.\n\n$$\n\\hat{y} = X \\hat{\\beta}\n$$\n\nwhich is shorthand for this:\n\n$$\n\\displaylines{\n\\begin{align}\n\\begin{bmatrix} \n  \\hat{y}_1 \\\\ \\hat{y}_2 \\\\ \\hat{y}_3 \\\\ \\vdots \\\\ \\hat{y}_{n-2} \\\\ \\hat{y}_{n-1} \\\\ \\hat{y}_n\n\\end{bmatrix} & =\n\\begin{bmatrix}\n  1 & x_{11} & \\cdots & x_{1,k-1} & x_{1,k} \\\\ \n  1 & x_{21} & \\cdots & x_{2,k-1} & x_{2,k} \\\\ \n  1 & x_{31} & \\cdots & x_{3,k-1} & x_{3,k} \\\\ \n  \\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\\n  1 & x_{n-2,1} & \\cdots & x_{n-2,k-1} & x_{n-2,k} \\\\ \n  1 & x_{n-1,1} & \\cdots & x_{n-1,k-1} & x_{n-1,k} \\\\ \n  1 & x_{n,1} & \\cdots & x_{n,k-1} & x_{n,k}\n\\end{bmatrix}\n\\begin{bmatrix} \n  \\hat{\\beta}_0 \\\\\n  \\hat{\\beta}_1 \\\\\n  \\vdots \\\\\n  \\hat{\\beta}_{k-1} \\\\\n  \\hat{\\beta}_{k}\n\\end{bmatrix}\n\\\\ \\\\ & = \n\\begin{bmatrix} \n  \\hat{\\beta}_0 + \\hat{\\beta}_{1}x_{1,1} + \\cdots + \\hat{\\beta}_{k-1}x_{1,k-1} + \\hat{\\beta}_{k}x_{1,k} \\\\\n  \\hat{\\beta}_0 + \\hat{\\beta}_{1}x_{2,1} + \\cdots + \\hat{\\beta}_{k-1}x_{2,k-1} + \\hat{\\beta}_{k}x_{2,k} \\\\\n  \\hat{\\beta}_0 + \\hat{\\beta}_{1}x_{3,1} + \\cdots + \\hat{\\beta}_{k-1}x_{3,k-1} + \\hat{\\beta}_{k}x_{3,k} \\\\\n  \\vdots \\\\\n  \\hat{\\beta}_0 + \\hat{\\beta}_{1}x_{n-2,1} + \\cdots + \\hat{\\beta}_{k-1}x_{2,k-1} + \\hat{\\beta}_{k}x_{n-2,k} \\\\\n  \\hat{\\beta}_0 + \\hat{\\beta}_{1}x_{n-1,1} + \\cdots + \\hat{\\beta}_{k-1}x_{2,k-1} + \\hat{\\beta}_{k}x_{n-1,k} \\\\\n  \\hat{\\beta}_0 + \\hat{\\beta}_{1}x_{n,1} + \\cdots + \\hat{\\beta}_{k-1}x_{2,k-1} + \\hat{\\beta}_{k}x_{n,k} \\\\\n\\end{bmatrix}\n\\end{align}\n}\n$$\n\nThe residual is the difference between the true outcome and the model prediction.\n\n$$\n\\hat{\\epsilon} = y_i -\\hat{y}_i\n$$\n\nwhich is shorthand for this:\n\n$$\n\\begin{bmatrix}\n  \\hat{\\epsilon_1} \\\\\n  \\hat{\\epsilon_2} \\\\\n  \\hat{\\epsilon_3} \\\\\n  \\vdots \\\\\n  \\hat{\\epsilon_{n-2}} \\\\\n  \\hat{\\epsilon_{n-1}} \\\\\n  \\hat{\\epsilon_{n}}\n\\end{bmatrix} =\n\\begin{bmatrix} \n    y_1 \\\\ y_2 \\\\ y_3 \\\\ \\vdots \\\\ y_{n-2} \\\\ y_{n-1} \\\\ y_n\n\\end{bmatrix} -\n\\begin{bmatrix} \n  \\hat{y}_1 \\\\ \\hat{y}_2 \\\\ \\hat{y}_3 \\\\ \\vdots \\\\ \\hat{y}_{n-2} \\\\ \\hat{y}_{n-1} \\\\ \\hat{y}_n\n\\end{bmatrix} \n$$\n\nOur aim is to find the optimum vector of coefficients, $\\hat{\\beta}^*$, that minimizes the sum of squared residuals:\n\n$$\n\\min_{\\beta} \\left( \\epsilon^{\\intercal}\\epsilon \\right)\n$$\n\n\n### 1. Expand the sum of squared residuals\n\nThe first step involves expanding the sum of squared residuals, and substituting in $X \\hat{\\beta}$ for $\\hat{y}$.\n$$\n\\displaylines{\n\\begin{align}\n\\sum_{i=1}^n{\\hat{\\epsilon}_i^2} & = \\hat{\\epsilon}^{\\intercal}\\hat{\\epsilon} \n\\\\& =(y-X\\hat{\\beta})^{\\intercal}(y-X\\hat{\\beta}) \n\\\\& = y^{\\intercal}y - y^{\\intercal}X\\hat{\\beta}-\n\\hat{\\beta}^{\\intercal} X^{\\intercal}y+\n\\hat{\\beta}^{\\intercal}X^{\\intercal}X\\hat{\\beta} \n\\\\& = y^{\\intercal}y - 2y^{\\intercal}X\\hat{\\beta}\n+\\hat{\\beta}^{\\intercal}X^{\\intercal}X\\hat{\\beta}\n\\end{align}\n}\n$$\n\n::: {.column-margin}\nNote we can simply add the two middle terms, since are both scalars:\n\n$$\n\\displaylines{\ny^{\\intercal}X\\hat{\\beta} =\n\\hat{\\beta}^{\\intercal} X^{\\intercal}y \\\\\n\\because \\underset{1 \\times n}{y^{\\intercal}} \\times \n\\underset{n \\times k}{X} \\times \n\\hat{\\underset{k \\times 1}{\\beta}}\n = \n\\hat{\\underset{1 \\times k}{\\beta}^{\\intercal}} \\times\n\\underset{k \\times n}{X^{\\intercal}} \\times\n\\underset{n \\times 1}{y} \n}\n$$\n\n:::\n\n### 2. Partially differentiate RSS with respect to beta\n\nThe second step involves partially differentiating the cost function with respect to its parameters, to understand how it changes as the coefficients vary.\n\n$$\n\\displaylines{\n\\begin{align}\n\\frac{\\partial}{\\partial\\hat{\\beta}}\\hat{\\epsilon}^{\\intercal}\\hat{\\epsilon} & \\equiv\n\\begin{bmatrix}\n    \\frac{\\partial}{\\partial\\hat{\\beta}_1}\\hat{\\epsilon}^{\\intercal}\\hat{\\epsilon} \\\\\n    \\frac{\\partial}{\\partial\\hat{\\beta}_2}\\hat{\\epsilon}^{\\intercal}\\hat{\\epsilon} \\\\\n    \\vdots \\\\\n    \\frac{\\partial}{\\partial\\hat{\\beta}_k}\\hat{\\epsilon}^{\\intercal}\\hat{\\epsilon}\n\\end{bmatrix} \n\\\\ & = \\frac{d}{d\\hat{\\beta}}(\ny^{\\intercal}y - 2y^{\\intercal}X\\hat{\\beta}\n+\\hat{\\beta}^{\\intercal}X^{\\intercal}X\\hat{\\beta}) \\\\ & = 0 - 2X^{\\intercal}y +((X^{\\intercal}X)\\hat{\\beta} + (X^{\\intercal}X)^{\\intercal}\\hat{\\beta})\n\\\\ & = -2X^{\\intercal}y +  2((X^{\\intercal}X)\\hat{\\beta})\n\\end{align}\n}\n$$\n\n::: {.column-margin}\nTwo matrix differentiation rules used here for reference:\n\n$$\n\\displaylines{\n\\frac{\\partial}{dx}(Ax) = A^{\\intercal}x \\\\ \n\\frac{\\partial}{dx}(xAx) = Ax + A^{\\intercal}x\n}\n$$\n\nAnd note $X^{\\intercal}X = (X^{\\intercal}X)^{\\intercal}$ by definition, so we can add the two last terms.\n:::\n\n### 3. Find the coefficient values at the stationary point\n\nNow we find the choices of $\\beta$ where the partial differential is equal to zero. These stationary points for the cost function are either at its maximum or minimum.\n\nFor OLS - we actually only find one unique solution!\n\n$$\n\\displaylines{\n\\begin{align}\n\\cancel{2}X^{\\intercal}y +  \\cancel{2}((X^{\\intercal}X)\\hat{\\beta}) & = 0 \\\\\n\\therefore (X^{\\intercal}X)\\hat{\\beta} & = X^{\\intercal}y \\\\\n\\therefore \\hat{\\beta} & = (X^{\\intercal}X)^{-1}X^{\\intercal}y\n\\end{align}\n}\n$$\n\n::: {.column-margin}\nNote the need to invert $X^{\\intercal}X$. This is only possible for a full rank matrix.\n:::\n\nThe first term is the (inverse) variance matrix of $X$. This term normalizes the coefficient with respect to the magnitude of $X$.\n\nThe second term is the covariance matrix between $X$ and $y$. This incorporates the linear relationship between the two in the coefficient.\n\nHence, the coefficient can be interpreted as the estimated change in $y$ given a one unit change in $X$.\n\n### 4. Check the stationary point is a global minimum (hessian matrix)\n\nFinally, we derive the **hessian matrix**, by double-differentiating the cost function with respect to the coefficients:\n\n$$\n\\displaylines{\n\\frac{\\partial^2}{\\partial\\hat{\\beta}\\partial\\hat{\\beta}^{\\intercal}}\n\\left(\n  \\hat{\\epsilon}^{\\intercal}\\hat{\\epsilon} \n\\right)\n\\\\ = \\frac{\\partial}{\\partial \\hat{\\beta}^{\\intercal}} \\left( -2X^{\\intercal}y +  2((X^{\\intercal}X)\\hat{\\beta}) \\right)\n\\\\ = 2(X^{\\intercal}X)^{\\intercal}\n}\n$$\n\nSince $X^{\\intercal}X$ is clearly positive definite, the cost function is convex. Thus, we know our unique solution for $\\beta$ where the partial differential is at zero is indeed a **global minimum** for the cost function.\n\n### 5. Coding this up in python\n\n::: {#001e19f5 .cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\nfrom scipy.linalg import qr, solve_triangular\n\ndef quick_matrix_invert(X: np.ndarray) -> np.ndarray:\n    \"\"\" Find the inverse of a matrix, using QR factorization \"\"\"\n    Q, R = qr(X)\n    X_inv = solve_triangular(R, np.identity(X.shape[1])).dot(Q.transpose())\n    return X_inv\n\ndef derive_coefs(y, X) -> np.ndarray:\n    \"\"\" This is a function to calculate coefficients \"\"\"\n    XTX = X.T.dot(X)\n    XTY = X.T.dot(y)\n    XTX_inv = quick_matrix_invert(XTX)\n    beta = XTX_inv.dot(XTY)\n    return beta\n\ndef predict(X,coefs)-> np.ndarray:\n    \"\"\" This is a function to calculate the predictions \"\"\"\n    return X.dot(coefs)\n\ndef RSS(y,X,coefs)-> np.ndarray:\n    \"\"\" This is a function to calculate the RSS \"\"\"\n    residual = y - predict(X,coefs)\n    return residual.T.dot(residual)\n\n# Example usage\n\nnp.random.seed(1)\nn, k = 4, 2\nsigma_sq = 1\n\nX = np.hstack([ \n  np.ones(n).reshape(n,1),\n  np.random.normal(size=(n,k-1)) \n])\nbeta = np.random.normal(size=(k,1))\ny = X.dot(beta) + np.random.normal(loc=0,scale=sigma_sq,size=(n,1))\n```\n:::\n\n\n::: {#e6212e1c .cell execution_count=2}\n``` {.python .cell-code}\nimport pandas as pd\nstep_size = 25\ncoefs = list()\nidx = np.arange(step_size,y.shape[0]+1,step_size).astype(int)\nfor i in idx:\n    ty = y[:int(i)]\n    tX = X[:int(i),:]\n    tcoefs = derive_coefs(ty,tX)\n    tyhat = predict(tX,tcoefs)\n    tRSS = RSS(ty,tX,tcoefs)\n    coefs.append(np.append(tcoefs,tRSS))\nresults = pd.DataFrame(\n  coefs, columns=['β0','β1','RSS']\n).set_index(idx).rename_axis('n')\nresults['RMSE'] = np.sqrt(results['RSS']/results.index)\nresults[['β0','β1','RMSE']]\n```\n\n::: {.cell-output .cell-output-display execution_count=44}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>β0</th>\n      <th>β1</th>\n      <th>RMSE</th>\n    </tr>\n    <tr>\n      <th>n</th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n::: {#6429e851 .cell execution_count=3}\n``` {.python .cell-code}\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nfrom scipy.stats import norm\n\nX = X[np.argsort(X[:,1]),:]\ncoefs = derive_coefs(y,X)\ny_hat = predict(X,coefs)\n\nfig = make_subplots(\n  rows=2,cols=1,\n  subplot_titles=(\n    \"Plot of y against X\",\n    \"Plot of RSS against choice of β0 and β1\",\n    ),\n  specs=[ [dict(type='scatter',l=0.2,r=0.2)],[dict(type='scatter3d')] ],\n    # [{\"type\": \"scatter\", \"l\": 0.2, \"r\": 0.2, \"b\":0}],\n    # [{\"type\": \"scatter3d\", \"b\": 0.1, \"t\": 0}]\n  # ],\n  row_heights = [3,5],\n  horizontal_spacing = 0,\n)\n\nfig.add_trace(\n  go.Scatter(\n    x=X[:,1],\n    y=y.flatten(), \n    mode='markers', name='Observations',\n    marker=dict(size=2, color=\"blue\"),\n  ),\n)\nfig.add_trace(\n  go.Scatter(\n    x=X[:,1],\n    y=y_hat.flatten(),\n    mode='lines', name='Model',\n    line=dict(color=\"red\"),\n  ),\n)\n\nfig.update_xaxes(title=dict(text=\"X\", font_size=16),row=1, col=1)\nfig.update_yaxes(title=dict(text=\"y\", font_size=16),row=1, col=1)\n\n# a, b = np.arange(-1,1,0.05), np.arange(-1,1,0.05)\n# av, bv = np.meshgrid(a, b)\n# all_coefs = np.dstack([av,bv])\n# cv = np.zeros(shape=av.shape)\n\n# for idx in range(len(all_coefs)):\n#   for idy in range(len(all_coefs[idx])):\n#       cv[idx,idy] = RSS(y,X,all_coefs[idx,idy].reshape(k,1))[0]\n\n# fig.add_trace(\n#   go.Surface(\n#     x=av, y=bv, z=cv, \n#     opacity=0.2,\n#     showscale=False,\n#     contours = dict(z=dict(show=True,size=50,start=0,end=2500))\n#   ),\n#   row=2, col=1\n# )\n\n# 3d plot of data\nfig.add_trace(\n  go.Scatter3d(\n    x=coefs[0], \n    y=coefs[1],\n    z=RSS(y,X,coefs)[0],\n    mode='markers', name='Optimum choice of coefficients',\n    marker=dict(symbol='cross'),\n  ), row=2, col=1,\n)\n\nfig.update_layout(\n    scene = dict(\n      aspectratio=dict(x=1.5,y=1.8,z=1),\n      xaxis_title=\"β0\",\n      yaxis_title=\"β1\",\n      zaxis_title=\"RSS\",\n    ),\n  showlegend=False,\n)\n\nfig.show()\n```\n\n::: {.cell-output .cell-output-display}\n```{=html}\n<div>                            <div id=\"73978f5c-1803-42f1-8b09-384f2d4571f0\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"73978f5c-1803-42f1-8b09-384f2d4571f0\")) {                    Plotly.newPlot(                        \"73978f5c-1803-42f1-8b09-384f2d4571f0\",                        [{\"marker\":{\"color\":\"blue\",\"size\":2},\"mode\":\"markers\",\"name\":\"Observations\",\"x\":[-1.0729686221561705,-0.6117564136500754,-0.5281717522634557,1.6243453636632417],\"y\":[-1.1282743180278676,1.5121817875099255,2.400054451815186,3.0855160582780137],\"type\":\"scatter\"},{\"line\":{\"color\":\"red\"},\"mode\":\"lines\",\"name\":\"Model\",\"x\":[-1.0729686221561705,-0.6117564136500754,-0.5281717522634557,1.6243453636632417],\"y\":[0.4273991411959158,0.9454711703356442,1.0393604350404637,3.457247233003234],\"type\":\"scatter\"},{\"marker\":{\"symbol\":\"cross\"},\"mode\":\"markers\",\"name\":\"Optimum choice of coefficients\",\"x\":[1.6326470098163746],\"y\":[1.1232834248204464],\"z\":[4.730953108900469],\"type\":\"scatter3d\",\"scene\":\"scene\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"},\"margin\":{\"b\":0,\"l\":0,\"r\":0,\"t\":30}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.2,0.8],\"title\":{\"font\":{\"size\":16},\"text\":\"X\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.71875,1.0],\"title\":{\"font\":{\"size\":16},\"text\":\"y\"}},\"scene\":{\"domain\":{\"x\":[0.0,1.0],\"y\":[0.0,0.46875]},\"aspectratio\":{\"x\":1.5,\"y\":1.8,\"z\":1},\"xaxis\":{\"title\":{\"text\":\"\\u03b20\"}},\"yaxis\":{\"title\":{\"text\":\"\\u03b21\"}},\"zaxis\":{\"title\":{\"text\":\"RSS\"}}},\"annotations\":[{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"Plot of y against X\",\"x\":0.5,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":1.0,\"yanchor\":\"bottom\",\"yref\":\"paper\"},{\"font\":{\"size\":16},\"showarrow\":false,\"text\":\"Plot of RSS against choice of \\u03b20 and \\u03b21\",\"x\":0.5,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":0.46875,\"yanchor\":\"bottom\",\"yref\":\"paper\"}],\"showlegend\":false},                        {\"responsive\": true}                    ).then(function(){\n                            \nvar gd = document.getElementById('73978f5c-1803-42f1-8b09-384f2d4571f0');\nvar x = new MutationObserver(function (mutations, observer) {{\n        var display = window.getComputedStyle(gd).display;\n        if (!display || display === 'none') {{\n            console.log([gd, 'removed!']);\n            Plotly.purge(gd);\n            observer.disconnect();\n        }}\n}});\n\n// Listen for the removal of the full notebook cells\nvar notebookContainer = gd.closest('#notebook-container');\nif (notebookContainer) {{\n    x.observe(notebookContainer, {childList: true});\n}}\n\n// Listen for the clearing of the current output cell\nvar outputEl = gd.closest('.output');\nif (outputEl) {{\n    x.observe(outputEl, {childList: true});\n}}\n\n                        })                };                });            </script>        </div>\n```\n:::\n:::\n\n\n## Final reflections\n\nUnlike logistic regression, or the multiple hidden-layer structure of neural networks, we can \"jump\" straight to the optimum coefficients for OLS. Why can we do this? Well chiefly its because OLS is a bit of a special case:\n\n* **The minima is a global minima:** The hessian matrix is positive definite, and hence the cost function is strictly convex. This means we know that when a choice of coefficients is found that ensure the partially differentiated cost function is equal zero, this minima is also a global one, not a local one.\n* **There is only one solution for the optimum coefficient:** We assume that the matrix is full rank (every feature provides additional predictive power) and that the number of predictors is smaller than the number of obervations. This also means that partially differentiating is okay to do!\n* **A closed-form solution can be found** The predictions are generated from $X$ using a simple, purely algebraic function, i.e. the sum-product of $X$ by $\\beta$. This means we can find an analytical solution to the optimal choice $\\beta^*$. Note this often isn't possible since non-linear activation functions (i.e. link functions) are often transcendental.\n\nWe will dive into this in another post.\n\nFin.\n\n",
    "supporting": [
      "ols_coefs_multivariate_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n<script type=\"text/javascript\">\nwindow.PlotlyConfig = {MathJaxConfig: 'local'};\nif (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\nif (typeof require !== 'undefined') {\nrequire.undef(\"plotly\");\nrequirejs.config({\n    paths: {\n        'plotly': ['https://cdn.plot.ly/plotly-2.27.0.min']\n    }\n});\nrequire(['plotly'], function(Plotly) {\n    window._Plotly = Plotly;\n});\n}\n</script>\n\n"
      ]
    }
  }
}