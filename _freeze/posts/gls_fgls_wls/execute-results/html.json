{
  "hash": "9cd926961078832f6740e89a8cd37320",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Generalized Least Squares\"\nauthor: \"Chris Kelly\"\ndate: '02-21-24'\nimage: '../images/gls_fgls_wls.jpeg'\ncategories: [Linear Models, Generalized Least Squares]\nformat:\n  html:\n    code-fold: true\n    toc: true\n    code-links:\n      - text: OLS class\n        icon: file-code\n        href: ../src/ols_blue.py\n      - text: Sandwich class\n        icon: file-code\n        href: ../src/ols_sandwich_estimators.py\n      - text: GLS class\n        icon: file-code\n        href: ../src/gls_fgls_wls.py\ndraft: true\n---\n\n::: {.callout-tip}\n### What are we exploring?\nApplying a weight matrix to correct for non-homoskedastic error variance can be more efficient than OLS with sandwich errors, aka GLS.\n:::\n\n## Introduction\n\nAs seen when exploring [sandwich estimators](ols_sandwich_estimators.html), the assumption of homoskedasticity is often violated in real-world data. This can lead to inefficient estimates and incorrect inference too. \n\nSandwich estimators correct for the variance by adjusting the standard error after OLS estimation. However, another approach is to correct for the variance before estimation by applying a weight matrix to the data before fitting. This is known as Generalized Least Squares (GLS). Weighted least squares (WLS) is a special case of GLS.\n\n## The Motivation\n\nFirst - let’s state that:\n\n* $E(\\epsilon|X)=0$\n* $V(\\epsilon \\epsilon^{\\intercal}|X)=\\sigma^2\\underset{n \\times n}{\\Omega}$\n\nThis means that although the assumption of endogeneity is not violated, the assumption of homoskedasticity is. And more specifically, that the variance of the error term can be decomposed between into the constant variance $\\sigma^2$ and a covariance matrix $\\Omega$.\n\nNow according to Cholesky decomposition, if $\\Omega$ is symmetric positive definite, then there exists a lower triangular matrix $\\mathrm{P}$ such that:\n\n$$\n\\displaylines{\n\\Omega=(\\mathrm{P}^{\\intercal}\\mathrm{P})^{-1} = \\mathrm{P}^{-1}(\\mathrm{P}^{\\intercal})^{-1} \\\\\n\\therefore \\mathrm{P} \\Omega \\mathrm{P}^{\\intercal} = I\n}\n$$\n\nSo, if we transform all variables by $\\mathrm{P}$:\n\n* We get the following form: $\\mathrm{P} y = \\mathrm{P} X\\beta + \\mathrm{P} \\epsilon$\n* Then the expected error is still zero (i.e. consistency): $E[\\mathrm{P} \\epsilon] = \\mathrm{P} E[\\epsilon] = 0$\n* But the variance is now homoskedastic: $V[\\mathrm{P} \\epsilon] = \\mathrm{P} V[\\epsilon] \\mathrm{P}^{\\intercal} = \\sigma^2 \\mathrm{P} \\Omega \\mathrm{P}^{\\intercal} = \\sigma^2 I$\n\n:::{.column-margin}\nIt might be apparent now that Weighted Least Squares (WLS) is a special case of GLS, where $\\mathrm{P}$ is an error covariance matrix has zero off-digonal elements. \n:::\n\nThis is the motivation behind GLS. We can transform the data by $\\mathrm{P}$ to make the error variance homoskedastic, and then apply OLS to the transformed data.\n\nIn fact, we can use this to jump straight to the solution for the GLS estimator!\n$$\n\\displaylines{\n\\beta_{GLS} = \n[(\\mathrm{P} X)^{\\intercal}(\\mathrm{P} X)]^{-1}[(\\mathrm{P} X)^{\\intercal}(\\mathrm{P} y)] \\\\\n= \n[X^{\\intercal}\\mathrm{P}^{\\intercal}\\mathrm{P} X]^{-1}\n[X^{\\intercal}(\\mathrm{P}^{\\intercal}\\mathrm{P} y)] \\\\\n= [X^{\\intercal}\\Omega^{-1} X]^{-1}X^{\\intercal}(\\Omega^{-1} y)\n}\n$$\n\nHowever, this isn't feasible unless we know what $\\mathrm{P}$ is! We usually have to estimate this.\n\n## Feasible Generalized Least Squares\n\nThe feasible GLS estimator is a two-step process:\n\n* Run a normal OLS regression. Estimate the error covariance matrix, $\\Omega$, using the residuals from the OLS regression.\n* Estimate $\\mathrm{P}$ using the Cholesky decomposition of $\\Omega$, and transform $y$ and $X$ by $\\mathrm{P}$\n\nWhilst GLS is more efficient, FGLS is only *asymptotically* more efficient, where the error covariance matrix is consistently estimated. In fact, for a small sample size, FGLS can be actually less efficient than OLS - and often it is even biased! It is only for large samples that FGLS would be preferred, as it is consistent. \n\nThus some authors prefer OLS, and use a sandwich estimator instead. Finally - its worth noting that we can still apply the sandwich estimator to the FGLS coefficients as well.\n\n## Coding it up from scratch\n\nWe can inherit from the OLS class, as this does a lot of the leg work! We need to make two changes though:\n\n* Add an extra method to estimate the covariance matrix and apply the Cholesky decomposition. \n* Change the `_estimate_ls_coefs` method to utilise this covariance matrix.\n\nFirst we take the sandwich parent class we defined previously in the [sandwich estimator post](ols_sandwich_estimators.qmd.html) (note this also inherits from the OLS class - see that post [here](ols_blue.html)):\n\n<details>\n<summary>Sandwich class (source code):</summary>\n```{.python include='../src/ols_blue.py' }\n```\n</details>\n\n\n\nNow we can define a child GLS class, which utilises the parent classes in two key ways:\n\n* Uses the error covariance estimation methods from the sandwich class to estimate $\\Omega$\n* Uses the OLS class functionality to estimate the coefficients (and then other methods from OLS and sandwich for the standard errors etc)\n\n::: {#e3608ef1 .cell execution_count=2}\n``` {.python .cell-code}\nclass LS(sandwich):\n\n    def __init__(\n        self, \n        y: Optional[np.ndarray] = None, \n        X: Optional[np.ndarray] = None,\n        omega: Optional[np.ndarray] = None\n        ) -> None:\n        \"\"\"Initializes the LS class to run an least-squares regression\"\"\"\n        super().__init__(y, X)\n        self.omega = omega\n        self.P = None\n\n    def _estimate_gls_coefs(self, y: np.ndarray, X: np.ndarray, omega: np.ndarray):\n        \"\"\"Estimates the GLS coefficients given a vector y and matrix X\"\"\"\n        try:\n            P = np.linalg.cholesky(omega)\n            PX = P.dot(X)\n            Py = P.dot(y)\n            coefs, XTOX_inv = self._estimate_ols_coefs(Py,PX)\n        except:\n            omega_inv = np.linalg.inv(omega)\n            XTO = X.T.dot(omega_inv)\n            XTOX = XTO.dot(X)\n            XTOX_inv = self._quick_matrix_invert(XTOX)\n            XTOY = XTO.dot(y)\n            coefs = XTOX_inv.dot(XTOY)\n        return coefs, XTOX_inv\n        \n    def fit(\n        self,\n        y: Optional[np.ndarray] = None,\n        X: Optional[np.ndarray] = None,\n        omega: Optional[np.ndarray] = None,\n        fgls = None,\n    ):\n        self._clear_fitted_attributes()\n        y = self._get_y(y)\n        X, exog_names = self._get_X(X)\n        if y is None or X is None:\n            raise ValueError('X and y is required for fitting')\n        if len(y) != X.shape[0]:\n            raise ValueError(\"y and X must be the same size.\")\n        self.y, self.X, self.exog_names = y, X, exog_names\n        self.n, self.k = X.shape\n        self.DoF = self.n - self.k\n        if omega is not None:\n            self.omega = omega\n        if self.omega is None:\n            self.beta, self.var_X_inv = self._estimate_ols_coefs(y,X)\n        if self.omega is not None or fgls is not None:\n            if self.omega is not None and fgls is not None:\n                raise ValueError('Cannot specify both omega and fgls')\n            elif fgls is not None:\n                self._assess_fit()\n                if type(fgls) == str:\n                      if fgls == \"homoskedastic\":\n                          self.omega = self._homoskedastic()\n                      elif fgls == \"heteroskedastic\":\n                          self.omega = self._heteroskedastic()\n                elif type(fgls) == np.ndarray:\n                    self.omega = self._clustered(fgls)\n            self._clear_fitted_attributes()\n            self.beta, self.var_X_inv = self._estimate_gls_coefs(y,X,self.omega)\n```\n:::\n\n\nLet's now compare it to statsmodels:\n\n::: {#827da0f9 .cell execution_count=3}\n``` {.python .cell-code}\nnp.random.seed(42)\nn, k = 200, 2\nsigma_sq = 1\nbeta = np.random.normal(size=(k,1))\nX = np.hstack([ \n  np.ones(n).reshape(n,1),\n  np.random.normal(size=(n,k-1)) \n  ])\ny = X.dot(beta) + np.random.normal(loc=0,scale=sigma_sq,size=(n,1))\ncl = np.repeat(np.arange(10), n/10)\nmodel = LS(y,X)\nmodel.fit()\nmodel.summary()\n```\n\n::: {.cell-output .cell-output-display execution_count=57}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Coefficient</th>\n      <th>Standard Error</th>\n      <th>Lower bound</th>\n      <th>Upper bound</th>\n      <th>test-statistic</th>\n      <th>p-value</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.570588</td>\n      <td>0.070282</td>\n      <td>0.431990</td>\n      <td>0.709186</td>\n      <td>8.118521</td>\n      <td>4.939418e-14</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-0.044316</td>\n      <td>0.075569</td>\n      <td>-0.193339</td>\n      <td>0.104707</td>\n      <td>-0.586427</td>\n      <td>5.582568e-01</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n::: {#ebc36cd1 .cell execution_count=4}\n``` {.python .cell-code}\nmodel.fit(omega=np.diagflat(np.arange(1,n+1)))\nmodel.summary()\n```\n\n::: {.cell-output .cell-output-display execution_count=58}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Coefficient</th>\n      <th>Standard Error</th>\n      <th>Lower bound</th>\n      <th>Upper bound</th>\n      <th>test-statistic</th>\n      <th>p-value</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.554508</td>\n      <td>0.007028</td>\n      <td>0.540648</td>\n      <td>0.568368</td>\n      <td>78.895699</td>\n      <td>1.468003e-151</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-0.129972</td>\n      <td>0.007649</td>\n      <td>-0.145057</td>\n      <td>-0.114888</td>\n      <td>-16.991774</td>\n      <td>1.563731e-40</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n::: {#e32487ca .cell execution_count=5}\n``` {.python .cell-code}\nmodel.omega = None\nmodel.fit(fgls='homoskedastic')\nmodel.summary()\n```\n\n::: {.cell-output .cell-output-display execution_count=59}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Coefficient</th>\n      <th>Standard Error</th>\n      <th>Lower bound</th>\n      <th>Upper bound</th>\n      <th>test-statistic</th>\n      <th>p-value</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.570588</td>\n      <td>0.070770</td>\n      <td>0.431029</td>\n      <td>0.710147</td>\n      <td>8.062601</td>\n      <td>6.974507e-14</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-0.044316</td>\n      <td>0.076093</td>\n      <td>-0.194372</td>\n      <td>0.105741</td>\n      <td>-0.582387</td>\n      <td>5.609684e-01</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n::: {#e1bfb5bc .cell execution_count=6}\n``` {.python .cell-code}\nmodel.omega = None\nmodel.fit(fgls='heteroskedastic')\nmodel.summary()\n```\n\n::: {.cell-output .cell-output-display execution_count=60}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Coefficient</th>\n      <th>Standard Error</th>\n      <th>Lower bound</th>\n      <th>Upper bound</th>\n      <th>test-statistic</th>\n      <th>p-value</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.787920</td>\n      <td>0.075926</td>\n      <td>0.638194</td>\n      <td>0.937647</td>\n      <td>10.377539</td>\n      <td>2.005613e-20</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.281053</td>\n      <td>0.083791</td>\n      <td>0.115816</td>\n      <td>0.446290</td>\n      <td>3.354224</td>\n      <td>9.537270e-04</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n::: {#90f49bba .cell execution_count=7}\n``` {.python .cell-code}\nmodel.omega = None\nmodel.fit(fgls=cl)\nmodel.summary()\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n/Users/Chris/dev/blog-1/src/ols_blue.py:135: RuntimeWarning:\n\ninvalid value encountered in sqrt\n\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=61}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Coefficient</th>\n      <th>Standard Error</th>\n      <th>Lower bound</th>\n      <th>Upper bound</th>\n      <th>test-statistic</th>\n      <th>p-value</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.570588</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-0.044316</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n::: {#6ac03db7 .cell execution_count=8}\n``` {.python .cell-code}\nmodel.omega\n```\n\n::: {.cell-output .cell-output-display execution_count=62}\n```\narray([[ 0.89932625,  0.79360054, -1.3556773 , ...,  0.        ,\n         0.        ,  0.        ],\n       [ 0.79360054,  0.70030406, -1.1963025 , ...,  0.        ,\n         0.        ,  0.        ],\n       [-1.3556773 , -1.1963025 ,  2.04359757, ...,  0.        ,\n         0.        ,  0.        ],\n       ...,\n       [ 0.        ,  0.        ,  0.        , ...,  1.61626309,\n        -2.1636842 , -0.92289636],\n       [ 0.        ,  0.        ,  0.        , ..., -2.1636842 ,\n         2.89651438,  1.23547724],\n       [ 0.        ,  0.        ,  0.        , ..., -0.92289636,\n         1.23547724,  0.52697961]])\n```\n:::\n:::\n\n\n::: {#15c327be .cell execution_count=9}\n``` {.python .cell-code}\nimport statsmodels.api as sm \nmodel = sm.GLS(y,X).fit()\n```\n:::\n\n\n",
    "supporting": [
      "gls_fgls_wls_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}