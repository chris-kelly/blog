{
  "hash": "90747ba40b718ff7596726bcb22d35f2",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Deriving Logistic Regression Coefficients\"\nauthor: \"Chris Kelly\"\ndate: '02-21-25'\ncategories: [Maximum Likelihood, Generalized Linear Models]\nformat:\n  html:\n    code-fold: true\n    toc: true\nimage: '../images/glm_logistic.png'\n---\n\n\n::: {.callout-tip}\n### What are we exploring?\nDeriving coefficients to predict the likelihood of a binary outcome using maximum likelihood estimation and the logit link funciton.\n:::\n\n# The bernoulli PMF\n\nImagine a basketball player is taking free throws.\n\nFor each trial $i$ (a freethrow attempt), we observe the outcome $y_i$ as being 1 if there is a success (i.e. the freethrow is made), or 0 if it is a failure (a miss). \n\nWe can denote $p$ as the player's fixed probability for success (the likelihood they make the free throw, e.g. 90%). It follows that the probability of failure as $1-p$ (i.e. a 10% chance of missing).\n\nThis can be formalised as the following (the probability mass function of the bernoulli distribution):\n\n$$\n\\displaylines{\n\\begin{align}\nP(y_i) = \n\\begin{cases}\n  p & \\text{if}\\ y_i=1 \\\\\n  1-p & \\text{if}\\ y_i=0\n\\end{cases}\n\\end{align}\n}\n$$\n\nWhich is equivalent to the following:\n\n$$\nP(y_i) = p^{y_i}(1-p)^{1-y_i}\n$$\n\n:::{.column-margin}\nSince $g(x)^0 = 1$:\n\n* if $y_i=1$, $p^{1}(1-p)^{0} = p$\n* if $y_i=0$, $p^{0}(1-p)^{1} = 1-p$\n:::\n\nNow imagine that, rather than $p$ being fixed for all attempts, there are some external variables that influence the shooter (for example, whether the game is at home or away). Let's denote these relevant variables $X$.\n\nWe can then denote the probability of success as the following:\n\n$$\nP(y_i|X_i) = P(y_i=1|X_i)^{y_i}(1-P(y_i=1|X_i))^{1-y_i}\n$$\n\n# Maximum Likelihood\n\nImagine we observe the player make the first two freethrows but miss the third. If we guess the probability of them scoring is 80%, then the chance of what we observed occuring in that order is $80\\%\\times80\\%\\times20\\%=12.8\\%$. They could also have got 2/3 by missing the first (scoring the last two) or missing the second (scoring first and third). So the overall probability is $3\\times8.1\\%=38.4\\%$\n\nWe might make a better guess - say that the probability of them scoring is 67%, since they made 2/3. Then the probability of what we observed occuring is $3\\times(67\\%\\times67\\%\\times33\\%)=44\\%$.\n\nWhat we have done is estimated the maximum likelihood - the value of $\\hat{p}$ that maximises the chance of observing the outcomes saw.\n\nHowever, in our problem, \n\n\n* But \n\n$$\np(y|X\\beta) =\n\\underbrace{\n  \\prod_{y_i=0}^{N}{ p(y_i=1|X_i\\beta)^{y_i} }\n}_{y_i=1} \\times\n\\underbrace{\n  \\prod_{y_i=1}^{N}{ (1-p(y_i=1|X_i\\beta))^{1-y_i} }    \n}_{y_i=0}\n$$\n\n<!-- In reality, we don't observe $p_i$ (the true probability of making the free throw), only $y_i$ (whether they made the free throw or not). We can use these binary observations to make estimates for $\\hat{p_i}$ though (e.g. if they made 85/100 freethrows in away games previously, our best guess for the probability of making one away is 85%). -->\n\n<!-- We might collect $N$ trials in our sample (say analyse the shooter's  freethrows so far this season, against many different teams, point differentials etc). We want to learn from this past experience to estimate the probability that they make the next one. -->\n\n<!-- In other words, our aim is to learn the relationship between all $X$ and $p$ (so we can infer e.g. the impact of taking freethrows at home or away), and this should allow us to make good predictions for future freethrows. -->\n\n<!-- Across $N$ observations collected, we want to find the values of $\\beta$ that maximise the likelihood of observing all outcomes (the vector of results $y$). -->\n\nLet's split the outcomes between successes and failures. We thus derive the cost function:\n\n\n\n<!-- Maximum likelihood estimation aims to find the set of coefficients that maximises the likelihood of observing the evidence we have. We thus aim to find the coefficients $\\beta$ that maximise the likelihood of observing $y$ across all $n$ samples. -->\n\n### Taking the negative log-likelihood\n\nIn practice, dealing with a cost function made up of a sum product is tricky - it is easier to take the log and deal with addition instead. Further, rather than maximise, it is common to \"minimize\" cost functions, so the negative log-likelihood is usually used.\n\n:::{.column-margin}\nRecall that $\\log{\\left(ab\\right)} = \\log{\\left(a\\right)} + \\log{\\left(b\\right)}$\n:::\n\n$$\n\\displaylines{\n\\begin{align}\n& \\max_\\beta{p(y|\\beta,X)} \\\\\n= & \n\\max_\\beta{\\left\\{ \n  \\prod_{i=1}^{N}{ p(y_i=1|X_i\\beta)^{y_i} }\n  \\times\n  \\prod_{i=1}^{N}{ (1-p(y_i=1|X_i\\beta))^{1-y_i} }\n  \\right\\}}\n\\\\ \\\\ \\Rightarrow & \n\\min_\\beta{\\left\\{ -\\log{ \\left[\n  \\prod_{i=1}^{N}{ p(y_i=1|X_i\\beta)^{y_i} }\n  \\times\n  \\prod_{i=1}^{N}{ (1-p(y_i=1|X_i\\beta))^{1-y_i} }\n  \\right] } \\right\\}}\n\\\\ \\\\ = & \n\\min_\\beta{\\left\\{\n  \\sum_{i=1}^{N}{ -\\log{ \\left[ p(y_i=1|X_i\\beta)^{y_i} \\right] } } +\n  \\sum_{i=1}^{N}{ -\\log{ \\left[ (1-p(y_i=1|X_i\\beta))^{1-y_i}\\right] } }\n  \\right\\}}  \n\\\\ \\\\ = & \n\\min_\\beta{\\left\\{\n  \\sum_{i=1}^{N}{ -y_i\\log{ \\left[ p(y_i=1|X_i\\beta) \\right] } } +\n  \\sum_{i=1}^{N}{ -(1-y_i)\\log{ \\left[ 1-p(y_i=1|X_i\\beta)\\right] } }\n  \\right\\}}  \n\\end{align}\n}\n$$\n\n# Modelling the probability\n\n### Logistic activation function\n\nWe might assume that the the log-odds - the logarithm of the probability of success divided by the probability of failure - is linearly related to its predictors, i.e.\n\n$$\n\\text{logit}(E[Y_i|X_i]) = \\text{logit}(p_i) =\n\\ln{\\left(\\frac{p_i}{1-p_i}\\right)} = X_i \\beta\n$$\n\nThis is called a \"link function\" - the link between the outcome, $y$, and the linear predictors $X\\beta$. This specific link function is called the \"logit link function\".\n\nTo make predictions then for the probability of success, we need the inverse of the link function - sometimes called the \"activation function\" in the context of neural network. \n\nWe can derive the inverse of the logit link by rearranging it in terms of $p$:\n\n$$\n\\displaylines{\n\\begin{align}\n\\ln{\\left(\\frac{p}{1-p}\\right)} = X\\beta\n& \\Rightarrow \\frac{p}{1-p}\\ = \\exp{\\{X\\beta\\}}\n\\\\ \\\\\n& \\Rightarrow p = \\exp{\\{X\\beta\\}}(1-p)\n\\\\ \\\\\n& \\Rightarrow p - (1+\\exp{\\{X\\beta\\}}) = \\exp{\\{X\\beta\\}}\n\\\\ \\\\\n& \\Rightarrow p = \\frac{\\exp{\\{X\\beta\\}}}{1 + \\exp{\\{X\\beta\\}}}\n=  \\left( 1 + \\exp{\\{-X\\beta\\}} \\right)^{-1}\n\\end{align}\n}\n$$\n\nWe can see that this activation function \"squashes\" all outputs $X\\beta \\in [-\\infty,\\infty]$ between 0 and 1:\n\n::: {.cell}\n\n```{.r .cell-code}\nlogistic_dist <- function(x) {\n  return( (1+exp(-x))^-1 )\n}\nx = seq(-10,10,0.1)\nplot(\n  x=x,\n  y=logistic_dist(x),\n  type='l',col=\"blue\",,lty=1\n  )\nlines(\n  x=seq(-3,3,0.1),\n  y=(1/4)*seq(-3,3,0.1)+0.5,\n  type='l',col=\"red\",lty=2\n)\nlegend(\n  -10,1,\n  legend=c(\n    \"logistic activation\",\n    \"linear activation\"\n    ),\n  col=c(\"blue\",\"red\"),\n  lty=c(1,2)\n  )\n```\n\n::: {.cell-output-display}\n![](glm_logistic_files/figure-html/unnamed-chunk-1-1.png){width=864}\n:::\n:::\n\n\n:::{.column-margin}\nFor probabilities of between 0.3 to 0.7, we see that the logistic activation function maps very closely to that of a simply linear one. It is only at the more extreme probabilities that they diverge.\n:::\n\n\n### A latent variable model\n\nEconomists often frame binary regression as a \"latent variable model\". What this means is they frame the problem as though there is a hidden continuous variable we do not observe, $y_i^*$, and if it exceeds a certain threshold (usually zero) then there is a success:\n\n$$\n\\displaylines{\n\\begin{align}\ny_i^* & = X_i\\beta + \\epsilon_i\n\\\\ \\\\\ny_i & = \n\\begin{cases}\n  1 & \\text{if}\\ y_i^* > 0 & \\text{i.e. } -\\epsilon_i > X_i\\beta\\\\\n  0 & \\text{otherwise}\n\\end{cases}\n\\end{align}\n}\n$$\n\nIn other words, the latent variable $y_i^*$ is purely a function of its predictors $X_i$, their relationship to $p_i$ given by $\\beta$, and additive noise $\\epsilon_i$. If $y_i^*$ is positive, we observe a success (where $y_i = 1$).\n\nWe can thus formulate $P(y_i=1|X)$ as the likelihood that the addtive noise $\\epsilon_i$ is less than $X\\beta$, resulting in the probability $y_i^*$ above zero:\n\n$$\n\\displaylines{\n\\begin{align}\nP(y_i=1|X_i) \n& = P(y_i^* > 0 | X_i)\n\\\\\n& = P(X_i\\beta + \\epsilon_i > 0)\n\\\\\n& = P(\\epsilon_i > -X_i\\beta )\n\\\\\n& = P(\\epsilon_i < X_i\\beta ) & \\iff \\epsilon \\sim f(\\mu,s) \\text{ is symmetric}\n\\end{align}\n}\n$$\n\nSo a good assumption for the probabilistic process that generates the errors $\\epsilon$ is very important! \n\nThe error is often assumed to be generated from a logistic distribution, with location parameter $\\mu=0$ and scale parameter $s=1$:\n\n$$\n\\epsilon \\sim \\text{Logistic}(0,1)\n$$\n\nRecall the pdf of the logistic distribution is:\n\n$$\n\\displaylines{\n\\begin{align}\nf(x,\\mu,s) & = \n\\frac{\\exp{\\left\\{ -(x-\\mu)/s \\right\\}}}\n{s(1+\\exp{\\left\\{ -(x-\\mu)/s \\right\\})^2}}\n\\\\ \\\\\n\\therefore f(x,0,1) & = \n\\frac{\\exp{\\left\\{ -x \\right\\}}}\n{(1+\\exp{\\left\\{ -x \\right\\})^2}}\n\\end{align}\n}\n$$\n\nThen the CDF is the integral of the pdf:\n$$\n\\displaylines{\n\\begin{align}\n\\int {f(x,0,1) \\,dx} & = \n\\int{\n  \\frac{e^{-x}}\n  {(1+e^{-x})^2}\n\\,dx}\n\\\\ \\\\\n\\text{let } u = 1+e^{-x} & \\therefore \\frac{du}{dx} = -e^{-x} \n\\\\\n& \\therefore dx = -\\frac{du}{e^x}\n\\\\ \\\\\n\\therefore \n\\int {f(x,0,1) \\,dx} & = \n\\int {\\frac{e^x}{u^2} \\times -\\frac{du}{e^x}}\n\\\\ \n& = \\int {-u^{-2}\\,du}\n\\\\ \n& = u^{-1} + c\n\\\\ \n& = (1+\\exp{\\{-x\\}})^{-1} + c\n% \\therefore \n% P(y_i=1|X_i) & = P(X_i\\beta + \\epsilon_i > 0) \n% \\\\ & = \n% [(1+\\exp{\\{-(X_i\\beta+\\epsilon_i)\\}})^{-1} + c] -\n% [(1+\\exp{\\{-0\\}})^{-1} + c]\n% \\\\ & = \n% [(1+\\exp{\\{-(X_i\\beta+\\epsilon_i)\\}})^{-1}] -\n% \\frac{1}{2}\n\\end{align}\n}\n$$\n\nNote that this is the inverse of the logit function!\n\n$$\n\\displaylines{\n\\begin{align}\n\\text{logit}(x) \n& = \\ln{\\left(\\frac{x}{1-x}\\right)} \n\\\\\n\\therefore \\text{let } y & = \\ln{\\left(\\frac{x}{1-x}\\right)}\n\\\\\ne^y & = \\frac{x}{1-x}\n\\\\\ne^y - xe^y & = x\n\\\\\ne^y & = x(1+e^y)\n\\\\\nx & = \\frac{e^y}{1+e^y}\n= (1+\\exp{\\{e^{-x}\\}})^{-1}\n\\\\ \\\\\n\\therefore\nP(\\epsilon_i < X_i\\beta ) & = \\text{Logit}^{-1}(X\\beta)\n\\end{align}\n}\n$$\n\n# Optimal coefficients for the coefficient\n\n### Deriving the gradient with respect to the coefficients\n\nWe minimise the cost function by finding the optimum coefficient values $\\beta^*$ so that the partial differential is equal to zero.\n\n$$\n\\displaylines{\n\\begin{align}\n& \\frac{\\partial}{\\partial \\beta_j}p(y|\\beta,X) \\\\ \\\\\n= &\n\\frac{\\partial}{\\partial \\beta_j} \\left(\n\\sum_{i=1}^{N}{ -y_i\\log{ \\left[ p(y_i=1|X_i\\beta) \\right] } } +\n  \\sum_{i=1}^{N}{ -(1-y_i)\\log{ \\left[ 1-p(y_i=1|X_i\\beta)\\right] } }\n\\right) \\\\ \\\\\n= &\n\\sum_{i=1}^{N}{ y_i\n  \\frac{\\frac{\\partial}{\\partial \\beta_j}p(y_i=1|X_i\\beta)}{p(y_i=1|X_i\\beta)}\n  } +\n  \\sum_{i=1}^{N}{ (1-y_i)\n    \\frac{\\frac{\\partial}{\\partial \\beta_j}\\left(1-p(y_i=1|X_i\\beta)\\right)}{1-p(y_i=1|X_i\\beta)}\n    }\n\\end{align}\n}\n$$\n\nThis is as far as we can get, without now making some more assumptions. Let's imagine that we can model the \n\nGiven that: \n\n$$\n\\hat{p_i} = \\hat{p}(y_i=1|X_i \\hat{\\beta}) = \n\\frac{1}{1+\\exp{\\left\\{-X_i\\hat{\\beta}\\right\\}}}\n$$\n\nThen the partial differential of the probability with respect to feature $j$ is:\n\n$$\n\\displaylines{\n\\begin{align}\n& \\frac{\\partial }{\\partial \\beta_j}\n\\hat{p}(y_i=1|X_i \\hat{\\beta_j})\n\\\\ = &\n\\frac{\\partial }{\\partial \\beta_j} (1+\\exp{\\{-X_i\\hat{\\beta}\\}})^{-1}\n\\\\ = &\n\\frac{\\partial }{\\partial \\beta_j} -1(1+\\exp{\\{-X_i\\hat{\\beta}\\}})^{-2}\n\\times x_{ij}\\exp{\\{-X_i\\hat{\\beta}\\}}\n\\\\ = &\nx_{ij} \\left( \\frac{\\exp{\\{-X_i\\hat{\\beta}\\}}}{(1+\\exp{\\{-X_i\\hat{\\beta}\\}})^{2}} \\right)\n\\\\ = &\nx_{ij} \\left( \\frac{1}{(1+\\exp{\\{-X_i\\hat{\\beta}\\}})} \\times \\frac{\\exp{\\{-X_i\\hat{\\beta}\\}}}{(1+\\exp{\\{-X_i\\hat{\\beta}\\}})} \\right)\n\\\\ = &\nx_{ij} \\left( \\frac{1}{(1+\\exp{\\{-X_i\\hat{\\beta}\\}})} \\times \\left( 1 - \\frac{1}{(1+\\exp{\\{-X_i\\hat{\\beta}\\}})} \\right) \\right)\n\\\\ \\\\ = &\nx_{ij} ( \\hat{p}(y_i=1|X_i \\hat{\\beta_j}) \\times (1-\\hat{p}(y_i=1|X_i \\hat{\\beta_j})) )\n\\\\ \\\\ = &\nx_{ij} ( \\hat{p_i} \\times (1-\\hat{p_i}) )\n\\end{align}\n}\n$$\n\nAnd thus we can substitute this into our cost function:\n\n$$\n\\displaylines{\n\\begin{align}\n& \\frac{\\partial}{\\partial \\beta_j}p(y|\\beta,X) \\\\ \\\\\n= &\n\\sum_{i=1}^{N}{ y_i\n  \\frac{\\frac{\\partial}{\\partial \\beta_j}p(y_i=1|X_i\\beta)}{p(y_i=1|X_i\\beta)}\n  } +\n\\sum_{i=1}^{N}{ (1-y_i)\n  \\frac{\\frac{\\partial}{\\partial \\beta_j}\\left(1-p(y_i=1|X_i\\beta)\\right)}{1-p(y_i=1|X_i\\beta)}\n  } \\\\ \\\\\n= &\n\\sum_{i=1}^{N}{ y_i\n  \\frac{x_{ij} ( \\hat{p_i} \\times (1-\\hat{p_i}) )}{\\hat{p_i}}\n  } +\n\\sum_{i=1}^{N}{ (1-y_i)\n  \\frac{x_{ij} ( \\hat{p_i} \\times (1-\\hat{p_i}) )}{(1-\\hat{p_i})}\n  } \\\\ \\\\\n= &\n\\sum_{i=1}^{N}{ y_i\n  \\frac{x_{ij} ( \\cancel{\\hat{p_i}} \\times (1-\\hat{p_i}) )}{\\cancel{\\hat{p_i}}}\n  } +\n\\sum_{i=1}^{N}{ (1-y_i)\n  \\frac{x_{ij} ( \\hat{p_i} \\times \\cancel{(1-\\hat{p_i})} )}{\\cancel{(1-\\hat{p_i})}}\n  } \\\\ \\\\  \n= &\n\\sum_{i=1}^{N}{ y_ix_{ij} (1-\\hat{p_i})} +\n\\sum_{i=1}^{N}{ (1-y_i)x_{ij} \\hat{p_i}} \\\\ \\\\\n= &\n\\sum_{i=1}^{N}{ x_{ij} \\left[\n    y_i(1-\\hat{p_i}) (1-y_i)(\\hat{p_i})\n   \\right]\n  }\n\\end{align}\n}\n$$\n\nWhich is the coefficient from logistic regression.\n\nFin.",
    "supporting": [
      "glm_logistic_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}