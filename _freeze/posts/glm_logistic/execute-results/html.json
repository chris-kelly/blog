{
  "hash": "b27a7c2729a975e12fe1868e84ca5d37",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Deriving Logistic Regression Coefficients\"\nauthor: \"Chris Kelly\"\ndate: '02-21-25'\ncategories: [Maximum Likelihood, Generalized Linear Models]\nformat:\n  html:\n    code-fold: true\n    toc: true\nimage: '../images/glm_logistic.png'\n---\n\n\n::: {.callout-tip}\n### What are we exploring?\nDeriving coefficients to predict the likelihood of a binary outcome using maximum likelihood estimation and the logit link funciton.\n:::\n\n# Setting some intuition\n\nImagine a basketball player has a 90% chance of making a freethrow. \n\nFor freethrow attempt $i$, we observe the outcome, $y_i$, as being 1 if there is a success (i.e. the freethrow is made), or 0 if it is a failure (a miss). \n\nWe can denote $p$ as a fixed probability for success. It follows that the probability of failure as $1-p$ (i.e. a 10% chance of missing).\n\nThis can be formalised as the following (the probability mass function of the bernoulli):\n\n$$\n\\displaylines{\n\\begin{align}\np(y_i) = \n\\begin{cases}\n  p & \\text{if}\\ y_i=1 \\\\\n  1-p & \\text{if}\\ y_i=0\n\\end{cases}\n\\end{align}\n}\n$$\n\nWhich is equivalent to the following:\n\n$$\np(y_i) = p^{y_i}(1-p)^{1-y_i}\n$$\n\n:::{.column-margin}\nSince $x^0 = 1$:\n\n* if $y_i=1$, $p^{1}(1-p)^{0} = p$\n* if $y_i=0$, $p^{0}(1-p)^{1} = 1-p$\n:::\n\nNow imagine that, rather than $p$ being fixed at one value, there are some external variables that influence the shooter (for example, whether the game is at home or away). Let's denote these relevant variables $X$, and the relationship they have on the probability of success as $\\beta$.\n\nWe can denote then denote the probability of success as the following:\n\n$$\np(y_i|X_i\\beta) = p(y_i=1|X_i\\beta)^{y_i}(1-p(y_i=1|X_i\\beta))^{1-y_i}\n$$\n\nNow imagine we observe the shooter take 100 freethrows (sample size $N$), against many different teams, point differentials etc. We want to learn from this past experience to estimate the probability that they make the next one.\n\n<!-- Note how this is modelling the probability $p_i$, but we only actually observe the binary outcome $y_i$. In econometrics, we often assume there is some underlying threshold $y^*$,  -->\n\n# Cost function for bernoulli regression\n\n### Applying the bernoulli pdf\n\nAcross $N$ observations collected, we want to find the values of $\\beta$ that maximise the likelihood of observing all outcomes (the vector of results $y$).\n\nLet's split the outcomes between successes and failures. We thus derive the cost function:\n\n$$\np(y|X\\beta) =\n\\underbrace{\n  \\prod_{i=1}^{N}{ p(y_i=1|X_i\\beta)^{y_i} }\n}_{y_i=1} \\times\n\\underbrace{\n  \\prod_{i=1}^{N}{ (1-p(y_i=1|X_i\\beta))^{1-y_i} }    \n}_{y_i=0}\n$$\n\nMaximum likelihood estimation aims to find the set of coefficients that maximises the likelihood of observing the evidence we have. We thus aim to find the coefficients $\\beta$ that maximise the likelihood of observing $y$ across all $n$ samples.\n\n### Taking the negative log-likelihood\n\nIn practice, dealing with a cost function made up of a sum product is tricky - it is easier to take the log and deal with addition instead. Further, rather than maximise, it is common to \"minimize\" cost functions, so the negative log-likelihood is usually used.\n\n:::{.column-margin}\nRecall that $\\log{\\left(ab\\right)} = \\log{\\left(a\\right)} + \\log{\\left(b\\right)}$\n:::\n\n$$\n\\displaylines{\n\\begin{align}\n& \\max_\\beta{p(y|\\beta,X)} \\\\\n= & \n\\max_\\beta{\\left\\{ \n  \\prod_{i=1}^{N}{ p(y_i=1|X_i\\beta)^{y_i} }\n  \\times\n  \\prod_{i=1}^{N}{ (1-p(y_i=1|X_i\\beta))^{1-y_i} }\n  \\right\\}}\n\\\\ \\\\ \\Rightarrow & \n\\min_\\beta{\\left\\{ -\\log{ \\left[\n  \\prod_{i=1}^{N}{ p(y_i=1|X_i\\beta)^{y_i} }\n  \\times\n  \\prod_{i=1}^{N}{ (1-p(y_i=1|X_i\\beta))^{1-y_i} }\n  \\right] } \\right\\}}\n\\\\ \\\\ = & \n\\min_\\beta{\\left\\{\n  \\sum_{i=1}^{N}{ -\\log{ \\left[ p(y_i=1|X_i\\beta)^{y_i} \\right] } } +\n  \\sum_{i=1}^{N}{ -\\log{ \\left[ (1-p(y_i=1|X_i\\beta))^{1-y_i}\\right] } }\n  \\right\\}}  \n\\\\ \\\\ = & \n\\min_\\beta{\\left\\{\n  \\sum_{i=1}^{N}{ -y_i\\log{ \\left[ p(y_i=1|X_i\\beta) \\right] } } +\n  \\sum_{i=1}^{N}{ -(1-y_i)\\log{ \\left[ 1-p(y_i=1|X_i\\beta)\\right] } }\n  \\right\\}}  \n\\end{align}\n}\n$$\n\n### Deriving the gradient with respect to the coefficients\n\nWe minimise the cost function by finding the optimum coefficient values $\\beta^*$ so that the partial differential is equal to zero.\n\n$$\n\\displaylines{\n\\begin{align}\n& \\frac{\\partial}{\\partial \\beta_j}p(y|\\beta,X) \\\\ \\\\\n= &\n\\frac{\\partial}{\\partial \\beta_j} \\left(\n\\sum_{i=1}^{N}{ -y_i\\log{ \\left[ p(y_i=1|X_i\\beta) \\right] } } +\n  \\sum_{i=1}^{N}{ -(1-y_i)\\log{ \\left[ 1-p(y_i=1|X_i\\beta)\\right] } }\n\\right) \\\\ \\\\\n= &\n\\sum_{i=1}^{N}{ y_i\n  \\frac{\\frac{\\partial}{\\partial \\beta_j}p(y_i=1|X_i\\beta)}{p(y_i=1|X_i\\beta)}\n  } +\n  \\sum_{i=1}^{N}{ (1-y_i)\n    \\frac{\\frac{\\partial}{\\partial \\beta_j}\\left(1-p(y_i=1|X_i\\beta)\\right)}{1-p(y_i=1|X_i\\beta)}\n    }\n\\end{align}\n}\n$$\n\nThis is as far as we can get, without now making some more assumptions. Let's imagine that we can model the \n\n# Logistic activation\n\nWe might assume that the the log-odds - the logarithm of the probability of success divided by the probability of failure - is linearly related to its predictors, i.e.\n\n$$\n\\displaylines{\n\\begin{align}\n\\ln{\\left(\\frac{p}{1-p}\\right)} = X\\beta + \\epsilon \n&& \\epsilon \\sim N(0,\\sigma^2)\n\\end{align}\n}\n$$\n\nThis is called a \"link function\" - the link between the outcome, $y$, and the linear predictors $X\\beta$. This specific link function is called the \"logit link function\".\n\nTo make predictions then for the probability of success, we need the inverse of the link function - sometimes called the \"activation function\" in the context of neural network. \n\nWe can derive the inverse of the logit link by rearranging it in terms of $p$:\n\n$$\n\\displaylines{\n\\begin{align}\nE\\left[\n  \\ln{\\left(\\frac{p}{1-p}\\right)}\n  \\right] = X\\beta\n& \\Rightarrow \\frac{p}{1-p}\\ = \\exp{\\{X\\beta\\}}\n\\\\ \\\\\n& \\Rightarrow p = \\exp{\\{X\\beta\\}}(1-p)\n\\\\ \\\\\n& \\Rightarrow p - (1+\\exp{\\{X\\beta\\}}) = \\exp{\\{X\\beta\\}}\n\\\\ \\\\\n& \\Rightarrow p = \\frac{\\exp{\\{X\\beta\\}}}{1 + \\exp{\\{X\\beta\\}}}\n=  \\left( 1 + \\exp{\\{-X\\beta\\}} \\right)^{-1}\n\\end{align}\n}\n$$\n\nWe can see that this activation function \"squashes\" all outputs $X\\beta \\in [-\\infty,\\infty]$ between 0 and 1:\n\n::: {.cell}\n\n```{.r .cell-code}\nlogistic_dist <- function(x) {\n  return( (1+exp(-x))^-1 )\n}\nx = seq(-10,10,0.1)\nplot(\n  x=x,\n  y=logistic_dist(x),\n  type='l',col=\"blue\",,lty=1\n  )\nlines(\n  x=seq(-3,3,0.1),\n  y=(1/4)*seq(-3,3,0.1)+0.5,\n  type='l',col=\"red\",lty=2\n)\nlegend(\n  -10,1,\n  legend=c(\n    \"logistic activation\",\n    \"linear activation\"\n    ),\n  col=c(\"blue\",\"red\"),\n  lty=c(1,2)\n  )\n```\n\n::: {.cell-output-display}\n![](glm_logistic_files/figure-html/unnamed-chunk-1-1.png){width=864}\n:::\n:::\n\n\n:::{.column-margin}\nFor probabilities of between 0.3 to 0.7, we see that the logistic activation function maps very closely to that of a simply linear one. It is only at the more extreme probabilities that they diverge.\n:::\n\nWe can now use this activation function to derive some coefficients.\n\n# Optimal coefficients for the coefficient\n\nGiven that: \n\n$$\n\\hat{p_i} = \\hat{p}(y_i=1|X_i \\hat{\\beta}) = \n\\frac{1}{1+\\exp{\\left\\{-X_i\\hat{\\beta}\\right\\}}}\n$$\n\nThen the partial differential of the probability with respect to feature $j$ is:\n\n$$\n\\displaylines{\n\\begin{align}\n& \\frac{\\partial }{\\partial \\beta_j}\n\\hat{p}(y_i=1|X_i \\hat{\\beta_j})\n\\\\ = &\n\\frac{\\partial }{\\partial \\beta_j} (1+\\exp{\\{-X_i\\hat{\\beta}\\}})^{-1}\n\\\\ = &\n\\frac{\\partial }{\\partial \\beta_j} -1(1+\\exp{\\{-X_i\\hat{\\beta}\\}})^{-2}\n\\times x_{ij}\\exp{\\{-X_i\\hat{\\beta}\\}}\n\\\\ = &\nx_{ij} \\left( \\frac{\\exp{\\{-X_i\\hat{\\beta}\\}}}{(1+\\exp{\\{-X_i\\hat{\\beta}\\}})^{2}} \\right)\n\\\\ = &\nx_{ij} \\left( \\frac{1}{(1+\\exp{\\{-X_i\\hat{\\beta}\\}})} \\times \\frac{\\exp{\\{-X_i\\hat{\\beta}\\}}}{(1+\\exp{\\{-X_i\\hat{\\beta}\\}})} \\right)\n\\\\ = &\nx_{ij} \\left( \\frac{1}{(1+\\exp{\\{-X_i\\hat{\\beta}\\}})} \\times \\left( 1 - \\frac{1}{(1+\\exp{\\{-X_i\\hat{\\beta}\\}})} \\right) \\right)\n\\\\ \\\\ = &\nx_{ij} ( \\hat{p}(y_i=1|X_i \\hat{\\beta_j}) \\times (1-\\hat{p}(y_i=1|X_i \\hat{\\beta_j})) )\n\\\\ \\\\ = &\nx_{ij} ( \\hat{p_i} \\times (1-\\hat{p_i}) )\n\\end{align}\n}\n$$\n\nAnd thus we can substitute this into our cost function:\n\n$$\n\\displaylines{\n\\begin{align}\n& \\frac{\\partial}{\\partial \\beta_j}p(y|\\beta,X) \\\\ \\\\\n= &\n\\sum_{i=1}^{N}{ y_i\n  \\frac{\\frac{\\partial}{\\partial \\beta_j}p(y_i=1|X_i\\beta)}{p(y_i=1|X_i\\beta)}\n  } +\n\\sum_{i=1}^{N}{ (1-y_i)\n  \\frac{\\frac{\\partial}{\\partial \\beta_j}\\left(1-p(y_i=1|X_i\\beta)\\right)}{1-p(y_i=1|X_i\\beta)}\n  } \\\\ \\\\\n= &\n\\sum_{i=1}^{N}{ y_i\n  \\frac{x_{ij} ( \\hat{p_i} \\times (1-\\hat{p_i}) )}{\\hat{p_i}}\n  } +\n\\sum_{i=1}^{N}{ (1-y_i)\n  \\frac{x_{ij} ( \\hat{p_i} \\times (1-\\hat{p_i}) )}{(1-\\hat{p_i})}\n  } \\\\ \\\\\n= &\n\\sum_{i=1}^{N}{ y_i\n  \\frac{x_{ij} ( \\cancel{\\hat{p_i}} \\times (1-\\hat{p_i}) )}{\\cancel{\\hat{p_i}}}\n  } +\n\\sum_{i=1}^{N}{ (1-y_i)\n  \\frac{x_{ij} ( \\hat{p_i} \\times \\cancel{(1-\\hat{p_i})} )}{\\cancel{(1-\\hat{p_i})}}\n  } \\\\ \\\\  \n= &\n\\sum_{i=1}^{N}{ y_ix_{ij} (1-\\hat{p_i})} +\n\\sum_{i=1}^{N}{ (1-y_i)x_{ij} \\hat{p_i}} \\\\ \\\\\n= &\n\\sum_{i=1}^{N}{ x_{ij} \\left[\n    y_i(1-\\hat{p_i}) (1-y_i)(\\hat{p_i})\n   \\right]\n  }\n\\end{align}\n}\n$$\n\nWhich is the coefficient from logistic regression.\n\nFin.",
    "supporting": [
      "glm_logistic_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}