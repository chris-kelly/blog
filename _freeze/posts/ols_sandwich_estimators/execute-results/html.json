{
  "hash": "3870fe122ee60158b552f4b6015b90db",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Sandwiches: robust covariance error estimation\"\nauthor: \"Chris Kelly\"\ndate: '02-22-24'\ncategories: [Linear Models, Gauss-Markov, Standard errors]\nimage: '../images/ols_sandwich_estimators.png'\nformat:\n  html:\n    code-fold: true\n    toc: true\n    code-links:\n      - text: OLS class\n        icon: file-code\n        href: ../src/ols_blue.py\n      - text: Sandwich class\n        icon: file-code\n        href: ../src/ols_sandwich_estimators.py\n    other-links:\n      - text: Deriving the variance of the OLS coefficient\n        href: ols_blue.html#coefficient-variance-for-ols\n      - text: Using a weight matrix to correct for non-spherical errors \n        href: gls_fgls_wls.html\n---\n\n::: {.callout-tip}\n### What are we exploring?\nEstimating the correct coefficient variance when relaxing homoskedastic error assumptions\n:::\n\n## Introducing sandwiches\n\nThe [variance for the OLS coefficient estimator](ols_blue.html#coefficient-variance-for-ols) is equal to the following:\n\n$$\n\\displaylines{\nV(\\hat{\\beta}) =\n(X^{\\intercal}X)^{-1}X^{\\intercal}E[\\epsilon\\epsilon^{\\intercal}]X(X^{\\intercal}X)^{-1}\n}\n$$\n\nThis can be though to as a sandwich:\n\n- The **‚Äúbread‚Äù** either side: $(X^{\\intercal}X)^{-1}X^{\\intercal}$ on the left and its transpose $X(X^{\\intercal}X)^{-1}$ on the right\n- The **‚Äúmeat‚Äù** in the middle: what we assume for $E[\\epsilon\\epsilon^{\\intercal}]$\n    - Note that this is the same as the error variance, since $V[\\epsilon]=E[\\epsilon\\epsilon^{\\intercal}]-E[\\epsilon]E[\\epsilon^{\\intercal}]$ and $E[\\epsilon] = 0$\n\nOur coefficient will only be **efficient** if these assumptions about the expected error are correct! We will explore what happens when the errors are assumed to be homoskedastic, heteroskedastic or clustered.\n\n## Salmon bagel: Spherical Errors üêü\n\nUsual OLS is efficient [if the true model has \"spherical errors\"](ols_blue.html#coefficient-variance-assuming-spherical-errors). What does this mean in practice?\n\n- Errors are **homoskedastic**: $V(\\epsilon_i)=\\sigma^2$ for all observations\n- Errors are **serially uncorrelated**: $cov(\\epsilon_i,\\epsilon_{j\\neq i})=0$\n\nWhat does this look like for $E[\\epsilon\\epsilon^{\\intercal}]$?\n\n- The diagonal of the matrix is a constant value (scalar), $\\sigma^2$\n- The off-diagonals are all zero\n\n$$\n\\hat{\\sigma}^2\\underset{n\\times n}{I} = \n\\begin{bmatrix}\n\\hat{\\sigma}^2 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\ \n0 & \\hat{\\sigma}^2 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\ \n0 & 0 & \\hat{\\sigma}^2 & 0 & 0 & 0 & 0 & 0 & 0\\\\ \n0 & 0 & 0 & \\hat{\\sigma}^2 & 0 & 0 & 0 & 0 & 0\\\\ \n0 & 0 & 0 & 0 & \\hat{\\sigma}^2 & 0 & 0 & 0 & 0\\\\ \n0 & 0 & 0 & 0 & 0 & \\hat{\\sigma}^2 & 0 & 0 & 0\\\\ \n0 & 0 & 0 & 0 & 0 & 0 & \\hat{\\sigma}^2 & 0 & 0\\\\ \n0 & 0 & 0 & 0 & 0 & 0 & 0 & \\hat{\\sigma}^2 & 0\\\\ \n0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & \\hat{\\sigma}^2\n\\end{bmatrix}\n$$\n\nA good estimation of the constant error variance $\\sigma^2$ is to apply the standard formula to the residuals (i.e. method of moments):\n\n$$\n\\hat{\\sigma^2}=\\frac{1}{n-k}\\sum{\\hat{\\epsilon_i}^2} \\equiv \\frac{\\epsilon^{\\intercal}\\epsilon }{n-k} \n$$\n\n::: {.callout-tip}\n## Thus our \"salmon sandwich\" is:\n$$\n\\underset{k \\times k}{(X^{\\intercal}X)}^{-1}\n\\underset{k \\times n}\n{X^{\\intercal}}\n\\left(\\frac{1}{n-k} \\times \\underset{1 \\times 1}{(\\epsilon^{\\intercal}\\epsilon)}\n\\times\\underset{n \\times n}{I} \\right)\n\\underset{n \\times k}{X}\n\\underset{k \\times k}{(X^{\\intercal}X)}^{-1}\n$$\n:::\n\n::: {.column-margin}\nNote that under spherical errors, the sandwich can be simplified:\n$$\n\\displaylines{\n\\begin{align}\nV[\\hat{\\beta}] \n& = (X^{\\intercal}X)^{-1}X^{\\intercal}E[\\epsilon\\epsilon^{\\intercal}]X(X^{\\intercal}X)^{-1} \n\\\\ & = (X^{\\intercal}X)^{-1}X^{\\intercal}\\sigma^2IX(X^{\\intercal}X)^{-1} \n\\\\ & = \\sigma^2(X^{\\intercal}X)^{-1}\\cancel{X^{\\intercal}X}\\cancel{(X^{\\intercal}X)^{-1}} \n\\\\ & = \\sigma^2(X^{\\intercal}X)^{-1}\n\\end{align}\n}\n$$\n:::\n\nIn this scenario, the only things that impact the standard error of the coefficient $\\beta_k$ is:\n\n- The variance of all the residuals, $\\sigma^2$\n- The variance of the feature $V(X_k)$\n\n## Ham sarnie: Heteroskedastic errors üçñ\n\nHeteroskedastic correction is needed if:\n\n- **Errors vary for every individual**: $V(\\epsilon_i)=\\sigma_i^2$ for all observations\n- But they are still independent aka **serially uncorrelated**: $cov(\\epsilon_i,\\epsilon_{j\\neq i})=0$\n\nWhat does this look like for $E[\\epsilon\\epsilon^{\\intercal}]$?\n\n- The diagonal of the matrix is the estimate of variance which is unique for each observation,  $\\sigma_i^2$\n- The off-diagonals are all zero\n    \n    $$\n    \\underset{n \\times n}{\\sigma^2} = \\begin{bmatrix}\n    \\sigma_1^2 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\ \n    0 & \\sigma_2^2 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\ \n    0 & 0 & \\sigma_3^2 & 0 & 0 & 0 & 0 & 0 & 0\\\\ \n    0 & 0 & 0 & \\sigma_4^2 & 0 & 0 & 0 & 0 & 0\\\\ \n    0 & 0 & 0 & 0 & \\sigma_5^2 & 0 & 0 & 0 & 0\\\\ \n    0 & 0 & 0 & 0 & 0 & \\sigma_6^2 & 0 & 0 & 0\\\\ \n    0 & 0 & 0 & 0 & 0 & 0 & \\sigma_7^2 & 0 & 0\\\\ \n    0 & 0 & 0 & 0 & 0 & 0 & 0 & \\sigma_8^2 & 0\\\\ \n    0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & \\sigma_{..}^2\\\\ \n    \\end{bmatrix}\n    $$\n    \n- A good estimation of the vector of heteroskedastic error variances $\\sigma^2$ is again to apply the standard formula to the residuals for each individual:\n\n$$\n\\hat{\\sigma_i^2}=\\frac{1}{n-k}\\sum{\\hat{\\epsilon_i}^2} \n$$\n\n::: {.callout-tip}\n## Thus our \"ham sandwich\" is:\n$$\n\\underset{k \\times k}{(X^{\\intercal}X)}^{-1}\n\\underset{k \\times n}{X^{\\intercal}}\n\\left(\\frac{1}{n-k} \\times \\underset{1 \\times n}{(\\epsilon\\odot \\epsilon)} ^{\\intercal}\n\\times\\underset{n \\times n}{I} \\right)\n\\underset{n \\times k}{X}\n\\underset{k \\times k}{(X^{\\intercal}X)}^{-1}\n$$\n:::\n\nSimilar to homoskedastic errors, the things that impacts the standard error of coefficient $k$ is:\n\n- The variance of all the individual errors, $\\sigma_i^2$\n- The variance of the feature $V(X_k)$\n\n## Cheese roll: Clustered Errors üßÄ\n\nCluster-robust errors are needed if:\n\n- **Errors vary for every individual**: $V(\\epsilon_i)=\\sigma_i^2$ for all observations i.e. still heteroskedastic\n- AND errors within the same cluster $C_l$ are **serially correlated**:\n$cov(\\epsilon_i,\\epsilon_{j}) \\neq 0 \\text{ if } \\epsilon_i,\\epsilon_j \\in C_l$\n    - Note - **errors between clusters are assumed not to be serially correlated** though i.e. $cov(\\epsilon_i,\\epsilon_{j}) =0 \\text{ if } \\epsilon_i \\in C_l,\\epsilon_j \\in C_{m \\neq l}$\n\n\nWhat does this look like for $E[\\epsilon\\epsilon^{\\intercal}]$?\n\n- The diagonal of the matrix is the estimate of variance which is unique for each observation,  $\\sigma_i^2$\n- The off-diagonals are also populated with the covariance - but only when they are both in the same cluster\n\nHere is an example where observations 1, 2 and 3 are in cluster A, observations 4 and 5 are in cluster B, observations 6, 7 and 8 are in cluster C etc.\n\n$$\n\\underset{n \\times n}{\\epsilon\\epsilon^{\\intercal}} \\sim \\begin{bmatrix}\n\\epsilon_1^2 & \\epsilon_1\\epsilon_2 & \\epsilon_1\\epsilon_3 & 0 & 0 & 0 & 0 & 0 & 0\\\\ \n\\epsilon_2\\epsilon_1 & \\epsilon_2^2 & \\epsilon_2\\epsilon_3 & 0 & 0 & 0 & 0 & 0 & 0\\\\ \n\\epsilon_3\\epsilon_1 & \\epsilon_3\\epsilon_2 & \\epsilon_3^2 & 0 & 0 & 0 & 0 & 0 & 0\\\\ \n0 & 0 & 0 & \\epsilon_4^2 & \\epsilon_4\\epsilon_5 & 0 & 0 & 0 & 0\\\\ \n0 & 0 & 0 & \\epsilon_5\\epsilon_4 & \\epsilon_5^2 & 0 & 0 & 0 & 0\\\\ \n0 & 0 & 0 & 0 & 0 & \\epsilon_6^2 & \\epsilon_6\\epsilon_7 & \\epsilon_6\\epsilon_8 & 0\\\\ \n0 & 0 & 0 & 0 & 0 & \\epsilon_7\\epsilon_6 & \\epsilon_7^2 & \\epsilon_7\\epsilon_8 & 0\\\\ \n0 & 0 & 0 & 0 & 0 & \\epsilon_8\\epsilon_6 & \\epsilon_8\\epsilon_7 & \\epsilon_8^2 & 0\\\\ \n0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & \\epsilon_{..}^2\\\\ \n\\end{bmatrix}\n$$\n\nAdditionally, we have to do a finite-sample correction as well on the degrees of freedom, based on the number of clusters $n_c$\n\n::: {.callout-tip}\n### Thus our cheese sandwich is:\n$$\n\\underset{k \\times k}{(X^{\\intercal}X)}^{-1}\n\\underset{k \\times n}{X^{\\intercal}}\n\\left(\n  \\underbrace{\\frac{n-1}{n-k}\\frac{n_c}{n_c-1}}_{\\text{Finite correction}}\n\\times \n\\underset{n \\times n}{(\\epsilon \\epsilon^{\\intercal})}\n\\right)\n\\underset{n \\times k}{X}\n\\underset{k \\times k}{(X^{\\intercal}X)}^{-1}\n$$\n:::\n\nIn this scenario, there are a few additional things that impact the standard error of coefficient $k$:\n\n- If errors are correlated within clusters, this will increase the error.\n- If features are correlated within clusters, this will also increase the error (due to the off-diagonals in the error variance matrix)\n- And if both the errors and feature correlations are the same sign, this will also increase the standard error.\n- As well as the variance of the individual errors, $\\sigma_i^2$, and the variance of each feature $V(X_k)$, as before\n\n## Coding it from scratch in python\n\nFirst we use the OLS parent class we defined previously in the [BLUE OLS post](ols_blue.html):\n\n<details>\n<summary>OLS class (source code):</summary>\n```{.python include='../src/ols_blue.py' }\n```\n</details>\n\n\n\nNow we can define two new classes:\n* A new class, `error_covariance` that returns an $n \\times n$ matrix of the estimated errors\n* a child `ols_sandwich` class, that adapts the standard errors of the existing `OLS` class:\n\n<details>\n<summary>Sandwich class (source code):</summary>\n```{.python include='../src/ols_sandwich_estimators.py' }\n```\n</details>\n\n\n\nNow let's test it! We will generate some data, run the OLS model, and compare our sandwich estimators to statsmodels:\n\n::: {#b2bcc292 .cell execution_count=3}\n``` {.python .cell-code}\nnp.random.seed(42)\nn, k = 50, 2\nsigma_sq = 1\nbeta = np.random.normal(size=(k,1))\nX = np.hstack([ \n    np.ones(n).reshape(n,1),\n    np.random.normal(size=(n,k-1)) \n    ])\ny = X.dot(beta) + np.random.normal(loc=0,scale=sigma_sq,size=(n,1))\ncl = np.repeat(np.arange(10), 5)\n\nour_mod = ols_sandwich(y,X)\nour_mod.fit()\n# our_mod.summary()\n\nimport statsmodels.api as sm\n\nstats_mod = sm.OLS(y, X)\n```\n:::\n\n\nFirst let's try homoskedastic errors:\n\n::: {#4cfa20f4 .cell execution_count=4}\n``` {.python .cell-code}\npd.DataFrame(\n  np.hstack([\n    our_mod.summary(se_correction=\"homoskedastic\")['Standard Error'].to_numpy().reshape(-1,1),\n    stats_mod.fit().bse.reshape(-1,1)\n    ]),\n  columns=['Our Standard Errors','StatsModel Standard Errors'],\n)\n```\n\n::: {.cell-output .cell-output-display execution_count=10}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Our Standard Errors</th>\n      <th>StatsModel Standard Errors</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.131679</td>\n      <td>0.131679</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.138426</td>\n      <td>0.138426</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nAnd now heteroskedastic:\n\n::: {#f7f1f258 .cell execution_count=5}\n``` {.python .cell-code}\npd.DataFrame(\n  np.hstack([\n    our_mod.summary(se_correction='heteroskedastic')['Standard Error'].to_numpy().reshape(-1,1),\n    stats_mod.fit(cov_type='HC1').bse.reshape(-1,1)\n    ]),\n  columns=['Our Standard Errors','StatsModel Standard Errors'],\n)\n```\n\n::: {.cell-output .cell-output-display execution_count=11}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Our Standard Errors</th>\n      <th>StatsModel Standard Errors</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.126831</td>\n      <td>0.126831</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.108043</td>\n      <td>0.108043</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nAnd finally, cluster-robust:\n\n::: {#f96cb4bc .cell execution_count=6}\n``` {.python .cell-code}\npd.DataFrame(\n  np.hstack([\n    our_mod.summary(se_correction=cl)['Standard Error'].to_numpy().reshape(-1,1),\n    stats_mod.fit(cov_type='cluster', cov_kwds={'groups': cl}).bse.reshape(-1,1)\n    ]),\n  columns=['Our Standard Errors','StatsModel Standard Errors'],\n)\n```\n\n::: {.cell-output .cell-output-display execution_count=12}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Our Standard Errors</th>\n      <th>StatsModel Standard Errors</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.106352</td>\n      <td>0.106352</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.067777</td>\n      <td>0.067777</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n## Some final reflections\n\nSandwich estimators correct for the variance by adjusting the standard error *after* OLS coefficient estimation. However, another approach is to correct for the variance *before* estimation - by first applying a weight matrix to the data before fitting. You can read more about this in the [GLS post](gls_fgls_wls.html)\n\n\nFin.\n\n",
    "supporting": [
      "ols_sandwich_estimators_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}