{
  "hash": "fe157e588998b8515929aca8fc458134",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Sandwiches: robust covariance error estimation\"\nauthor: \"Chris Kelly\"\ndate: '02-22-24'\ncategories: [Linear Models, Gauss-Markov, Standard errors]\nimage: '../images/ols_sandwich_estimators.png'\nformat:\n  html:\n    code-fold: true\n    toc: true\n    other-links:\n      - text: Deriving the variance of the OLS coefficient\n        href: ols_blue.html#coefficient-variance-for-ols\n---\n\n::: {.callout-tip}\n### What are we exploring?\nEstimating the correct coefficient variance when relaxing homoskedastic error assumptions\n:::\n\n## Introducing sandwiches\n\nThe [variance for the OLS coefficient estimator](ols_blue.html#coefficient-variance-for-ols) is equal to the following:\n\n$$\n\\displaylines{\nV(\\hat{\\beta}) =\n(X^{\\intercal}X)^{-1}X^{\\intercal}E[\\epsilon\\epsilon^{\\intercal}]X(X^{\\intercal}X)^{-1}\n}\n$$\n\nThis can be though to as a sandwich:\n\n- The **â€œbreadâ€** either side: $(X^{\\intercal}X)^{-1}X^{\\intercal}$ on the left and its transpose $X(X^{\\intercal}X)^{-1}$ on the right\n- The **â€œmeatâ€** in the middle: what we assume for $E[\\epsilon\\epsilon^{\\intercal}]$\n    - Note that this is the same as the error variance, since $V[\\epsilon]=E[\\epsilon\\epsilon^{\\intercal}]-E[\\epsilon]E[\\epsilon^{\\intercal}]$ and $E[\\epsilon] = 0$\n\nOur coefficient will only be **efficient** if these assumptions about the expected error are correct! We will explore what happens when the errors are assumed to be homoskedastic, heteroskedastic or clustered.\n\n## Salmon bagel: Spherical Errors ðŸŸ\n\nUsual OLS is efficient [if the true model has \"spherical errors\"](ols_blue.html#coefficient-variance-assuming-spherical-errors). What does this mean in practice?\n\n- Errors are **homoskedastic**: $V(\\epsilon_i)=\\sigma^2$ for all observations\n- Errors are **serially uncorrelated**: $cov(\\epsilon_i,\\epsilon_{j\\neq i})=0$\n\nWhat does this look like for $E[\\epsilon\\epsilon^{\\intercal}]$?\n\n- The diagonal of the matrix is a constant value (scalar), $\\sigma^2$\n- The off-diagonals are all zero\n\n$$\n\\hat{\\sigma}^2\\underset{n\\times n}{I} = \n\\begin{bmatrix}\n\\hat{\\sigma}^2 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\ \n0 & \\hat{\\sigma}^2 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\ \n0 & 0 & \\hat{\\sigma}^2 & 0 & 0 & 0 & 0 & 0 & 0\\\\ \n0 & 0 & 0 & \\hat{\\sigma}^2 & 0 & 0 & 0 & 0 & 0\\\\ \n0 & 0 & 0 & 0 & \\hat{\\sigma}^2 & 0 & 0 & 0 & 0\\\\ \n0 & 0 & 0 & 0 & 0 & \\hat{\\sigma}^2 & 0 & 0 & 0\\\\ \n0 & 0 & 0 & 0 & 0 & 0 & \\hat{\\sigma}^2 & 0 & 0\\\\ \n0 & 0 & 0 & 0 & 0 & 0 & 0 & \\hat{\\sigma}^2 & 0\\\\ \n0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & \\hat{\\sigma}^2\n\\end{bmatrix}\n$$\n\nA good estimation of the constant error variance $\\sigma^2$ is to apply the standard formula to the residuals (i.e. method of moments):\n\n$$\n\\hat{\\sigma^2}=\\frac{1}{n-k}\\sum{\\hat{\\epsilon_i}^2} \\equiv \\frac{\\epsilon^{\\intercal}\\epsilon }{n-k} \n$$\n\n::: {.callout-tip}\n## Thus our \"salmon sandwich\" is:\n$$\n\\underset{k \\times k}{(X^{\\intercal}X)}^{-1}\n\\underset{k \\times n}\n{X^{\\intercal}}\n\\left(\\frac{1}{n-k} \\times \\underset{1 \\times 1}{(\\epsilon^{\\intercal}\\epsilon)}\n\\times\\underset{n \\times n}{I} \\right)\n\\underset{n \\times k}{X}\n\\underset{k \\times k}{(X^{\\intercal}X)}^{-1}\n$$\n:::\n\n::: {.column-margin}\nNote that under spherical errors, the sandwich can be simplified:\n$$\n\\displaylines{\n\\begin{align}\nV[\\hat{\\beta}] \n& = (X^{\\intercal}X)^{-1}X^{\\intercal}E[\\epsilon\\epsilon^{\\intercal}]X(X^{\\intercal}X)^{-1} \n\\\\ & = (X^{\\intercal}X)^{-1}X^{\\intercal}\\sigma^2IX(X^{\\intercal}X)^{-1} \n\\\\ & = \\sigma^2(X^{\\intercal}X)^{-1}\\cancel{X^{\\intercal}X}\\cancel{(X^{\\intercal}X)^{-1}} \n\\\\ & = \\sigma^2(X^{\\intercal}X)^{-1}\n\\end{align}\n}\n$$\n:::\n\nIn this scenario, the only things that impact the standard error of the coefficient $\\beta_k$ is:\n\n- The variance of all the residuals, $\\sigma^2$\n- The variance of the feature $V(X_k)$\n\n## Ham sarnie: Heteroskedastic errors ðŸ–\n\nHeteroskedastic correction is needed if:\n\n- **Errors vary for every individual**: $V(\\epsilon_i)=\\sigma_i^2$ for all observations\n- But they are still independent aka **serially uncorrelated**: $cov(\\epsilon_i,\\epsilon_{j\\neq i})=0$\n\nWhat does this look like for $E[\\epsilon\\epsilon^{\\intercal}]$?\n\n- The diagonal of the matrix is the estimate of variance which is unique for each observation,  $\\sigma_i^2$\n- The off-diagonals are all zero\n    \n    $$\n    \\underset{n \\times n}{\\sigma^2} = \\begin{bmatrix}\n    \\sigma_1^2 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\ \n    0 & \\sigma_2^2 & 0 & 0 & 0 & 0 & 0 & 0 & 0\\\\ \n    0 & 0 & \\sigma_3^2 & 0 & 0 & 0 & 0 & 0 & 0\\\\ \n    0 & 0 & 0 & \\sigma_4^2 & 0 & 0 & 0 & 0 & 0\\\\ \n    0 & 0 & 0 & 0 & \\sigma_5^2 & 0 & 0 & 0 & 0\\\\ \n    0 & 0 & 0 & 0 & 0 & \\sigma_6^2 & 0 & 0 & 0\\\\ \n    0 & 0 & 0 & 0 & 0 & 0 & \\sigma_7^2 & 0 & 0\\\\ \n    0 & 0 & 0 & 0 & 0 & 0 & 0 & \\sigma_8^2 & 0\\\\ \n    0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & \\sigma_{..}^2\\\\ \n    \\end{bmatrix}\n    $$\n    \n- A good estimation of the vector of heteroskedastic error variances $\\sigma^2$ is again to apply the standard formula to the residuals for each individual:\n\n$$\n\\hat{\\sigma_i^2}=\\frac{1}{n-k}\\sum{\\hat{\\epsilon_i}^2} \n$$\n\n::: {.callout-tip}\n## Thus our \"ham sandwich\" is:\n$$\n\\underset{k \\times k}{(X^{\\intercal}X)}^{-1}\n\\underset{k \\times n}{X^{\\intercal}}\n\\left(\\frac{1}{n-k} \\times \\underset{1 \\times n}{(\\epsilon\\odot \\epsilon)} ^{\\intercal}\n\\times\\underset{n \\times n}{I} \\right)\n\\underset{n \\times k}{X}\n\\underset{k \\times k}{(X^{\\intercal}X)}^{-1}\n$$\n:::\n\nSimilar to homoskedastic errors, the things that impacts the standard error of coefficient $k$ is:\n\n- The variance of all the individual errors, $\\sigma_i^2$\n- The variance of the feature $V(X_k)$\n\n## Cheese roll: Clustered Errors ðŸ§€\n\nCluster-robust errors are needed if:\n\n- **Errors vary for every individual**: $V(\\epsilon_i)=\\sigma_i^2$ for all observations i.e. still heteroskedastic\n- AND errors within the same cluster $C_l$ are **serially correlated**:\n$cov(\\epsilon_i,\\epsilon_{j}) \\neq 0 \\text{ if } \\epsilon_i,\\epsilon_j \\in C_l$\n    - Note - **errors between clusters are assumed not to be serially correlated** though i.e. $cov(\\epsilon_i,\\epsilon_{j}) =0 \\text{ if } \\epsilon_i \\in C_l,\\epsilon_j \\in C_{m \\neq l}$\n\n\nWhat does this look like for $E[\\epsilon\\epsilon^{\\intercal}]$?\n\n- The diagonal of the matrix is the estimate of variance which is unique for each observation,  $\\sigma_i^2$\n- The off-diagonals are also populated with the covariance - but only when they are both in the same cluster\n\nHere is an example where observations 1, 2 and 3 are in cluster A, observations 4 and 5 are in cluster B, observations 6, 7 and 8 are in cluster C etc.\n\n$$\n\\underset{n \\times n}{\\epsilon\\epsilon^{\\intercal}} \\sim \\begin{bmatrix}\n\\epsilon_1^2 & \\epsilon_1\\epsilon_2 & \\epsilon_1\\epsilon_3 & 0 & 0 & 0 & 0 & 0 & 0\\\\ \n\\epsilon_2\\epsilon_1 & \\epsilon_2^2 & \\epsilon_2\\epsilon_3 & 0 & 0 & 0 & 0 & 0 & 0\\\\ \n\\epsilon_3\\epsilon_1 & \\epsilon_3\\epsilon_2 & \\epsilon_3^2 & 0 & 0 & 0 & 0 & 0 & 0\\\\ \n0 & 0 & 0 & \\epsilon_4^2 & \\epsilon_4\\epsilon_5 & 0 & 0 & 0 & 0\\\\ \n0 & 0 & 0 & \\epsilon_5\\epsilon_4 & \\epsilon_5^2 & 0 & 0 & 0 & 0\\\\ \n0 & 0 & 0 & 0 & 0 & \\epsilon_6^2 & \\epsilon_6\\epsilon_7 & \\epsilon_6\\epsilon_8 & 0\\\\ \n0 & 0 & 0 & 0 & 0 & \\epsilon_7\\epsilon_6 & \\epsilon_7^2 & \\epsilon_7\\epsilon_8 & 0\\\\ \n0 & 0 & 0 & 0 & 0 & \\epsilon_8\\epsilon_6 & \\epsilon_8\\epsilon_7 & \\epsilon_8^2 & 0\\\\ \n0 & 0 & 0 & 0 & 0 & 0 & 0 & 0 & \\epsilon_{..}^2\\\\ \n\\end{bmatrix}\n$$\n\nAdditionally, we have to do a finite-sample correction as well on the degrees of freedom, based on the number of clusters $n_c$\n\n::: {.callout-tip}\n### Thus our cheese sandwich is:\n$$\n\\underset{k \\times k}{(X^{\\intercal}X)}^{-1}\n\\underset{k \\times n}{X^{\\intercal}}\n\\left(\n  \\underbrace{\\frac{n-1}{n-k}\\frac{n_c}{n_c-1}}_{\\text{Finite correction}}\n\\times \n\\underset{n \\times n}{(\\epsilon \\epsilon^{\\intercal})}\n\\right)\n\\underset{n \\times k}{X}\n\\underset{k \\times k}{(X^{\\intercal}X)}^{-1}\n$$\n:::\n\nIn this scenario, there are a few additional things that impact the standard error of coefficient $k$:\n\n- If errors are correlated within clusters, this will increase the error.\n- If features are correlated within clusters, this will also increase the error (due to the off-diagonals in the error variance matrix)\n- And if both the errors and feature correlations are the same sign, this will also increase the standard error.\n- As well as the variance of the individual errors, $\\sigma_i^2$, and the variance of each feature $V(X_k)$, as before\n\n## Coding it from scratch in python\n\nFirst we use the usual OLS class we defined previously in the [BLUE OLS post](ols_blue.html):\n\n::: {#09caf671 .cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np, pandas as pd\nfrom typing import Optional, Tuple\nfrom scipy.linalg import qr, solve_triangular\nfrom scipy.stats import norm, t\n\nclass OLS:\n\n    def _convert_types(self, z) -> np.ndarray:\n        \"\"\" Re-shape and convert y to numpy array to work nicely with rest of functions \"\"\"\n        if type(z) in [pd.DataFrame, pd.Series]:\n            z2 = z.to_numpy()\n        if type(z) == list:\n            z2 = np.array(z)\n        if type(z) == np.ndarray:\n            z2 = z\n        else:\n            raise TypeError('Array must be a pandas series/dataframe, numpy array or list')\n        return z2\n             \n    def _get_y(self, y: Optional = None) -> np.ndarray:\n        \"\"\"Re-shape and convert y to numpy array to work nicely with rest of functions\"\"\"\n        if y is None:\n            y = self.y\n        return self._convert_types(y).reshape(-1)\n\n    def _get_X(self, X: Optional = None) -> Tuple[np.ndarray, np.ndarray]:\n        \"\"\"Re-shape and convert X to numpy array to work nicely with rest of functions\n        Also return names for summarising in coefficient table\"\"\"\n        if X is None:\n            X = self.X\n        X2 = self._convert_types(X)\n        if type(X) == pd.DataFrame:\n            exog_names = np.array(X.columns)\n        elif type(X) == pd.Series:\n            exog_names = np.array(['Unamed Exog Feature'])\n        else:\n            exog_names = np.arange(X2.shape[1])\n        X2 = X2.reshape(-1,len(exog_names))\n        return (X2, exog_names)\n        \n    def __init__(\n        self, \n        y: Optional[np.ndarray] = None, \n        X: Optional[np.ndarray] = None\n        ) -> None:\n        \"\"\"Initializes the OLS class to run an least-squares regression\"\"\"\n        if y is not None:\n            self.y = self._get_y(y)\n            self.n = len(self.y)\n        if X is not None:\n            self.X, self.exog_names = self._get_X(X)\n            self.k = self.X.shape[1]\n        if y is not None and X is not None and len(self.y) != self.X.shape[0]:\n            raise ValueError(\"y and X must be the same size.\")\n        self.beta, self.RSS, self.beta_se, self.conf_int, self.test_stat, self.p_val = None, None, None, None, None, None\n\n    def _quick_matrix_invert(self, X: np.ndarray) -> np.ndarray:\n        \"\"\" Find the inverse of a matrix, using QR factorization \"\"\"\n        Q, R = qr(X)\n        X_inv = solve_triangular(R, np.identity(X.shape[1])).dot(Q.transpose())\n        return X_inv\n\n    def fit(\n        self,\n        y: Optional[np.ndarray] = None,\n        X: Optional[np.ndarray] = None,\n        intercept: bool = True,\n    ):\n        \"\"\"Estimates the OLS coefficients given a vector y and matrix X\"\"\"\n        # Import data\n        y = self._get_y(y)\n        X, exog_names = self._get_X(X)\n        if y is None or X is None:\n            raise ValueError('X and y is required for fitting')\n        if len(y) != X.shape[0]:\n            raise ValueError(\"y and X must be the same size.\")\n        # Store some metadata\n        self.y, self.X, self.exog_names = y, X, exog_names\n        self.n, self.k = X.shape\n        self.DoF = self.n - self.k\n        # Estimate coefficients\n        XTX = X.T.dot(X)\n        XTY = X.T.dot(y)\n        XTX_inv = self._quick_matrix_invert(XTX)\n        coefs = XTX_inv.dot(XTY)\n        # Store estiamtions\n        self.beta, self.var_X_inv = (coefs, XTX_inv)\n\n    def _check_if_fitted(self):\n        \"\"\"Quick helper function that raises an error if the model has not been fitted already\"\"\"\n        if self.beta is None:\n            raise ValueError('Need to fit the model first - run fit()')\n        else:\n            return True\n\n    def predict(\n        self,\n        X: Optional[np.ndarray] = None,\n    ) -> np.ndarray:\n        \"\"\"Predict values for y. Returns fitted values if X not provided.\"\"\"\n        self._check_if_fitted()\n        X2, exog_names = self._get_X(X)\n        y_hat = X2.dot(self.beta)\n        if X is None:\n            self.y_hat = y_hat\n        return y_hat\n\n    def assess_fit(\n        self,\n        y: Optional[np.ndarray] = None,\n        X: Optional[np.ndarray] = None,\n    ) -> float:\n        \"\"\"Returns the unadjusted R^2\"\"\"\n        self._check_if_fitted()\n        if (y is None and X is not None) or (y is not None and X is None):\n            raise ValueError('Need to either provide both X and y, (or provide neither and R^2 is based on the X and y used for fitting)')\n        else:\n            y, (X, exog_names) = self._get_y(y), self._get_X(X)\n        y_hat = self.predict(self.X)\n        residuals = (y - y_hat).reshape(-1, 1)\n        RSS = residuals.T.dot(residuals)\n        TSS = (y - y.mean()).T.dot(y - y.mean())\n        unadj_r_squared = 1 - RSS/TSS\n        \n        if (y == self.y).all() and (X == self.X).all():\n            self.residuals = residuals\n            self.RSS = RSS\n            self.TSS = TSS\n            self.unadj_r_squared = unadj_r_squared\n        return unadj_r_squared\n\n    def standard_error(self,) -> np.ndarray:\n        \"\"\"Returns the standard errors for the coefficients from the fitted model\"\"\"\n        if self.RSS is None:\n            self._check_if_fitted()\n            self.assess_fit()\n        sigma_sq = self.RSS / float(self.DoF) * np.identity(len(self.beta))\n        var_b = sigma_sq.dot(self.var_X_inv)\n        self.beta_se = np.sqrt(np.diag(var_b))\n        return self.beta_se\n\n    def confidence_intervals(self, size = 0.95):\n        \"\"\"Returns the confidence intervals for the coefficients from the fitted model\"\"\"\n        if self.beta_se is None:\n            self._check_if_fitted()\n            self.standard_error()\n        alpha = 1-(1-size)/2\n        self.conf_int = np.array([\n            self.beta - t.ppf(alpha, self.DoF) * self.beta_se,\n            self.beta + t.ppf(alpha, self.DoF) * self.beta_se\n        ])\n        return self.conf_int\n\n    def test_statistic(self) -> np.ndarray:\n        \"\"\"Returns the test statistics for the coefficients from the fitted model\"\"\"\n        if self.conf_int is None:\n            self._check_if_fitted()\n            self.conf_int = self.confidence_intervals()\n        self.test_stat = self.beta.flatten() / self.beta_se\n        return self.test_stat\n\n    def p_value(self, z_dist: bool = False) -> np.ndarray:\n        \"\"\"Returns the p-values for the coefficients from the fitted model.\"\"\"\n        if self.test_stat is None:\n            self._check_if_fitted()\n            self.test_stat = self.test_statistic()\n        if z_dist:\n            self.p_val = [norm.cdf(-abs(z)) + 1 - norm.cdf(abs(z)) for z in self.test_stat]\n        else:\n            self.p_val = [2 * t.sf(abs(x), self.DoF) for x in self.test_stat]\n        return self.p_val\n\n    def summary(self, z_dist: bool = False) -> pd.DataFrame:\n        \"\"\"Returns the coefficients, standard errors, test statistics and p-values in a Pandas DataFrame.\"\"\"\n        if self.p_val is None:\n            self._check_if_fitted()\n            self.p_value(z_dist)\n        summary = pd.DataFrame(\n            data={\n                'Coefficient': self.beta.flatten(),\n                'Standard Error': self.beta_se,\n                'Lower bound': self.conf_int[0],\n                'Upper bound': self.conf_int[1],\n                'test-statistic': self.test_stat,\n                'p-value': self.p_val,\n            },\n            index=self.exog_names,\n        )\n        return summary    \n```\n:::\n\n\nNow we can define a sandwich class, that adapts the standard errors of the existing OLS regression model:\n\n::: {#3da4a50e .cell execution_count=2}\n``` {.python .cell-code}\nclass sandwich():\n\n    def __init__(self, model: OLS) -> None:\n        \"\"\"The sandwich class adapts the standard errors of existing OLS regressions\"\"\"\n        if not model._check_if_fitted():\n            raise ValueError('Fit model before using with Sandwich estimator')\n        else:\n            self.model = model\n\n    def _sandwich(self, meat: np.ndarray, bread: Optional[np.ndarray] = None) -> None:\n        \"\"\"Helper function to return a 'sandwich' from bread and meat\"\"\"\n        self.model.beta_se = None\n        self.model.conf_int = None\n        self.model.test_stat = None\n        self.model.p_val = None\n        if bread is None:\n            bread = self.model.var_X_inv\n        sandwich = bread.dot(meat).dot(bread)\n        self.model.beta_se = np.sqrt(np.diag(sandwich))\n    \n    def homoskedastic(self) -> pd.DataFrame:\n        \"\"\"Return the table of coefficients assuming homoskedastic error variance\"\"\"\n        e = self.model.residuals\n        ee = e.T.dot(e) / float(self.model.DoF)\n        XeeX = (X.T * ee).dot(X)\n        self._sandwich(XeeX)\n        return self.model.summary()\n\n    def heteroskedastic(self) -> np.ndarray:\n        \"\"\"Return the coefficient standard errors assuming heteroskedastic error variance ('HC0')\"\"\"\n        ee = np.diagflat(self.model.residuals**2)\n        XeeX = X.T.dot(ee).dot(X)\n        self._sandwich(XeeX)\n        return self.model.summary()\n\n    def cluster_robust(self, clusters: np.ndarray) -> np.ndarray:\n        \"\"\"Return the coefficient standard errors allowing for clustered error variance (and heteroskedasticity)\"\"\"\n        def cluster_XeeX(cluster_index):\n            j = clusters == cluster_index\n            _X, _e = self.model.X[j, :], self.model.residuals[j]\n            _eX = _e.T.dot(_X)\n            _XeeX = _eX.T.dot(_eX)\n            return _XeeX\n        clusters = clusters.flatten()\n        cluster_XeeX = [cluster_XeeX(i) for i in np.unique(clusters)]\n        n_cl = len(np.unique(clusters))\n        n, k = self.model.n, self.model.k\n        # finite-sample correction factor.    # sum XeeX across all clusters\n        meat = ((n - 1) / (n - k)) * (n_cl / (n_cl - 1)) * np.sum(cluster_XeeX, axis=0)\n        # summed across all clusters - requires averaging to ensure V is consistent, so needs many clusters\n        # https://cameron.econ.ucdavis.edu/research/Cameron_Miller_JHR_2015_February.pdf (p.7-9)\n        self._sandwich(meat)\n        return self.model.summary()    \n```\n:::\n\n\nNow let's test it! We will generate some data, run the OLS model, and compare our sandwich estimators to statsmodels:\n\n::: {#e732f1a9 .cell execution_count=3}\n``` {.python .cell-code}\nnp.random.seed(42)\nn, k = 50, 2\nsigma_sq = 1\nbeta = np.random.normal(size=(k,1))\nX = np.hstack([ \n    np.ones(n).reshape(n,1),\n    np.random.normal(size=(n,k-1)) \n    ])\ny = X.dot(beta) + np.random.normal(loc=0,scale=sigma_sq,size=(n,1))\ncl = np.repeat(np.arange(10), 5)\n\nOLS_model = OLS(y,X)\nOLS_model.fit()\nOLS_model.summary()\n\nimport statsmodels.api as sm\n\nstats_mod = sm.OLS(y, X)\n```\n:::\n\n\nFirst let's try homoskedastic errors:\n\n::: {#7d9e36be .cell execution_count=4}\n``` {.python .cell-code}\nour_mod = sandwich(OLS_model)\n\npd.DataFrame(\n  np.hstack([\n    our_mod.homoskedastic()['Standard Error'].to_numpy().reshape(-1,1),\n    stats_mod.fit().bse.reshape(-1,1)\n    ]),\n  columns=['Our Standard Errors','StatsModel Standard Errors'],\n)\n```\n\n::: {.cell-output .cell-output-display execution_count=25}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Our Standard Errors</th>\n      <th>StatsModel Standard Errors</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.131679</td>\n      <td>0.131679</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.138426</td>\n      <td>0.138426</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nAnd now heteroskedastic:\n\n::: {#6bbd9fc1 .cell execution_count=5}\n``` {.python .cell-code}\npd.DataFrame(\n  np.hstack([\n    our_mod.heteroskedastic()['Standard Error'].to_numpy().reshape(-1,1),\n    stats_mod.fit(cov_type='HC0').bse.reshape(-1,1)\n    ]),\n  columns=['Our Standard Errors','StatsModel Standard Errors'],\n)\n```\n\n::: {.cell-output .cell-output-display execution_count=26}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Our Standard Errors</th>\n      <th>StatsModel Standard Errors</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.124268</td>\n      <td>0.124268</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.105861</td>\n      <td>0.105861</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nAnd finally, cluster-robust:\n\n::: {#db767b19 .cell execution_count=6}\n``` {.python .cell-code}\npd.DataFrame(\n  np.hstack([\n    our_mod.cluster_robust(cl)['Standard Error'].to_numpy().reshape(-1,1),\n    stats_mod.fit(cov_type='cluster', cov_kwds={'groups': cl}).bse.reshape(-1,1)\n    ]),\n  columns=['Our Standard Errors','StatsModel Standard Errors'],\n)\n```\n\n::: {.cell-output .cell-output-display execution_count=27}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Our Standard Errors</th>\n      <th>StatsModel Standard Errors</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.106352</td>\n      <td>0.106352</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.067777</td>\n      <td>0.067777</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nFin.\n\n",
    "supporting": [
      "ols_sandwich_estimators_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}