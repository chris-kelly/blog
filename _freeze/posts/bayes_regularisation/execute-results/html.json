{
  "hash": "0fe7b2f706a1a106a98a197a5a91ee5a",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Bayesian: using priors for regularisation\"\nauthor: \"Chris Kelly\"\ndate: '02-21-24'\ncategories: []\nformat:\n  html:\n    code-fold: true\n    toc: true\ndraft: true\n---\n\n\n# Regularization\n\nRegularization applies a penalty term to the cost function when large coefficients are estimated. For example, we can alter the RSS (residual sum of squares) cost function in the following way:\n\n$$\n\\min_\\beta{\\left[\\sum_{i=1}^N{\\epsilon_i^2}-\\frac{\\lambda}{2}\\sum_{k=1}^K{\\beta_k^2}\\right]}\n$$\n\nThe penalty term $\\lambda /2$ increases the cost if larger coefficients are estimated, resulting in smaller coefficients being estimated to minimize the the cost function:\n\n$$\n\\min_{\\beta_j}{\\left[\\sum_{i=1}^N{\\epsilon_i^2}-\\frac{\\lambda}{2}\\sum_{k=1}^K{\\beta_k^2}\\right]}\n\\Rightarrow \\sum_{i=1}^N 2\\epsilon_i\\left(\\frac{\\partial\\epsilon_i}{\\partial\\beta_j}\\right)-\\lambda\\beta_j=0 \\\\\n$$\n\nAs such, we see that minimizing the cost function will artificially reduce the coefficents depending on the size of $\\lambda$. This is an example of L2 regularization (ridge). Selecting a good value for $\\lambda$ can be found through cross-validation.\n\nHowever, L2 regularization will never zero-out a coefficient. Instead, we can similarly derive the same penalty through L1 regularisation (LASSO) which actively zeroes out coefficients:\n\n$$\n\\min_\\beta{\\left[\\sum_{i=1}^N{\\epsilon_i^2}-\\frac{\\lambda}{2}\\sum_{k=1}^K{\\mid\\beta_k\\mid}\\right]}\n$$\n\nEmpirically though, LASSO is shown to be unreliable, zero-es out coefficients that are small but significant.\n\nA more robust method is the elastic net, which combines L1 and L2 regularization. It is often implemented in the following way:\n\n$$\n\\min_\\beta{\\left[\\sum_{i=1}^N{\\epsilon_i^2}-\\frac{\\lambda}{2}\\left(\\alpha\\sum_{k=1}^K{\\mid\\beta_k\\mid}+(1-\\alpha)\\sum_{k=1}^K{\\beta_k^2}\\right)\\right]}\n$$\n\n\nHowever, regularization in a frequentist model can be a blunt instrument when thinking about regularization of coefficients. This is because $\\lambda$ is a single penalty for all coefficients, so if poorly specified, can either include a very large number of irrelevant predictors, or over-shink included coefficients. In particular, it might drop small but signficant features.\nAn ideal method should thus only induce weak shrinkage on large coefficients, and stronger shrinkage to zero on less relevant effects.\nCue bayesian regression:\n\n# Maximising likelihood to solve linear regression:\n\nFirst let's solve the original linear regression problem by maximum likelihood as per the bayesian paradigm. Rather than simply minimizing the residual sum of squares (as usual with the OLS loss function), we want to find the beta that maximises the likelihood of observing the evidence we have, knowing $y \\sim N(X'\\beta,1)$.\nIn other words, the probability of observing $y$ given our data and estimated model parameters is a function of the normal probability density of our squared residuals:\n\n$$\np(y|\\beta,X) = \\prod_{i=1}^{n}{\\frac{1}{\\sqrt{2\\pi}}}e^{-\\frac{1}{2}\\epsilon_i^2}\n$$\nWe can take the negative log of the likelihood function to make it easier to differentiate:\n\n$$\n\\max_\\beta{p(y|\\beta,X)} \\Rightarrow \\min_\\beta\\left[{-\\log{\\left(\\prod_{i=1}^{n}{\\frac{1}{\\sqrt{2\\pi}}}e^{-\\frac{1}{2}\\epsilon_i^2}\\right)}}\\right] = \\\\\n\\min_\\beta{\\left[ -\\sum{\\log{\\left(\\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{1}{2}\\epsilon_i^2}\\right)}} \\right]} \\\\\n= \\min_\\beta{\\left[ -\\sum{\\log{\\left(\\frac{1}{\\sqrt{2\\pi}}\\right)}} + -\\sum{\\log{\\left(e^{-\\frac{1}{2}\\epsilon_i^2}\\right)}} \\right]} \\\\\n= \\min_\\beta{\\left[ -\\sum{\\log{((2\\pi)^{-\\frac{1}{2}})}} + -\\sum{\\left(-\\frac{1}{2}\\epsilon_i^2\\right)} \\right]} \\\\\n= \\min_\\beta{\\left[ \\frac{1}{2}\\sum{\\log{(2\\pi)}} + -\\sum{\\left(-\\frac{1}{2}(y_i-X_i'\\beta)^2\\right)} \\right]} \\\\\n= \\min_\\beta{\\left[ \\frac{1}{2}\\sum{\\log{(2\\pi)}} + \\frac{1}{2}(Y-X\\beta)^T(Y-X\\beta) \\right]} \\\\\n= \\min_\\beta{\\left[ \\frac{1}{2}\\sum{\\log{(2\\pi)}} + \\frac{1}{2}\\epsilon^T\\epsilon \\right]} \\\\\n\\leftrightarrow \\beta^*=\\arg\\min_\\beta{\\left[ \\frac{1}{2}\\epsilon^T\\epsilon \\right]}\n$$\nNow when we minimise the log-likelihood cost function by differentiating it with respect to $\\beta$ and setting it to zero in order to derive the optimum coefficient, the constant $\\log{(2\\pi)}$ drops out, and we are left with differentiating $\\frac{d}{d\\beta}\\epsilon^T\\epsilon=0$ - the exact equivalent as with frequentist OLS.\nWe can rewrite this in terms of the bayesian paradigm if we think of $\\beta$ as a random variable (rather than a fixed quantity as per frequenist thinking):\n$$\np(\\beta|X,Y)=\\frac{p(Y|\\beta,X)p(\\beta|X)}{p(Y|X)}=\\frac{p(Y|\\beta,X)p(\\beta|X)}{\\int p(Y|X,\\beta)p(\\beta|X)d\\beta}\n$$\nWhere:\n\n* $p(Y|\\beta,X)$ is the likelihood function (where we maximise the log-likelihood as above)\n* $p(Y|X)$ is the evidence (the data we feed into the model)\n* $p(\\beta|X)$ is the prior\n\nIf we assume $\\beta$ is fixed, then $p(\\beta|X)=1$, and thus we get $= \\min_\\beta{\\left[\\epsilon^T\\epsilon \\right]} \\Rightarrow \\beta^*=(X^TX)^{-1}X^TY$ (as per OLS).\n\n# Using informative priors to apply regularization:\n\nHowever, we might want to pick a more informative prior for two reasons (1) including past evidence from previous models, that we want to update over time with new evidence or (2) we want to eliminate some of our predictors to create a more parsimonious model in a systematic way.\n\nFor (2), picking a prior that is concentrated at zero can help achieve this - for example, using a laplace distribution:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nplot(x=seq(-10,10,0.1), y = sapply(seq(-10,10,0.1), FUN = function(x) exp(-abs(x)/0.5)/(2*0.5)), type='l',main='Laplace(gamma=0.5)')\n```\n\n::: {.cell-output-display}\n![](bayes_regularisation_files/figure-html/unnamed-chunk-1-1.png){width=864}\n:::\n:::\n\n\nThis is given by the following density function:\n\n$$\np(\\beta|X) = p(\\beta) = \\prod_{k=1}^{K}{\\frac{1}{2\\gamma}e^{-\\mid\\beta\\mid/{\\gamma}}}\n$$\nWhich if set as the prior $p(\\beta|X)$, results in the following:\n\n$$\n\\max_\\beta{p(\\beta|y,X)} = \\max_\\beta{p(y|beta,X)p(\\beta|X)} \\Rightarrow \\\\\n= \\min_\\beta{\\left[-\\log{(p(y|\\beta,X)p(\\beta|X))}\\right]} = \\min_\\beta\\left[{-\\log{(p(y|\\beta,X))} -\\log{(p(\\beta|X))}}\\right] \\\\\n= \\min_\\beta{\\left[ \\frac{1}{2}\\sum{\\log{(2\\pi)}} + \\frac{1}{2}\\epsilon^T\\epsilon -\\log{\\left(\\prod_{k=1}^{K}{\\frac{1}{2\\gamma}e^{-\\mid\\beta\\mid/{\\gamma}}}\\right)} \\right]} \\\\\n= \\min_\\beta{\\left[ \\frac{1}{2}\\sum{\\log{(2\\pi)}} + \\frac{1}{2}\\epsilon^T\\epsilon -\\sum_{k=1}^{K}{\\log{\\left(\\frac{1}{2\\gamma}e^{-\\mid\\beta\\mid/{\\gamma}}\\right)}} \\right]} \\\\\n= \\min_\\beta{\\left[ \\frac{1}{2}\\sum{\\log{(2\\pi)}} + \\frac{1}{2}\\epsilon^T\\epsilon -\\sum_{k=1}^{K}{\\log{\\left(\\frac{1}{2\\gamma}\\right)}} -\\sum_{k=1}^{K}{\\log{\\left(e^{-\\mid\\beta\\mid/{\\gamma}}\\right)}} \\right]} \\\\\n= \\min_\\beta{\\left[ \\frac{1}{2}\\sum{\\log{(2\\pi)}} + \\frac{1}{2}\\epsilon^T\\epsilon -\\sum_{k=1}^{K}{\\log{\\left(\\frac{1}{2\\gamma}\\right)}} +\\frac{1}{\\gamma}\\sum_{k=1}^{K}{\\mid\\beta\\mid} \\right]} \\\\\n\\leftrightarrow \\beta^*=\\arg\\min_\\beta{\\left[ \\frac{1}{2}\\epsilon^T\\epsilon +\\frac{1}{\\gamma}\\sum_{k=1}^{K}{\\mid\\beta\\mid} \\right]}\n$$\nWhen we minimize with respect to beta, the constants drop out, so we are left with minimizing $\\min_\\beta{\\left[\\epsilon^T\\epsilon + (2/\\gamma)\\sum_{k=1}^{K}{\\mid\\beta\\mid} \\right]} \\\\$, which is equivalent to L1 Lasso regresion (where $2/\\gamma$ is the regularization parameter).\n(Note that if we set the prior instead to the normal centered at zero, then we derive L2 ridge regression).\n\nHowever, the 'LASSO prior' is not truly sparse when the likelihood function yields a non-zero coefficent. Although there is sparsity in the mode of the prior, when combining this with the likelihood distribution, the sparsity does not continue into the posterior distribution.\n\n!SHOW GRAPH OF THIS!\n\nConsequently, we can combine a discrete and a continuous prior to construct a 'spike-and-slab' prior which fully zero-es out small coefficients:\n",
    "supporting": [
      "bayes_regularisation_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}