{
  "hash": "6fccb343af2333daecc1216d3fd6fd2b",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Logistic Regression as a latent variable model\"\nauthor: \"Chris Kelly\"\ndate: '02-25-24'\ncategories: [Maximum Likelihood, Generalized Linear Models]\nformat:\n  html:\n    code-fold: true\n    toc: true\nimage: '../images/glm_logistic.png'\n---\n\n\n\n\n::: {.callout-tip}\n### What are we exploring?\nTaking a \"latent variable\" approach to modelling bernoulli probabilities using maximum likelihood estimation and the logit link function.\n:::\n\n## A latent variable model\n\nImagine a basketball player is taking free throws.\n\nFor each trial $i$ (a freethrow attempt), we observe the outcome $y_i$ as being 1 if there is a success (i.e. the freethrow is made), or 0 if it is a failure (a miss). \n\nWhen the player takes the freethrow, there are exogenous features $X_i$ that influence their likelihood of making it (e.g. whether they are home or away). Also there is some randomness, $\\varepsilon_i$ that is unpredictable: a 99% accurate shooter still has a 1% chance of missing.\n\nEconomists often frame this problem as a \"latent variable model\". What this means is they frame the problem as though there is a hidden continuous variable we do not observe, $y_i^*$, and if it exceeds a certain threshold (usually zero) then there is a success (e.g. the freethrow is made):\n\n$$\n\\displaylines{\n\\begin{align}\ny_i^* & = X_i\\beta + \\varepsilon_i\n\\\\ \\\\\ny_i & = \n\\begin{cases}\n  1 & \\text{if}\\ y_i^* > 0 & \\text{i.e. } -\\varepsilon_i > X_i\\beta\\\\\n  0 & \\text{otherwise}\n\\end{cases}\n\\end{align}\n}\n$$\n\nIn other words, the latent variable $y_i^*$ is purely a function of its predictors $X_i$, their relationship to $y_i^*$ given by $\\beta$, and some additive noise $\\varepsilon_i$. If $y_i^*$ is positive, we observe a success (i.e. $y_i = 1$).\n\nWe can thus formulate the probability of success $P(y_i=1|X)$ as the likelihood that the additive noise $\\varepsilon_i$ is less than $X\\beta$, resulting in the probability $y_i^*$ above zero:\n\n$$\n\\displaylines{\n\\begin{align}\np_i & = P(y_i=1|X_i) \\\\\n& = P(y_i^* > 0 | X_i)\n\\\\\n& = P(X_i\\beta + \\varepsilon_i > 0)\n\\\\\n& = P(\\varepsilon_i > -X_i\\beta )\n\\\\\n& = P(\\varepsilon_i < X_i\\beta ) & \\iff \\varepsilon_i \\sim f(\\mu,s) \\text{ is symmetric}\n\\end{align}\n}\n$$\n\nSo a good assumption for the probabilistic process that generates the error $\\varepsilon_i$ is very important! We will come back to this shortly, after discussing the Bernoulli probability mass function.\n\n## The Bernoulli PMF\n\nEven if a player has a 99% of making freethrows, they would be expected to miss 1 in 100. We can capture this through the probability mass function of the Bernoulli distribution:\n\n$$\n\\displaylines{\n\\begin{align}\nP(y_i) = \n\\begin{cases}\n  p_i & \\text{if}\\ y_i=1 \\\\\n  1-p_i & \\text{if}\\ y_i=0\n\\end{cases}\n\\end{align}\n}\n$$\n\nWhich is equivalent to the following:\n\n$$\n\\displaylines{\n\\begin{align}\nP(y_i) \n& = p_i^{y_i}(1-p_i)^{1-y_i} \\\\ \\\\\n& = P(\\varepsilon_i < X_i\\beta)^{y_i}\n(1-P(\\varepsilon_i < X_i\\beta))^{1-y_i}\n\\end{align}\n}\n$$\n\n:::{.column-margin}\nSince $g(x)^0 = 1$:\n\n* if $y_i=1$, $p^{1}(1-p)^{0} = p$\n* if $y_i=0$, $p^{0}(1-p)^{1} = 1-p$\n:::\n\nUntil now, we have just been looking at the likelihood of making a success for a single trial $i$. But we want to find values for $\\beta$ that optimize predictions across all trials, to learn the impact of $X$ so we can better predict $y$ next time - aka maximum likelihood estimation.\n\n## Maximum Likelihood Estimation\n\nAssuming each trial is independent (a big assumption for free throws!) - the probability of making two then missing one is $p_i \\times p_i \\ \\times (1-p_i)$. In other words - the combined probability is the multiplication of the individual probabilities.\n\nWe can generalize this to a sample size $N$ as the following:\n\n$$\n\\displaylines{\n\\begin{align}\np(y|X) \n& =\n\\prod_{i=1}^{N}{ \n  p_i^{y_i} (1-p_i)^{1-y_i}\n  }\n\\\\ & =\n\\underbrace{\n  \\prod_{y_i=0}^{n_1}{ p_i^{y_i} }\n}_{y_i=1} \\times\n\\underbrace{\n  \\prod_{y_i=1}^{n_0}{ (1-p_i)^{1-y_i} }    \n}_{y_i=0}\n\\end{align}\n}\n$$\n\nIn practice, it is common to minimize the negative log-likelihood, which is shown to be equivalent to maximising the likelihood directly (since the logarithm is a monotonic function):\n\n$$\n\\displaylines{\n\\begin{align}\n& \\max_\\beta{p(y|X)} \\\\\n= & \n\\max_\\beta{\\left\\{ \n  \\prod_{i=1}^{N}{ \\bigg(\n    p_i^{y_i} \\times (1-p_i)^{1-y_i}\n  \\bigg)}\n  \\right\\}}\n\\\\ \\\\ \\equiv & \n\\min_\\beta{\\left\\{ -\\log{ \\left[\n  \\prod_{i=1}^{N}{ \\bigg(\n    p_i^{y_i}  \\times (1-p_i)^{1-y_i}\n  \\bigg) }\n  \\right] } \\right\\}}\n\\\\ \\\\ = & \n\\min_\\beta{\\left\\{\n  -\\sum_{i=1}^{N}{ \\bigg(\n    \\log{ \\left[ p_i^{y_i} \\right] } } +\n    \\log{ \\left[ (1-p_i)^{1-y_i}\\right] }\n    \\bigg)\n  \\right\\}}  \n\\\\ \\\\ = & \n\\min_\\beta{\\left\\{\n  -\\sum_{i=1}^{N}{ \\bigg(\n    y_i\\log{ \\left[ p_i \\right] } \n    (1-y_i)\\log{ \\left[ 1-p_i\\right] } \n  \\bigg) }\n  \\right\\}}\n\\\\ \\\\ = & \n\\min_\\beta{\\left\\{\n  -\\sum_{i=1}^{N}{ \\bigg(\n    y_i \\log{ [ P(\\varepsilon_i < X \\beta) ] } + \n    (1-y_i) \\log{ [ \n      1-P(\\varepsilon_i < X \\beta)] }\n    } \\bigg)\n  \\right\\}}    \n\\end{align}\n}\n$$\n\n:::{.column-margin}\nRecall that $\\log{\\left(ab\\right)} = \\log{\\left(a\\right)} + \\log{\\left(b\\right)}$\n:::\n\nWe now - almost - have a well defined problem we can solve! We just need to make an assumption for the distribution of errors $\\varepsilon$.\n\n## Assuming errors come from a logistic distribution\n\nThe errors are often assumed to be generated from a logistic distribution, with location parameter $\\mu=0$ and scale parameter $s=1$:\n\n$$\n\\epsilon \\sim \\text{Logistic}(0,1)\n$$\n\nWhy a logistic? Well because is highly similar to a normal distribution, but with fatter tails, so its seen as an approximation that is more robust. Furthermore, it has easier algebra to unpick - which we will show shortly.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlogistic_func <- function(x) {\n  return( exp(-x)*(1+exp(-x))^-2 )\n}\nnormal_func <- function(x) {\n  return( (2*pi)^(-0.5) * exp(-0.5 * x^2) )\n}\nx = seq(-10,10,0.1)\nplot(\n  x=x,\n  y=normal_func(x),\n  type='l',col=\"blue\",lty=1\n  )\nlines(\n  x=x,\n  y=logistic_func(x),\n  type='l',col=\"red\",lty=1\n)\nlegend(\n  -10,0.4,\n  legend=c(\n    \"Normal PDF\",\n    \"Logistic function\"\n    ),\n  col=c(\"blue\",\"red\"),\n  lty=c(1,1)\n  )\n```\n\n::: {.cell-output-display}\n![](glm_logit_files/figure-html/unnamed-chunk-1-1.png){width=864}\n:::\n:::\n\n\nFor now, you might recall the pdf of the logistic distribution is:\n\n$$\n\\displaylines{\n\\begin{align}\nf(x,\\mu,s) & = \n\\frac{e^{-(x-\\mu)/s}}\n{s(1+e^{-(x-\\mu)/s})^2}\n\\\\ \\\\\n\\therefore f(x,0,1) & = \n\\frac{e^{-x}}\n{(1+e^{-x})^2}\n\\end{align}\n}\n$$\n\nThen we can obtain the CDF by taking the integral of the pdf:\n\n$$\n\\displaylines{\n\\begin{align}\n\\int {f(x,0,1) \\,dx} & = \n\\int{\n  \\frac{e^{-x}}\n  {(1+e^{-x})^2}\n\\,dx}\n\\\\ \\\\\n\\text{let } u = 1+e^{-x} & \\therefore \\frac{du}{dx} = -e^{-x} \n\\\\\n& \\therefore dx = -\\frac{du}{e^{-x}}\n\\\\ \\\\ \\therefore\n\\int {f(x,0,1) \\,dx} & = \n\\int {\\frac{e^{-x}}{u^2} \\times -\\frac{du}{e^{-x}}}\n\\\\ & \n= \\int {-u^{-2}\\,du}\n\\\\ \n& = u^{-1} + c\n\\\\ \n& = (1+e^{-x})^{-1} + c\n\\end{align}\n}\n$$\n\nAnd hence, we derive the \"logisitic function\":\n\n$$\n\\displaylines{\n\\begin{align}\np(y_i|X_i) \n& = p(\\varepsilon_i < X_i\\beta) \\\\\n& = (1+e^{-X_i\\beta})^{-1} \\\\\n& = \\frac{1}{1+e^{-X_i\\beta}} \\\\ \\\\\n& = \\text{logistic}(X_i\\beta)\n\\end{align}\n}\n$$\n\nSo we now have a mapping of $X$ to $y$, given by $\\beta$ and the activation function: the \"logistic function\".\n\nWe can see that this activation function \"squashes\" all outputs $X\\beta \\in [-\\infty,\\infty]$ between 0 and 1:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlogistic_dist <- function(x) {\n  return( (1+exp(-x))^-1 )\n}\nx = seq(-10,10,0.1)\nplot(\n  x=x,\n  y=logistic_dist(x),\n  type='l',col=\"blue\",,lty=1\n  )\nlines(\n  x=seq(-3,3,0.1),\n  y=(1/4)*seq(-3,3,0.1)+0.5,\n  type='l',col=\"red\",lty=2\n)\nlegend(\n  -10,1,\n  legend=c(\n    \"logistic activation\",\n    \"linear activation\"\n    ),\n  col=c(\"blue\",\"red\"),\n  lty=c(1,2)\n  )\n```\n\n::: {.cell-output-display}\n![](glm_logit_files/figure-html/unnamed-chunk-2-1.png){width=864}\n:::\n:::\n\n\n:::{.column-margin}\nFor probabilities of between 0.3 to 0.7, we see that the logistic activation function maps very closely to that of a simply linear one. It is only at the more extreme probabilities that they diverge.\n:::\n\n## Linearity in terms of log-odds!\n\nTo further intuition, it can also be useful to rearrange the regression in terms of $X_i\\beta$.\n\nBy doing this, we find that we are fitting a model where the \"log-odds\" are linearly related to its predictors:\n\n:::{.column-margin}\nLog-odds means taking the logarithm of the probability of success divided by the probability of failure\n:::\n\n$$\n\\displaylines{\n\\begin{align}\np(y_i|X_i) = p_i & = \\frac{1}{1+e^{-X_i\\beta}} \\\\\n\\therefore 1 + e^{-X_i\\beta} & = \\frac{1}{p_i} \\\\\n\\therefore e^{-X_i\\beta} & = \\frac{1}{p_i} - \\frac{p_i}{p_i} = \\frac{1-p_i}{p_i} \\\\\n\\therefore e^{X_i\\beta} & = \\frac{p_i}{1-p_i} \\\\\n\\therefore \\ln{\n  \\left\\{ \\frac{p_i}{1-p_i} \\right\\}\n  } & = X_i\\beta \n\\end{align}\n}\n$$\n\nThis is a \"link function\" - the link between the outcome, $y$, and the linear predictors $X$ via $\\beta$. This specific link function is called the \"logit function\".\n\nAnd so it is now clear the inverse logit is the logistic function:\n\n$$\n\\displaylines{\n\\begin{align}\n\\text{logit}(x) & = \\ln{\n  \\left\\{ \\frac{x}{1-x} \\right\\}\n  } \\\\ \n\\text{logistic}(x)\n& = \\frac{1}{1+e^{-x}} \\\\\n& = \\text{logit}^{-1}(x) \\\\\n\\end{align}\n}  \n$$\n\n<!-- \n$$\n\\displaylines{\n\\begin{align}\nP(y|X) = p = &\n-\\sum_{i=1}^{N}{ y_i\\log{ \\left[ \n  P(\\varepsilon_i < X \\beta) \n  \\right] } + (1-y_i)\\log{ \\left[ \n  1-P(\\varepsilon_i < X \\beta)\n  \\right] } }\n\\\\ \n= &\n-\\sum_{i=1}^{N}{ y_i\\log{ \\left[ \n  (1+e^{-X_i\\beta})\n  \\right] } + (1-y_i)\\log{ \\left[ \n  1-(1+e^{-X_i\\beta})\n  \\right] } }\n\\end{align}\n}\n$$ \n-->\n\n## Optimising the coefficients\n\nWe minimise the cost function by finding the optimum coefficient values $\\beta^*$ so that the partial differential is equal to zero.\n\n$$\n\\displaylines{\n\\begin{align}\n& \\frac{\\partial}{\\partial \\beta_j}p(y|\\beta,X) \\\\ \\\\\n= &\n\\frac{\\partial}{\\partial \\beta_j} \\left(\n\\sum_{i=1}^{N}{ -y_i\\log{ \\left[ p_i \\right] } } +\n  \\sum_{i=1}^{N}{ -(1-y_i)\\log{ \\left[ 1-p_i\\right] } }\n\\right) \\\\ \\\\\n= &\n\\sum_{i=1}^{N}{ y_i\n  \\frac{\\partial p_i/\\partial \\beta_j}{p_i}\n  } +\n\\sum_{i=1}^{N}{ (1-y_i)\n  \\frac{(1-\\partial p_i)/\\partial \\beta_j}{1-p_i}\n  }\n\\end{align}\n}\n$$\n\nThis is as far as we can get without modelling $P(\\varepsilon_i < X \\beta)$. So let's look at substituting $p_i$, $1-p_i$, $\\partial p_i/\\partial \\beta$ and $\\partial (1-p_i)/\\partial \\beta$ into our first moment condition to derive the optimal coefficients:\n\n$$\n\\displaylines{\n\\begin{align}\np_i & = P(\\varepsilon_i < X \\beta) = (1+e^{-X\\beta})^{-1}\n\\\\ & = \\frac{1}{1+e^{-X\\beta}} \n\\\\\n\\therefore 1-p_i & = 1 - (1+e^{-X\\beta})^{-1} = \\frac{1+e^{-X\\beta}}{1+e^{-X\\beta}} - \\frac{1}{1+e^{-X\\beta}} \n\\\\ & = \\frac{e^{-X\\beta}}{1+e^{-X\\beta}}  \\\\\n\\therefore \\frac{\\partial p_i}{\\partial \\beta_j}\n& = -1(1+e^{-X\\beta})^{-2} \\times -x_je^{-X\\beta} \n\\\\ & = \\frac{-x_j \\times e^{-X\\beta}}{(1+e^{-X\\beta})^2} \n\\\\\n\\therefore \\frac{\\partial (1-p_i)}{\\partial \\beta_j} & = -1(1+e^{-X\\beta})^{-2} \\times x_j \\times e^{-X\\beta} \n\\\\ & = \\frac{x_j \\times e^{-X\\beta}}{(1+e^{-X\\beta})^2}\n\\end{align}\n}\n$$\n\n$$\n\\displaylines{\n\\begin{align}\n\\therefore \\frac{\\partial}{\\partial \\beta_j}p(y|\\beta,X) & =\n\\sum_{i=1}^{N}{ y_i\n  \\frac{\\partial p_i/\\partial \\beta_j}{p_i}\n  } +\n\\sum_{i=1}^{N}{ (1-y_i)\n  \\frac{\\partial(1-\\partial p_i)/ \\beta_j}{1-p_i}\n  } \\\\\n& =\n\\sum_{i=1}^{N}{ y_i\n  \\frac{\\frac{-x_j \\times e^{-X\\beta}}{(1+e^{-X\\beta})^2}}{\\frac{1}{1+e^{-X\\beta}}}\n  } +\n\\sum_{i=1}^{N}{ (1-y_i)\n  \\frac{\\frac{x_j \\times e^{-X\\beta}}{(1+e^{-X\\beta})^2}}{\\frac{e^{-X\\beta}}{1+e^{-X\\beta}}}\n  }\n\\\\\n& =\n\\sum_{i=1}^{N}{ -x_jy_i\n  \\frac{ e^{-X\\beta}}{1+e^{-X\\beta}}\n  } +\n\\sum_{i=1}^{N}{ x_j(1-y_i)\n  \\frac{1}{1+e^{-X\\beta}}\n  }    \n\\\\\n& =\n-x_j\\sum_{i=1}^{N}{ \n  y_i \\times  \\left(1-p_i\\right) + \n  (1-y_i) \\times p_i\n  }    \n\\\\\n\\end{align}\n}\n$$\n\nThus there is no closed form solution like OLS. However, given the cost function is convex, using an optimization like Newton Raphson will find the optimum coefficients.\n\nFin.",
    "supporting": [
      "glm_logit_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}