{
  "hash": "21df77e2338c863e69be7e80b598cc44",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Minimizing RSS equivalent to MLE for OLS\"\nauthor: \"Chris Kelly\"\ndate: '02-08-24'\ncategories: [differentation, cost functions, MSE, MAE]\nformat:\n  html:\n    code-fold: true\ndraft: true\n---\n\n\n::: {.callout-tip}\n## What we are exploring\nShowing that OLS produces the same coefficients as MLE when gaussian errors are used.\n:::\n\n# Quick recap of OLS: deriving optimal coefficients through least squares\n\nThere are many options to choose for the intercept (alpha) and the slope (beta) to best a line that best fits the data, but we want to find the ones that best fit the data:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nrequire(plotly)\nset.seed(1)\nX = 0:5\ny = 5 + 2*X + runif(6,-0.5,1)\nplot_ly(type='scatter',mode='lines',line=list(dash='dot')) %>%\n  add_trace(x=X,y=1+1*X,name='alpha=1,beta=0.5') %>%\n  add_trace(x=X,y=5+1*X,name='alpha=5,beta=0.5') %>%\n  add_trace(x=X,y=9+1*X,name='alpha=9,beta=0.5') %>%\n  add_trace(x=X,y=1+2*X,name='alpha=1,beta=2') %>%\n  add_trace(x=X,y=5+2*X,name='alpha=5,beta=2') %>%\n  add_trace(x=X,y=9+2*X,name='alpha=9,beta=2') %>%\n  add_trace(x=X,y=1+4*X,name='alpha=1,beta=4') %>%\n  add_trace(x=X,y=5+4*X,name='alpha=5,beta=4') %>%\n  add_trace(x=X,y=9+4*X,name='alpha=9,beta=4') %>%\n  add_trace(x=X,y=y,mode='markers',name='Observed data',line=list(color='rgba(0,0,0,0)'),marker=list(size=10)) %>%\n  layout(xaxis=list(title='X'), yaxis=list(title='y'))\n```\n\n::: {.cell-output-display}\n\n```{=html}\n<div class=\"plotly html-widget html-fill-item\" id=\"htmlwidget-d62a0dd1ba32e0541469\" style=\"width:100%;height:361px;\"></div>\n<script type=\"application/json\" data-for=\"htmlwidget-d62a0dd1ba32e0541469\">{\"x\":{\"visdat\":{\"1115a496cc73\":[\"function () \",\"plotlyVisDat\"]},\"cur_data\":\"1115a496cc73\",\"attrs\":{\"1115a496cc73\":{\"mode\":\"lines\",\"line\":{\"dash\":\"dot\"},\"alpha_stroke\":1,\"sizes\":[10,100],\"spans\":[1,20],\"type\":\"scatter\"},\"1115a496cc73.1\":{\"mode\":\"lines\",\"line\":{\"dash\":\"dot\"},\"alpha_stroke\":1,\"sizes\":[10,100],\"spans\":[1,20],\"type\":\"scatter\",\"x\":[0,1,2,3,4,5],\"y\":[1,2,3,4,5,6],\"name\":\"alpha=1,beta=0.5\",\"inherit\":true},\"1115a496cc73.2\":{\"mode\":\"lines\",\"line\":{\"dash\":\"dot\"},\"alpha_stroke\":1,\"sizes\":[10,100],\"spans\":[1,20],\"type\":\"scatter\",\"x\":[0,1,2,3,4,5],\"y\":[5,6,7,8,9,10],\"name\":\"alpha=5,beta=0.5\",\"inherit\":true},\"1115a496cc73.3\":{\"mode\":\"lines\",\"line\":{\"dash\":\"dot\"},\"alpha_stroke\":1,\"sizes\":[10,100],\"spans\":[1,20],\"type\":\"scatter\",\"x\":[0,1,2,3,4,5],\"y\":[9,10,11,12,13,14],\"name\":\"alpha=9,beta=0.5\",\"inherit\":true},\"1115a496cc73.4\":{\"mode\":\"lines\",\"line\":{\"dash\":\"dot\"},\"alpha_stroke\":1,\"sizes\":[10,100],\"spans\":[1,20],\"type\":\"scatter\",\"x\":[0,1,2,3,4,5],\"y\":[1,3,5,7,9,11],\"name\":\"alpha=1,beta=2\",\"inherit\":true},\"1115a496cc73.5\":{\"mode\":\"lines\",\"line\":{\"dash\":\"dot\"},\"alpha_stroke\":1,\"sizes\":[10,100],\"spans\":[1,20],\"type\":\"scatter\",\"x\":[0,1,2,3,4,5],\"y\":[5,7,9,11,13,15],\"name\":\"alpha=5,beta=2\",\"inherit\":true},\"1115a496cc73.6\":{\"mode\":\"lines\",\"line\":{\"dash\":\"dot\"},\"alpha_stroke\":1,\"sizes\":[10,100],\"spans\":[1,20],\"type\":\"scatter\",\"x\":[0,1,2,3,4,5],\"y\":[9,11,13,15,17,19],\"name\":\"alpha=9,beta=2\",\"inherit\":true},\"1115a496cc73.7\":{\"mode\":\"lines\",\"line\":{\"dash\":\"dot\"},\"alpha_stroke\":1,\"sizes\":[10,100],\"spans\":[1,20],\"type\":\"scatter\",\"x\":[0,1,2,3,4,5],\"y\":[1,5,9,13,17,21],\"name\":\"alpha=1,beta=4\",\"inherit\":true},\"1115a496cc73.8\":{\"mode\":\"lines\",\"line\":{\"dash\":\"dot\"},\"alpha_stroke\":1,\"sizes\":[10,100],\"spans\":[1,20],\"type\":\"scatter\",\"x\":[0,1,2,3,4,5],\"y\":[5,9,13,17,21,25],\"name\":\"alpha=5,beta=4\",\"inherit\":true},\"1115a496cc73.9\":{\"mode\":\"lines\",\"line\":{\"dash\":\"dot\"},\"alpha_stroke\":1,\"sizes\":[10,100],\"spans\":[1,20],\"type\":\"scatter\",\"x\":[0,1,2,3,4,5],\"y\":[9,13,17,21,25,29],\"name\":\"alpha=9,beta=4\",\"inherit\":true},\"1115a496cc73.10\":{\"mode\":\"markers\",\"line\":{\"dash\":\"dot\",\"color\":\"rgba(0,0,0,0)\"},\"alpha_stroke\":1,\"sizes\":[10,100],\"spans\":[1,20],\"type\":\"scatter\",\"x\":[0,1,2,3,4,5],\"y\":[4.89826299471315,7.0581858494551852,9.3592800450278446,11.862311684992164,12.802522896556184,15.847584527451545],\"name\":\"Observed data\",\"marker\":{\"size\":10},\"inherit\":true}},\"layout\":{\"margin\":{\"b\":40,\"l\":60,\"t\":25,\"r\":10},\"xaxis\":{\"domain\":[0,1],\"automargin\":true,\"title\":\"X\"},\"yaxis\":{\"domain\":[0,1],\"automargin\":true,\"title\":\"y\"},\"hovermode\":\"closest\",\"showlegend\":true},\"source\":\"A\",\"config\":{\"modeBarButtonsToAdd\":[\"hoverclosest\",\"hovercompare\"],\"showSendToCloud\":false},\"data\":[{\"mode\":\"lines\",\"line\":{\"color\":\"rgba(31,119,180,1)\",\"dash\":\"dot\"},\"type\":\"scatter\",\"marker\":{\"color\":\"rgba(31,119,180,1)\",\"line\":{\"color\":\"rgba(31,119,180,1)\"}},\"error_y\":{\"color\":\"rgba(31,119,180,1)\"},\"error_x\":{\"color\":\"rgba(31,119,180,1)\"},\"xaxis\":\"x\",\"yaxis\":\"y\",\"frame\":null},{\"mode\":\"lines\",\"line\":{\"color\":\"rgba(255,127,14,1)\",\"dash\":\"dot\"},\"type\":\"scatter\",\"x\":[0,1,2,3,4,5],\"y\":[1,2,3,4,5,6],\"name\":\"alpha=1,beta=0.5\",\"marker\":{\"color\":\"rgba(255,127,14,1)\",\"line\":{\"color\":\"rgba(255,127,14,1)\"}},\"error_y\":{\"color\":\"rgba(255,127,14,1)\"},\"error_x\":{\"color\":\"rgba(255,127,14,1)\"},\"xaxis\":\"x\",\"yaxis\":\"y\",\"frame\":null},{\"mode\":\"lines\",\"line\":{\"color\":\"rgba(44,160,44,1)\",\"dash\":\"dot\"},\"type\":\"scatter\",\"x\":[0,1,2,3,4,5],\"y\":[5,6,7,8,9,10],\"name\":\"alpha=5,beta=0.5\",\"marker\":{\"color\":\"rgba(44,160,44,1)\",\"line\":{\"color\":\"rgba(44,160,44,1)\"}},\"error_y\":{\"color\":\"rgba(44,160,44,1)\"},\"error_x\":{\"color\":\"rgba(44,160,44,1)\"},\"xaxis\":\"x\",\"yaxis\":\"y\",\"frame\":null},{\"mode\":\"lines\",\"line\":{\"color\":\"rgba(214,39,40,1)\",\"dash\":\"dot\"},\"type\":\"scatter\",\"x\":[0,1,2,3,4,5],\"y\":[9,10,11,12,13,14],\"name\":\"alpha=9,beta=0.5\",\"marker\":{\"color\":\"rgba(214,39,40,1)\",\"line\":{\"color\":\"rgba(214,39,40,1)\"}},\"error_y\":{\"color\":\"rgba(214,39,40,1)\"},\"error_x\":{\"color\":\"rgba(214,39,40,1)\"},\"xaxis\":\"x\",\"yaxis\":\"y\",\"frame\":null},{\"mode\":\"lines\",\"line\":{\"color\":\"rgba(148,103,189,1)\",\"dash\":\"dot\"},\"type\":\"scatter\",\"x\":[0,1,2,3,4,5],\"y\":[1,3,5,7,9,11],\"name\":\"alpha=1,beta=2\",\"marker\":{\"color\":\"rgba(148,103,189,1)\",\"line\":{\"color\":\"rgba(148,103,189,1)\"}},\"error_y\":{\"color\":\"rgba(148,103,189,1)\"},\"error_x\":{\"color\":\"rgba(148,103,189,1)\"},\"xaxis\":\"x\",\"yaxis\":\"y\",\"frame\":null},{\"mode\":\"lines\",\"line\":{\"color\":\"rgba(140,86,75,1)\",\"dash\":\"dot\"},\"type\":\"scatter\",\"x\":[0,1,2,3,4,5],\"y\":[5,7,9,11,13,15],\"name\":\"alpha=5,beta=2\",\"marker\":{\"color\":\"rgba(140,86,75,1)\",\"line\":{\"color\":\"rgba(140,86,75,1)\"}},\"error_y\":{\"color\":\"rgba(140,86,75,1)\"},\"error_x\":{\"color\":\"rgba(140,86,75,1)\"},\"xaxis\":\"x\",\"yaxis\":\"y\",\"frame\":null},{\"mode\":\"lines\",\"line\":{\"color\":\"rgba(227,119,194,1)\",\"dash\":\"dot\"},\"type\":\"scatter\",\"x\":[0,1,2,3,4,5],\"y\":[9,11,13,15,17,19],\"name\":\"alpha=9,beta=2\",\"marker\":{\"color\":\"rgba(227,119,194,1)\",\"line\":{\"color\":\"rgba(227,119,194,1)\"}},\"error_y\":{\"color\":\"rgba(227,119,194,1)\"},\"error_x\":{\"color\":\"rgba(227,119,194,1)\"},\"xaxis\":\"x\",\"yaxis\":\"y\",\"frame\":null},{\"mode\":\"lines\",\"line\":{\"color\":\"rgba(127,127,127,1)\",\"dash\":\"dot\"},\"type\":\"scatter\",\"x\":[0,1,2,3,4,5],\"y\":[1,5,9,13,17,21],\"name\":\"alpha=1,beta=4\",\"marker\":{\"color\":\"rgba(127,127,127,1)\",\"line\":{\"color\":\"rgba(127,127,127,1)\"}},\"error_y\":{\"color\":\"rgba(127,127,127,1)\"},\"error_x\":{\"color\":\"rgba(127,127,127,1)\"},\"xaxis\":\"x\",\"yaxis\":\"y\",\"frame\":null},{\"mode\":\"lines\",\"line\":{\"color\":\"rgba(188,189,34,1)\",\"dash\":\"dot\"},\"type\":\"scatter\",\"x\":[0,1,2,3,4,5],\"y\":[5,9,13,17,21,25],\"name\":\"alpha=5,beta=4\",\"marker\":{\"color\":\"rgba(188,189,34,1)\",\"line\":{\"color\":\"rgba(188,189,34,1)\"}},\"error_y\":{\"color\":\"rgba(188,189,34,1)\"},\"error_x\":{\"color\":\"rgba(188,189,34,1)\"},\"xaxis\":\"x\",\"yaxis\":\"y\",\"frame\":null},{\"mode\":\"lines\",\"line\":{\"color\":\"rgba(23,190,207,1)\",\"dash\":\"dot\"},\"type\":\"scatter\",\"x\":[0,1,2,3,4,5],\"y\":[9,13,17,21,25,29],\"name\":\"alpha=9,beta=4\",\"marker\":{\"color\":\"rgba(23,190,207,1)\",\"line\":{\"color\":\"rgba(23,190,207,1)\"}},\"error_y\":{\"color\":\"rgba(23,190,207,1)\"},\"error_x\":{\"color\":\"rgba(23,190,207,1)\"},\"xaxis\":\"x\",\"yaxis\":\"y\",\"frame\":null},{\"mode\":\"markers+lines\",\"line\":{\"color\":\"rgba(0,0,0,0)\",\"dash\":\"dot\"},\"type\":\"scatter\",\"x\":[0,1,2,3,4,5],\"y\":[4.89826299471315,7.0581858494551852,9.3592800450278446,11.862311684992164,12.802522896556184,15.847584527451545],\"name\":\"Observed data\",\"marker\":{\"color\":\"rgba(31,119,180,1)\",\"size\":10,\"line\":{\"color\":\"rgba(31,119,180,1)\"}},\"error_y\":{\"color\":\"rgba(31,119,180,1)\"},\"error_x\":{\"color\":\"rgba(31,119,180,1)\"},\"xaxis\":\"x\",\"yaxis\":\"y\",\"frame\":null}],\"highlight\":{\"on\":\"plotly_click\",\"persistent\":false,\"dynamic\":false,\"selectize\":false,\"opacityDim\":0.20000000000000001,\"selected\":{\"opacity\":1},\"debounce\":0},\"shinyEvents\":[\"plotly_hover\",\"plotly_click\",\"plotly_selected\",\"plotly_relayout\",\"plotly_brushed\",\"plotly_brushing\",\"plotly_clickannotation\",\"plotly_doubleclick\",\"plotly_deselect\",\"plotly_afterplot\",\"plotly_sunburstclick\"],\"base_url\":\"https://plot.ly\"},\"evals\":[],\"jsHooks\":[]}</script>\n```\n\n:::\n:::\n\n\nWe can see that the choices of alpha and beta that minimize the sum of squared residuals are 5 and 2 respectively:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nalpha_options <- seq(3,7,1) # seq(4,6,1) # \nbeta_options <- 2^seq(0,2,0.5) # 2^seq(0.5,1.5,0.5) # \ngraph_options <- \"plot_ly(type='scatter',mode='lines') %>% add_trace(x=X,y=y,mode='markers',name='Observed data') %>% \"\ncost_matrix <- matrix(nrow=length(alpha_options),ncol=length(beta_options),dimnames = list(alpha_options, beta_options))\nfor(alpha in 1:length(alpha_options)) {\n  for(beta in 1:length(beta_options)) {\n    graph_options <- paste0(graph_options, paste0(\" add_trace(x=X,y=\",alpha_options[alpha],\"+\",beta_options[beta],\"*X\",\",name='alpha=\",alpha_options[alpha],\",beta=\",round(beta_options[beta],1),\"') %>%  \\n\"))\n    cost_matrix[alpha,beta] <- sum((y-alpha_options[alpha]-beta_options[beta]*X)^2)\n  }\n}\ngraph_options <- paste0(graph_options, ' layout()')\n# eval(parse(text=graph_options))\nplot_ly(x=alpha_options,y=beta_options,z =~cost_matrix,showscale=FALSE,reversescale=TRUE) %>% \n  layout(scene=list(xaxis=list(title='beta'),yaxis=list(title='alpha'),zaxis=list(title='RSS'))) %>% \n  add_surface(contours = list(z = list(project=list(z=TRUE)))) %>%\n  layout(scene = list(camera = list(eye = list(x = -1.5,y = 1.5,z = 1.5))))\n```\n\n::: {.cell-output-display}\n\n```{=html}\n<div class=\"plotly html-widget html-fill-item\" id=\"htmlwidget-b5ed4906484ed42468d8\" style=\"width:100%;height:361px;\"></div>\n<script type=\"application/json\" data-for=\"htmlwidget-b5ed4906484ed42468d8\">{\"x\":{\"visdat\":{\"1115a15050e0c\":[\"function () \",\"plotlyVisDat\"]},\"cur_data\":\"1115a15050e0c\",\"attrs\":{\"1115a15050e0c\":{\"x\":[3,4,5,6,7],\"y\":[1,1.4142135623730951,2,2.8284271247461903,4],\"z\":{},\"showscale\":false,\"reversescale\":true,\"alpha_stroke\":1,\"sizes\":[10,100],\"spans\":[1,20],\"type\":\"surface\",\"contours\":{\"z\":{\"project\":{\"z\":true}}},\"inherit\":true}},\"layout\":{\"margin\":{\"b\":40,\"l\":60,\"t\":25,\"r\":10},\"scene\":{\"xaxis\":{\"title\":\"beta\"},\"yaxis\":{\"title\":\"alpha\"},\"zaxis\":{\"title\":\"RSS\"},\"camera\":{\"eye\":{\"x\":-1.5,\"y\":1.5,\"z\":1.5}}},\"hovermode\":\"closest\",\"showlegend\":false},\"source\":\"A\",\"config\":{\"modeBarButtonsToAdd\":[\"hoverclosest\",\"hovercompare\"],\"showSendToCloud\":false},\"data\":[{\"colorbar\":{\"title\":\"cost_matrix\",\"ticklen\":2},\"colorscale\":[[\"0\",\"rgba(68,1,84,1)\"],[\"0.0416666666666667\",\"rgba(70,19,97,1)\"],[\"0.0833333333333333\",\"rgba(72,32,111,1)\"],[\"0.125\",\"rgba(71,45,122,1)\"],[\"0.166666666666667\",\"rgba(68,58,128,1)\"],[\"0.208333333333333\",\"rgba(64,70,135,1)\"],[\"0.25\",\"rgba(60,82,138,1)\"],[\"0.291666666666667\",\"rgba(56,93,140,1)\"],[\"0.333333333333333\",\"rgba(49,104,142,1)\"],[\"0.375\",\"rgba(46,114,142,1)\"],[\"0.416666666666667\",\"rgba(42,123,142,1)\"],[\"0.458333333333333\",\"rgba(38,133,141,1)\"],[\"0.5\",\"rgba(37,144,140,1)\"],[\"0.541666666666667\",\"rgba(33,154,138,1)\"],[\"0.583333333333333\",\"rgba(39,164,133,1)\"],[\"0.625\",\"rgba(47,174,127,1)\"],[\"0.666666666666667\",\"rgba(53,183,121,1)\"],[\"0.708333333333333\",\"rgba(79,191,110,1)\"],[\"0.75\",\"rgba(98,199,98,1)\"],[\"0.791666666666667\",\"rgba(119,207,85,1)\"],[\"0.833333333333333\",\"rgba(147,214,70,1)\"],[\"0.875\",\"rgba(172,220,52,1)\"],[\"0.916666666666667\",\"rgba(199,225,42,1)\"],[\"0.958333333333333\",\"rgba(226,228,40,1)\"],[\"1\",\"rgba(253,231,37,1)\"]],\"showscale\":false,\"x\":[3,4,5,6,7],\"y\":[1,1.4142135623730951,2,2.8284271247461903,4],\"z\":[[161.57977877043456,94.956988221897817,32.956388334494903,9.7108072374214451,105.70960746261559],[109.92348277404241,55.727099096698524,11.300092338102758,12.907324983415007,144.05331146622345],[70.267186777650267,28.497209971499231,1.6437963417106123,28.103842729408569,194.39701546983133],[42.610890781258121,13.267320846299942,3.9875003453184665,55.30036047540213,256.74071947343919],[26.954594784865975,10.037431721100649,18.33120434892632,94.496878221395704,331.08442347704704]],\"reversescale\":true,\"type\":\"surface\",\"contours\":{\"z\":{\"project\":{\"z\":true}}},\"frame\":null}],\"highlight\":{\"on\":\"plotly_click\",\"persistent\":false,\"dynamic\":false,\"selectize\":false,\"opacityDim\":0.20000000000000001,\"selected\":{\"opacity\":1},\"debounce\":0},\"shinyEvents\":[\"plotly_hover\",\"plotly_click\",\"plotly_selected\",\"plotly_relayout\",\"plotly_brushed\",\"plotly_brushing\",\"plotly_clickannotation\",\"plotly_doubleclick\",\"plotly_deselect\",\"plotly_afterplot\",\"plotly_sunburstclick\"],\"base_url\":\"https://plot.ly\"},\"evals\":[],\"jsHooks\":[]}</script>\n```\n\n:::\n:::\n\n\nHowever, we don't need to do this through an exhaustive gridsearch through all the parameters to minimize the sum of squared residuals, but can yield this by differentiating our regression function with respect to our parameters to find its minimum point.\n\nThis is simple to see if our features are orthogonal (i.e. in a univariate regression, the intercept is a one-dimensional vector of 1s, and the X a one-dimensional vector) as we can apply partial differentiation by $\\beta_j$:\n\n$$\n\\min_\\beta{\\left[\\sum_{i=1}^N{\\epsilon_i^2}\\right]}\n\\Rightarrow \\frac{\\partial}{\\partial\\beta_j} \\sum_{i=1}^N{\\epsilon_i^2}=\\frac{\\partial}{\\partial\\epsilon} \\sum_{i=1}^N{\\epsilon_i^2} \\frac{\\partial\\epsilon}{\\partial\\beta_j} = \\sum_{i=1}^N 2\\epsilon_i\\left(\\frac{\\partial\\epsilon_i}{\\partial\\beta_j}\\right)=0 \\\\\n\\epsilon_i = y_i-\\beta_0-\\beta_1x_i \\Rightarrow \\\\\n\\sum_{i=1}^N2( y_i-\\beta_0-\\beta_1x_i)\\left(\\frac{\\partial( y_i-\\beta_0-\\beta_1x_i)}{\\partial\\beta_j}\\right)=0 \\\\\n\\text{if j=0:} \\\\ \n\\Rightarrow \\sum_{i=1}^N2( y_i-\\beta_0-\\beta_1x_i)(-1)=0 \\\\\n\\Rightarrow n\\beta_0 = \\sum_{i=1}^N( y_i-\\beta_1x_i)=\\sum_{i=1}^N{y_i} - \\beta_1\\sum_{i=1}^N{x_i} \\\\\n\\Rightarrow \\beta_0 = \\frac{\\sum_{i=1}^N{y_i}}{n} - \\beta_1\\frac{\\sum_{i=1}^N{x_i}}{n}=\\bar{y}-\\beta_1\\bar{x} \\\\\n\\text{if j=1:} \\\\ \n\\Rightarrow \\sum_{i=1}^N2( y_i-\\beta_0-\\beta_1x_i)(-x_i)=0 \\\\\n\\Rightarrow \\sum_{i=1}^N2( y_i-(\\bar{y}-\\beta_1\\bar{x})-\\beta_1x_i)(-x_i)=0 \\\\\n\\Rightarrow \\sum_{i=1}^N( y_i-\\bar{y}-\\beta_1(x_i-\\bar{x}))(-x_i)\n=\\sum_{i=1}^N( y_ix_i-\\bar{y}x_i-\\beta_1x_i^2+\\beta_1\\bar{x}x_i)\n=\\sum_{i=1}^N{y_ix_i}-\\bar{y}\\sum_{i=1}^N{x_i}-\\beta_1\\sum_{i=1}^N{x_i}^2+\\beta_1\\bar{x}\\sum_{i=1}^N{x_i}) \\\\\n=\\sum_{i=1}^N{y_ix_i}-\\bar{y}(N\\bar{x})-\\beta_1\\sum_{i=1}^N{x_i}^2+\\beta_1\\bar{x}(N\\bar{x})=0 \\\\\n\\Rightarrow \\beta_1 = \\frac{\\sum_{i=1}^N{y_ix_i}-N\\bar{x}\\bar{y}}{\\sum_{i=1}^N{x_i^2}-N\\bar{x}^2}\n= \\frac{\\sum_{i=1}^N{y_ix_i}-N\\bar{x}\\bar{y}+(N\\bar{x}\\bar{y}-N\\bar{x}\\bar{y})}{\\sum_{i=1}^N{x_i^2}-N\\bar{x}^2+(N\\bar{x}^2-N\\bar{x}^2)} \\\\\n= \\frac{\\sum_{i=1}^N{y_ix_i}-\\sum_{i=1}^N\\bar{x}\\bar{y}+(\\bar{x}\\sum_{i=1}^N{y_i}-\\bar{y}\\sum_{i=1}^N{x_i})}{\\sum_{i=1}^N{x_i^2}-N\\bar{x}^2+(\\bar{x}\\sum_{i=1}^N{x_i}-\\bar{x}\\sum_{i=1}^N{x_i})}\n= \\frac{\\sum_{i=1}^N{(y_i-\\bar{y})(x_i-\\bar{x})}}{\\sum_{i=1}^N{(x_i-\\bar{x})^2}} \\\\\n= \\frac{cov(x,y)}{var(x)}\n$$\n\n(If the regression is multivariate, and the features are not perfectly orthogonal - i.e. is some multicollinearity - then this doesn't perfectly hold, and can yield different coefficients)\n\nWe can see then that the best unbiased linear estimator (BLUE) for the intercept and slope is derived from minimizing the sum of squared residuals. These derivations also derive two interesting properties:\n\n* From line 5, that the mean error is zero: $\\sum_{i=1}^N2( y_i-\\beta_0-\\beta_1x_i)(-1)=0 \\Rightarrow \\frac{1}{n}\\sum_{i=1}^N(\\epsilon_i)=0$\n* From line 9, that X is deterministic, and not correlated with the error term: $\\sum_{i=1}^N2( y_i-\\beta_0-\\beta_1x_i)(-x_i)=0 \\Rightarrow \\sum_{i=1}^N(\\epsilon_ix_i)=0$ (note that this is equal to the $cov(x_i,e_i)=\\sum_{i=1}^N(\\epsilon_ix_i)-\\sum_{i=1}^N(\\bar{\\epsilon}\\bar{x})$ since the mean error $\\bar{\\epsilon}$ is zero)\n\n\n# Maximising likelihood to solve linear regression:\n\nFirst let's solve the original linear regression problem by maximum likelihood. \n\nRather than simply minimizing the residual sum of squares (as usual with the OLS loss function), we want to find the beta that maximises the likelihood of observing the evidence we have, knowing $y \\sim N(X'\\beta,1)$.\nIn other words, the probability of observing $y$ given our data and estimated model parameters is a function of the normal probability density of our squared residuals:\n\n$$\np(y|\\beta,X) = \\prod_{i=1}^{n}{\\frac{1}{\\sqrt{2\\pi}}}e^{-\\frac{1}{2}\\epsilon_i^2}\n$$\nWe can take the negative log of the likelihood function to make it easier to differentiate:\n\n$$\n\\max_\\beta{p(y|\\beta,X)} \\Rightarrow \\min_\\beta\\left[{-\\log{\\left(\\prod_{i=1}^{n}{\\frac{1}{\\sqrt{2\\pi}}}e^{-\\frac{1}{2}\\epsilon_i^2}\\right)}}\\right] = \\\\\n\\min_\\beta{\\left[ -\\sum{\\log{\\left(\\frac{1}{\\sqrt{2\\pi}}e^{-\\frac{1}{2}\\epsilon_i^2}\\right)}} \\right]} \\\\\n= \\min_\\beta{\\left[ -\\sum{\\log{\\left(\\frac{1}{\\sqrt{2\\pi}}\\right)}} + -\\sum{\\log{\\left(e^{-\\frac{1}{2}\\epsilon_i^2}\\right)}} \\right]} \\\\\n= \\min_\\beta{\\left[ -\\sum{\\log{((2\\pi)^{-\\frac{1}{2}})}} + -\\sum{\\left(-\\frac{1}{2}\\epsilon_i^2\\right)} \\right]} \\\\\n= \\min_\\beta{\\left[ \\frac{1}{2}\\sum{\\log{(2\\pi)}} + -\\sum{\\left(-\\frac{1}{2}(y_i-X_i'\\beta)^2\\right)} \\right]} \\\\\n= \\min_\\beta{\\left[ \\frac{1}{2}\\sum{\\log{(2\\pi)}} + \\frac{1}{2}(Y-X\\beta)^T(Y-X\\beta) \\right]} \\\\\n= \\min_\\beta{\\left[ \\frac{1}{2}\\sum{\\log{(2\\pi)}} + \\frac{1}{2}\\epsilon^T\\epsilon \\right]} \\\\\n\\leftrightarrow \\beta^*=\\arg\\min_\\beta{\\left[ \\frac{1}{2}\\epsilon^T\\epsilon \\right]}\n$$\nNow when we minimise the log-likelihood cost function by differentiating it with respect to $\\beta$ and setting it to zero in order to derive the optimum coefficient, the constant $\\log{(2\\pi)}$ drops out, and we are left with differentiating $\\frac{d}{d\\beta}\\epsilon^T\\epsilon=0$ - the exact equivalent as with frequentist OLS.\nWe can rewrite this in terms of the bayesian paradigm if we think of $\\beta$ as a random variable (rather than a fixed quantity as per frequenist thinking):\n$$\np(\\beta|X,Y)=\\frac{p(Y|\\beta,X)p(\\beta|X)}{p(Y|X)}=\\frac{p(Y|\\beta,X)p(\\beta|X)}{\\int p(Y|X,\\beta)p(\\beta|X)d\\beta}\n$$\nWhere:\n\n* $p(Y|\\beta,X)$ is the likelihood function (where we maximise the log-likelihood as above)\n* $p(Y|X)$ is the evidence (the data we feed into the model)\n* $p(\\beta|X)$ is the prior\n\nIf we assume $\\beta$ is fixed, then $p(\\beta|X)=1$, and thus we get $= \\min_\\beta{\\left[\\epsilon^T\\epsilon \\right]} \\Rightarrow \\beta^*=(X^TX)^{-1}X^TY$ (as per OLS).",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<script src=\"../site_libs/htmlwidgets-1.6.4/htmlwidgets.js\"></script>\n<script src=\"../site_libs/plotly-binding-4.10.4/plotly.js\"></script>\n<script src=\"../site_libs/typedarray-0.1/typedarray.min.js\"></script>\n<script src=\"../site_libs/jquery-3.5.1/jquery.min.js\"></script>\n<link href=\"../site_libs/crosstalk-1.2.1/css/crosstalk.min.css\" rel=\"stylesheet\" />\n<script src=\"../site_libs/crosstalk-1.2.1/js/crosstalk.min.js\"></script>\n<link href=\"../site_libs/plotly-htmlwidgets-css-2.11.1/plotly-htmlwidgets.css\" rel=\"stylesheet\" />\n<script src=\"../site_libs/plotly-main-2.11.1/plotly-latest.min.js\"></script>\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}