{
  "hash": "6dd9efb991a601760429ae52aa151b6b",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"NLP: Data Mining Intro\"\nauthor: \"Chris Kelly\"\ndate: '02-21-24'\ncategories: [NLP, TF-IDF, cosine similarity, LSA, topic extraction]\nformat:\n  html:\n    code-fold: true\n    toc: true\nexecute: \n  enabled: true\ndraft: true\n---\n\n# Quick introduction:\n\nAlgorithms like dealing with numbers - they like structure, for input data to be *tidy*. So how can an algorithm start to process unstructured text? And even then - start to extract anything meaningful from it?\n\n<img src=\"https://media.giphy.com/media/eMBKXi56D0EXC/giphy.gif\">\n\n### Some definitions/background\n\nWhen text data is provided, it goes through the process of \"tokenisation\". This involves splitting the text into smaller pieces, that the algorithm can encode into 0 and 1s (one-hot encoding). In our example, each unique word in a menu will be a \"token\".\n\nA \"document\" is a collection of tokens associated with a particular sample. In our example, each document will be a restaurant menu. An \"embedding\" is the attempt to create numerical representation of that document.\n\nThe \"corpus\" is a collection of all documents that the model can learn from. In our example, it the entire colletion of menus.\n\n# TF-IDF: text-frequency inverse-document-frequency\n\n### What is it used for?\n\nText-frequency Inverse-Document-Frequency (TF-IDF) is a technique to find the most important words in a document, or alternatively formulated to find the most important letters in a word. \n\nIt is good at determining 'global' statistics. By this, we mean it compares the frequency of words and letters across the entire corpus. It does not capture more detailed semantic relationships, particularly since it doesn't care about the ordering of words in a document.\n\n### What is the intuition?\n\nTF-IDF hypothesises that tokens that appear more frequently, especially those that tend to be rarer, are more important for that document.\n\nFor example, let's take two takeaway orders:\n* Chicken Kurma Masala, Chicken Tikka Masala, Chicken Curry, Saag Aloo, Pilau Rice, Plain Rice\n* Chicken Chow Mein Noodles, Crispy Beef, Chicken and Cashew nut curry, Prawn Crackers, Egg Fried Rice, Steamed rice, Vegetable Noodles\n\n<img src=\"https://media.giphy.com/media/12xu9HYTRo4Eg0/giphy.gif\">\n\nThe word 'Chicken' appears the most in the first order, followed by 'masasla' and 'Rice'. This is *text frequency*.\n\nBut that's only half the story - because chicken and rice are quite common words. So we need a way to measure whether a word is rarer. The word 'masala' only appears in the first order, whereas 'chicken' and 'rice' appears in both orders. Hence, the rareness of a word can be determined by how infrequently it appears in all the documents. This is *inverse document frequency*.\n\nCombining text-frequency and inverse-document frequency scores for each token give it a TF-IDF score. This way, we can derive that the word 'masala' is the most important word from the first order, since it has both high TF and IDF scores.\n\nTF-IDF can also be done for words, for example splitting the word 'manner' into character tokens:\n\n* 'n' appears twice in the word\n* 'm' is rarer\n\nSo we are much more likely to guess that the word 'm_nn_' is manner, whereas seeing '_a__er' is far less informative.\n\n#### Text Frequency\n\nLet's say we wanted to build something to classify cuisines. \nWe might take orders from two different menus, and first try to identify which words are the some of the more important.\n\n* Chicken Kurma Masala, Chicken Tikka Masala, Chicken Curry, Saag Aloo, Pilau Rice, Plain Rice\n* Chicken Chow Mein Noodles, Crispy Beef, Chicken and Cashew nut curry, Prawn Crackers, Egg Fried Rice, Steamed rice, Vegetable Noodles\n\nBut how can we do this in an automated way?\n\n\n<img src=\"https://media.giphy.com/media/B8Bp8MfpmKbWU/giphy.gif\">\n\n\nWe might think that **words that are repeated many times in an order are important. This is called 'text frequency'.**\n\nSo let's count them:\n\n::: {#cell-2 .cell execution_count=1}\n``` {.python .cell-code}\ntakeaway_orders = ['Chicken Kurma Masala, Chicken Tikka Masala, Chicken Curry, Pilau Rice, Plain Rice'\n                   , 'Chicken Chow Mein Noodles, Crispy Beef, Chicken curry, Egg Fried Rice, Plain rice, Vegetable Noodles']\n# unique_tokens = set(' ,'.join(takeaway_orders).replace(',','').split(' '))\nfrom sklearn.feature_extraction.text import CountVectorizer\ncnt_vec = CountVectorizer()\ncount_mat = cnt_vec.fit_transform(takeaway_orders)\nimport pandas as pd\npd.DataFrame(data = count_mat.todense(), index = ['Order_1', 'Order_2'], columns = cnt_vec.get_feature_names_out())\n```\n\n::: {.cell-output .cell-output-display execution_count=1}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>beef</th>\n      <th>chicken</th>\n      <th>chow</th>\n      <th>crispy</th>\n      <th>curry</th>\n      <th>egg</th>\n      <th>fried</th>\n      <th>kurma</th>\n      <th>masala</th>\n      <th>mein</th>\n      <th>noodles</th>\n      <th>pilau</th>\n      <th>plain</th>\n      <th>rice</th>\n      <th>tikka</th>\n      <th>vegetable</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Order_1</th>\n      <td>0</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>Order_2</th>\n      <td>1</td>\n      <td>2</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>2</td>\n      <td>0</td>\n      <td>1</td>\n      <td>2</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nCool, so the most populated word from the first order is chicken, followed by masala and rice. The second order has chicken, noodles and rice as the most populated words.\n\nTwo things to note:\n* Rather than taking the absolute counts, we might log the counts instead. This is because we might want to capture the concept of diminishing returns - the additional marginal importance we expect between having the word 'Masala' appear once vs twice between documents is greater than appearing nine times vs 10 times.  This is more important for longer documents, and the logging our counts captures this concept.*\n* In this instance, we count 'uni-grams', with one token per word. In general implementation, we can count 'bi-grams' such as 'Chicken Kurma' and 'Kurma Masala', 'tri-grams' etc. See more under cosine-similarity\n\n#### Inverse Document Frequency\n\nWe are only getting half the information here then. For example in the first order, chicken in general is a common word, so it appearing frequently is less informative, whereas masala is a rare word, and more informative. We thus need to **introduce an additional concept of word uniqueness**, or equivalently the opposite of how frequently it appears across our entire 'corpus' of menu orders.\n\n<img src=\"https://media.giphy.com/media/5vR6pNsjhoKwo/giphy.gif\">\n\nEnter our  heroine Karen Spärck Jones, with the concept of ‘inverse document frequency’. This is the idea that it is not just how often a word appears, **but how unique the word is across all sentences (or ‘documents’), that determines how important it is. TF-IDF helps use this to turn words into vectors.**\n\nFor example, a rare word like Masala appears in many 'documents' (orders), so will have a low document frequency, and thus will have a high inverse document frequency score.\n\nWe usually calculte inverse document frequency using the following logic:\n\n$$\n\\text{idf} = 1+\\ln \\left(\\frac{\\text{\\# docs in corpus}}{\\text{\\# docs term appears in}} \\right)\n$$\n\nIn other words, the word chicken appears in both docs, so the idf score is $1+\\ln(\\frac{2}{2})=1$. \nOn the other hand, the word masala only appears in one doc, so it gets an IDF score of  $1+\\ln(\\frac{2}{1})\\sim1.7$\n\n::: {#cell-4 .cell execution_count=2}\n``` {.python .cell-code}\nfrom sklearn.feature_extraction.text import TfidfTransformer\nidf_vec = TfidfTransformer(smooth_idf=False,norm=None)\nidf_mat = idf_vec.fit_transform(count_mat)\npd.DataFrame(data = idf_mat/count_mat, index = ['Order_1', 'Order_2'], columns = cnt_vec.get_feature_names_out())\n```\n\n::: {.cell-output .cell-output-display execution_count=2}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>beef</th>\n      <th>chicken</th>\n      <th>chow</th>\n      <th>crispy</th>\n      <th>curry</th>\n      <th>egg</th>\n      <th>fried</th>\n      <th>kurma</th>\n      <th>masala</th>\n      <th>mein</th>\n      <th>noodles</th>\n      <th>pilau</th>\n      <th>plain</th>\n      <th>rice</th>\n      <th>tikka</th>\n      <th>vegetable</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Order_1</th>\n      <td>NaN</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1.693147</td>\n      <td>1.693147</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1.693147</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.693147</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>Order_2</th>\n      <td>1.693147</td>\n      <td>1.0</td>\n      <td>1.693147</td>\n      <td>1.693147</td>\n      <td>1.0</td>\n      <td>1.693147</td>\n      <td>1.693147</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>1.693147</td>\n      <td>1.693147</td>\n      <td>NaN</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>NaN</td>\n      <td>1.693147</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n::: {#cell-5 .cell execution_count=3}\n``` {.python .cell-code}\nfrom sklearn.feature_extraction.text import TfidfVectorizer\ntfidf_vec = TfidfVectorizer(sublinear_tf=True)\ntfidf_mat = tfidf_vec.fit_transform(takeaway_orders)\npd.DataFrame(data = tfidf_mat.todense(), index = ['Order_1', 'Order_2'], columns = tfidf_vec.get_feature_names_out())\n```\n\n::: {.cell-output .cell-output-display execution_count=3}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>beef</th>\n      <th>chicken</th>\n      <th>chow</th>\n      <th>crispy</th>\n      <th>curry</th>\n      <th>egg</th>\n      <th>fried</th>\n      <th>kurma</th>\n      <th>masala</th>\n      <th>mein</th>\n      <th>noodles</th>\n      <th>pilau</th>\n      <th>plain</th>\n      <th>rice</th>\n      <th>tikka</th>\n      <th>vegetable</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Order_1</th>\n      <td>0.000000</td>\n      <td>0.459492</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.218951</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.307727</td>\n      <td>0.521028</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.307727</td>\n      <td>0.218951</td>\n      <td>0.370715</td>\n      <td>0.307727</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>Order_2</th>\n      <td>0.269369</td>\n      <td>0.324505</td>\n      <td>0.269369</td>\n      <td>0.269369</td>\n      <td>0.191658</td>\n      <td>0.269369</td>\n      <td>0.269369</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.269369</td>\n      <td>0.456081</td>\n      <td>0.000000</td>\n      <td>0.191658</td>\n      <td>0.324505</td>\n      <td>0.000000</td>\n      <td>0.269369</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nCool, so combining text frequency and inverse document frequency now reveals the most important word in the first order is 'Masala', and it is 'noodles'. Nice.\n\nFinally, we don't have to limit ourselves to sentences, and can split stings into character tokens, applying the same logic: for the word $\\text{queen}$, we would find the letters $q$ and $e$ to be the most informative because of their rarity (IDF) and being repeated (TF). This could be useful in predicting the word being types or correcting mispelling - let's jump into a character-level example when discussing cosine similarity.\n\n# Cosine similarlity\n\n### What is it used for?\n\nCosine similarity can help measure how similar two words or documents are. For example, we could better match a search to a result using this, whereas 'keywords' would just weight every word equally. \n\n<img src=\"https://media.giphy.com/media/13cgadB959Y0BW/giphy.gif\">\n\n### What is the intuition?\n\nTF-IDF creates a row of scores for each token in the text, for example the first order had high TF-IDF scores for masala, chicken and rice, and low scores for noodles and fried. If we have another document that has high TF-IF scores for masala, chicken and rice, and low scores for noodles and fried, we might think it is similar to the first order. Cosine similarity gives a measure between one and zero as to how similar the two texts are.\n\n#### Vectorization\n\nTo take this further, what we have done using TF-IDF is a form of 'vectorization'. If we were to plot the first order in 16 dimensional space, with one axis for each word, the line remains at zero for the 'beef' axis, travels 0.46 along the 'chicken' axis, etc.\n\n**We can then measure the angle (the cosine) between these lines to get an idea of how similar two documents are.** Cosine similarity is often used to match, how example, a search string to a result. \n\nThis can feel a bit abstract, but hang on in there because this is a key part of intution!\n\nLet's dive into this with an example we can visualise in 3 dimensional space to make this clearer.\n\n\n\n",
    "supporting": [
      "nlp_mining_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}