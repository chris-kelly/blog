{
  "hash": "9cd9f19d9f6eed902e8a85b4bbf4dc00",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"NLP: Data Mining Intro\"\nauthor: \"Chris Kelly\"\ndate: '02-24-24'\ncategories: [NLP, TF-IDF, cosine similarity, LSA, topic extraction]\nformat:\n  html:\n    code-fold: true\n    toc: true\nexecute: \n  enabled: true\n---\n\n::: {#cell-1 .cell execution_count=1}\n``` {.python .cell-code}\nimport numpy as np\nimport pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n```\n:::\n\n\n:::{.callout-tip}\nIntroducing TF-IDF, cosine-similarity and Latent Semantic Analysis\n:::\n\n# Quick introduction:\n\nAlgorithms like dealing with numbers - they like structure, for input data to be *tidy*. So how can an algorithm start to process unstructured text? And even then - start to extract anything meaningful from it?\n\n<img src=\"https://media.giphy.com/media/eMBKXi56D0EXC/giphy.gif\">\n\n### Some definitions/background\n\nA \"token\" is a small piece of text. When text data is provided it is split into smaller consistuent pieces (e.g. a sentence is split into words, usually after stemming and stop-word removal). In our example, the tokenisation step involves one-hot encoding each unique word.\n\nA \"document\" is a collection of tokens associated with a particular sample. In our example, each document will be a restaurant menu. \n\nAn \"embedding\" is an attempt to create numerical representation of that document in a vector. In our example, we hope that the most important tokens will have the largest values.\n\nThis can be useful for similarity search as we will see later (e.g. this user liked this restaurant, so let's recommend them a restaurant with a similar vector).\n\nFinally, the \"corpus\" is a collection of all documents that the model can learn from. In our example, it the entire colletion of menus.\n\n# Text-mining with TF-IDF:\n\n### What is it used for?\n\nAlgorithms like dealing with numbers - they like data to be structured and numerical. So how can an algorithm start to process unstructured text? And even then - start to extract anything meaningful from it?\n\nText-frequency Inverse-Document-Frequency (TF-IDF) is a technique to find the most important tokens in a document. It is thus a form of information retrieval/text-mining.\n\n<!-- It is good at determining 'global' statistics. By this, we mean it contrasts  the frequency of tokens in a document vs their prevalence across the entire corpus. It does not capture more detailed semantic relationships, particularly since it doesn't care about the ordering of words in a document. -->\n\n<!-- Note further that it is heuristic-based. This means that although the TF-IDF -->\n\n### What is the intuition?\n\nTF-IDF is based on the intuition that tokens that appear more frequently, especially those that are rarer across the entire corpus, are the most important ones relating to that document.\n\nFor example, let's contrast two takeaway menus:\n\n* Chicken Kurma Masala, Chicken Tikka Masala, Chicken Curry, Saag Aloo, Pilau Rice, Plain Rice\n* Chicken Chow Mein Noodles, Crispy Beef, Chicken and Cashew nut curry, Prawn Crackers, Egg Fried Rice, Steamed rice, Vegetable Noodles\n\n<img src=\"https://media.giphy.com/media/12xu9HYTRo4Eg0/giphy.gif\">\n\nThe word 'Chicken' appears the most in the first menu, followed by 'masasla' and 'Rice'. This is \"text frequency\".\n\nBut that's only half the story - since chicken and rice are quite common words in general. So we need a way to measure if a word is rare. The word 'masala' only appears in the first menu, whereas 'chicken' and 'rice' appears in both orders. Hence, the rareness of a word can be determined by how infrequently it appears across all the documents in the corpus. This is \"inverse document frequency\".\n\nCombining text-frequency and inverse-document frequency scores for each token give it a TF-IDF score. This way, we can derive that the word 'masala' is the most important word from the first order, since it has both high TF and IDF scores.\n\n::: {.column-margin}\nTF-IDF can also be done for words, for example splitting the word `manner` into character tokens:\n\n* `n` appears twice in the word\n* `m` is rarer\n\nImagine now that three of the letters are dropped. We are much more likely to guess that the word `m_nn_` could be manner, whereas seeing `_a__er` is far less informative.\n:::\n\n#### Text Frequency\n\n<img src=\"https://media.giphy.com/media/B8Bp8MfpmKbWU/giphy.gif\">\n\nWe might think that **words that are repeated many times in the menu are more characteristic of that restaurant**\n\nSo let's count them:\n\n::: {#cell-5 .cell execution_count=2}\n``` {.python .cell-code}\nmenus = [\n    'Chicken Kurma Masala, Chicken Tikka Masala, Chicken Curry, Pilau Rice, Plain Rice', \n    'Chicken Chow Mein Noodles, Crispy Beef, Chicken curry, Egg Fried Rice, Plain rice, Vegetable Noodles'\n    ]\n\ncnt_vec = CountVectorizer()\ncount_mat = cnt_vec.fit_transform(menus)\npd.DataFrame(\n    data = count_mat.todense(), \n    index = ['Order_1', 'Order_2'], \n    columns = cnt_vec.get_feature_names_out()\n    )\n```\n\n::: {.cell-output .cell-output-display execution_count=2}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>beef</th>\n      <th>chicken</th>\n      <th>chow</th>\n      <th>crispy</th>\n      <th>curry</th>\n      <th>egg</th>\n      <th>fried</th>\n      <th>kurma</th>\n      <th>masala</th>\n      <th>mein</th>\n      <th>noodles</th>\n      <th>pilau</th>\n      <th>plain</th>\n      <th>rice</th>\n      <th>tikka</th>\n      <th>vegetable</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Order_1</th>\n      <td>0</td>\n      <td>3</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>1</td>\n      <td>2</td>\n      <td>1</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>Order_2</th>\n      <td>1</td>\n      <td>2</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>2</td>\n      <td>0</td>\n      <td>1</td>\n      <td>2</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nThe most common token in the first document is `chicken`, followed by `masala` and `rice`. The second menu also has `chicken` as its most common word, followed `noodles` and `rice`.\n\nIn practice, the counts are usually logged. This is a heuristic for diminishing returns - we expect the additional marginal importance for having a token appear once vs twice is greater than it appearing nine times vs 10 times. This is particularly important for longer documents with many tokens.\n\n$$\nTF = \\log(1 + \\text{\\# tokens in document})\n$$\n\n(we add one as $log(0)$ is undefined)\n\n::: {#cell-7 .cell execution_count=3}\n``` {.python .cell-code}\npd.DataFrame(\n    data = np.log(count_mat.todense()+1), \n    index = ['menu_1','menu_2'],\n    columns = cnt_vec.get_feature_names_out()\n    ).round(2)\n```\n\n::: {.cell-output .cell-output-display execution_count=3}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>beef</th>\n      <th>chicken</th>\n      <th>chow</th>\n      <th>crispy</th>\n      <th>curry</th>\n      <th>egg</th>\n      <th>fried</th>\n      <th>kurma</th>\n      <th>masala</th>\n      <th>mein</th>\n      <th>noodles</th>\n      <th>pilau</th>\n      <th>plain</th>\n      <th>rice</th>\n      <th>tikka</th>\n      <th>vegetable</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>menu_1</th>\n      <td>0.00</td>\n      <td>1.39</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.69</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.69</td>\n      <td>1.1</td>\n      <td>0.00</td>\n      <td>0.0</td>\n      <td>0.69</td>\n      <td>0.69</td>\n      <td>1.1</td>\n      <td>0.69</td>\n      <td>0.00</td>\n    </tr>\n    <tr>\n      <th>menu_2</th>\n      <td>0.69</td>\n      <td>1.10</td>\n      <td>0.69</td>\n      <td>0.69</td>\n      <td>0.69</td>\n      <td>0.69</td>\n      <td>0.69</td>\n      <td>0.00</td>\n      <td>0.0</td>\n      <td>0.69</td>\n      <td>1.1</td>\n      <td>0.00</td>\n      <td>0.69</td>\n      <td>1.1</td>\n      <td>0.00</td>\n      <td>0.69</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n#### Inverse Document Frequency\n\nText-frequency on its own is insufficient. For example in the first order, chicken in general is a common word, so it appearing frequently in a document doesn't provide much information. Masala on the other hand is a rare word, so if it appears frequently this is very informative. \n\nWe thus need to **introduce an additional concept of word uniqueness**, or equivalently the opposite of how frequently it appears across our entire 'corpus' of menu orders.\n\n<img src=\"https://media.giphy.com/media/5vR6pNsjhoKwo/giphy.gif\">\n\nEnter Karen Spärck Jones, with the concept of ‘inverse document frequency’. This is simply the idea that it is not just how often a word appears, **but how unique the word is across all sentences (or ‘documents’), that determines how important it is.**\n\nFor example, a rare word like `Masala` only appears in a few documents, so it will have a low document frequency, and thus a high inverse document frequency score.\n\nWe often calculate inverse document frequency using the following logic:\n\n$$\n\\text{idf} = 1+\\ln \\left(\\frac{1+\\text{\\# docs in corpus}}{1+\\text{\\# docs token appears in}} \\right)\n$$\n\nThe word chicken appears in both docs, so the idf score is $1+\\ln(\\frac{1+2}{1+2})=1$. \nOn the other hand, the word masala only appears in one doc, so it gets an IDF score of  $1+\\ln(\\frac{1+2}{1+1})\\sim1.4$\n\n::: {#cell-9 .cell execution_count=4}\n``` {.python .cell-code}\nidf_vec = TfidfTransformer(smooth_idf=True,norm=None)\nidf_mat = idf_vec.fit_transform((count_mat > 0).astype(int))\npd.DataFrame(\n    data = idf_mat.todense(),\n    index = ['menu_1', 'menu_2'], \n    columns = cnt_vec.get_feature_names_out()\n).round(1)\n```\n\n::: {.cell-output .cell-output-display execution_count=4}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>beef</th>\n      <th>chicken</th>\n      <th>chow</th>\n      <th>crispy</th>\n      <th>curry</th>\n      <th>egg</th>\n      <th>fried</th>\n      <th>kurma</th>\n      <th>masala</th>\n      <th>mein</th>\n      <th>noodles</th>\n      <th>pilau</th>\n      <th>plain</th>\n      <th>rice</th>\n      <th>tikka</th>\n      <th>vegetable</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>menu_1</th>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.4</td>\n      <td>1.4</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.4</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>1.4</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>menu_2</th>\n      <td>1.4</td>\n      <td>1.0</td>\n      <td>1.4</td>\n      <td>1.4</td>\n      <td>1.0</td>\n      <td>1.4</td>\n      <td>1.4</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1.4</td>\n      <td>1.4</td>\n      <td>0.0</td>\n      <td>1.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>1.4</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\n### TF x IDF\n\nAnd to combine - TF-IDF is simply the multiplication between the TF and IDF scores, which \"combines\" the text-frequency and inverse-document-frequency concepts in the same token.\n\nNote that in practice, we do two \n\n::: {#cell-11 .cell execution_count=5}\n``` {.python .cell-code}\ntfidf_vec = TfidfVectorizer(sublinear_tf=True)\ntfidf_mat = tfidf_vec.fit_transform(menus)\npd.DataFrame(\n    data = tfidf_mat.todense(), \n    index = ['menu_1', 'menu_2'], \n    columns = tfidf_vec.get_feature_names_out()\n    ).round(2)\n```\n\n::: {.cell-output .cell-output-display execution_count=5}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>beef</th>\n      <th>chicken</th>\n      <th>chow</th>\n      <th>crispy</th>\n      <th>curry</th>\n      <th>egg</th>\n      <th>fried</th>\n      <th>kurma</th>\n      <th>masala</th>\n      <th>mein</th>\n      <th>noodles</th>\n      <th>pilau</th>\n      <th>plain</th>\n      <th>rice</th>\n      <th>tikka</th>\n      <th>vegetable</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>menu_1</th>\n      <td>0.00</td>\n      <td>0.46</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.22</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.31</td>\n      <td>0.52</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.31</td>\n      <td>0.22</td>\n      <td>0.37</td>\n      <td>0.31</td>\n      <td>0.00</td>\n    </tr>\n    <tr>\n      <th>menu_2</th>\n      <td>0.27</td>\n      <td>0.32</td>\n      <td>0.27</td>\n      <td>0.27</td>\n      <td>0.19</td>\n      <td>0.27</td>\n      <td>0.27</td>\n      <td>0.00</td>\n      <td>0.00</td>\n      <td>0.27</td>\n      <td>0.46</td>\n      <td>0.00</td>\n      <td>0.19</td>\n      <td>0.32</td>\n      <td>0.00</td>\n      <td>0.27</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nCool, so combining text frequency and inverse document frequency now reveals the most important word in the first menu is `masala`, and in the second one it is `noodles`. Nice.\n\nFinally, we don't have to limit ourselves to sentences, and can split stings into character tokens, applying the same logic: for the word $\\text{queen}$, we would find the letters $q$ and $e$ to be the most informative because of their rarity (IDF) and being repeated (TF). This could be useful in predicting the word being types or correcting mispelling - let's jump into a character-level example when discussing cosine similarity.\n\n# Cosine similarlity\n\n### What is it used for?\n\nCosine similarity can help measure how similar two words or documents are. For example, we could better match a search to a result using this, whereas 'keywords' would just weight every word equally. \n\n<img src=\"https://media.giphy.com/media/13cgadB959Y0BW/giphy.gif\">\n\n### What is the intuition?\n\nTF-IDF creates a row of scores for each token in the text, for example the first order had high TF-IDF scores for masala, chicken and rice, and low scores for noodles and fried. If we have another document that has high TF-IF scores for masala, chicken and rice, and low scores for noodles and fried, we might think it is similar to the first order. Cosine similarity gives a measure between one and zero as to how similar the two texts are.\n\n#### Vectorization\n\nTo take this further, what we have done using TF-IDF is a form of 'vectorization'. If we were to plot the first order in 16 dimensional space, with one axis for each word, the line remains at zero for the 'beef' axis, travels 0.46 along the 'chicken' axis, etc.\n\n**We can then measure the angle (the cosine) between these lines to get an idea of how similar two documents are.** Cosine similarity is often used to match, how example, a search string to a result. \n\nThis can feel a bit abstract, but hang on in there because this is a key part of intution!\n\nLet's dive into this with an example we can visualise in 3 dimensional space to make this clearer.\n\n\n",
    "supporting": [
      "nlp_mining_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}