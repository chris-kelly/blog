{
  "hash": "b3985c2d0ca195475bacf13f8b290eb1",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Shrinkage priors for Lasso and Ridge\"\nauthor: \"Chris Kelly\"\ndate: '02-24-24'\ncategories: [Bayes, MLE, Regularization, Laplace]\nformat:\n  html:\n    code-fold: true\n    toc: true\n    other-links:\n        - text: MLE for Linear regression\n          href: mle_ols_normal.html\nimage: '../images/bayes_lasso.png'\nbibliography: ../references.bib\n---\n\n\n::: {.callout-tip}\n### What are we exploring?\nShowing Laplace priors in bayesian regression are equivalent to Lasso regularization\n:::\n\n\n<!-- # Regularization\n\nRegularization applies a penalty term to the cost function for large coefficients estimates. For example, we can alter the RSS (residual sum of squares) cost function in the following way:\n\n$$\n\\min_\\beta{\\left[\\sum_{i=1}^N{\\epsilon_i^2}-\\frac{\\lambda}{2}\\sum_{k=1}^K{\\beta_k^2}\\right]}\n$$\n\nThe penalty term $\\lambda /2$ increases the cost if larger coefficients are estimated, resulting in smaller coefficients being estimated to minimize the the cost function:\n\n$$\n\\min_{\\beta_j}{\\left[\\sum_{i=1}^N{\\epsilon_i^2}-\\frac{\\lambda}{2}\\sum_{k=1}^K{\\beta_k^2}\\right]}\n\\Rightarrow \\sum_{i=1}^N 2\\epsilon_i\\left(\\frac{\\partial\\epsilon_i}{\\partial\\beta_j}\\right)-\\lambda\\beta_j=0 \\\\\n$$\n\nAs such, we see that minimizing the cost function will artificially reduce the coefficents depending on the size of $\\lambda$. This is an example of L2 regularization (ridge). Selecting a good value for $\\lambda$ can be found through cross-validation.\n\nHowever, L2 regularization will never zero-out a coefficient. Instead, we can similarly derive the same penalty through L1 regularisation (LASSO) which actively zeroes out coefficients:\n\n$$\n\\min_\\beta{\\left[\\sum_{i=1}^N{\\epsilon_i^2}-\\frac{\\lambda}{2}\\sum_{k=1}^K{\\mid\\beta_k\\mid}\\right]}\n$$\n\nEmpirically though, LASSO is shown to be unreliable, zero-es out coefficients that are small but significant.\n\nA more robust method is the elastic net, which combines L1 and L2 regularization. It is often implemented in the following way:\n\n$$\n\\min_\\beta{\\left[\\sum_{i=1}^N{\\epsilon_i^2}-\\frac{\\lambda}{2}\\left(\\alpha\\sum_{k=1}^K{\\mid\\beta_k\\mid}+(1-\\alpha)\\sum_{k=1}^K{\\beta_k^2}\\right)\\right]}\n$$\n\n\nHowever, regularization in a frequentist model can be a blunt instrument when thinking about regularization of coefficients. This is because $\\lambda$ is a single penalty for all coefficients, so if poorly specified, can either include a very large number of irrelevant predictors, or over-shink included coefficients. In particular, it might drop small but signficant features.\nAn ideal method should thus only induce weak shrinkage on large coefficients, and stronger shrinkage to zero on less relevant effects.\nCue bayesian regression: -->\n\n# Bayesian Linear regression:\n\nWhen performing ordinary linear regression using maximum likelihood, we model the noise around $y$ as being generated from a gaussian distributed process - conditional on the data $X$ and estimated model parameters $\\beta$ and $\\sigma$:\n\n$$\n\\displaylines{\n\\begin{align}\np(y|\\beta,X,\\sigma) \n& = \\prod_{i=1}^{n}{\\frac{1}{\\sigma\\sqrt{2\\pi}}}\\exp{\\left\\{-\\frac{1}{2\\sigma^2}\\epsilon_i^2\\right\\}} \\\\\n& \\sim \\prod_{i=1}^{n}{\\frac{1}{\\sigma\\sqrt{2\\pi}}}\\exp{\\left\\{-\\frac{1}{2\\sigma^2}\n\\left( y - X \\beta \\right)^2\n\\right\\}}\n\\end{align}\n}\n$$\n\nWe can formulate our estimate of the coefficient in a bayesian way if we model $\\beta$ as a random variable (rather than a fixed quantity as per frequenist thinking):\n\n$$\n\\displaylines{\n\\begin{align}\np(\\beta|X,y,\\sigma) \n& =\n\\frac{\n  p(y|\\beta,X,\\sigma)\n  p(\\beta|X,\\sigma)\n  }\n  {p(y|X,\\sigma)} \n\\\\ \\\\ & =\n\\frac{\n  p(y|\\beta,X,\\sigma)\n  p(\\beta|X,\\sigma)\n  }\n{\\int{}{} p(y|X,\\beta,\\sigma)p(\\beta|X,\\sigma) \\mathrm{d}\\beta}\n\\end{align}\n}\n$$\n\nWhere:\n\n* $p(Y|\\beta,X,\\sigma)$ is the likelihood function\n* $p(Y|X,\\sigma)$ is the evidence (the data we feed into the model)\n* $p(\\beta|X,\\sigma)$ is the prior for the coefficient\n\nNow in frequentist regression, we assume no prior at all: $\\beta$ is fixed, so $p(\\beta|X,\\sigma)=p(\\beta)=1$, regardless of the evidence observed. Thus we are just left with finding the coefficients that maximise $p(Y|\\beta,X,\\sigma)$.\n\nBy taking the negative log likelihood, we find this is identical to finding the coefficient values that minimise the sum of squared residuals [(see the derivation here)](mle_ols_normal.html):\n\n$$\n\\displaylines{\n\\begin{align}\n\\beta^* & =\\arg\\min_\\beta{\\left[ \n  \\sum_{i=1}^N{\\epsilon_i^2} \n\\right]}\n\\end{align}\n}\n$$\n\nHowever, we could use other types of priors, with mass around zero, to  apply regularization on our coefficients.\n\n# Using Laplace priors to shrink coefficients:\n\nRegularization aims to eliminate some of our predictors to create a more parsimonious model in a systematic way, and/or reduce their magnitude to prevent overfitting.\n\nPicking a prior for our coefficient that is concentrated at zero can help achieve this - for example, we could use a Laplace distribution, with a location parameter $\\mu$ of zero as visualised below:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlaplace_dist <- function(x, mu = 0, gamma = 1) {\n  laplace_pdf <- function(x,mu,gamma) {\n    return(\n      exp(-abs(x-mu)/gamma)/(2*gamma)\n    )\n  }\n  y = sapply(x, FUN = function(i) laplace_pdf(i,mu,gamma))\n  return(y)\n}\n\nx = seq(-10,10,0.1)\nplot(\n  x=x,\n  y=laplace_dist(x, gamma=1),\n  type='l',\n  main='Laplace(gamma=0.5)'\n  )\n```\n\n::: {.cell-output-display}\n![](bayes_lasso_files/figure-html/unnamed-chunk-1-1.png){width=864}\n:::\n:::\n\n\nNot only does the Laplace pdf increase when closer to zero, but it increases at an accelerating rate. Thus, we can imagine that the closer the likelihood estimate of the coefficient is to zero, the greater the influence of the prior.\n\n### Laplace priors ~ Lasso Regression\n\nRecall that the probability density function of Laplace is\n$$\nf(x|\\mu,\\gamma) =\n\\frac{1}{2\\gamma}\n\\exp{ \\left\\{ \n  \\frac{x - \\mu}{\\gamma} \\right\n\\} }\n$$\n\nThen the prior can be written as the maximum likelihood across the estimated coefficients each of the $K$ features in the model, for a given penalty importance $\\lambda$:\n$$\n\\displaylines{\n\\begin{align}\np(\\beta|X,\\sigma) \\sim p(\\beta)\n& = \\prod_{k=1}^{K}{ \n  \\frac{1}{2 \\gamma}\n  \\exp{\\left\\{ \\frac{ -\\mid \\beta - \\mu \\mid}{\\gamma} \\right\\}}\n}\n\\\\ \\\\ & = \\prod_{k=1}^{K}{ \n  \\frac{1}{2 \\gamma}\n  \\exp{\\left\\{ \\frac{ -\\mid \\beta \\mid}{\\gamma} \\right\\}}\n}\n\\end{align}\n}\n$$\n\n:::{.column-margin}\nThis is how the prior is defined in @tibshirani1996lasso. Since the prior is assumed before any data is observed, intuitively $\\beta$ should not need conditioning on $X$.\n\nHowever, @park2008lasso found that not conditioning on $\\sigma^2$ can result in non-unimodal posterior, so in practice a non-informative scale-invariant marginal prior $\\pi(\\sigma^2) = 1/\\sigma$ on $\\sigma^2$ is used.\n:::\n\nNow if set this as the prior, we can derive the cost function that we aim to minimize when $X$ is observed.\n\n$$\n\\displaylines{\n\\begin{align}\n\\max_{\\beta} {\\left\\{ p(\\beta|y,X,\\sigma) \\right\\}}\n& = \\max_\\beta{\n  \\left[ p(y|\\beta,X,\\sigma) \\times p(\\beta)\\right] \n} \\\\\n& \\sim \\min_\\beta{\n  \\left\\{ -\\log{\n    \\left[ p(y|\\beta,X,\\sigma) \\times p(\\beta)\\right] \n    }\\right\\}\n} \\\\\n& = \\min_\\beta{\n  \\left\\{ \n    -\\log{\\left[ p(y|\\beta,X,\\sigma) \\right]\n    -\\log{\\left[p(\\beta)\\right]}\n    }\\right\\}\n} \\\\\n& = \\min_\\beta \\left\\{ \n\\frac{1}{2} \\left( \n  \\cancel{ n \\log{[2 \\pi \\sigma^2]} }\n  + \\frac{1}{\\sigma^2} \\sum_{i=1}^N{\\epsilon_i^2}\n\\right)\n- \\log{\\left[ \n  \\prod_{k=1}^{K}{ \n    \\frac{1}{2\\gamma}\n    \\exp{\\left\\{ \\frac{ -\\mid \\beta \\mid}{\\gamma} \\right \\}}\n  }\n\\right]}\n\\right\\} \\\\\n& = \\min_\\beta \\left\\{ \n  \\frac{1}{2\\sigma^2} \\sum_{i=1}^N{\\epsilon_i^2}\n- \\sum_{k=1}^{K}{ \\log{ \\left[\n    \\frac{1}{2\\gamma}\n    \\exp{ \\left\\{ \\frac{ -\\mid \\beta \\mid}{\\gamma} \\right\\} }\n    \\right]\n    }\n  }\n\\right\\} \\\\\n& = \\min_\\beta \\left\\{ \n  \\frac{1}{2\\sigma^2} \\sum_{i=1}^N{\\epsilon_i^2}\n- \\cancel{ K \\log{ \\left[ \\frac{1}{2\\gamma} \\right] } }\n- \\sum_{k=1}^{K}{ \n    \\log{ \\left[ \\exp{\\left\\{ \\frac{ -\\mid \\beta \\mid}{\\gamma} \\right\\}} \\right] }\n  }  \n\\right\\} \\\\\n& = \\min_\\beta \\left\\{ \n  \\frac{1}{2\\sigma^2} \\sum_{i=1}^N{\\epsilon_i^2}\n+ \\frac{1}{\\gamma} \\sum_{k=1}^{K}{ \\mid \\beta \\mid}  \n\\right\\} \\\\ \\\\\n\\therefore \\beta^* \n& =\\arg\\min_\\beta{\\left\\{\n  \\frac{1}{2\\sigma^2} \\sum_{i=1}^N{\\epsilon_i^2}\n+ \\frac{1}{\\gamma}\\sum_{k=1}^{K}{\\mid\\beta\\mid} \n  \\right\\}\n}\n\\end{align}\n}\n$$\n\n::: {.column-margin}\nNote that terms that do not vary with the choice of $\\beta$ drop out\n:::\n\n::: {.callout-tip title=\"Key point\"}\nHence setting a Laplace prior on the coefficients is - almost - equivalent to running L1 regularization, where $1/\\gamma$ is the parameter influencing the penalty size.\n:::\n\nAlthough there is sparsity in the mode of the prior, when combining this with the likelihood distribution, the sparsity will not continue into the posterior distribution. In practice, if the posterior of $\\beta$ is sufficiently small, we would want to drop it - so a threshold value for the size at which coefficients are zero-oed out is set as a hyperparameter.\n\n# Using Gaussian priors to shrink coefficients:\n\nIn a similar way to before for Lasso, we set our coefficient priors to each have a Gaussian distribution, with a location parameter $\\mu=0$:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnormal_pdf <- function(x, mu = 0, sigma = 2) {\n  z = (x-mu)/sigma\n  y = (2*pi*sigma^2)^(-1/2) * exp(-0.5*z^2)\n  return(y)\n}\n\nx = seq(-10,10,0.1)\nplot(\n  x=x,\n  y=normal_pdf(x),\n  type='l',\n  main='Normal(sigma=1)'\n  )\n```\n\n::: {.cell-output-display}\n![](bayes_lasso_files/figure-html/unnamed-chunk-2-1.png){width=864}\n:::\n:::\n\n\n### Gaussian priors ~ Ridge Regression\n\nThe gaussian prior can be written as the maximum likelihood across the estimated coefficients each of the $K$ features in the model, for a given penalty importance $\\lambda$:\n\n$$\n\\displaylines{\n\\begin{align}\np(\\beta|X,\\sigma) \\sim p(\\beta)\n& = \\prod_{k=1}^{K}{\\frac{1}{\\sigma\\sqrt{2\\pi}}}\\exp{\\left\\{-\\frac{1}{2\\sigma^2}\\beta_k^2\\right\\}}\n\\end{align}\n}\n$$\n\nNow if set this as the prior, we can derive the cost function that we aim to minimize when $X$ is observed.\n\n$$\n\\displaylines{\n\\begin{align}\n\\max_{\\beta} {\\left\\{ p(\\beta|y,X,\\sigma) \\right\\}}\n& \\sim \\min_\\beta{\n  \\left\\{ \n    -\\log{\\left[ p(y|\\beta,X,\\sigma) \\right]\n    -\\log{\\left[p(\\beta)\\right]}\n    }\\right\\}\n} \\\\\n& = \\min_\\beta \\left\\{ \n\\frac{1}{2} \\left( \n  \\cancel{ n \\log{[2 \\pi \\sigma^2]} }\n  + \\frac{1}{\\sigma^2} \\sum_{i=1}^N{\\epsilon_i^2}\n\\right)\n- \\log{\\left[ \n  \\prod_{k=1}^{K}{\n    \\frac{1}{\\sigma\\sqrt{2\\pi}}\n    \\exp{\\left\\{-\\frac{1}{2\\sigma^2}\\beta_k^2\\right\\}}\n    }\n\\right]}\n\\right\\} \\\\\n& = \\min_\\beta \\left\\{ \n  \\frac{1}{2\\sigma^2} \\sum_{i=1}^N{\\epsilon_i^2}\n- \\sum_{k=1}^{K}{ \\log{ \\left[\n    \\frac{1}{\\sigma\\sqrt{2\\pi}}\n    \\exp{\\left\\{-\\frac{1}{2\\sigma^2}\\beta_k^2\\right\\}}\n    \\right]\n    }\n  }\n\\right\\} \\\\\n& = \\min_\\beta \\left\\{ \n  \\frac{1}{2\\sigma^2} \\sum_{i=1}^N{\\epsilon_i^2}\n- \\cancel{ K \\log{ \\left[ \\frac{1}{\\sigma\\sqrt{2\\pi}} \\right] } }\n- \\sum_{k=1}^{K}{ \n    \\log{ \\left[ \\exp{\\left\\{ -\\frac{1}{2\\sigma^2}\\beta_k^2 \\right\\}} \\right] }\n  }  \n\\right\\} \\\\\n& = \\min_\\beta \\left\\{ \n  \\frac{1}{2\\sigma^2} \\sum_{i=1}^N{\\epsilon_i^2}\n+ \\frac{1}{2\\sigma^2} \\sum_{k=1}^{K}{ \\beta^2 }  \n\\right\\} \\\\ \\\\\n\\therefore \\beta^* \n& =\\arg\\min_\\beta{\\left\\{\n  \\frac{1}{2\\sigma^2} \\sum_{i=1}^N{\\epsilon_i^2}\n+ \\frac{1}{2\\sigma^2} \\sum_{k=1}^{K}{ \\beta^2 }\n  \\right\\}\n}\n\\end{align}\n}\n$$\n\n::: {.callout-tip title=\"Key point\"}\nHence setting a Gaussian prior on the coefficients is - almost - equivalent to running L1 regularization, where the variance of the prior - $\\sigma^2$ is the parameter directly influencing the penalty size.\n:::",
    "supporting": [
      "bayes_lasso_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}