---
title: "Why OLS is so quick to optimize"
author: "Chris Kelly"
date: '02-20-24'
categories: []
format:
  html:
    code-fold: true
---

::: {.callout-tip}
## What we are exploring
Ordinary Linear Regression: why is it so quick to optimize?
:::

### Intro

OLS can jump straight to the optimum weights (coefficients) in one fell swoop. 
Neural networks usually need lots of steps to optimize their weights (parameters) though.
Why is this the case?

It is down to two things:

* Convexity of the cost function
* The common use of transcendental functions in neural networks

### 

Ordinary least squares regression minimizes the sum of squared residuals. What does this mean though?

A residual is the prediction error: for an observation $i$, it is the gap between the true outcome $y_i$ and the prediction for that outcome $\hat{y}_i$. 

In ordinary least squares regression, a prediction is generated by multiplying each of the features in $X$ by a vector of coefficients $\beta$, such that $\hat{y} = X^{\intercal}\beta$.

In order to minimize the sum of squared residuals, we thus want to find optimum values for the vector of coefficients, $\beta^*$. The good news is we can solve this in one fell (mathematical) swoop!

What we do is partially differentiate the cost function (the sum of squared residuals) as a function of each coefficients $\beta$ we can choose. We find the coefficients where the differential is equal to zero to find the global minima, and find there is a nice analytical solution to this.

Why can we do this? Well chiefly its because OLS is a bit of a special case:

* **The minima is a global minima:** The hessian matrix is positive definite, and hence the cost function is strictly convex. This means we know that when a choice of coefficients is found that ensure the partially differentiated cost function is equal zero, this minima is also a global one
* **There is only one solution for the optimum coefficient:** We assume that the matrix is full rank (every feature provides additional predictive power) and that the number of predictors is smaller than the number of obervations. This also means that partially differentiating is okay to do!
* **A closed-form solution can be found** The predictions are generated from $X$ using a simple, purely algebraic function, i.e. the sum-product of $X$ by $\beta$. This means we can find an analytical solution to the optimal choice $\beta^*$. Note this often isn't possible for e.g. neural networks, since their non-linear activation functions are often transcendental.

### 1. Expand the sum of squared residuals
$$
\displaylines{
\begin{align}
\sum_{i=1}^n{\hat{\epsilon}_i^2} & = \hat{\epsilon}^{\intercal}\hat{\epsilon} 
\\& =(y-X\hat{\beta})^{\intercal}(y-X\hat{\beta}) 
\\& = y^{\intercal}y - y^{\intercal}X\hat{\beta}-
\hat{\beta}^{\intercal} X^{\intercal}y+
\hat{\beta}^{\intercal}X^{\intercal}X\hat{\beta} 
\\& = y^{\intercal}y - 2y^{\intercal}X\hat{\beta}
+\hat{\beta}^{\intercal}X^{\intercal}X\hat{\beta}
\end{align}
}
$$

Note that:

$$
\displaylines{
y^{\intercal}X\hat{\beta} =
\hat{\beta}^{\intercal} X^{\intercal}y \\
\because \underset{1 \times n}{y^{\intercal}} \times 
\underset{n \times k}{X} \times 
\hat{\underset{k \times 1}{\beta}}
 = 
\hat{\underset{1 \times k}{\beta}^{\intercal}} \times
\underset{k \times n}{X^{\intercal}} \times
\underset{n \times 1}{y} 
\\ \text{i.e. they are both scalars}
}
$$

### Minimise residual sum of squares with respect to beta
$$
\displaylines{
\begin{align}
\frac{\partial}{\partial\hat{\beta}}\hat{\epsilon}^{\intercal}\hat{\epsilon} & \equiv
\begin{bmatrix}
    \frac{\partial}{\partial\hat{\beta}_1}\hat{\epsilon}^{\intercal}\hat{\epsilon} \\
    \frac{\partial}{\partial\hat{\beta}_2}\hat{\epsilon}^{\intercal}\hat{\epsilon} \\
    \vdots \\
    \frac{\partial}{\partial\hat{\beta}_k}\hat{\epsilon}^{\intercal}\hat{\epsilon}
\end{bmatrix} 
\\ & = \frac{d}{d\hat{\beta}}(
y^{\intercal}y - 2y^{\intercal}X\hat{\beta}
+\hat{\beta}^{\intercal}X^{\intercal}X\hat{\beta}) \\ & = 0 - 2X^{\intercal}y +((X^{\intercal}X)\hat{\beta} + (X^{\intercal}X)^{\intercal}\hat{\beta})
\\ & = -2X^{\intercal}y +  2((X^{\intercal}X)\hat{\beta})=0
\end{align}
}
$$

Note that:

$$
\displaylines{
\frac{\partial}{dx}(Ax) = A^{\intercal}x \text{ and } \frac{\partial}{dx}(xAx) = Ax + A^{\intercal}x
\\
\text{(and } X^{\intercal}X = (X^{\intercal}X)^{\intercal} \text{ by definition)} \\
}
$$

### Derive orthogonality condition i.e. strict exogeneity

$$
\displaylines{
\cancel{2}X^{\intercal}y +  \cancel{2}((X^{\intercal}X)\hat{\beta})=0 \\
\therefore -X^{\intercal}(y -X^{\intercal}\hat{\beta}) =
X^{\intercal}\hat{\epsilon} = 0 \\
}
$$

Note that:

* If a column in $X$ is a vector of ones (i.e. an intercept), then the sum (mean) of errors is zero: $E[\epsilon] = 0$
* Otherwise, there is no heteroskedasticity with respect to any regressor: $E[\epsilon|X] = 0$

### Derive optimal coefficients

$$
\displaylines{
\cancel{2}X^{\intercal}y +  \cancel{2}((X^{\intercal}X)\hat{\beta})=0 \\
\therefore (X^{\intercal}X)\hat{\beta}=X^{\intercal}y \\
\therefore \hat{\beta}=(X^{\intercal}X)^{-1}X^{\intercal}y
}
$$
