---
title: "Deriving Logistic Regression Coefficients"
author: "Chris Kelly"
date: '02-21-25'
categories: [Maximum Likelihood, Generalized Linear Models]
format:
  html:
    code-fold: true
    toc: true
image: '../images/glm_logistic.png'
---

::: {.callout-tip}
### What are we exploring?
Deriving coefficients to predict the likelihood of a binary outcome using maximum likelihood estimation and the logit link funciton.
:::

# Setting some intuition

Imagine a basketball player has a 90% chance of making a freethrow. 

For freethrow attempt $i$, we observe the outcome, $y_i$, as being 1 if there is a success (i.e. the freethrow is made), or 0 if it is a failure (a miss). 

We can denote $p$ as a fixed probability for success. It follows that the probability of failure as $1-p$ (i.e. a 10% chance of missing).

This can be formalised as the following (the probability mass function of the bernoulli):

$$
\displaylines{
\begin{align}
p(y_i) = 
\begin{cases}
  p & \text{if}\ y_i=1 \\
  1-p & \text{if}\ y_i=0
\end{cases}
\end{align}
}
$$

Which is equivalent to the following:

$$
p(y_i) = p^{y_i}(1-p)^{1-y_i}
$$

:::{.column-margin}
Since $x^0 = 1$:

* if $y_i=1$, $p^{1}(1-p)^{0} = p$
* if $y_i=0$, $p^{0}(1-p)^{1} = 1-p$
:::

Now imagine that, rather than $p$ being fixed at one value, there are some external variables that influence the shooter (for example, whether the game is at home or away). Let's denote these relevant variables $X$, and the relationship they have on the probability of success as $\beta$.

We can denote then denote the probability of success as the following:

$$
p(y_i|X_i\beta) = p(y_i=1|X_i\beta)^{y_i}(1-p(y_i=1|X_i\beta))^{1-y_i}
$$

Now imagine we observe the shooter take 100 freethrows (sample size $N$), against many different teams, point differentials etc. We want to learn from this past experience to estimate the probability that they make the next one.

<!-- Note how this is modelling the probability $p_i$, but we only actually observe the binary outcome $y_i$. In econometrics, we often assume there is some underlying threshold $y^*$,  -->

# Cost function for bernoulli regression

### Applying the bernoulli pdf

Across $N$ observations collected, we want to find the values of $\beta$ that maximise the likelihood of observing all outcomes (the vector of results $y$).

Let's split the outcomes between successes and failures. We thus derive the cost function:

$$
p(y|X\beta) =
\underbrace{
  \prod_{i=1}^{N}{ p(y_i=1|X_i\beta)^{y_i} }
}_{y_i=1} \times
\underbrace{
  \prod_{i=1}^{N}{ (1-p(y_i=1|X_i\beta))^{1-y_i} }    
}_{y_i=0}
$$

Maximum likelihood estimation aims to find the set of coefficients that maximises the likelihood of observing the evidence we have. We thus aim to find the coefficients $\beta$ that maximise the likelihood of observing $y$ across all $n$ samples.

### Taking the negative log-likelihood

In practice, dealing with a cost function made up of a sum product is tricky - it is easier to take the log and deal with addition instead. Further, rather than maximise, it is common to "minimize" cost functions, so the negative log-likelihood is usually used.

:::{.column-margin}
Recall that $\log{\left(ab\right)} = \log{\left(a\right)} + \log{\left(b\right)}$
:::

$$
\displaylines{
\begin{align}
& \max_\beta{p(y|\beta,X)} \\
= & 
\max_\beta{\left\{ 
  \prod_{i=1}^{N}{ p(y_i=1|X_i\beta)^{y_i} }
  \times
  \prod_{i=1}^{N}{ (1-p(y_i=1|X_i\beta))^{1-y_i} }
  \right\}}
\\ \\ \Rightarrow & 
\min_\beta{\left\{ -\log{ \left[
  \prod_{i=1}^{N}{ p(y_i=1|X_i\beta)^{y_i} }
  \times
  \prod_{i=1}^{N}{ (1-p(y_i=1|X_i\beta))^{1-y_i} }
  \right] } \right\}}
\\ \\ = & 
\min_\beta{\left\{
  \sum_{i=1}^{N}{ -\log{ \left[ p(y_i=1|X_i\beta)^{y_i} \right] } } +
  \sum_{i=1}^{N}{ -\log{ \left[ (1-p(y_i=1|X_i\beta))^{1-y_i}\right] } }
  \right\}}  
\\ \\ = & 
\min_\beta{\left\{
  \sum_{i=1}^{N}{ -y_i\log{ \left[ p(y_i=1|X_i\beta) \right] } } +
  \sum_{i=1}^{N}{ -(1-y_i)\log{ \left[ 1-p(y_i=1|X_i\beta)\right] } }
  \right\}}  
\end{align}
}
$$

### Deriving the gradient with respect to the coefficients

We minimise the cost function by finding the optimum coefficient values $\beta^*$ so that the partial differential is equal to zero.

$$
\displaylines{
\begin{align}
& \frac{\partial}{\partial \beta_j}p(y|\beta,X) \\ \\
= &
\frac{\partial}{\partial \beta_j} \left(
\sum_{i=1}^{N}{ -y_i\log{ \left[ p(y_i=1|X_i\beta) \right] } } +
  \sum_{i=1}^{N}{ -(1-y_i)\log{ \left[ 1-p(y_i=1|X_i\beta)\right] } }
\right) \\ \\
= &
\sum_{i=1}^{N}{ y_i
  \frac{\frac{\partial}{\partial \beta_j}p(y_i=1|X_i\beta)}{p(y_i=1|X_i\beta)}
  } +
  \sum_{i=1}^{N}{ (1-y_i)
    \frac{\frac{\partial}{\partial \beta_j}\left(1-p(y_i=1|X_i\beta)\right)}{1-p(y_i=1|X_i\beta)}
    }
\end{align}
}
$$

This is as far as we can get, without now making some more assumptions. Let's imagine that we can model the 

# Logistic activation

We might assume that the the log-odds - the logarithm of the probability of success divided by the probability of failure - is linearly related to its predictors, i.e.

$$
\displaylines{
\begin{align}
\ln{\left(\frac{p}{1-p}\right)} = X\beta + \epsilon 
&& \epsilon \sim N(0,\sigma^2)
\end{align}
}
$$

This is called a "link function" - the link between the outcome, $y$, and the linear predictors $X\beta$. This specific link function is called the "logit link function".

To make predictions then for the probability of success, we need the inverse of the link function - sometimes called the "activation function" in the context of neural network. 

We can derive the inverse of the logit link by rearranging it in terms of $p$:

$$
\displaylines{
\begin{align}
E\left[
  \ln{\left(\frac{p}{1-p}\right)}
  \right] = X\beta
& \Rightarrow \frac{p}{1-p}\ = \exp{\{X\beta\}}
\\ \\
& \Rightarrow p = \exp{\{X\beta\}}(1-p)
\\ \\
& \Rightarrow p - (1+\exp{\{X\beta\}}) = \exp{\{X\beta\}}
\\ \\
& \Rightarrow p = \frac{\exp{\{X\beta\}}}{1 + \exp{\{X\beta\}}}
=  \left( 1 + \exp{\{-X\beta\}} \right)^{-1}
\end{align}
}
$$

We can see that this activation function "squashes" all outputs $X\beta \in [-\infty,\infty]$ between 0 and 1:
```{r, fig.width=9}
logistic_dist <- function(x) {
  return( (1+exp(-x))^-1 )
}
x = seq(-10,10,0.1)
plot(
  x=x,
  y=logistic_dist(x),
  type='l',col="blue",,lty=1
  )
lines(
  x=seq(-3,3,0.1),
  y=(1/4)*seq(-3,3,0.1)+0.5,
  type='l',col="red",lty=2
)
legend(
  -10,1,
  legend=c(
    "logistic activation",
    "linear activation"
    ),
  col=c("blue","red"),
  lty=c(1,2)
  )
```

:::{.column-margin}
For probabilities of between 0.3 to 0.7, we see that the logistic activation function maps very closely to that of a simply linear one. It is only at the more extreme probabilities that they diverge.
:::

We can now use this activation function to derive some coefficients.

# Optimal coefficients for the coefficient

Given that: 

$$
\hat{p_i} = \hat{p}(y_i=1|X_i \hat{\beta}) = 
\frac{1}{1+\exp{\left\{-X_i\hat{\beta}\right\}}}
$$

Then the partial differential of the probability with respect to feature $j$ is:

$$
\displaylines{
\begin{align}
& \frac{\partial }{\partial \beta_j}
\hat{p}(y_i=1|X_i \hat{\beta_j})
\\ = &
\frac{\partial }{\partial \beta_j} (1+\exp{\{-X_i\hat{\beta}\}})^{-1}
\\ = &
\frac{\partial }{\partial \beta_j} -1(1+\exp{\{-X_i\hat{\beta}\}})^{-2}
\times x_{ij}\exp{\{-X_i\hat{\beta}\}}
\\ = &
x_{ij} \left( \frac{\exp{\{-X_i\hat{\beta}\}}}{(1+\exp{\{-X_i\hat{\beta}\}})^{2}} \right)
\\ = &
x_{ij} \left( \frac{1}{(1+\exp{\{-X_i\hat{\beta}\}})} \times \frac{\exp{\{-X_i\hat{\beta}\}}}{(1+\exp{\{-X_i\hat{\beta}\}})} \right)
\\ = &
x_{ij} \left( \frac{1}{(1+\exp{\{-X_i\hat{\beta}\}})} \times \left( 1 - \frac{1}{(1+\exp{\{-X_i\hat{\beta}\}})} \right) \right)
\\ \\ = &
x_{ij} ( \hat{p}(y_i=1|X_i \hat{\beta_j}) \times (1-\hat{p}(y_i=1|X_i \hat{\beta_j})) )
\\ \\ = &
x_{ij} ( \hat{p_i} \times (1-\hat{p_i}) )
\end{align}
}
$$

And thus we can substitute this into our cost function:

$$
\displaylines{
\begin{align}
& \frac{\partial}{\partial \beta_j}p(y|\beta,X) \\ \\
= &
\sum_{i=1}^{N}{ y_i
  \frac{\frac{\partial}{\partial \beta_j}p(y_i=1|X_i\beta)}{p(y_i=1|X_i\beta)}
  } +
\sum_{i=1}^{N}{ (1-y_i)
  \frac{\frac{\partial}{\partial \beta_j}\left(1-p(y_i=1|X_i\beta)\right)}{1-p(y_i=1|X_i\beta)}
  } \\ \\
= &
\sum_{i=1}^{N}{ y_i
  \frac{x_{ij} ( \hat{p_i} \times (1-\hat{p_i}) )}{\hat{p_i}}
  } +
\sum_{i=1}^{N}{ (1-y_i)
  \frac{x_{ij} ( \hat{p_i} \times (1-\hat{p_i}) )}{(1-\hat{p_i})}
  } \\ \\
= &
\sum_{i=1}^{N}{ y_i
  \frac{x_{ij} ( \cancel{\hat{p_i}} \times (1-\hat{p_i}) )}{\cancel{\hat{p_i}}}
  } +
\sum_{i=1}^{N}{ (1-y_i)
  \frac{x_{ij} ( \hat{p_i} \times \cancel{(1-\hat{p_i})} )}{\cancel{(1-\hat{p_i})}}
  } \\ \\  
= &
\sum_{i=1}^{N}{ y_ix_{ij} (1-\hat{p_i})} +
\sum_{i=1}^{N}{ (1-y_i)x_{ij} \hat{p_i}} \\ \\
= &
\sum_{i=1}^{N}{ x_{ij} \left[
    y_i(1-\hat{p_i}) (1-y_i)(\hat{p_i})
   \right]
  }
\end{align}
}
$$

Which is the coefficient from logistic regression.

Fin.