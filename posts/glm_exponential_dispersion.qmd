---
title: "Generalized Linear Models from scratch"
author: "Chris Kelly"
date: '05-15-24'
categories: []
format:
  html:
    code-fold: false
    toc: true
image: '../images/glm_exponential_dispersion.png'
---

## Introduction

There many types of generalized linear regression models: such as linear regression, logistic regression, poisson regression etc. Every one of these models is made up of a "random component" and a "systematic component". Each also has a "link function" that combines the random and systematic parts.

Let's take the example of a poisson regression:
$$
\displaylines{
\underbrace{y_i \sim Pois(\lambda_i)}_{\text{random}} \\
\underbrace{\ln{(\lambda_i)}}_{\text{link function}} = \underbrace{X_i^{\intercal}\beta}_{\text{systematic}}
}
$$

### Random component

The random component determines how we want to model the distribution of $y$. For example, if $y_i$ is a count outcome, then it could be well suited to a poisson distribution:

$$
y_i \sim Pois(\lambda_i)
$$

The mean rate of the count is $\lambda_i$ - so we expect $y_i$ to be around $\lambda_i$. However, for any individual observation $i$, the actual observed $y_i$ will vary above and below the mean rate $\lambda_i$. In fact, by using poisson we assume the variance of $y_i$ increases as the mean rate $\lambda_i$ increases too. And this is why it is called the "random component": since $y$ varies randomly around the mean, following the poisson distribution, it is a random variable.

!!!!
GRAPH OF POISSON
!!!!

But how do we find a good estimation for $\mathbb{E}[y_i|X_i]=\lambda_i$? Concretely, how do we best map our independent variables $X_i$ to $\lambda_i$? This is down to our systematic component and link function.

### Systematic component and link function

In most cases, the systematic component $\eta(X)$ is usually just a linear transformation of $X$, most often the result of multiplying each value by some good fitted coefficients $\beta$.

$$
\eta(X_i) = X_i^{\intercal}\beta
$$

The link function is a way of choosing how to map the systematic component to the natural parameter of the random component. For example, in poisson regression, we use a log link function, which means the systematic component predicts the natural log of the mean rate of the count. 

$$
\ln{(\lambda_i)} = \eta(X) = X^{\intercal}\beta
$$

Okay, so now we want to find the best values for $\beta$. These coefficients will transform our features $X$ to make the best predictions for $\ln{(\lambda_i)}$, given we want to predict $y$ as accurately as possible (but permit larger residuals when $\lambda_i$ is larger, following the poisson distribution).

To achieve this: we can try some initial coefficients, calculate the cost function and its first derivative, update the coefficients, and continue to minimize the cost function through gradient descent.

But how cumbersome that would be to do for every type of distribution we want to model! Wouldn't it be nice if we can derive a generic representation for the cost function and its first derivative, so that we can re-use the same code for every type of regression?

## Exponential Dispersion Family of Distributions

It can be shown that many common distributions can be reformulated into the "Exponential Dispersion Family of Distributions". This generic representation makes it easier to re-use the same code to run a regression, rather than hand calculate each pdf in different ways.

First let's define some generic notation for generalized linear models:

$$
\displaylines{
\underbrace{g{(\xi_i)}}_{\text{link function}} = \underbrace{\eta(X_i)}_{\text{systematic}} \\
\underbrace{y_i \sim f_y(\xi_i,\phi_i)}_{\text{random}}
}
$$

$\xi_i$ is a shape parameter, governing the shape of the distribution (e.g. in poisson, the expected value is the mean $\xi_i=\lambda_i$). In fact for many distributions, $\xi_i$ is the expected value of $y_i$, so $\xi_i = \mu_i$.

$\phi$ is a dispersion parameter, governing the spread of the data (e.g. gaussian has the standard deviation $\phi = \sigma$). It is not always necessary if this is determined by the shape parameter though(e.g. in poisson the variance is already equated to the expected rate $\lambda_i$).

Here are some examples of the common forms of poisson, bernoulli and gaussian probability distribution functions $y \sim f_{\theta_i}(\mu_i)$, along with some choices for link functions $g(\mu)$ to use in regression:

Type | Probability Density Function: | Link Function for Regression:
---|---|---
Count<br>$\xi_i = \mu_i \equiv \lambda_i$ | $y_i \sim Pois(\mu_i)$<br>$y_i \sim \frac{\mu_i^{y} \times e^{-\mu_i}}{y!}$ | $g(\mu) = \ln{[\mu_i]}$<br>(log-link)
Binary<br>$\xi_i = \mu_i \equiv p_i$ | $y_i \sim Bern(\mu_i)$<br>$y_i \sim \mu_i^y \times (1-\mu_i)^{(1-y)}$ | $g(\mu) = \ln{\left[\frac{\mu_i}{1-\mu_i}\right]}$<br>(logit-link)
Binary<br>$\xi_i = \mu_i \equiv p_i$ | $y_i \sim Bern(\mu_i)$<br>$y_i \sim \mu_i^y \times (1-\mu_i)^{(1-y)}$ | $g(\mu) = \Phi^{-1}[\mu_i]$<br>(probit-link)
Normal<br>$\xi_i = \mu_i$ | $y_i \sim N(\mu_i,\sigma^2)$<br>$y_i \sim \frac{1}{\sqrt{2\pi\sigma^2}}e^{-\frac{1}{2}\left(\frac{y_i-\mu_i}{\sigma^2}\right)^2}$ | $g(\mu) = \mu$<br>(identity-link)

Now all of these probability density functions can be rewritten into a the exponential family form! Which we show now:

$$
\displaylines{
\begin{align}
f(y;\xi,\phi) 
& = 
\exp{ \left\{ \frac{T(y) r(\xi) - b(\xi)}{a(\phi)} + c(y,\phi) \right\} } & \tag{1.1}
\\
Pois(y;\xi,\phi) = \frac{\mu_i^{y_i}e^{-\mu_i}}{y_i!}
& \equiv \exp{ \left\{ 
\frac{
  \underbrace{y_i}_{T(y_i)}
  \underbrace{\ln{ \left[ \xi_i \right]}}_{r(\xi_i)}
  - \underbrace{\mu_i}_{b(\xi_i)}
}{
  \underbrace{1}_{a(\phi)}
} 
- \underbrace{\ln{ \left[ y_i! \right] }}_{c(y,\phi)}
\right\}} \tag{1.2}
\\
Bern(y;\xi,\phi) = \xi_i^{y_i}(1-\xi_i)^{1-y_i} 
& \equiv \exp{ \left\{ \frac{
  \underbrace{y_i}_{T(y)}
\underbrace{ \ln{
  \left[\frac{\xi_i}{1-\xi_i} \right]
} }_{r(\xi_i)}
  + \underbrace{ \ln{[1-\xi_i]} }_{b(\xi_i)}
  }{
    \underbrace{1}_{a(\phi)}
  } 
  + \underbrace{0}_{c(y_i,\phi)}
\right\}} \tag{1.3}
\\
N(y;\xi,\phi) = \frac{1}{\sqrt{2 \pi \sigma^2}} \exp{ \left\{-\frac{(y_i-\xi_i)^2}{2\sigma^2} \right\} }
& \equiv \exp{\left\{
  \frac{
    \underbrace{y_i}_{T(y_i)} \underbrace{\xi_i}_{r(\xi_i)} - 
    \underbrace{\frac{\mu_i^2}{2}}_{b(\xi_i)}
  }{
    \underbrace{\sigma^2}_{a(\phi)}
  }
- 
\underbrace{
  \frac{1}{2}
  \left( 
    \frac{y_i^2}{\sigma^2} + 
    \ln{ \left[ 2 \pi \sigma^2 \right]}
  \right)
}_{c(y_i,\phi)}
\right\}} \tag{1.4}
\end{align}
}
$$

<!-- In fact, when we put these pdfs into the exponential family form, we uncover the following:

Term | Poisson | Logistic | Probit | Gaussian
--|--|--|--|---
$T(y_i)$ | $T(y_i)=y_i$ | $T(y_i)=y_i$ | $T(y_i)=y_i$ | $T(y_i)=y_i$
$r(\xi_i)$ | $r(\xi_i)=\ln[\xi_i]$ | $\displaylines{r(\xi_i)= \\ \ln[\frac{\xi_i}{1-\xi_i}]}$ | $\displaylines{r(\xi_i)= \\ \ln[\frac{\xi_i}{1-\xi_i}]}$ | $r(\xi_i)=\xi_i$
$b(\xi_i)$ | $b(\xi_i)=\xi_i$ | $\displaylines{b(\xi_i)= \\ \ln[1+e^{\xi_i}]}$ | $\displaylines{b(\xi_i)= \\ \ln[1+e^{\xi_i}]}$ | $\displaylines{b(\xi_i)= \\ \frac{\xi_i^2}{2}}$
$a(\phi)$ | $a(\phi)=1$ | $a(\phi)=1$ | $a(\phi)=1$ | $a(\phi)=\sigma^2$
$c(y_i,\phi)$ | $c(y_i,\phi) = \ln[y!]$ | $c(y_i,\phi) = 0$ | $c(y_i,\phi) = 0$ | $\displaylines{c(y_i,\phi) = \\ \frac{1}{2}\left( \frac{y_i^2}{\sigma^2} + \ln{ \left[ 2 \pi \sigma^2 \right]}\right)}$ -->

<!-- $\phi$ | $\phi=1$ | $\phi=1$ | $\phi=1$ | $\phi=\sigma$ -->
<!-- $\xi_i$ | $\xi_i=\lambda_i\equiv\mu_i$ | $\xi_i=p_i\equiv\mu_i$ | $\xi_i=p_i\equiv\mu_i$ | $\xi_i=\mu_i$ -->

That's a lot of terms! (e.g. $r(\xi_i)$, $a(\phi)$ etc). But now we have reformulated the bernoulli, poisson and gaussian distributions into this form, we can start to develop some intuition as to what each of these terms mean:

### The "canonical" link function

Firstly, you may have noticed that $r(\xi_i)$ looks very familiar... in fact, it is the link functions we commonly use for that regression! For example in logistic regression, we use the logit link function $g(\xi_i) = \ln{\left[\frac{\xi_i}{1-\xi_i}\right]} = \eta(X)$. So for any exponential distribution, $r(\xi_i)$ is the default choice for link function, the so-called "canonical link function".

In fact, if the canonical link function is used, and no transformation means $T(y_i) = y_i$, then we can simplify the notation, where $\theta_i = g(\xi_i)$:

$$
\displaylines{
\begin{align}
f(y;\xi,\phi) 
& = \exp{ \left\{ \frac{T(y) r(\xi) - b(\xi)}{a(\phi)} + c(y,\phi) \right\} }
\\ \\
\equiv f(y; \theta_i, \phi)
& = \exp{ \left\{ \frac{y \theta - b(\theta)}{a(\phi)} + c(y,\phi) \right\} } & \tag{2.1}
\\
\\
& \iff T(y) = y; \text{ } g(\xi) = r(\xi) = \theta
\end{align}
}
$$


The odd one out is the probit link function - as this is not the canonical link function. While the canonical link function for bernoulli is logit, the probit link is the inverse cumulative distribution function of the standard normal $g(\xi_i) = \Phi^{-1}[\xi_i] \neq r(\xi_i)$. So because it is not canonical, it's a bit more complicated to deal with (which we explore later).

### The integral of the inverse canonical link function

$b(\xi_i)$ seems slightly odd at first: it is the integral of the inverse of the canonical link function. It's easier to see this by running through each example:

* Poisson: 
  * The canonical link is log: $r(\xi_i) = \ln[\xi_i] = \eta(X)$. 
  * So the inverse-link is exponential: $\xi_i = b'(\eta(X)) = r^{-1}(\eta(X))=e^{\eta(X)}$.
  * So $r^{-1}(\xi_i) \equiv b'(\xi_i) = e^{\xi_i}$.
  * So the integral is: $b(\xi_i)=\int{b'(\xi_i)\,d\xi_i}=e^{\xi_i}$.
* Gaussian: 
  * The canonical link is the identity: $r(\xi_i) = \\xi_i = \eta(X)$. 
  * So the inverse-link is the identity: $\xi_i = b'(\eta(X)) = r^{-1}(\eta(X))=\eta(X)$.
  * So $r^{-1}(\xi_i) \equiv b'(\xi_i) = \xi_i$.
  * So the integral is: $b(\xi_i)=\int{b'(\xi_i)\,d\xi_i}= \frac{\xi_i^2}{2}$.
* Bernoulli: 
  * The canonical link is logit: $r(\xi_i) = \ln\left[ \frac{\xi_i}{1-\xi_i} \right] = \eta(X)$. 
  * So the inverse-link is logistic: $\xi_i = b'(\eta(X)) = r^{-1}(\eta(X))=\frac{e^{\eta(X)}}{1+e^{\eta(X)}}$.
  * So $r^{-1}(\xi_i) \equiv b'(\xi_i) = \frac{e^{\xi_i}}{1+e^{\xi_i}}$.
  * So the integral is: $b(\xi_i)=\int{b'(\xi_i)\,d\xi_i}=\ln{(1+e^{\xi_i})}$.

Another way to think about this is to notice that $b'(\xi_i)$ is the "activation function" in the outer layer of a neural network. I.e. is the function that maps the output of the network to the final prediction.

<!-- Term | Poisson | Gaussian | Bernoulli 
--|--|--|--
canonical link <br> $r(\xi_i)$ | $r(\xi_i) = \ln[\xi_i] = \eta(X)$ | $r(\xi_i) = \xi_i = \eta(X)$ | $r(\xi_i) = \ln\left[ \frac{\xi_i}{1-\xi_i} \right] = \eta(X)$
inverse-link <br> $r^{-1}(\eta(X))$ | $\xi_i = b'(\eta(X)) = r^{-1}(\eta(X))=e^{\eta(X)}$ | $\xi_i = b'(\eta(X)) = r^{-1}(\eta(X))=\eta(X)$ | $\xi_i = b'(\eta(X)) = r^{-1}(\eta(X))=\frac{e^{\eta(X)}}{1+e^{\eta(X)}}$
$\therefore b'(\xi_i) = r^{-1}(\xi_i)$ | $r^{-1}(\xi_i) \equiv b'(\xi_i) = e^{\xi_i}$ | $r^{-1}(\xi_i) \equiv b'(\xi_i) = \xi_i$ | $r^{-1}(\xi_i) \equiv b'(\xi_i) = \frac{e^{\xi_i}}{1+e^{\xi_i}}$
$\therefore b(\xi_i)=\int{b'(\xi_i)\,d\xi_i}$ | $b(\xi_i) = e^{\eta(X)}$ | $b(\xi_i) = \frac{\xi_i^2}{2}$ | $b(\xi_i) = \frac{e^{\eta(X)}}{1+e^{\eta(X)}}$ -->

### Other terms

The other terms are less interesting! But here is a quick summary for completeness:

Parameter / Function | intuition
---|:--------
$T(y)$ is a transformation of $y$ | It is the identity for all the distributions we are looking at, so can be ignored for now.
$\phi$ is the "dispersion parameter", | A parameter for the expected dispersion of $y_i$ around $\mu_i$. For example, gaussian regression has $\phi=\sigma$, the standard deviation of all the residuals (assuming homoskedasticity). However, it is not needed for one parameter distributions, like the poisson, where we already assume the variance $\mathbb{V}[y_i] = \lambda_i$ (i.e. already determined by $\xi_i$).
$a(.)$ is a normalizing function  | A function that normalizes the pdf using the dispersion parameter $\phi$. E.g. for gaussian regression, it is the variance of the residuals, $a(\phi) = \sigma^2$. Again this isn't needed for one parameter distributions.
$c(.)$ an adjustment function | A function that adjusts the normalized pdf so that it sums to one. For example, the exponential form of the poisson distribution would sum to more than one if it wasn't included. 

## Solving for any exponential family using maximum likelihood estimation

So far we have reformulated poisson, binomial and normal regressions into the exponential family form. So now we want to derive our "one-use formula" to estimate the best coefficients for any GLM via maximum likelihood estimation!

### Cost function

Let's define our generic cost function in negative log-likelihood form assuming the canonical link function is used (so in terms of $\theta_i$):

:::{.column-margin}
Maximising the the likelihood is hard, because it involves calculating the total product across every observation. Instead, taking the log likelihood makes everything a sum, far easier to calculate. Also, taking the negative ensures we are looking to minimize the cost.
:::
$$
\displaylines{
\begin{align}
L(\theta) 
& = \prod_{i=1}^n{
  f(y_i;\theta_i,\phi)
}
\\ & = \prod_{i=1}^n{
  \exp{ \left\{
    \frac{y \theta - b(\theta)}{a(\phi)} 
    + c(y,\phi) 
  \right\} }
}
\\ \therefore \mathcal{L}(\theta) 
& = -\ln \left\{ \prod_{i=1}^n{
  \exp{ \left\{ 
    \frac{y_i \theta_i - b(\theta_i)}{a(\phi)} 
    + c(y_i,\phi) 
  \right\} }
} \right\}
\\ & = -\sum_{i=1}^n{ \left\{
  \frac{y_i \theta_i - b(\theta_i)}{a(\phi)} 
  + c(y_i,\phi)
\right\} }
\\ & = -\left(
\frac{1}{a(\phi)} 
\sum_{i=1}^n{ \bigg\{
    y_i \theta_i - b(\theta_i)
\bigg\} } + 
\sum_{i=1}^n{ \bigg\{
  c(y_i,\phi)
\bigg\}} \right)
\end{align}
}
$$

### Score

Next we want to minimize this generic cost function with respect to $\beta$. Common methods include "Newton-Raphson", "Fisher-Scoring", "Iteratively-reweighted Least Squares" or "Gradient Descent".

To execute any of these, we need to derive the the "score" (or "informant"): the first derivative of the negative log likelihood with respect to $\beta$:

$$
\displaylines{
\begin{align}
\arg \min_\beta \left[ \mathcal{L}(\theta) \right]
& = \arg \min_\beta \left[ -\left(
\frac{1}{a(\phi)} 
\sum_{i=1}^n{ \bigg\{
    y_i \theta_i - b(\theta_i)
\bigg\} } + 
\sum_{i=1}^n{ \bigg\{
  c(y_i,\phi)
\bigg\}}
\right)\right]
\\ 
& = \arg \min_\beta \left[ -\left(
\frac{1}{a(\phi)} 
\sum_{i=1}^n{ \bigg\{
    y_i X_i^{\intercal}\beta - b(X_i^{\intercal}\beta)
\bigg\} } + 
\sum_{i=1}^n{ \bigg\{
  c(y_i,\phi)
\bigg\}}
\right)\right] 
\\ & \because \theta = \eta(X) \text{ in the canonical case}
\\
\\
\therefore \frac{\partial \mathcal{L}(\theta)}{\partial\beta_i} 
& = -\frac{1}{a(\phi)} \sum_{i=1}^n{ \bigg\{
      y_i X_i  - \frac{\partial b(\theta)}{\partial \theta} X_i
  \bigg\} } 
\\ & \because c() \text{ is constant with respect to } \beta
\\
& =\underbrace{\frac{1}{a(\phi)}}_{\text{Constant}}  \times \sum_{i=1}^n{ \bigg\{ \left( b'(\theta) - y_i \right) X_i \bigg\} }
\end{align}
}
$$

:::{.column-margin}
Only $\theta$ changes with respect to the choice of $\beta$. So $a(\phi)$ and $c(y_i,\phi)$, which are not a function of $\theta$, drop out.
:::

So hopefully this now gives a bit more intuition behind why $b(\theta)$ is the integral of the inverse canonical link. Since $b'(\theta)$ is the activation function, we can see that the score function is just the difference between the predicted value and the actual value, multiplied by the feature. This is the same as the gradient of the cost function used for gradient descent!

This also uncovers an interesting property of GLMs - that the average prediction $b'(\theta)$ must be equal to the average value of Y too:

$$
\displaylines{
\begin{align}
\frac{\partial \mathcal{L}(\theta)}{\partial\beta_i} 
& = 0 \text{ (cost is minimized at stationary point)}
\\
& = \cancel{\frac{1}{a(\phi)}}  \times \sum_{i=1}^n{ \bigg\{ \left( b'(\theta) - y_i \right) \cancel{X_i} \bigg\} }
\\
\therefore 
\sum_{i=1}^n{ y_i } & = \sum_{i=1}^n{ b'(\theta) } 
\\ & \div N 
\\ \\
\Rightarrow \bar{y} & = \mathbb{E}[y] = b'(\theta)
\end{align}
}
$$

### Hessian

Finally, we need to derive the Hessian matrix, which is the second derivative of the cost function with respect to $\beta$. This is required for second order optimization methods, like Newton's method, which can converge faster than gradient descent:

$$
\displaylines{
\begin{align}
\frac{\partial^2 \mathcal{L}(\theta)}{\partial\beta_i^2} 
& = \frac{\partial}{\partial\beta_i}\left( 
  \frac{1}{a(\phi)} \sum_{i=1}^n{ \bigg\{ \left( b'(\theta) - y_i \right) X_i \bigg\} }
\right)
\\
& = -\frac{1}{a(\phi)} \sum_{i=1}^n{ \bigg\{
      \frac{\partial b'(\theta)}{\partial \theta} X_i^2
  \bigg\} } 
\end{align}
}
$$

Well done on getting this far!. We can now start to write out some python to create this ourselves.

### Poisson

Now let's have a look at poisson, with mean rate of $\lambda_i = \mu_i = \xi_i$:

$$
\displaylines{
\begin{align}
f_y(\xi_i,\phi)
& = 
\frac{\mu_i^{y_i}e^{-\mu_i}}{y_i!}
\\ & = \exp{ \left\{ 
\ln{ \left[ \frac{\mu_i^{y_i} \times e^{-\mu_i}}{y_i!} \right]}
\right\}}
\\ & = \exp{ \left\{ 
\ln{ \left[ \mu_i^{y_i} \right]}
+ \ln{ \left[ e^{-\mu_i} \right] }
- \ln{ \left[ y_i! \right] }
\right\}}
\\ & = \exp{ \left\{ 
y_i \ln{ \left[ \mu_i \right]}
-\mu_i
- \ln{ \left[ y_i! \right] }
\right\}}
\\ & \equiv \exp{ \left\{ 
\frac{
  \underbrace{y_i}_{T(y_i)}
  \underbrace{\ln{ \left[ \mu_i \right]}}_{r(\xi_i)}
  - \underbrace{\mu_i}_{b(\xi_i)}
}{
  \underbrace{1}_{a(\phi)}
} 
- \underbrace{\ln{ \left[ y_i! \right] }}_{c(y,\phi)}
\right\}}
\\ & = \exp{ \left\{ \frac{T(y_i) r(\xi_i) - b(\xi_i)}{a(\phi)} + c(y_i,\phi) \right\} } & \tag{1.2}
\end{align}
}
$$




So when reformulating the poisson distribution as an exponential dispersion family, we find:

* $T(.) = I(.)$ i.e. it is simply the identity, so $T(y) = y$
* $r(\xi_i) = \ln{[\mu_i]}$
* 
* Since we are using the log-link, our choice of link function is canonical: $r(\xi_i) = \ln{[\mu_i]} = g(\mu_i)$. So we can simplify to $\theta_i$!
* $b(\theta_i) = \mu_i = \exp\{\theta_i\}$.
  * Note that the differential of this with respect to $\theta_i$ is still the exponential function, $\frac{\partial b(\theta_i)}{\partial \theta_i}=\exp\{{\theta_i}\}$. So the differential is the inverse of the log-link function!
* We can ignore $a(\phi)$. This term usually helps normalize the pdf by its dispersion, but the poisson distribution only has one parameter: the variance is identical to the mean, $\mathbb{V}[y_i] = \mu_i$. So no normalisation is needed.
* Since the exponential form of the poisson distribution actually sums to more than one, we need to adjust it using $c(y_i,\phi)=\ln{ \left[ y_i! \right] }$.

### Bernoulli (Logit vs probit)

Let's take the bernoulli pdf with expected success probability $p_i=\mu_i$, and reformulate it to the exponential dispersion family form:

$$
\displaylines{
\begin{align}
f(y;\theta,\phi)
& = \mu_i^{y_i}(1-\mu_i)^{1-y_i} \\
& = \exp{ \left\{\ln{\left[\mu_i^{y_i}(1-\mu_i)^{(1-y_i)} \\\right]} \right\}} \\
& = \exp{ \left\{
  \ln{\left[\mu_i^{y_i}\right]} + \ln{\left[(1-\mu_i)^{(1-y_i)}\right]} 
  \right\}} \\
& = \exp{ \left\{ y_i \ln{[\mu_i]} + (1-y_i)\ln{[1-\mu_i]} \right\}} \\
& = \exp{ \left\{ y_i \ln{
  \left[\frac{\mu_i}{1-\mu_i} \right]} + \ln{[1-\mu_i]} \right\}} \\
& = \exp{ \left\{ \frac{
  \underbrace{y_i}_{T(y)}
\underbrace{ \ln{
  \left[\frac{\mu_i}{1-\mu_i} \right]
} }_{r(\xi_i)}
  + \underbrace{ \ln{[1-\mu_i]} }_{b(\xi_i)}
  }{
    \underbrace{1}_{a(\phi)}
  } 
  + \underbrace{0}_{c(y_i,\phi)}
\right\}} 
\\
& = \exp{ \left\{ \frac{y_i 
\underbrace{ \ln{
  \left[\frac{\mu_i}{1-\mu_i} \right]
} }_{\theta_i}
  + \underbrace{ \ln{[1-\mu_i]} }_{b(\theta_i)}
  }{
    \underbrace{1}_{a(\phi)}
  } 
  + \underbrace{0}_{c(y_i,\phi)}
\right\}} 
\\ & = \exp{ \left\{ \frac{y_i \theta_i - b(\theta_i)}{a(\phi)} + c(y_i,\phi) \right\} }
\end{align}
}
$$

So we find that:

* Now when using the logit-link, our choice of link function is canonical: $r(\xi_i) = \ln{\left[\frac{\mu_i}{1-\mu_i} \right]} = g(\mu_i)$. So we can simplify to $\theta_i$ when using logistic regression!
  * However, this isn't the case for probit, since $r(\xi_i) = \ln{\left[\frac{\mu_i}{1-\mu_i} \right]} \neq g(\mu_i) = \Phi^{-1}[\mu_i]$. So to conduct probit regression, we need to use the non-canonical form.
* $b(\theta_i) = \ln{[1-\mu_i]} = \ln{[1+\exp\{\theta_i\}]}$[^1].
  * Note that the differential of this with respect to $\theta$ is just the logistic function, $\frac{\partial b(\theta_i)}{\partial\theta_i}=\frac{\exp\{\theta_i\}}{1+\exp\{\theta_i\}}$. This differential is the inverse of the logit link function.
* We can ignore $a(\phi)$. This term usually helps normalize the pdf by its dispersion, but the bernoulli distribution only has one parameter: the variance is $\mathbb{V}[y_i] = \mu_i \times (1-\mu_i)$
* We can also ignore $c(y_i,\phi)$. Since the exponential form of the bernoulli distribution already sums to one, we do not need to adjust it.

:::{.column-margin}
See footnotes for deriving $\ln{[1-p]} = \ln{[1+\exp\{X_i^{\intercal}\beta\}]}$.
:::

### Gaussian

$$
\displaylines{
\begin{align}
f(y;\theta,\phi) 
& = \frac{1}{\sqrt{2 \pi \sigma^2}} \exp{ \left\{-\frac{(y_i-\mu_i)^2}{2\sigma^2} \right\} }
\\ & =
\exp{\left\{
\ln{ \left[ \frac{1}{\sqrt{2 \pi \sigma^2}} 
\exp{ \left\{ -\frac{(y_i-\mu_i)^2}{2\sigma^2} \right\} }
\right]}
\right\}}
\\ & =
\exp{\left\{
\ln{ \left[ \frac{1}{\sqrt{2 \pi \sigma^2}} \right]}
- \frac{(y_i-\mu_i)^2}{2\sigma^2}
\right\}}
\\ & =
\exp{\left\{
\cancel{\ln{ \left[ 1 \right]}}
+ \ln{ \left[ 2 \pi \sigma^2 \right]}^{-1/2}
- \frac{y_i^2+\mu_i^2-2y\mu_i}{2\sigma^2}
\right\}}
\\ & =
\exp{\left\{
- \frac{1}{2}\ln{ \left[ 2 \pi \sigma^2 \right]}
- \frac{y_i^2}{2\sigma^2}
- \frac{\frac{\mu_i^2}{2}}{\sigma^2}
+ \frac{\cancel{2}y_i\mu_i}{\cancel{2}\sigma^2}
\right\}}
\\ & =
\exp{\left\{
  \frac{
    y \underbrace{\mu_i}_{\theta_i} - 
    \underbrace{\frac{\mu_i^2}{2}}_{b(\theta_i)}
  }{
    \underbrace{\sigma^2}_{a(\phi)}
  }
- 
\underbrace{
  \frac{1}{2}
  \left( 
    \frac{y_i^2}{\sigma^2} + 
    \ln{ \left[ 2 \pi \sigma^2 \right]}
  \right)
}_{c(y_i,\phi)}
\right\}}
\\ & = \exp{ \left\{ \frac{y_i \theta_i - b(\theta_i)}{a(\phi)} + c(y_i,\phi) \right\} }
\end{align}
}
$$

So we find that:

* Since we are using the identity-link, our choice of link function is canonical: $r(\xi_i) = \xi_i = g(\xi_i)$. So we can simplify to $\theta_i$!
* $b(\theta_i) = \frac{\mu_i^2}{2}$. You might recognise this squared term as part of the usual cost function for least squares regression!
  * Note that the differential of this with respect to $\theta_i$ is the identity, $\frac{\partial b(\theta_i)}{\partial \theta_i}=\theta_i$. It is the inverse of the link function (again the identity).
* Finally, we have a use for $a(\phi)$. The gaussian distribution has two parameters, a second relating to its variance, so we need to normalize our cost function by this measure of the dispersion of our data.
* And again, we have a use $c(y_i,\phi)$. Without this correction, the exponential form of the gaussian distribution sums to more than one.



$$
\displaylines{
\begin{align}
f(y;\xi,\phi) 
& = 
\exp{ \left\{ \frac{T(y) r(\xi) - b(\xi)}{a(\phi)} + c(y,\phi) \right\} } & \tag{1.1}
\\
\equiv f(y; \theta_i, \phi)
& = \exp{ \left\{ \frac{y \theta - b(\theta)}{a(\phi)} + c(y,\phi) \right\} } & \tag{1.2}
\\
\\
& \iff T(y) = y; g(\xi) = r(\xi) = \theta
\end{align}
}
$$

Now when reformulating the probability density functions into the exponential family form, a good choice for the link function $g(.)$ often "reveals itself": i.e. the "canonical" choice of $g(\xi_i) = r(\xi_i)$.

If the chosen link function is this canonical one, then we can further simplify the non-canonical form $(1.1)$ to its more common canonical form $(1.2), iff:

1. $T(y) = y$ i.e. it happens to be the identity function.
1. $g(\xi) = r(\xi)$ i.e. $r(\xi)$ is identical to the chosen link function $g(\xi)$. If we are using a linear predictor too, so $r(\xi_i)=\theta_i = \eta(X_i) = X_i^{\intercal}\beta$, then we actually find $b(\xi) = b(\theta_i)$ too, hence simplifying to $(1.2)$.

Now it is actually true that both these conditions are met for poisson, logistic and gaussian regression! However isn't true for probit, because the choice of link function is not "canonical", i.e. $r(\mu_i) \neq \Phi^{-1}[\mu_i]$. We will go through this later though.

### Intuition behind the exponential form

There's a lot of terms in the formula above. We will try to give some intuition behind $(1.2)$, but don't worry if this still seems hard to get your head around: we are going to show how to reformulate each of the poisson, binomial and gaussian distributions into an exponential dispersion family form after this, to provide further intuition:

Parameter / Function | intuition
---|:--------
$\theta$ is the "natural parameter" | This is a parameter influencing the shape of the distribution. It is clearly related to location, but sometimes the dispersion too. For example, in bernoulli, the mean (expected) $p_i = \mu_i$, so $\theta_i = g(\mu_i)=\ln{\left(\frac{\mu_i}{1-\mu_i}\right)}=\ln{\left(\frac{p_i}{1-p_i}\right)}$. However, since $\mathbb{V}[y]=p_i(1-p_i)$, it also governs the dispersion of the bernoulli too. 
$\phi$ is the "dispersion parameter", | A parameter for the expected dispersion of $y_i$ around $\mu_i$. For example, gaussian regression has $\phi=\sigma$, the standard deviation of all the residuals (assuming homoskedasticity). However, it is not needed for one parameter distributions, like the poisson, where we already assume the variance $\mathbb{V}[y_i] = \lambda_i$ (i.e. already determind by $\theta_i$).
$a(.)$ is a normalizing function  | A function that normalizes the pdf using the dispersion parameter $\phi$. E.g. for gaussian regression, it is the variance of the residuals, $a(\phi) = \sigma^2$. Again this isn't needed for one parameter distributions.
$c(.)$ an adjustment function | A function that adjusts the normalized pdf so that it sums to one. For example, the exponential form of the poisson distribution would sum to more than one if it wasn't included.
$b(.)$ is the integral of the inverse-link function | It helps to look at $b'(\theta)$ first. Whereas the link function maps $\mu \rightarrow \theta$, i.e. $g(\mu) = \theta$, the inverse of the link function maps the other way $\theta \rightarrow \mu$ i.e. $b'(\theta)=g^{-1}(\theta)=\mu$. You might also recognise this as the "activation function" in the outer layer of neural networks. Then, the integral $b(\theta) = \int{b'(\theta)\,d\theta}$ by definition. <br> For example, poisson regression has log-link function: $g(\mu) = \ln{(\mu)} = \theta$. So the inverse-link function is the exponential: $b'(\theta) = g'(\mu)= \exp{(\theta)}$. It just so happens the integral is the same in this case: $b(\theta) = \int{\exp{\{\theta\}}} = \exp{\{\theta\}}$



## Coding it up from scratch

### Base class for canonical exponential dispersion family

Let's start creating a generic base class then that we can use for any distribution that can be represented by the canonical exponential dispersion family:
```{python}

class base_canonical_exponential_dispersion_family():

    def __init__(self,y,X,b,phi,a,c,seed=0):
      """
      y:    Dependent variable. A 1d vector of numeric values 
      X:    Independent variables. A 2d matrix of numeric values 
      b:    Function for the integral of the activation function. See db for more details.
      db:   Function for the first derivative of b(θ) with respect to theta.
            Note that this should be the inverse of the link function
            E.g. for poisson reg, g(µ) = ln(µ) = η(X) = θ. So b(θ) = exp(θ).
      phi:  Parameter: the "dispersion parameter", φ. 
            A measure of the dispersion of the distribution
            E.g. for gaussian reg, φ is the standard deviation of residuals.
      a:    Function of φ. Normalizes the pdf using the dispersion parameter. 
            E.g. for gaussian reg, it is the variance of the residuals a(φ) = φ^2.
      c:    Function for y and φ. Adjusts the likelihood so that the pdf sums to 1.
      """

      self.y = np.array(y).reshape(-1,1)
      self.X = np.array(X).reshape(y.shape[0],-1)
      self.b = b
      self.phi = phi
      self.a = a
      self.c = c
      
      np.random.seed(seed)
      
      # Initialize beta with random values
      self.n, self.k = self.X
      self.beta = np.random.normal(0, 0.5, self.k).reshape((self.k,1))

    def canonical_theta(X, beta):
      """ In the canonical form, theta is simply θ = η(X)= X'β """
      return X.dot(beta)

    def negative_log_likelihood(self, beta):
        """ Negative log likelihood i.e. the current cost
        """
        y, X = self.y, self.X
        theta = self.canonical_theta(self.X, beta)
        phi = self.phi
        a, b, c = self.a, self.b, self.c
        log_likelihood = ( y * _theta - b(_theta) ) / a(phi) + c(y,phi)
        J = -1 * log_likelihood
        return J

    def informant(self, theta=None, phi=None):
        """ First derivative of the cost function with respect to theta """
        y, X, a, b = self.y, self.X, self.a, self.b
        if theta is None:
            theta = self.canonical_theta()
        if phi is None:
            phi = self.phi
        dJ = ( X.T/a(phi) ).dot( db( theta ) - y )
        return dJ

    def hessian(self, theta, phi):
        """ Second derivative of the cost function with respect to theta """
        a, d2b, X = self.a, self.d2b, self.X
        V = self.var_y(theta, phi)
        d2J = X.T.dot( V / a(phi)**2 ).dot(X)
        return d2J

    def update_beta(self, fit_type='Netwon-Raphson'):
        """ A single step towards optimizing beta """
        beta, X = self.beta, self.X
        theta, phi =  self.theta(), self.phi()
        d2J = self.hessian( theta, phi )
        learning_rate = np.linalg.solve( d2J, np.eye(X.shape[1]) )
        if fit_type in ['Netwon-Raphson']:
            dJ = self.score_function( theta, phi )
        else:
            raise ValueError('Please select "Newton-Raphson". "IRLS" coming soon')
        beta -= learning_rate.dot(dJ)
        return beta

    def fitting_mle(self, max_iter = 100, fit_type='Netwon-Raphson', epsilon = 1e-8):
        """ Fitting using MLE """
        for i in range(max_iter):
            old_beta = self.beta.copy()
            new_beta = self.update_beta(fit_type=fit_type)
            self.beta = new_beta
            if (np.abs(new_beta - old_beta)/(0.1 + np.abs(new_beta)) <= epsilon).all():
                print("Fully converged by iteration " + str(i))
                break
            if (i == max_iter):
                print("Warning - coefficients did not fully converge within " + str(max_iter) + " iterations.")
        self.fitted = True
        self.unadj_r_squared = -1
        self.residuals = (self.y - self.db(self.theta())).reshape(-1,1)
      
```

### Base class for canonical exponential dispersion family

And now we have our base class, we can utilise it for specific classes of the poisson, bernoulli and gaussian distributions:

```{python}
class gaussian_family(base_canonical_exponential_dispersion_family):

  def __init__(self,y,X,seed = 0):

      def b(theta):
          """ Integral of the activation function """
          return 0.5*theta**2

      def db(theta):
          """ Activation function is identity (inverse of the indentity function):"""
          return theta

      def d2b(theta):
          """ Differential of the activation function """
          return 1

      def dispersion(y, theta):
          """ Std.dev of residuals = sqrt(RSS / DoF) """
          n, k = self.n, self.k
          std_dev = np.sqrt(np.sum( (y - theta)**2 ) / ( n - k ) )
          return std_dev

      def a(phi):
          """ Variance of residuals """
          return phi**2

      def c(y,phi):
          """ Adjustment needed so pdf sums to one """
          return -0.5 * ( y**2/phi + np.log(2*pi*phi))

      super().__init__(y,X,b,db,d2b,dispersion,a,c)

class bernoulli_family(base_canonical_exponential_dispersion_family):

  def __init__(self,y,X,seed = 0):
      
      def b(theta):
          """ Integral of the activation function """
          return np.log(1 + np.exp(theta))

      def db(theta):
          """ Activation function is logistic (inverse of the logit function):"""
          return np.exp(theta)/(1 + np.exp(theta))

      def d2b(theta):
          """ Differential of the activation function """
          return np.exp(theta)/(1 + np.exp(theta))**2

      def dispersion(y, theta):
          """ Not needed - one parameter distribution """
          return 1

      def a(dispersion):
          """ Not needed - one parameter distribution """
          return 1

      def c(y, dispersion):
          """ No adjustment needed (pdf already sums to one) """
          return 0

      super().__init__(y,X,b,db,d2b,dispersion,a,c) 

class poisson_family(base_canonical_exponential_dispersion_family):

  def __init__(self,y,X,seed = 0):
      
      def b(theta):
          """ Integral of the activation function """
          return np.exp(theta)

      def db(theta):
          """ Activation function is exponential (inverse of the log-link function):"""
          return np.exp(theta)

      def d2b(theta):
          """ Differential of the activation function """
          return np.exp(theta)

      def dispersion(y, theta):
          """ Not needed - one parameter distribution """
          return 1

      def a(dispersion):
          """ Not needed - one parameter distribution """
          return 1

      def c(y, dispersion):
          """ Adjustment needed so pdf sums to one """
          return np.vectorize(-np.log(np.math.factorial(y)))

      super().__init__(y,X,b,db,d2b,dispersion,a,c)
```

[^1]:
  Showing how $\ln[1-p] = -\ln[1+e^{\theta}]$
$$
\displaylines{
\begin{align} 
\theta & = \ln{\left[\frac{p}{1-p} \right]}
\\ \text{ (1) Put } p \text{ in terms of } \theta \text{:}
\\ \therefore e^{\theta} & = \frac{p}{1-p} & \text{raise by }e
\\ \therefore e^{\theta}-pe^{\theta} & = p & \times (1-p)
\\ \therefore e^{\theta} & = p(1+e^\theta) & + pe^\theta
\\ \therefore p & = \frac{e^{\theta}}{1+e^\theta} & \div (1+e^\theta)
\\\\ 
\\ \text{ (2) Substitute in } p = \frac{e^{\theta}}{1+e^\theta}
\\
\Rightarrow \ln[1-p] 
& = \ln\left[1-\frac{e^{\theta}}{1+e^\theta} \right]
\\ & \equiv \ln\left[\frac{1+e^\theta}{1+e^\theta}-\frac{e^{\theta}}{1+e^\theta} \right]
\\ & = \ln\left[\frac{1}{1+e^\theta} \right]
\\ & \equiv \ln\left[(1+e^\theta \right)^{-1}]
\\ & \equiv -\ln\left[1+e^\theta \right]
\end{align}
}
$$

