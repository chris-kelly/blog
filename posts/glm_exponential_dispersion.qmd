---
title: "GLM: exponential dispersion families"
author: "Chris Kelly"
date: '02-21-24'
categories: []
format:
  html:
    code-fold: true
    toc: true
draft: true
---

It is cumbersome to need to write unique code for each type of regression. Instead, it can be shown that many common distributions can be reformulated into the "Exponential Dispersion Family of Distributions". This generic representation makes it easier to re-use the same code to run a regression, rather than hand calculate each pdf in different ways.

Concretely, it means many probability distribution functions (pdf) can be represented in the following way:

$$
f(y;\theta,\phi) = \exp{ \left[ \frac{y \theta - b(\theta)}{a(\phi)} + c(y,\phi) \right] }
$$

This means we only need to use one formula to estimate each family via maximum likelihood estimation!

$$
\displaylines{
\begin{align}
L(\theta) 
& = \prod_{i=1}^n{
  f(y_i;\theta_i,\phi)
}
\\ & = \prod_{i=1}^n{
  \exp{ \left\{
    \frac{y \theta - b(\theta)}{a(\phi)} 
    + c(y,\phi) 
  \right\} }
}
\\ \therefore \mathcal{L}(\theta) 
& = \ln \left[ \prod_{i=1}^n{
  \exp{ \left\{ 
    \frac{y_i \theta_i - b(\theta_i)}{a(\phi)} 
    + c(y_i,\phi) 
  \right\} }
} \right]
\\ & = \sum_{i=1}^n{ \left\{
  \frac{y_i \theta_i - b(\theta_i)}{a(\phi)} 
  + c(y_i,\phi)
\right\} }
\\ & = 
\frac{1}{a(\phi)} 
\sum_{i=1}^n{ \bigg\{
    y_i \theta_i - b(\theta_i)
\bigg\} } + 
\sum_{i=1}^n{ \bigg\{
  c(y_i,\phi)
\bigg\}}
\end{align}
}
$$

And now we can look at what happens when we minimize the cost function with respect to $\beta$:

$$
\displaylines{
\begin{align}
\arg \min_\beta \left[ \mathcal{L}(\theta) \right]
& = \arg \min_\beta \left[
\frac{1}{a(\phi)} 
\sum_{i=1}^n{ \bigg\{
    y_i \theta_i - b(\theta_i)
\bigg\} } + 
\sum_{i=1}^n{ \bigg\{
  c(y_i,\phi)
\bigg\}}
\right]
\\ & \equiv \arg \min_\beta \left[
  \sum_{i=1}^n{ \bigg\{
      y_i \theta_i - b(\theta_i)
  \bigg\} }
\right]
\end{align}
}
$$

:::{.column-margin}
We assume that only $\theta$ changes with respect to the choice of $\beta$.
:::

How can we think of this intuitively?

* $\theta$ is our link function. For example, for logistic regression, $\theta = X_i^\intercal\beta = \ln{\left[\frac{p}{1-p}\right]}$
* $a(\phi)$ is 

### Bernoulli

Let's take the bernoulli pdf with success probability $p$, and raise it to have an exponential form:

$$
\displaylines{
\begin{align}
f(y;\theta,\phi)
& = p^y(1-p)^{1-y} \\
& = \exp{ \left\{\ln{\left[p^y (1-p)^{(1-y)}\right]} \right\}} \\
& = \exp{ \left\{
  \ln{\left[p^y\right]} + \ln{\left[(1-p)^{(1-y)}\right]} 
  \right\}} \\
& = \exp{ \left\{ y \ln{[p]} + (1-y)\ln{[1-p]} \right\}} \\
& = \exp{ \left\{ y \ln{
  \left[\frac{p}{1-p} \right]} + \ln{[1-p]} \right\}} \\
& = \exp{ \left\{ \frac{y 
\underbrace{ \ln{
  \left[\frac{p}{1-p} \right]
} }_{\theta}
  + \underbrace{ \ln{[1-p]} }_{b(\theta)}
  }{
    \underbrace{1}_{a(\phi)}
  } 
  + \underbrace{0}_{c(y,\phi)}
\right\}} 
\\ & = \exp{ \left[ \frac{y \theta - b(\theta)}{a(\phi)} + c(y,\phi) \right] }
\end{align}
}
$$

Thus:
* We thus find the link function, $\theta=\ln{\left[\frac{p}{1-p} \right]} = X_i^\intercal\beta$
* And $b(\theta) = \ln{[1-p]} = \ln{[1+\exp\{X_i^{\intercal}\beta\}]}$[^1].

:::{.column-margin}
See footnotes for deriving $\ln{[1-p]} = \ln{[1+\exp\{X_i^{\intercal}\beta\}]}$.
:::

So now if we look to minimize $\beta$, we find we are simply minimizing the sum of squares:

$$
\displaylines{
\begin{align}
\arg \min_\beta \left[
  \sum_{i=1}^n{ \bigg\{
      y_i \theta_i - b(\theta_i)
  \bigg\} }
\right]
& = 
\arg \min_\beta \left[
  \sum_{i=1}^n{ \bigg\{
      y_i \theta_i - \ln[1-p]
  \bigg\} }
\right]
\\ & = 
\arg \min_\beta \left[
  \sum_{i=1}^n{ \bigg\{
      y_i X_i^{\intercal}\beta - 
      \ln{[1+\exp\{X_i^{\intercal}\beta\}]}
  \bigg\} }
\right]
\end{align}
}
$$

Which we can show is equivalent to the usual cost function for logistic regression:

$$
\displaylines{
\begin{align}
& -\sum_{i=1}^{N}{ \bigg\{
  y_i\ln{ [ p_i ] } +
  (1-y_i)\ln{ [ 1-p_i ] } 
\bigg\} }
\\ = &
-\sum_{i=1}^{N}{ \bigg\{
  y_i \bigg( 
    \ln{[ p_i ]} - \ln{ [ 1-p_i ] }
  \bigg) + 
  \ln[1-p_i]
  \bigg\}
} 
\\ = &
-\sum_{i=1}^{N}{ \bigg\{
  y_i \bigg( 
    \ln{ \left[ \frac{p_i}{1-p_i} \right] }
  \bigg) + 
  \ln[1-p_i]
  \bigg\}
} 
\\ = &
-\sum_{i=1}^{N}{ \bigg\{
  y_i \theta - b(\theta) 
  \bigg\}
} 
\end{align}
}
$$

### Poisson

Let's have a look at poisson, with mean rate of $\lambda$:

$$
\displaylines{
\begin{align}
f(y;\theta,\phi) 
& = 
\frac{\lambda^ye^{-\lambda}}{y!}
\\ & = \exp{ \left\{ 
\ln{ \left[ \frac{\lambda^y \times e^{-\lambda}}{y!} \right]}
\right\}}
\\ & = \exp{ \left\{ 
\ln{ \left[ \lambda^y \right]}
+ \ln{ \left[ e^{-\lambda} \right] }
- \ln{ \left[ y! \right] }
\right\}}
\\ & = \exp{ \left\{ 
y \ln{ \left[ \lambda \right]}
-\lambda
- \ln{ \left[ y! \right] }
\right\}}
\\ & = \exp{ \left\{ 
\frac{y 
  \underbrace{\ln{ \left[ \lambda \right]}}_{\theta}
  - \underbrace{\lambda}_{b(\theta)}
}{
  \underbrace{1}_{a(\phi)}
} 
- \underbrace{\ln{ \left[ y! \right] }}_{c(y,\phi)}
\right\}}
\\ & = \exp{ \left[ \frac{y \theta - b(\theta)}{a(\phi)} + c(y,\phi) \right] }
\end{align}
}
$$

Thus:
$b(\theta) = \lambda = \exp\big\{X_i^{\intercal}\beta\big\}$.

So now if we look to minimize $\beta$, we find..

$$
\displaylines{
\begin{align}
\arg \min_\beta \left[
  \sum_{i=1}^n{ \bigg\{
      y_i \theta_i - b(\theta_i)
  \bigg\} }
\right]
& = 
\arg \min_\beta \left[
  \sum_{i=1}^n{ \bigg\{
      y_i \theta_i - \lambda
  \bigg\} }
\right]
\\ & = 
\arg \min_\beta \left[
  \sum_{i=1}^n{ \bigg\{
      y_i X_i^{\intercal}\beta - \exp\big\{X_i^{\intercal}\beta\big\} 
  \bigg\} }
\right]
\end{align}
}
$$



### Gaussian

$$
\displaylines{
\begin{align}
f(y;\theta,\phi) 
& = \frac{1}{\sqrt{2 \pi \sigma^2}} \exp{ \left\{-\frac{(y-\mu)^2}{2\sigma^2} \right\} }
\\ & =
\exp{\left\{
\ln{ \left[ \frac{1}{\sqrt{2 \pi \sigma^2}} 
\exp{ \left\{ -\frac{(y-\mu)^2}{2\sigma^2} \right\} }
\right]}
\right\}}
\\ & =
\exp{\left\{
\ln{ \left[ \frac{1}{\sqrt{2 \pi \sigma^2}} \right]}
- \frac{(y-\mu)^2}{2\sigma^2}
\right\}}
\\ & =
\exp{\left\{
\cancel{\ln{ \left[ 1 \right]}}
+ \ln{ \left[ 2 \pi \sigma^2 \right]}^{-1/2}
- \frac{y^2+\mu^2-2y\mu}{2\sigma^2}
\right\}}
\\ & =
\exp{\left\{
- \frac{1}{2}\ln{ \left[ 2 \pi \sigma^2 \right]}
- \frac{y^2}{2\sigma^2}
- \frac{\frac{\mu^2}{2}}{\sigma^2}
+ \frac{\cancel{2}y\mu}{\cancel{2}\sigma^2}
\right\}}
\\ & =
\exp{\left\{
  \frac{
    y \underbrace{\mu}_{\theta} - 
    \underbrace{\frac{\mu^2}{2}}_{b(\theta)}
  }{
    \underbrace{\sigma^2}_{a(\phi)}
  }
- 
\underbrace{
  \frac{1}{2}
  \left( 
    \frac{y^2}{\sigma^2} + 
    \ln{ \left[ 2 \pi \sigma^2 \right]}
  \right)
}_{c(y,\phi)}
\right\}}
\end{align}
}
$$

Thus, $b(\theta) = \frac{\mu^2}{2} = \frac{(X_i^{\intercal}\beta)^2}{2}$.

So now if we look to minimize $\beta$, we find we are simply minimizing the sum of squares:

$$
\displaylines{
\begin{align}
\arg \min_\beta \left[
  \sum_{i=1}^n{ \bigg\{
      y_i \theta_i - b(\theta_i)
  \bigg\} }
\right]
& = 
\arg \min_\beta \left[
  \sum_{i=1}^n{ \bigg\{
      y_i \theta_i - \frac{\mu^2}{2}
  \bigg\} }
\right]
\\ & = 
\arg \min_\beta \left[
  \sum_{i=1}^n{ \bigg\{
      y_i X_i^{\intercal}\beta - \frac{(X_i^{\intercal}\beta)^2}{2}
  \bigg\} }
\right]
\\ & = 
\arg \min_\beta \left[ \frac{1}{2}
  \sum_{i=1}^n{ \bigg\{
      (y_i -X_i^{\intercal}\beta)^2
  \bigg\} }
\right]
\\ & \equiv
\arg \min_\beta \left[
  \sum_{i=1}^n{ \varepsilon_i^2}
\right]
\end{align}
}
$$

[^1]:
  Showing how $\ln[1-p] = -\ln[1+\exp{\{\theta\}}]$
  $$
  \displaylines{
  \begin{align} 
  \theta & = \ln{\left[\frac{p}{1-p} \right]}
  \\ \therefore e^{\theta} & = \frac{p}{1-p}
  \\ \therefore e^{\theta}-pe^{\theta} & = p
  \\ \therefore e^{\theta} & = p(1+e^\theta)
  \\ \therefore p & = \frac{e^{\theta}}{1+e^\theta}
  \\ \therefore \ln[1-p] 
  & = \ln\left[1-\frac{e^{\theta}}{1+e^\theta} \right]
  \\ & = \ln\left[\frac{1+e^\theta}{1+e^\theta}-\frac{e^{\theta}}{1+e^\theta} \right]
  \\ & = \ln\left[\frac{1}{1+e^\theta} \right]
  \\ & = \ln\left[1+e^\theta \right]^{-1}
  \\ & = -\ln\left[1+e^\theta \right]
  \end{align}
  }
  $$
