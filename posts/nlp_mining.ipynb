{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"NLP: Data Mining Intro\"\n",
    "author: \"Chris Kelly\"\n",
    "date: '02-21-24'\n",
    "categories: [NLP, TF-IDF, cosine similarity, LSA, topic extraction]\n",
    "format:\n",
    "  html:\n",
    "    code-fold: true\n",
    "    toc: true\n",
    "execute: \n",
    "  enabled: true\n",
    "draft: true\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick introduction:\n",
    "\n",
    "Algorithms like dealing with numbers - they like structure, for input data to be *tidy*. So how can an algorithm start to process unstructured text? And even then - start to extract anything meaningful from it?\n",
    "\n",
    "<img src=\"https://media.giphy.com/media/eMBKXi56D0EXC/giphy.gif\">\n",
    "\n",
    "### Some definitions/background\n",
    "\n",
    "When text data is provided, it goes through the process of \"tokenisation\". This involves splitting the text into smaller pieces, that the algorithm can encode into 0 and 1s (one-hot encoding). In our example, each unique word in a menu will be a \"token\".\n",
    "\n",
    "A \"document\" is a collection of tokens associated with a particular sample. In our example, each document will be a restaurant menu. \n",
    "\n",
    "An \"embedding\" is an attempt to create numerical representation of that document in a vector. This can be useful for similarity search as we will see later (e.g. this user liked this restaurant, so let's recommend them a restaurant with a similar vector).\n",
    "\n",
    "Finally, the \"corpus\" is a collection of all documents that the model can learn from. In our example, it the entire colletion of menus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF: text-frequency inverse-document-frequency\n",
    "\n",
    "### What is it used for?\n",
    "\n",
    "Text-frequency Inverse-Document-Frequency (TF-IDF) is a technique to find the most important words in a document, or alternatively formulated to find the most important letters in a word.\n",
    "\n",
    "It is good at determining 'global' statistics. By this, we mean it contrasts  the frequency of tokens in a document vs their prevalence across the entire corpus. It does not capture more detailed semantic relationships, particularly since it doesn't care about the ordering of words in a document.\n",
    "\n",
    "<!-- Note further that it is heuristic-based. This means that although the TF-IDF -->\n",
    "\n",
    "### What is the intuition here?\n",
    "\n",
    "TF-IDF is based on the intuition that tokens that appear more frequently, especially those that tend to be rarer across the corpus, are the most important ones in that document.\n",
    "\n",
    "For example, let's contrast two takeaway menus:\n",
    "* Chicken Kurma Masala, Chicken Tikka Masala, Chicken Curry, Saag Aloo, Pilau Rice, Plain Rice\n",
    "* Chicken Chow Mein Noodles, Crispy Beef, Chicken and Cashew nut curry, Prawn Crackers, Egg Fried Rice, Steamed rice, Vegetable Noodles\n",
    "\n",
    "<img src=\"https://media.giphy.com/media/12xu9HYTRo4Eg0/giphy.gif\">\n",
    "\n",
    "The word 'Chicken' appears the most in the first order, followed by 'masasla' and 'Rice'. This is *text frequency*.\n",
    "\n",
    "But that's only half the story - because chicken and rice are quite common words in general. So we need a way to measure whether a word is rarer. The word 'masala' only appears in the first order, whereas 'chicken' and 'rice' appears in both orders. Hence, the rareness of a word can be determined by how infrequently it appears in all the documents. This is *inverse document frequency*.\n",
    "\n",
    "Combining text-frequency and inverse-document frequency scores for each token give it a TF-IDF score. This way, we can derive that the word 'masala' is the most important word from the first order, since it has both high TF and IDF scores.\n",
    "\n",
    "::: {.column-margin}\n",
    "TF-IDF can also be done for words, for example splitting the word `manner`` into character tokens:\n",
    "\n",
    "* `n`` appears twice in the word\n",
    "* `m`` is rarer\n",
    "\n",
    "Imagine now that three of the letters are dropped. We are much more likely to guess that the word `m_nn_` could be manner, whereas seeing `_a__er` is far less informative.\n",
    ":::\n",
    "\n",
    "#### Text Frequency\n",
    "\n",
    "Let's say we wanted to build something to classify cuisines. We might first take orders from two different menus, and try to identify which words are the some of the most important.\n",
    "\n",
    "But how can we do this in an automated way?\n",
    "\n",
    "<img src=\"https://media.giphy.com/media/B8Bp8MfpmKbWU/giphy.gif\">\n",
    "\n",
    "We might think that **words that are repeated many times in the menu are more characteristic of that restaurant. This is called 'text frequency'.**\n",
    "\n",
    "So let's count them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>beef</th>\n",
       "      <th>chicken</th>\n",
       "      <th>chow</th>\n",
       "      <th>crispy</th>\n",
       "      <th>curry</th>\n",
       "      <th>egg</th>\n",
       "      <th>fried</th>\n",
       "      <th>kurma</th>\n",
       "      <th>masala</th>\n",
       "      <th>mein</th>\n",
       "      <th>noodles</th>\n",
       "      <th>pilau</th>\n",
       "      <th>plain</th>\n",
       "      <th>rice</th>\n",
       "      <th>tikka</th>\n",
       "      <th>vegetable</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Order_1</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Order_2</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         beef  chicken  chow  crispy  curry  egg  fried  kurma  masala  mein  \\\n",
       "Order_1     0        3     0       0      1    0      0      1       2     0   \n",
       "Order_2     1        2     1       1      1    1      1      0       0     1   \n",
       "\n",
       "         noodles  pilau  plain  rice  tikka  vegetable  \n",
       "Order_1        0      1      1     2      1          0  \n",
       "Order_2        2      0      1     2      0          1  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "takeaway_orders = ['Chicken Kurma Masala, Chicken Tikka Masala, Chicken Curry, Pilau Rice, Plain Rice'\n",
    "                   , 'Chicken Chow Mein Noodles, Crispy Beef, Chicken curry, Egg Fried Rice, Plain rice, Vegetable Noodles']\n",
    "# unique_tokens = set(' ,'.join(takeaway_orders).replace(',','').split(' '))\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cnt_vec = CountVectorizer()\n",
    "count_mat = cnt_vec.fit_transform(takeaway_orders)\n",
    "import pandas as pd\n",
    "pd.DataFrame(data = count_mat.todense(), index = ['Order_1', 'Order_2'], columns = cnt_vec.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool, so the most populated word from the first order is `chicken`, followed by `masala` and `rice`. The second order has chicken, noodles and rice as the most populated words.\n",
    "\n",
    "Two things to note:\n",
    "* Rather than taking the absolute counts, we might log the counts instead. This is because we might want to capture the concept of diminishing returns - the additional marginal importance we expect between having the word 'Masala' appear once vs twice between documents is greater than appearing nine times vs 10 times.  This is more important for longer documents, and the logging our counts captures this concept.*\n",
    "* In this instance, we count 'uni-grams', with one token per word. In general implementation, we can count 'bi-grams' such as 'Chicken Kurma' and 'Kurma Masala', 'tri-grams' etc. See more under cosine-similarity\n",
    "\n",
    "#### Inverse Document Frequency\n",
    "\n",
    "We are only getting half the information here then. For example in the first order, chicken in general is a common word, so it appearing frequently is less informative, whereas masala is a rare word, and more informative. We thus need to **introduce an additional concept of word uniqueness**, or equivalently the opposite of how frequently it appears across our entire 'corpus' of menu orders.\n",
    "\n",
    "<img src=\"https://media.giphy.com/media/5vR6pNsjhoKwo/giphy.gif\">\n",
    "\n",
    "Enter Karen Spärck Jones, with the concept of ‘inverse document frequency’. This is the idea that it is not just how often a word appears, **but how unique the word is across all sentences (or ‘documents’), that determines how important it is. TF-IDF helps use this to turn words into vectors.**\n",
    "\n",
    "For example, a rare word like Masala appears in many 'documents' (orders), so will have a low document frequency, and thus will have a high inverse document frequency score.\n",
    "\n",
    "We usually calculate inverse document frequency using the following logic:\n",
    "\n",
    "$$\n",
    "\\text{idf} = 1+\\ln \\left(\\frac{\\text{\\# docs in corpus}}{\\text{\\# docs term appears in}} \\right)\n",
    "$$\n",
    "\n",
    "In other words, the word chicken appears in both docs, so the idf score is $1+\\ln(\\frac{2}{2})=1$. \n",
    "On the other hand, the word masala only appears in one doc, so it gets an IDF score of  $1+\\ln(\\frac{2}{1})\\sim1.7$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>beef</th>\n",
       "      <th>chicken</th>\n",
       "      <th>chow</th>\n",
       "      <th>crispy</th>\n",
       "      <th>curry</th>\n",
       "      <th>egg</th>\n",
       "      <th>fried</th>\n",
       "      <th>kurma</th>\n",
       "      <th>masala</th>\n",
       "      <th>mein</th>\n",
       "      <th>noodles</th>\n",
       "      <th>pilau</th>\n",
       "      <th>plain</th>\n",
       "      <th>rice</th>\n",
       "      <th>tikka</th>\n",
       "      <th>vegetable</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Order_1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.693147</td>\n",
       "      <td>1.693147</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.693147</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.693147</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Order_2</th>\n",
       "      <td>1.693147</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.693147</td>\n",
       "      <td>1.693147</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.693147</td>\n",
       "      <td>1.693147</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.693147</td>\n",
       "      <td>1.693147</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.693147</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             beef  chicken      chow    crispy  curry       egg     fried  \\\n",
       "Order_1       NaN      1.0       NaN       NaN    1.0       NaN       NaN   \n",
       "Order_2  1.693147      1.0  1.693147  1.693147    1.0  1.693147  1.693147   \n",
       "\n",
       "            kurma    masala      mein   noodles     pilau  plain  rice  \\\n",
       "Order_1  1.693147  1.693147       NaN       NaN  1.693147    1.0   1.0   \n",
       "Order_2       NaN       NaN  1.693147  1.693147       NaN    1.0   1.0   \n",
       "\n",
       "            tikka  vegetable  \n",
       "Order_1  1.693147        NaN  \n",
       "Order_2       NaN   1.693147  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "idf_vec = TfidfTransformer(smooth_idf=False,norm=None)\n",
    "idf_mat = idf_vec.fit_transform(count_mat)\n",
    "pd.DataFrame(data = idf_mat/count_mat, index = ['Order_1', 'Order_2'], columns = cnt_vec.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF x IDF\n",
    "\n",
    "And to finish - TF-IDF is simply the multiplication between the TF and IDF scores, which \"combines\" the text-frequency and inverse-document-frequency concepts in the same token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>beef</th>\n",
       "      <th>chicken</th>\n",
       "      <th>chow</th>\n",
       "      <th>crispy</th>\n",
       "      <th>curry</th>\n",
       "      <th>egg</th>\n",
       "      <th>fried</th>\n",
       "      <th>kurma</th>\n",
       "      <th>masala</th>\n",
       "      <th>mein</th>\n",
       "      <th>noodles</th>\n",
       "      <th>pilau</th>\n",
       "      <th>plain</th>\n",
       "      <th>rice</th>\n",
       "      <th>tikka</th>\n",
       "      <th>vegetable</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Order_1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.459492</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.218951</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.307727</td>\n",
       "      <td>0.521028</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.307727</td>\n",
       "      <td>0.218951</td>\n",
       "      <td>0.370715</td>\n",
       "      <td>0.307727</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Order_2</th>\n",
       "      <td>0.269369</td>\n",
       "      <td>0.324505</td>\n",
       "      <td>0.269369</td>\n",
       "      <td>0.269369</td>\n",
       "      <td>0.191658</td>\n",
       "      <td>0.269369</td>\n",
       "      <td>0.269369</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.269369</td>\n",
       "      <td>0.456081</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.191658</td>\n",
       "      <td>0.324505</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.269369</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             beef   chicken      chow    crispy     curry       egg     fried  \\\n",
       "Order_1  0.000000  0.459492  0.000000  0.000000  0.218951  0.000000  0.000000   \n",
       "Order_2  0.269369  0.324505  0.269369  0.269369  0.191658  0.269369  0.269369   \n",
       "\n",
       "            kurma    masala      mein   noodles     pilau     plain      rice  \\\n",
       "Order_1  0.307727  0.521028  0.000000  0.000000  0.307727  0.218951  0.370715   \n",
       "Order_2  0.000000  0.000000  0.269369  0.456081  0.000000  0.191658  0.324505   \n",
       "\n",
       "            tikka  vegetable  \n",
       "Order_1  0.307727   0.000000  \n",
       "Order_2  0.000000   0.269369  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vec = TfidfVectorizer(sublinear_tf=True)\n",
    "tfidf_mat = tfidf_vec.fit_transform(takeaway_orders)\n",
    "pd.DataFrame(data = tfidf_mat.todense(), index = ['Order_1', 'Order_2'], columns = tfidf_vec.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cool, so combining text frequency and inverse document frequency now reveals the most important word in the first order is 'Masala', and it is 'noodles'. Nice.\n",
    "\n",
    "Finally, we don't have to limit ourselves to sentences, and can split stings into character tokens, applying the same logic: for the word $\\text{queen}$, we would find the letters $q$ and $e$ to be the most informative because of their rarity (IDF) and being repeated (TF). This could be useful in predicting the word being types or correcting mispelling - let's jump into a character-level example when discussing cosine similarity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cosine similarlity\n",
    "\n",
    "### What is it used for?\n",
    "\n",
    "Cosine similarity can help measure how similar two words or documents are. For example, we could better match a search to a result using this, whereas 'keywords' would just weight every word equally. \n",
    "\n",
    "<img src=\"https://media.giphy.com/media/13cgadB959Y0BW/giphy.gif\">\n",
    "\n",
    "### What is the intuition?\n",
    "\n",
    "TF-IDF creates a row of scores for each token in the text, for example the first order had high TF-IDF scores for masala, chicken and rice, and low scores for noodles and fried. If we have another document that has high TF-IF scores for masala, chicken and rice, and low scores for noodles and fried, we might think it is similar to the first order. Cosine similarity gives a measure between one and zero as to how similar the two texts are.\n",
    "\n",
    "#### Vectorization\n",
    "\n",
    "To take this further, what we have done using TF-IDF is a form of 'vectorization'. If we were to plot the first order in 16 dimensional space, with one axis for each word, the line remains at zero for the 'beef' axis, travels 0.46 along the 'chicken' axis, etc.\n",
    "\n",
    "**We can then measure the angle (the cosine) between these lines to get an idea of how similar two documents are.** Cosine similarity is often used to match, how example, a search string to a result. \n",
    "\n",
    "This can feel a bit abstract, but hang on in there because this is a key part of intution!\n",
    "\n",
    "Let's dive into this with an example we can visualise in 3 dimensional space to make this clearer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
