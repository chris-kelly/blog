---
title: "Deriving the Central Limit Theorem"
author: "Chris Kelly"
date: '06-14-24'
# image: '../images/'
categories: [Law of Large Numbers, Central Limit Theorem]
draft: true
format:
  html:
    code-fold: true
    toc: true
    # code-links:
    #   - text: OLS class
    #     icon: file-code
    #     href: ../src/ols_blue.py
    #   - text: Sandwich class
    #     icon: file-code
    #     href: ../src/ols_sandwich_estimators.py
    #   - text: GLS class
    #     icon: file-code
    #     href: ../src/gls_fgls_wls.py
    # other-links:
    #   - text: Correcting SE for non-spherical errors
    #     href: ols_sandwich_estimators.html
---

$$
\phi_X(t) = \mathbb{E}[e^{itX}] \approx 1 + it\mu - \frac{\sigma^2 t^2}{2}
$$

## Intro

The central limit theorem states that, under the right conditions, the distribution of the sample mean $\bar{X}_n$ converges to a normal distribution as the sample size $n \rightarrow \infty$. 

This is a fundamental result in statistics, and is the reason why the normal distribution is so widely used in hypothesis testing and confidence intervals.

But why does the sample mean converge to a normal distribution? In this post, we'll derive the classical Central Limit Theorem from first principles.

## Defining terms

For the classical CLT, we state that if we sample a large number $n$ of independent observations from the same (read identical) distribution of the random variable $X$, and calculate the mean $\bar{X}_n$ of this sample, then not only would the expected mean tend to the true mean $\lim_{n \rightarrow \infty}\mathbb{E}[\bar{X}_n] \rightarrow \mu$ as per weak law of large numbers, but the this sample mean will also vary around the true mean following to the normal distribution: $\lim_{n \rightarrow \infty}\bar{X}_n \sim N(\mu,\frac{\sigma^2}{\sqrt{n}})$

First, we define a random variable $X_i$, that is independently and identically distributed with mean $\mu$ and variance $\sigma^2$:

$$
X_i \overset{iid}{\sim}(\mu,\sigma^2)
$$

Note that we make no assumptions about its distribution at all: for example, it could be bernoulli distributed, with $\mu = p$ and $\sigma^2 = p(1-p)$. But according to the CLT, despite this underlying distribution, we will always find that its sample mean will follow a normal distribution.

Now let's define the sample mean of $n$ observations of $X_i$ as $\bar{X}_n$:

$$
\bar{X}_n = \frac{1}{n}\sum_{i=1}^{n}X_i
$$

So we want to determine if the distribution of $\bar{X}_n$ converges to a normal distribution as $n \rightarrow \infty$.

<!-- And as per the frequentist defintion, let's define a random variable where we collect many of these sample means $N$ times:

$$
S_n = \sum_{i=1}^{N}\bar{X}_n
$$

So what we what to determine, is what distribution does $S_n$ follow as $n \rightarrow \infty$.

Well we can derive the expected values and its variance:

$$
\displaylines{
\begin{align}
\mathbb{E}[S_n] 
& = \mathbb{E}\left[\sum_{i=1}^{N}\bar{X}_n\right] \\
& = \mathbb{E}[N \times \bar{X}_n] \quad \text{as identical} \\
& = N\times \mathbb{E}[\bar{X}_n] \\
& = N\mu \\ \\
\mathbb{V}[S_n] 
& = \mathbb{V}\left[\sum_{i=1}^{N}\bar{X}_n\right] \\
& = \mathbb{V}\left[N \times \bar{X}_n\right] \quad \text{as identical} \\
& = N^2\mathbb{V}[\bar{X}_n] \\
& = N^2\frac{\sigma^2}{N} \\
& = N\sigma^2
\end{align}
}
$$

So it follows a normal distribution, then we would find the following:

$$
S_n \sim Norm(N\mu, N\sigma^2) \quad \text{as} \quad n \rightarrow \infty
$$
-->

## Characteristic functions

Characteristic functions are the Fourier transformation of the probability density functions of random variables. Characteristic functions can be used to fully describe the probability distributions they transform, and can be reverse-transformed perfectly to the original probability distribution too.

So why is this relevant? Well they can be used as an alternative (read: easier) route to derive analytical results, such as the central limit theorem, rather than through the probability density functions directly. And if we can derive the central limit theorem using characteristic functions of random variables, then we can safely infer that the same applies if perfectly reverse-transformed to the probability distributions of those same random variables.

For any random variable $X$, the characteristic function is a transformation defined as:

$$
\phi_X(t) = \mathbb{E}[e^{itX}]
$$

where $i=\sqrt{-1}$ i.e. the imaginary unit, and $t$ is a real number.

Let's derive the characteristic function of the bernoulli distribution as an example:

$$
\displaylines{
\begin{align}
\phi_X(t) 
& = \mathbb{E}[e^{itX}] \\ \\
& = \int_{-\infty}^{\infty} e^{itx} \times \mathbb{P}(X=x) \\
& \equiv \left[ e^{itx} \times \mathbb{P}(X=0) \right]
 + \left[ e^{itx} \times \mathbb{P}(X=1) \right] \\ \\
& =  \left( e^{it(0)} \times (1-p) \right) 
+ \left( e^{it(1)} \times p \right) \\
& =  (1-p) + p e^{it}
\end{align}
}
$$

And here is the characteristic function of the normal distribution:
$$
\displaylines{
\begin{align}
\phi_X(t)
& = \mathbb{E}[e^{itX}] \\
& = \int_{-\infty}^{\infty} e^{itx} \times \frac{1}{\sqrt{2\pi}\sigma} e^{-\frac{(x-\mu)^2}{2\sigma^2}} \, dx \\
& = \frac{1}{\sqrt{2\pi}} \int_{-\infty}^{\infty} \exp{\left\{itx -\frac{(x-\mu)^2}{2\sigma^2} \right\} } \, dx \\
& = \frac{1}{\sqrt{2\pi}} \int_{-\infty}^{\infty} \exp{\left\{
    -\frac{1}{2\sigma^2} \left(-2\sigma^2itx + \left( x^2 + \mu^2 - 2x\mu \right) \right)
\right\} } \, dx \\ 
& = \frac{1}{\sqrt{2\pi}} \int_{-\infty}^{\infty} \exp{\left\{
    -\frac{1}{2\sigma^2} \left( 
    \underbrace{(x - \mu - \sigma^2 it)^2}_{
        x^2 + \mu^2 - \sigma^4t^2 - 2\sigma^2itx - 2x\mu + 2\mu i\sigma^2t
        } 
        + \sigma^4t^2 + 2\mu i\sigma^2t
        \right)
\right\} } \, dx \\ 
& = \frac{1}{\sqrt{2\pi}} \int_{-\infty}^{\infty} \exp{\left\{
    -\frac{1}{2\sigma^2} 
    \left(x - \mu + i\sigma^2t \right)^2
        - \frac{1}{2}\sigma^2t^2 + i\mu t
\right\} } \, dx \\ 
& = \frac{1}{\sqrt{2\pi}} \exp\left\{ i\mu t - \frac{\sigma^2t^2}{2} \right\} \underbrace{ \int_{-\infty}^{\infty} \exp{\left\{
    -\frac{1}{2\sigma^2} 
    \left(x - \mu + i\sigma^2t \right)^2
\right\} } \, dx }_{\text{Gaussian integral} \Rightarrow\sqrt{2\pi}} \\ \\
& = \exp\left\{i\mu t - \frac{\sigma^2t^2}{2} \right\}
\end{align}
}
$$

::: {.column-margin}
[See this post](fundamentals_gaussian_integral.html) on the gaussian integral for more details for why $\int_{-\infty}^{+\infty}{e^{-z^2}\,dx}=\sqrt{\pi}$
:::

**So we are looking to show that the characteristic function of ${S}_n$ converges to the characteristic function of the standard normal distribution as $n \rightarrow \infty$.**

## Characteristic function of the sample means:

<!-- So let's define the characteristic function for the sum of sample means $S_n$:

$$
\displaylines{
\begin{align}
\phi_{S_n}(t) 
& = \mathbb{E}[e^{itS_n}] \\
& = \mathbb{E}\left[e^{it\sum_{i=1}^{N}\bar{X}_n}\right] \\
& = \mathbb{E}\left[\prod_{i=1}^{n}e^{it\bar{X}_n}\right] & \because e^{(a+b)}=e^a\times e^b\\
& = \prod_{i=1}^{N}\mathbb{E}\left[e^{it\bar{X}_n}\right] & \text{by independence} \\
& = \left(\mathbb{E}\left[e^{it\bar{X}_n}\right]\right)^N \\
& = \left(\phi_{\bar{X}_n}(t)\right)^N
\end{align}
}
$$

So we now see that the characteristic function of the sum of sample means is the $N$th power of the characteristic function of the sample mean. -->

And what is the characteristic function of the sample mean? Well in a very similar way, we can derive this by 
using the definition of the characteristic function:

$$
\displaylines{
\begin{align}
\phi_{\bar{X}_n}(t)
& = \mathbb{E}[e^{it\bar{X}_n}] \\
& = \mathbb{E}\left[e^{it\left(\frac{1}{n}\sum_{i=1}^{n}X_i\right)}\right] \\
& = \mathbb{E}\left[\prod_{i=1}^{n}e^{it\frac{1}{n}X_i}\right] & \because e^{(a+b)}=e^a\times e^b\\
& = \prod_{i=1}^{n}\mathbb{E}\left[e^{it\frac{1}{n}X_i}\right] & \text{by independence} \\
& = \left(\mathbb{E}\left[e^{it\frac{1}{n}X_i}\right]\right)^n & \text{as identical} \\
& = \left(\phi_{X_i}\left(\frac{t}{n}\right)\right)^n
\end{align}
}
$$

So we see that the characteristic function of the sample mean is the characteristic function of the random variable $X_i$ divided by $n$, raised to the $n$th power.

We can formulate our characteristic function of the random variable $X_i$ as a taylor series:

$$
\displaylines{
\begin{align}
\phi_\bar{X}(t)
& = \mathbb{E}\left[\exp{\left\{\frac{it}{n}X_i\right\}}\right] \\
& = \mathbb{E}\left[\sum_{k=0}^{\infty} \frac{ \left( \frac{it}{n}X_i\right)^k}{k!}\right] 
& \because e^x = \sum_{k=0}^{\infty}{\frac{x^k}{k!}}
\\
& = \mathbb{E}\left[
    \frac{ \left( \frac{it}{n}X_i\right)^0}{0!} + 
    \frac{ \left( \frac{it}{n}X_i\right)^1}{1!} + 
    \frac{ \left( \frac{it}{n}X_i\right)^2}{2!} + 
    \frac{ \left( \frac{it}{n}X_i\right)^3}{3!} + \dots
\right] 
& \because f(x) = \sum_{n=0}^{\infty}{ \frac{f^k(a)}{k!}(x-a)^k }
\\
& = \mathbb{E}\left[
    1
    + \frac{it}{n}X_i
    - \frac{t^2}{2n^2}X_i^2
    + \frac{it^3}{6n^3}X_i^3
    + \dots
\right] \\
& = \mathbb{E}[1] + it\mathbb{E}[X_i] - \frac{t^2}{2!}\underbrace{\mathbb{E}[X_i^2]}_{\mathbb{V}[X_i]+\mathbb{E}[X_i]^2} + \frac{it^3}{3!}\mathbb{E}[X_i^3] + \ldots \\
& = 1 + it\mu - \frac{t^2}{2!}\left( \sigma^2+\mu^2 \right) + \frac{it^3}{3!}(\mu^3 + 3\mu\sigma^2) + \ldots
\end{align}
}
$$

$$
\displaylines{
\begin{align}
\phi_X(t)
& = \mathbb{E}[e^{itX_i}] \\
& = \mathbb{E}\left[1 + itX_i + \frac{(itX_i)^2}{2!} + \frac{(itX_i)^3}{3!} + \ldots \right] \\
& = \mathbb{E}[1] + it\mathbb{E}[X_i] - \frac{t^2}{2!}\underbrace{\mathbb{E}[X_i^2]}_{\mathbb{V}[X_i]+\mathbb{E}[X_i]^2} + \frac{it^3}{3!}\mathbb{E}[X_i^3] + \ldots \\
& = 1 + it\mu - \frac{t^2}{2!}\left( \sigma^2+\mu^2 \right) + \frac{it^3}{3!}(\mu^3 + 3\mu\sigma^2) + \ldots
\end{align}
}
$$


$$
\displaylines{
\begin{align}
\phi_X(t) 
& = \mathbb{E}[e^{it\bar{X}_n}] \\
& = \mathbb{E}\left[1 + it\bar{X}_n + \frac{(it\bar{X}_n)^2}{2!} + \frac{(it\bar{X}_n)^3}{3!} + \ldots \right] \\
& = \mathbb{E}[1] + it\mathbb{E}[\bar{X}_n] - \frac{t^2}{2!}\underbrace{\mathbb{E}[\bar{X}_n^2]}_{\mathbb{V}[\bar{X}_n]+\mathbb{E}[\bar{X}_n]^2} + \frac{it^3}{3!}\mathbb{E}[\bar{X}_n^3] + \ldots \\
& = 1 + it\mu - \frac{t^2}{2!}\left( \frac{\sigma^2}{N}+\mu^2 \right) + \frac{it^3}{3!}(\mu^3 + 3\mu\sigma^2) + \ldots
\end{align}
}
$$


So let's derive the characteristic function of the sample mean:


So we now see that the characteristic function of the sample mean is the $n$th power of the characteristic function of the random variable $X_i$.



Now it can be helpful to remember that the exponential function can be written in various forms, such as the infinite sum:

$$
e^x = \lim_{n \rightarrow \infty} \left(1 + \frac{x}{n}\right)^n
$$


Now let's state some properties of the distribution of the sample mean, if we take a sample of size $n$, estimate the mean from it, and repeat this proceedure $N$ times:

$$
\displaylines{
\begin{align}
\bar{X}_n
& = \frac{1}{n}\sum_{i=1}^{n}X_i
\\ \\
\therefore \mathbb{E}\left[\bar{X}_n\right]
& = \mathbb{E}\left[\frac{1}{N}\sum_{i=1}^{N} \bar{X}_n \right] \\
& = \frac{1}{N}\sum_{i=1}^{N} \mathbb{E}\left[\bar{X}_n\right] \\ 
& = \frac{1}{N}\left(N \times \mu \right) & \because \text{WLLN}  \\
& = \mu
\\ \\
\therefore \mathbb{V}\left[\bar{X}_n\right]
& = \mathbb{V}\left[\frac{1}{N}\sum_{i=1}^{n} X_i \right] \\
& = \frac{1}{N^2}\sum_{i=1}^{n} \mathbb{V}\left[X_i\right] \\
& = \frac{1}{N^2}\left(N \times \sigma^2 \right) \\ 
& = \frac{\sigma^2}{N}
\end{align}
}
$$

::: {.column-margin}
So we see that the sample mean is an unbiased estimator of the true mean, and the variance of the sample mean decreases as the sample size increases. This is the same result we derived in the weak law of large numbers.
:::

We can easily standardise this, to create a variable with a mean of zero and variance of one (which is easier to work with to prove the central limit theorem - which we will see now we introduce characteristic functions):

$$
\displaylines{
\begin{align}
Z_N & = \frac{\bar{X}_N - \mu}{\sigma/\sqrt{N}} \\
\mathbb{E}[Z_N] & = \frac{\mathbb{E}[\bar{X}_N] - \mu}{\sigma/\sqrt{N}} = 0 \\
\mathbb{V}[Z_N] & = \frac{\mathbb{V}[\bar{X}_N]}{\sigma^2/N} = 1
\end{align}
}
$$

Now it can be helpful to remember that the exponential function can be written in various forms, such as the infinite sum:

$$
e^x = \lim_{n \rightarrow \infty} \left(1 + \frac{x}{n}\right)^n
$$

And hence this infinite sum can also be written as a Taylor series:

## Taylor Series

As a reminder, the Taylor series is a way to approximate a function as an infinite sum of terms. The $n$th term of the Taylor series is given by:
$$
\displaylines{
\begin{align}
f(x) & = f(a) + \frac{f^1(a)}{1!}(x-a) + \frac{f^2(a)}{2!}(x-a)^2 +  \ldots + \\
& = \sum_{n=0}^{\infty}{ \frac{f^n(a)}{n!}(x-a)^n }
\end{align}
}
$$

Here is a plot of the Taylor series approximation of a function $f(x) = 20 - 17x - 6x^2 + 8x^3 + 3x^4 + 4x^5 + x^6$ around the point $a=-2.5$:

```{python}
import plotly.graph_objects as go
from math import factorial
import numpy as np

x = np.linspace(-4.5,2.5,100)
c = [20, -17, -6, 8, 3, 4, 1]
y = [np.sum([c*x**n for n,c in enumerate(c)]) for x in x]
a = -2.5

true_plot = go.Scatter(x=x,y=y,mode='lines',name="True function")

dc = c.copy()
y_hat = np.zeros(len(x))
for i in range(len(c)):    
    coef = np.sum([c*a**n for n,c in enumerate(dc)])/factorial(i)
    y_hat += coef*(x-a)**i
    if i == 0:
        fig = go.Figure(data = [
            go.Scatter(mode='markers',x=x,y=y_hat,name="Taylor approximation"),
            true_plot
            ])
        frames = []
    frames.append(go.Frame(
        data=[go.Scatter(x=x,y=y_hat,name="Taylor approximation"), true_plot], name= f'frame{i}'
    ))
    dc = [n*c for n,c in enumerate(dc)][1:]

fig.frames = frames

updatemenus = [dict(
        buttons = [dict(args = [None, {
            "frame": {"duration": 800, "redraw": False}, "fromcurrent": True, "transition": {"duration": 300}
            }],
            label = "Play", method = "animate"),
            dict(args = [[None], {
                "frame": {"duration": 0, "redraw": False},"mode": "immediate","transition": {"duration": 0}}],
                label = "Pause",method = "animate")
                ],
        direction = "left", pad = {"r": 10, "t": 87}, showactive = False, type = "buttons",
        x = 0.1, xanchor = "right", y = 0, yanchor = "top"
    )]  

sliders = [dict(steps = [
    dict(method= 'animate',args= [
        [f'frame{i}'],
        dict(mode= 'immediate',frame= dict(duration=400, redraw=False),transition=dict(duration= 0))
        ],
        label=f'{i}') for i in range(len(frames))], 
        active=0,transition= dict(duration= 0),y=0, x=0,
        currentvalue=dict(font=dict(size=12), prefix='polynomial: ', visible=True, xanchor= 'center'),len=1.0)
        ]

fig.update_layout(updatemenus=updatemenus,sliders=sliders)
```

So how does this relate to the exponential function? Well the exponential function can be written as a Taylor series:

$$ 
e^{x} = \sum_{n=0}^{\infty} \frac{x^n}{n!} = 1 + x + \frac{x^2}{2!} + \frac{x^3}{3!} + \ldots
$$

Still not convinced? Well remember that $\frac{d(e^x)}{dx}=e^x$ by definition. So if we take the derivative of the Taylor series of $e^x$, we should get back to the original function:

$$
\displaylines{
\begin{align}
\frac{d(e^x)}{dx} & = \frac{d}{dx} \left(1 + x + \frac{x^2}{2!} + \frac{x^3}{3!} + \ldots \right) \\
& = 0 + 1 + \frac{2x}{2!} + \frac{3x^2}{3!} + \ldots \\
& = 1 + x + \frac{x^2}{2!} + \frac{x^3}{3!} + \ldots \\
& = e^x
\end{align}
}
$$

So now let's reformulate our generic characteristic function as a Taylor series:

$$
\displaylines{
\begin{align}
\phi_X(t) 
& = \mathbb{E}[e^{it\bar{X}_n}] \\
& = \mathbb{E}\left[1 + it\bar{X}_n + \frac{(it\bar{X}_n)^2}{2!} + \frac{(it\bar{X}_n)^3}{3!} + \ldots \right] \\
& = \mathbb{E}[1] + it\mathbb{E}[\bar{X}_n] - \frac{t^2}{2!}\underbrace{\mathbb{E}[\bar{X}_n^2]}_{\mathbb{V}[\bar{X}_n]+\mathbb{E}[\bar{X}_n]^2} + \frac{it^3}{3!}\mathbb{E}[\bar{X}_n^3] + \ldots \\
& = 1 + it\mu - \frac{t^2}{2!}\left( \frac{\sigma^2}{N}+\mu^2 \right) + \frac{it^3}{3!}(\mu^3 + 3\mu\sigma^2) + \ldots
\end{align}
}
$$

$$
\displaylines{
\begin{align}
\exp{\left\{ i\mu t - \frac{\sigma^2t^2}{2} \right\}}
= & \sum_{n=0}^{\infty} \frac{1}{n!}\left(it\mu - \frac{1}{2}\sigma^2 t^2\right)^n \\
= & \frac{1}{0!}\left(it\mu - \frac{1}{2}\sigma^2 t^2\right)^0
+ \frac{1}{1!}\left(it\mu - \frac{1}{2}\sigma^2 t^2\right)^1
+ \frac{1}{2!}\left(it\mu - \frac{1}{2}\sigma^2 t^2\right)^2
+ \ldots \\
\\
= 1 + & \left(it\mu - \frac{1}{2}\sigma^2 t^2\right) + \frac{1}{2!}\left(it\mu - \frac{1}{2}\sigma^2 t^2\right)^2 + \ldots \\
+ \ldots \\
\end{align}
}
$$



And if we use our standardised variable $Z_N$ instead, with $\mathbb{E}[Z_N]=0$ and $\mathbb{V}[Z_N]=\mathbb{E}[Z_N^2]=1$ we find it is highly simplified:

$$
\displaylines{
\begin{align}
\phi_{Z_N}(t)
& = \mathbb{E}[e^{itZ_N}] \\
& = \mathbb{E}\left[1 + itZ_N + \frac{(itZ_N)^2}{2!} + \frac{(itZ_N)^3}{3!} + \ldots \right] \\
& = \mathbb{E}[1] + it \underbrace{\mathbb{E}[Z_N]}_{=0} - \frac{t^2}{2!} \underbrace{\mathbb{E}[Z_N^2]}_{=1} + \frac{it^3}{3!}\mathbb{E}[Z_N^3] + \ldots \\
& = 1 - \frac{t^2}{2} + \ldots
\end{align}
}
$$

And now to tie it all together, we can show that the taylor expansion of the normal distribution's characteristic function is the same:

$$
\displaylines{\begin{align}
\exp\left\{\frac{-t^2}{2}\right\} & 
= 1 - \frac{t^2}{2} + \ldots
\end{align}}
$$

So the sample mean for a collection for independent $n$ samples taken from the same distribution converges to a normal distribution as $n \rightarrow \infty$.

<!-- 

$$
\displaylines{
\begin{align}
\frac{\mathbb{E}[Z]}{a} & \ge \mathbb{P}(Z \ge a) & \text{(Markov's inequality)} \\
\text{let} \quad Z & = |X-\mu| & (\text{New random variable } Z) \\
\implies
\frac{\mathbb{E}[|X-\mu|]}{a} & \ge \mathbb{P}(|X - \mu| \ge a)
\end{align}
}
$$

Why can we say this? Well because Markov's inequality states that $\frac{\mathbb{E}[X]}{a} \ge \mathbb{P}(X \ge a)$ for *any* random variable $X$. So if we define a new random variable $Z=|X-\mu|$, then the same inequality must apply.

Now let's transform the right-hand side of the inequality:

$$
\displaylines{
\begin{align}
\mathbb{P}(|X - \mu| \ge a) & \equiv \mathbb{P}((X - \mu)^2 \ge a^2) \\ \\
\implies \frac{\mathbb{E}[|X-\mu|]}{a} & \ge \mathbb{P}((X - \mu)^2 \ge a^2)
\end{align}
}
$$

To do this, we have to convince ourselves that $\mathbb{P}(X - \mu \ge a) \equiv \mathbb{P}((X - \mu)^2 \ge a^2)$. 

So if we take our dice example from before:

--->

```{python}
# x_subtract_mu = x - Ex
# pxmu_le_a = [np.sum(p[x_subtract_mu >= a]) - Ex for a in x]
# abs_x_subtract_mu = np.abs(x_subtract_mu)
# abs_pxmu_le_a = [np.sum(p[abs_x_subtract_mu >= a]) for a in x]

# fig = Figure()
# fig.add_trace(Bar(x=x, y=pxmu_le_a, name='P(X-µ >= a)'))
# fig.add_trace(Bar(x=x, y=x_subtract_mu/x, name='E[X-µ]/a'))
# fig.add_trace(Bar(x=x, y=abs_pxmu_le_a, name='P(|X-µ| >= a)'))
# fig.add_trace(Bar(x=x, y=abs_x_subtract_mu/x, name='E[|X-µ|]/a'))

# # Update layout for grouped bar chart
# fig.update_layout(barmode='group', title='Markov\'s inequality for a dice role', xaxis_title='a (Dice roll)', legend=dict(x=0.37,y=0.95))

# fig.show()
```

<!-- We can define the sine and cosine functions using a taylor series. Since we know the derivative of sine is cosine, and the derivative of cosine  -->


<!-- First let's look at the probability side of things. Remember that $\mu$ is 3.5:

* If the threshold $a$ is $3$, then the probabiity is zero: no dice roll $X$ would be far enough away to ensure the absolute distance from the expected value is greater than $a$.
    * e.g. $|6-3.5| = 2.5 < 3$
    * e.g. $|1-3.5| = 2.5 < 3$
* If the threshold $a$ is $2$, then this only occurs if the dice roll is $1$ or $6$. So $P(X=1) + P(X=6) = 1/6 + 1/6 = 1/3$:
    * e.g. $|6-3.5| = |1-3.5| = 2.5 > 2$
    * e.g. $|5-3.5| = |2-3.5| = 1.5 < 2$
* And finally, if the threshold of $a$ is $1$, and $\mu=3.5$, then only rolling a $1,2,4$ or $6$ will ensure $|X-\mu| \ge a, hence $P(X=1) + P(X=2) + P(X=4) + P(X=6) = 2/3$:
    * e.g. $|6-3.5| = |1-3.5| = 2.5 > 1$
    * e.g. $|5-3.5| = |2-3.5| = 1.5 > 1$
    * e.g. $|4-3.5| = |3-3.5| = 0.5 < 1$

Now let's look at the expectation side. Given $\mu=3.5$, it is expected that the absolute distance increases with larger distance from 3.5 (so rolls of $1$ and $6$). However, it is then scaled by $\frac{1}{a}$: -->